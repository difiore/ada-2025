[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Data Analysis",
    "section": "",
    "text": "Overview\n\n\n\n\n\n\n\n\n\n\n\nThis course provides an overview of methods and tools for applied data analysis. It is geared toward research in biological anthropology and evolutionary biology, but the material covered is applicable to a wide range of natural, social science, and humanities disciplines. Students will receive practical, hands-on training in various data science tools and workflows, including data acquisition and wrangling, exploratory data analysis and visualization, statistical analysis and interpretation, and literate programming and version control.\nStatistical topics to be covered include basic descriptive and inferential statistics, hypothesis testing, basic regression and ANOVA, generalized linear modeling, and mixed effects modeling. Statistical inference will be considered from a frequentist perspective, introducing both parametric and resampling techniques. If we have time, I will also introduce a Bayesian perspective, although this approach will not be tackled at a particularly advanced level. Additional methods and tools will also be covered based on time and student interest (e.g., geospatial data analysis, phylogenetic comparative methods, social network analysis, text corpus construction and mining, population genetic analysis) and on how quickly the class feels we can move forward.\nThe course particularly emphasizes the development of solid data science skills, focusing on the practical side of data manipulation, analysis, and visualization. Students will learn to use the statistical programming language R as well as many other useful software tools (e.g., shell scripts, text editors, databases, query languages, and version control systems).\n\n\nThis class is supported by DataCamp, an intuitive online learning platform for data science. Learn R, Python, and SQL the way you learn best, through a combination of short expert videos and hands-on-the-keyboard exercises. Take over 100+ courses by expert instructors on topics such as importing data, data visualization or machine learning and learn faster through immediate and personalised feedback on every exercise.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "00-logistics.html",
    "href": "00-logistics.html",
    "title": "Course Logistics",
    "section": "",
    "text": "Learning Objectives\nAt the conclusion of this course, students will be able to:",
    "crumbs": [
      "Preliminaries",
      "Course Logistics"
    ]
  },
  {
    "objectID": "00-logistics.html#learning-objectives",
    "href": "00-logistics.html#learning-objectives",
    "title": "Course Logistics",
    "section": "",
    "text": "understand and articulate key concepts and methods in applied data science; acquire, manipulate, and manage data from varied sources; conduct exploratory data analyses; test statistical hypotheses; build models to classify and make predictions about data; and evaluate model performance;\nuse modern tools for data analysis (e.g., the Unix command line, version control systems, the R programming environment, web APIs) and apply “best practices” in data science and data management;\ninteract with both local and remote data sources to store, query, process, and analyze data presented in a variety of common formats (e.g., delimited text files, structured text files, various database systems);\ncomfortably write their own simple computer programs/scripts for data management, statistical analysis, visualization, and more specialized applications;\ndesign and implement reproducible data science workflows that take a project from data acquisition to analysis to presentation and organize their work using a version control system;\nand apply all of these tools to questions of interest in the natural and social sciences.",
    "crumbs": [
      "Preliminaries",
      "Course Logistics"
    ]
  },
  {
    "objectID": "00-logistics.html#prerequisites",
    "href": "00-logistics.html#prerequisites",
    "title": "Course Logistics",
    "section": "Prerequisites",
    "text": "Prerequisites\nAt least one semester of introductory statistics is recommended. Prior programming experience is not expected, but would be helpful!",
    "crumbs": [
      "Preliminaries",
      "Course Logistics"
    ]
  },
  {
    "objectID": "00-structure.html",
    "href": "00-structure.html",
    "title": "Structure",
    "section": "",
    "text": "This course is divided into three main sections.\nIn Part I, we will introduce and practice using the statistical programming software R, the RStudio integrated development environment, and the R package ecosystem. We will also cover programming/scripting fundamentals as implemented in R (functions, flow control) and practice using version control systems (e.g., git and GitHub) as we build up our skills for conducting reproducible research. We will use all of these tools to practice data wrangling and perform exploratory data analysis and visualizations.\nIn Part II, we will cover basic statistical and probability theory and methods of statistical inference. We will discuss classical null hypothesis significance testing and more contemporary methods based on permutation methods and, if time permits, I may also introduce alternative Bayesian approaches to inference. In this section, we will cover a variety of linear modeling topics, including simple and multivariate regression, ANOVA and ANCOVA, generalized linear modeling, and mixed effects modeling, as well as regression diagnostics and tools for model selection.\nFinally, in Part III, I hope to introduce a few additional and more specialized data analysis and visualization topics. Assuming we get there, Part III will introduce a mish-mash of (hopefully useful and interesting!) topics and tools, e.g., working with geospatial data and phylogenetic trees, network analysis, machine learning, natural language processing, image analysis, etc. Past experience suggests that I am proposing an ambitious amount of material to cover, so we likely will not get to some of these more specialized kinds of analyses. Still, if there’s a topic you are particularly excited about exploring, let me know and I will see what we can do!",
    "crumbs": [
      "Preliminaries",
      "Structure"
    ]
  },
  {
    "objectID": "00-course-schedule.html",
    "href": "00-course-schedule.html",
    "title": "Course Schedule",
    "section": "",
    "text": "Part I - Using R and RStudio",
    "crumbs": [
      "Preliminaries",
      "Course Schedule"
    ]
  },
  {
    "objectID": "00-course-schedule.html#part-i---using-r-and-rstudio",
    "href": "00-course-schedule.html#part-i---using-r-and-rstudio",
    "title": "Course Schedule",
    "section": "",
    "text": "An Introduction to R\n\nModules\n\nModule 01 - Getting Started with R\nModule 02 - Getting Started with RStudio\nModule 03 - Extending the Functionality of R\nModule 04 - Fundamentals of the R Language\n\n\n\nTopics\n\nHistory of R\n\nRelation to other languages and statistics software\n\nInstalling R and RStudio\nUsing R and RStudio in the cloud\nSetting up your RStudio workspace\n\nPanels: Source, Console, Environment/History, Other Views\n\nConfiguration and customization\n\nSetting the working directory\nSaving workspaces\n\nR documentation and getting help\n\nThe ? command\nVignettes\nStack Overflow\n\nR Basics\n\nUsing R interactively\nVariables and assignment\nPackages\n\nInstalling and updating\nDependencies\n\nR objects\n\nObject types - Vectors, simple functions, and environments\nClasses and attributes of objects\nScripting and sourcing scripts\n\n\n\n\n\nSuggested Readings\n\nIntroduction to Data Science\n\nChapter 1 - Introduction\nChapter 2 - R Basics\n\nR in Action, Second Edition\n\nChapter 1 - Getting Started\nChapter 2 - Creating a Dataset\n\n\n\n\nOther Useful Readings\n\nThe Book of R\n\nChapter 1 - Getting Started\nChapter 2 - Numerics, Arithmetic, Assignment, and Vectors\n\nR Programming for Data Science\n\nChapter 3 - History and Overview of R\nChapter 5 - R Nuts and Bolts\n\nStatistics: An Introduction Using R\n\nChapter 1 - Fundamentals\nAppendix: Essentials of the R Language\n\nAdvanced R, First Edition\n\nChapter 2 - Data Structures\n\nModern Data Science with R\n\nAppendix B: An Introduction to R and RStudio\n\n\n\n\n\nVersion Control and Reproducibility\n\nModules\n\nModule 05 - Basics of Version Control\nModule 06 - Reproducible Research Using RStudio\nSome recommendations on Programming Style\n\n\n\nTopics\n\nGood programming practices\n\nVersion control with git and GitHub\nData workflow with R projects using local and remote repositories\nReproducible research using Rmarkdown and Quarto\nProgramming conventions and style\n\n\n\n\nSuggested Readings\n\nIntroduction to Data Science\n\nChapter 39 - Git and GitHub\n\nEssentials of Data Science\n\nChapter 11 - R with Style\n\n\n\n\nOther Useful Readings\n\nHappy Git and GitHub for the useR\nIntroduction to Data Science\n\nChapter 37 - Accessing the terminal and installing Git\nChapter 38 - Organizing with Unix\nChapter 40 - Reproducible projects with RStudio and RMarkdown/Quarto\n\n\n\n\n\nData Science Preliminaries\n\nModules\n\nModule 07 - Additional Data Structures in R\nModule 08 - Getting Data into R\n\n\n\nTopics\n\nWorking with data\n\nThe Tao of text\nMore object types - matrices, n-dimensional arrays, lists, data frames, and other tabular structures (e.g., data tables and “tibbles”)\nSubsetting and filtering data structures\n\nSingle bracket ([]) notation\nDouble bracket ([[]]) notation\n$ notation\n\nFactors\nClass coercion and conversion\nSpecial data values - NA, NaN, Inf\nGetting data in and out of R\n\nFrom “.csv” files - {readr}\nFrom Excel - {readxl} and others\nFrom Dropbox - {rdrop2}\nFrom other online resources - {curl}\nFrom databases - {RMySQL}, {RSQLite}, {RPostgreSQL} and others\n\n\n\n\n\nSuggested Readings\n\nThe Book of R\n\nChapter 3 - Matrices and Arrays\nChapter 5 - Lists and Data Frames\n\nR in Action\n\nChapter 4 - Basic Data Management\n\n\n\n\nOther Useful Readings\n\nThe Book of R\n\nChapter 4 - Non-Numeric Values\nChapter 6 - Special Values, Classes, and Coercion\nChapter 8 - Reading and Writing Files\n\nAdvanced R\n\nChapter 4 - Subsetting\n\nR for Data Science\n\nChapter 7 - Data Import\n\n\n\n\n\nExploratory Data Analysis\n\nModules\n\nModule 09 - Exploratory Data Analysis\n\n\n\nTopics\n\nSummarizing and visualizing data\n\nBasic descriptive statistics\nTidying and reshaping data with {tidyr}\nSimple plotting (boxplots, histograms, scatterplots) with {base} R, {ggplot2}, and others\n\n\n\n\nSuggested Readings\n\nIntroduction to Data Science\n\nChapter 5 - The {tidyverse}\n\nR in Action\n\nChapter 6 - Basic Graphs\nChapter 7 - Basic Statistics\n\n\n\n\nOther Useful Readings\n\nThe Book of R\n\nChapter 13 - Elementary Statistics\nChapter 14 - Basic Data Visualization\n\nR for Data Science\n\nChapter 5 - Data Tidying\n\n\n\n\n\nData Wrangling and Programming\n\nModules\n\nModule 10 - Data Wrangling with {dplyr}\nModule 11 - Functions and Flow Control\n\n\n\nTopics\n\nManipulating data\n\n{dplyr} functions - select(), filter(), arrange(), rename(), mutate(), group_by(), summarize()\nChaining and piping data\n\nR programming practices\n\nWriting functions\n\nArgument lists\nDefault values\n\nProgram flow control\n\nConditional statements\nfor() loops\nwhile() loops\n\n\n\n\n\nSuggested Readings\n\nIntroduction to Data Science\n\nChapter 4 - Programming Basics\n\n\n\n\nOther Useful Readings\n\nThe Book of R\n\nChapter 9 - Calling Functions\nChapter 10 - Conditions and Loops\nChapter 11 - Writing Functions\n\nR for Data Science\n\nChapter 3 - Data Transformation\n\nR in Action\n\nChapter 5 - Advanced Data Management",
    "crumbs": [
      "Preliminaries",
      "Course Schedule"
    ]
  },
  {
    "objectID": "00-typography-and-formatting.html",
    "href": "00-typography-and-formatting.html",
    "title": "Typography and Formatting",
    "section": "",
    "text": "To the extent possible, I have tried to follow the following typographic and formatting conventions throughout the course modules.\n\nProgram names are written in bold italics:\n\nR\nRStudio\n\nFunctions, commands, and R code are written as inline code, e.g., x &lt;- 5, or in code blocks:\n\n\nx &lt;- 5\nprint(x)\n\n\nCode output appears in a colored cell, prefaced by “##”:\n\n\n\n## [1] 5\n\n\n\nPackage names are written in {curly braces}:\n\n{tidyverse}\n{lubridate}\n\nVariable names (for vectors, data frames and other tabular data, etc.) are written in bold:\n\nx\ny\nairline_flights\n\nColumn names within a data frame are also written in bold, whether referred to separately or as part of the table:\n\norigin\ndestination\nairline_flights$origin\nairline_flights[[“destination”]]\n\nFilenames and file types are written in “quotation marks”:\n\n“myData.R”\n“.csv”\n\nFull file path names are written in “quotation marks”:\n\n“/Users/Tony/Desktop/myData.R”\n“~/Desktop/myData.R”\n“C:\\Documents and Settings\\Anthony Di Fiore\\Desktop\\myData.R”\n\nDirectory names are written as inline code followed by a slash:\n\nimg/, src/\n\nImportant concepts are written in italics when first referred to:\n\nworking directory\nenvironment\nnamespace\n\nMenu names and menu commands are written in bold:\n\nFile &gt; New File &gt; R Script\n\nArgument values or values to be replaced are written in lowercase text between &lt;angle brackets&gt;, where that entire text, brackets included, should be replaced with the text being asked for:\n\nsetwd(\"&lt;your working directory&gt;\")\n“&lt;your file name&gt;.csv”\n\nConstants are written in italics:\n\npi\n\nArgument names and assignments are written as inline code:\n\nSet na.rm=TRUE\nSet data=df\nSet filename=\"~/Users/Tony/Desktop/output.csv\"\n\nNames of RStudio panes are written in bold:\n\nConsole\nEnvironment/History\n\nNames of RStudio tabs within panes are written in italics:\n\nHistory\nPlots\nHelp\nGit\n\nNames of dialog boxes are written in italics:\n\nGlobal Options\n\nButton names and sections within dialog boxes are written in quotation marks:\n\n“OK”\n“Cancel”\n\nCheck box names within dialog boxes are written in inline code:\n\nRestore .RData into workspace at startup\n\nR object class names are written in bold:\n\nnumeric\nfunction\n\ngit repository branch names are written as inline code:\n\nmain\norigin/main\n\nFull URLs/links are written in inline code and may include hyperlinks:\n\nhttps://cran.r-project.org/\nhttps://difiore.github.io/ada-2024/",
    "crumbs": [
      "Preliminaries",
      "Typography and Formatting"
    ]
  },
  {
    "objectID": "00-programming-style-guide.html",
    "href": "00-programming-style-guide.html",
    "title": "Programming Style Guide",
    "section": "",
    "text": "File Type Conventions",
    "crumbs": [
      "Preliminaries",
      "Programming Style Guide"
    ]
  },
  {
    "objectID": "00-programming-style-guide.html#file-type-conventions",
    "href": "00-programming-style-guide.html#file-type-conventions",
    "title": "Programming Style Guide",
    "section": "",
    "text": "Use the uppercase “.R” extension for files containing R code\nUse the “.RData” extension for files that contain binary data\nUse the “.Rmd” extension for RMarkdown documents\nUse the “.qmd” extension for Quarto documents\nUse lowercase file extensions for other standard filetypes (e.g., “.csv”, “.jpg”, “.docx”, “.xlsx”)",
    "crumbs": [
      "Preliminaries",
      "Programming Style Guide"
    ]
  },
  {
    "objectID": "00-programming-style-guide.html#stylistic-conventions",
    "href": "00-programming-style-guide.html#stylistic-conventions",
    "title": "Programming Style Guide",
    "section": "Stylistic Conventions",
    "text": "Stylistic Conventions\n\nUse a space before and after the standard backwards assignment operator &lt;- and other infix operators (except for = used in function arguments), but not around parentheses or brackets:\n\nx &lt;- \"Hello\"\n\nEven though the syntax is valid, avoid using = for assignment, except when assigning values to named arguments in a function:\n\nrnorm(n=1000, mean=50, sd=10)\n\nEven though the syntax is valid, do not abbreviate TRUE and FALSE to T and F\nGenerally avoid using the forward assignment operator -&gt; except at the end of a sequence of piped operations:\n\ndf &lt;- df %&gt;% select(name, age, sex, body_weight)\ndf |&gt; select(name, age, sex, body_weight) -&gt; df\n\nUse a space after a comma when listing the arguments of a function:\n\nx &lt;- c(4, 5, 6, 7)\npaste(\"Data science\", \"is\", \"cool\", sep=\" \")",
    "crumbs": [
      "Preliminaries",
      "Programming Style Guide"
    ]
  },
  {
    "objectID": "00-programming-style-guide.html#programming-conventions",
    "href": "00-programming-style-guide.html#programming-conventions",
    "title": "Programming Style Guide",
    "section": "Programming Conventions",
    "text": "Programming Conventions\n\nUse simple, single characters for temporary variables, like indices:\n\nx &lt;- 1:10\nfor (i in 1:100) {print(i)}\n\nWhenever possible, use short, descriptive names for variables:\n\nrate &lt;- 5.6\nsex &lt;- c(\"M\", \"F\", \"F\", \"M\", \"F\", \"M\" \"F\", \"M\")\n\nFor longer, multi-word variable, function, or argument names, use either camelCase or snake_case:\n\ninterestRate &lt;- 0.45\nsay_hello &lt;- function(x) {print(paste0(\"Hello, \",x))}\nprint_n_rows &lt;- function(x, n_rows=10) {print(x[n_rows,])}\n\nInclude default values in your function definitions:\n\nn_rows=10 in the preceding example\n\nInclude error checking in your functions\nFor support files that contain a single function, name the file to match the name of the function defined in the file:\n\n“prettyPrint.R” for a file that contains the function prettyPrint()\n“rescale_image.R” for a file that contains the function rescale_image()",
    "crumbs": [
      "Preliminaries",
      "Programming Style Guide"
    ]
  },
  {
    "objectID": "00-programming-style-guide.html#code-formatting-conventions",
    "href": "00-programming-style-guide.html#code-formatting-conventions",
    "title": "Programming Style Guide",
    "section": "Code Formatting Conventions",
    "text": "Code Formatting Conventions\n\nTry to keep lines of code to less than 80 characters\nUse comments liberally to make notes about what your code does\n\nR ignores lines starting with the hashtag character (#) as well as text after this character (until it encounters a line break)\n\n\n\n# assign `g`, the constant for gravitational acceleration\ng &lt;- 9.80665  # units are m/s^2\n\n\nUse a single # to introduce a comment, and separate comments from code with a single empty line before the comment\n\n\nx &lt;- 3\n\n# Now add 2 to x...\nx &lt;- x + 2\n\n\nIn RStudio, use four dashes ---- at the end of a comment line to indicate a section… this should allow for code folding in your scripts:\n\n\n# Section 1 ----\nx &lt;- 5\ny &lt;- 3\nz &lt;- x + y^2\n\n\nNOTE: In RStudio, you can highlight several lines and then use ⌘-SHIFT-C to comment/uncomment multiple lines simultaneously.\n\n\nUse indentation to identify (nested) blocks of code:\n\nUse spaces rather than the invisible tab (\\t) character for indentation\nUse consistent indentation (e.g., 2 spaces, 4 spaces) to keep your code looking neat\n\n\n\n\n\n\n\n\n\n\n\n\nUse a linter (see the “Addins” section under the Tools menu or in the RStudio toolbar) to catch common style “errors”\n\n\nNOTE: In RStudio, you can use highlight a chunk of code within an R code block and then use ⌘-SHIFT-A to try to neatly and consistently reformat your code. Also, when working in the text editor in RStudio, holding the option (⌥) key while selecting with the cursor allows you to highlight/edit replace text in multiple rows simultaneously.",
    "crumbs": [
      "Preliminaries",
      "Programming Style Guide"
    ]
  },
  {
    "objectID": "00-programming-style-guide.html#version-control-system",
    "href": "00-programming-style-guide.html#version-control-system",
    "title": "Programming Style Guide",
    "section": "Version Control System",
    "text": "Version Control System\n\nFinally, always, always, always use a version control system!!! 😃 See Module 5 and Module 6 for more details.",
    "crumbs": [
      "Preliminaries",
      "Programming Style Guide"
    ]
  },
  {
    "objectID": "00-resources.html",
    "href": "00-resources.html",
    "title": "Resources",
    "section": "",
    "text": "Texts\nThere are no required texts for this course, but we will be covering useful material from a number of the following books, all of which are excellent resources for learning basic to intermediate level statistics and R programming.\nThese are available in print or electronic format directly from the publishers - e.g., No Starch Press, O’Reilly Media, Inc., Manning Publications Co. - or from Amazon.com.",
    "crumbs": [
      "Preliminaries",
      "Resources"
    ]
  },
  {
    "objectID": "00-resources.html#texts",
    "href": "00-resources.html#texts",
    "title": "Resources",
    "section": "",
    "text": "Davies, T.M. (2016). The Book of R: A First Course in Programming and Statistics. No Starch Press.\nBaumer, B.S., Kaplan, D.T., & Horton, N.J. (2021). Modern Data Science with R (Second Edition). Chapman & Hall/CRC. (link to web version)[https://mdsr-book.github.io/mdsr2e/]\nIsmay, C. & Kim, A.Y. (2019). Statistical Inference via Data Science: A ModernDive into R and the Tidyverse. Chapman & Hall/CRC. (link to web version)[https://moderndive.com/]\nIrizarry, R.A. (2019). Introduction to Data Science. Lean Publishing.\nKabacoff, R. (2022). R in Action: Data Analysis and Graphics with R (Third Edition). Manning Publications Co.\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for Data Science (Second Edition). O’Reilly Media, Inc. (link to web version)[https://r4ds.hadley.nz/]",
    "crumbs": [
      "Preliminaries",
      "Resources"
    ]
  },
  {
    "objectID": "00-resources.html#other-resources",
    "href": "00-resources.html#other-resources",
    "title": "Resources",
    "section": "Other Resources",
    "text": "Other Resources\n\nCheatsheets\n\nPosit Cheatsheets Resource\nBase R\nAdvanced R\nR Reference Card\nMarkdown and GitHub Flavored Markdown\nRMarkdown 1\nRMarkdown 2\nRMarkdown Reference Guide\nLearning RMarkdown\nRStudio IDE\nData Import\nData Transformation with {dplyr}\nData Wrangling with {dplyr} and {tidyr}\nTypes of Regression (R in Action Table 8.1)\nRegression Syntax (R in Action Table 8.2)\nUseful Functions for Regression Models (R in Action Table 8.3)\n{leaflet} for Interactive Mapping\nBasics of Probability\n{shiny} Tutorial 1\n{shiny} Tutorial 2\n\n\n\nSoftware Tools\n\nProgramming Languages\n\nR (MacOS, Windows, Linux)\nPython (MacOS, Windows, Linux)\nJulia (MacOS, Windows, Linux)\n\n\n\nText and Markdown Editors and Publishing Software\n\nBB Edit (MacOS)\nMarkdownPad2 (Windows)\nNotepad++ (Windows)\nObsidian (MacOS)\nPandoc (MacOS, Windows, Linux)\nQuarto (MacOS, Windows, Linux)\nVisual Studio Code (MacOS, Windows, Linux)\n\n\n\nIDEs\n\nRStudio Desktop (R, Python) (MacOS, Windows, Linux)\nRStudio Cloud/Posit Cloud (R, Python) (browser)\nJupyterLab (R, Python, Julia) (MacOS, Windows, Linux, browser)\nPyCharm (Python) (MacOS)\n\n\n\nVersion Control Tools\n\ngit (MacOS, Windows, Linux)\nGitHub (Website)\nGitHub Desktop (MacOS, Windows)\n\n\n\n\nWeb Resources\n\nCRAN (Comprehensive R Archive Network)\nDataCamp\nSoftware Carpentry\nData Carpentry\nrOpenSci\nStack Overflow\nOnline R Exercises\nR-bloggers\nMockaroo\n\n\n\nBooks\n\nStatistical Modeling in Biology\n\nBolker, B.M. (2008). Ecological Models and Data in R. Princeton University Press.\nIrizarry, R.A. & Love, M.I. (2015). Data Analysis for the Life Sciences. Lean Publishing.\nQuinn, G.P. & Keough, M.J. (2002). Experimental Design and Data Analysis for Biologists. Cambridge University Press.\n\n\n\nR and Basic Statistics\n\nCaffo, B. (2015). Statistical Inference for Data Science. Lean Publishing.\nCaffo, B. (2016). Regression Models for Data Science in R. Lean Publishing.\nChihara, L.M. & Hesterberg, T.C. (2018). Mathematical Statistics with Resampling and R. John Wiley & Sons, Inc.\nCrawley, M.J. (2014). Statistics: An Introduction Using R. (Second Edition). John Wiley & Sons, Inc.\nDalgaard, P. (2008). Introductory Statistics with R (Second Edition). Springer.\nDiez, D., Çetinkaya-Rundel, M., & Barr, C.D. (2019). OpenIntro Statistics (Fourth Edition). OpenIntro.org.\nIrizarry, R.A. (2019). Introduction to Data Science. Lean Publishing.\nIsmay, C. & Kim, A.Y. (2019). Statistical Inference via Data Science: A ModernDive into R and the Tidyverse. Chapman & Hall/CRC.\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning. Springer.\nShahbaba, B. (2012). Biostatistics with R. Springer.\nWolfe, D.A. & Schneider, G. (2017). Intuitive Introductory Statistics. Springer.\n\n\n\nR Programming\n\nDavies, T.M. (2016). The Book of R: A First Course in Programming and Statistics. No Starch Press.\nKabacoff, R. (2022). R in Action: Data Analysis and Graphics with R (Third Edition). Manning Publications Co.\nMatloff, N. (2011). The Art of R Programming. No Starch Press.\nPeng, R. (2020). R Programming for Data Science. Lean Publishing.\nPeng, R. (2016). Exploratory Data Analysis with R. Lean Publishing.\nWickham, H. (2015). Advanced R. Chapman & Hall/CRC.\nWickham, H. (2019). Advanced R. (Second Edition). Chapman & Hall/CRC.\nZuur, A.F., Ieno, E.N., & Meesters, E.H.W.G. (2009). A Beginner’s Guide to R. Springer.\n\n\n\nR Reference\n\nAdler, J. (2009). R in a Nutshell. O’Reilly Media, Inc.\nCrawley, M.J. (2012). The R Book (Second Edition). John Wiley & Sons, Inc.\nEkstrøm, C. T. (2016). The R Primer (Second Edition). Chapman & Hall/CRC.\nGardener, M. (2012). The Essential R Reference. John Wiley & Sons, Inc.\nLong, J.D. & Teetor, P. (2019). R Cookbook (Second Edition). O’Reilly Media, Inc.\n\n\n\nR Graphics\n\nChang, W. (2013). R Graphics Cookbook. O’Reilly Media, Inc.\nWickham, H. (2016). ggplot2: Elegant Graphics for Data Analysis (Second Edition). Springer.\n\n\n\nData Science\n\nBaumer, B.S., Kaplan, D.T., & Horton, N.J. (2017). Modern Data Science with R. Chapman & Hall/CRC.\nBruce, P. & Bruce, A. (2017). Practical Statistics for Data Scientists. O’Reilly Media, Inc.\nCady, F. (2017). The Data Science Handbook. John Wiley & Sons, Inc.\nGrus, J. (2015). Data Science from Scratch. O’Reilly Media, Inc.\nIsmay, C. & Kim, A.Y. (2019). Statistical Inference via Data Science: A ModernDive into R and the Tidyverse. Chapman & Hall/CRC.\nLarose, C.D. & Larose D.T. (2019). Data Science Using Python and R. John Wiley & Sons, Inc.\nMaillund, T. (2016). Introduction to Data Science and Statistical Programming in R. Lean Publishing.\nMcNicholas, P.D. & Tait, P.A. (2019). Data Science with Julia. Chapman & Hall/CRC.\nPearson, R.K. (2018). Exploratory Data Analysis Using R. Chapman & Hall/CRC.\nPeng, R.D. & Matsui, E. (2015). The Art of Data Science. Lean Publishing.\nWickham, H. & Grolemund, G. (2017). R for Data Science. O’Reilly Media, Inc.\nWilliams, G.J. (2017). The Essentials of Data Science. Chapman & Hall/CRC.\nZumel, N. & Mount, J. (2020). Practical Data Science with R, Second Edition. Manning Publications Co.\n\n\n\nData Visualization\n\nDale, K. (2016). Data Visualization with Python and JavaScript. O’Reilly Media, Inc.\nHealy, K. (2018). Data Visualization: A Practical Introduction. Princeton University Press.\nThomas, S.A. (2015). Data Visualization with JavaScript. No Starch Press.\nWilke, C.O. (2019) Fundamentals of Data Visualization. O’Reilly Media, Inc.\n\n\n\nSpatial Data Analysis\n\nBivand, R.S., Pebesma, E., & Gómez-Rubio, V. (2013). Applied Spatial Data Analysis with R (Second Edition). Springer.\nBrundson, C. & Comber, L. (2019). An Introduction to R for Spatial Analysis and Mapping (Second Edition). SAGE.\nBrunsdon, C. & Singleton, A.D. (Eds.). (2015). Geocomputation: A Practical Primer. Los Angeles: SAGE.\nLovelace, R., Nowosad, J., & Muenchow, J. (2019). Geocomputation with R. Chapman & Hall/CRC.\n\n\n\nR and Bayesian Statistics\n\nBolstad, W.M. & Curran, J.M. (2017). Introduction to Bayesian Statistics (Third Edition). John Wiley & Sons, Inc.\nKruschke, J.K. (2015). Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan (Second Edition). Elsevier.\nMcElreath, R. (2019). Statistical Rethinking: A Bayesian Course with Examples in R and Stan (Second Edition). Chapman & Hall/CRC.\n\n\n\nGeneral and Generalized Regression, Mixed Effects, and Multilevel/Hierarchical Modeling\n\nBurnham, K.P. & Anderson, D.R. (2002). Model Selection and Multimodel Inference. Springer.\nDunn, P.K. & Smyth, G.K. (2018). Generalized Linear Models With Examples in R. Springer.\nFox, J. (2016). Applied Regression Analysis and Generalized Linear Models (Third Edition). SAGE.\nFox, J. & Weisberg, S. (2019). An R Companion to Applied Regression. SAGE.\nGelman, A. & Hill, J. (2007). Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press.\nJames, G,, Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning. Springer.\nHoffman, J.P. (2022). Linear Regression Models: Applications in R. Chapman & Hall/CRC.\nZuur, A.F., Ieno, E.N., Walker, N.J., Savaliev, A.A., & Smith, G.M. (2009). Mixed Effects Models and Extensions in Ecology with R. Springer.\n\n\n\nWeb Scraping, Text Mining, and Text Analysis\n\nFriedl, J.E.F. (2000). Mastering Regular Expressions (Third Edition). O’Reilly Media, Inc.\nMitchell, R. (2015). Web Scraping with Python. O’Reilly Media, Inc.\nNolan, D. & Temple Lang, D. (2014). XML and Web Technologies for Data Sciences with R. Springer.\nSilge, J. & Robinson, D. (2017). Text Mining with R: A Tidy Approach. O’Reilly Media, Inc.\n\n\n\nR Packages, R Markdown, Quarto, and Reproducible Research\n\nGandrud, C. (2020). Reproducible Research with R and RStudio (Third Edition). Chapman & Hall/CRC.\nWickham, H. (2015). R Packages. O’Reilly Media, Inc.\nXie, Y. (2017). Bookdown: Authoring Books and Technical Documents with RMarkdown. Chapman & Hall/CRC.\nXie, Y., Allaire, J.J., & Grolemund, G. (2018). R Markdown: The Definitive Guide. Chapman & Hall/CRC.\n\n\n\ngit and Unix Shell Tools\n\nAlbing, C., Vossen, J.P., & Newham, C. (2007). Bash Cookbook. O’Reilly Media, Inc.\nBarrett, D.J. (2016). Linux Pocket Guide: Essential Commands (Third Edition). O’Reilly Media, Inc.\nChacon, S. & Straub, B. (2014). Pro Git (Second Edition). Apress.\nDougherty, D. & Robbins, A. (1998). Sed and Awk (Second Edition). O’Reilly Media, Inc.\nNewham, C. & Rosenblatt, B. (2005). Learning the bash Shell (Third Edition). O’Reilly Media, Inc.\nRobbins, A. (2006). UNIX in a Nutshell (Fourth Edition). O’Reilly Media, Inc.\n\n\n\nData Science, Statistics, and Programming in Python\n\nBeazley, D. & Jones, B.K. (2013). Python Cookbook (Third Edition). O’Reilly Media, Inc.\nDowney, A.B. (2012). Think Python. O’Reilly Media, Inc.\nDowney, A.B. (2014). Think Stats (Second Edition). O’Reilly Media, Inc.\nDowney, A.B. (2023). Modeling and Simulation in Python. No Starch Press.\nKazil, J. & Jarmul, K. (2016). Data Wrangling with Python. O’Reilly Media, Inc.\nLubanovic, B. (2014). Introducing Python. O’Reilly Media, Inc.\nLee, K.D. (2011). Python Programming Fundamentals. Springer.\nLutz, M. (2013). Learning Python (Fifth Edition). O’Reilly Media, Inc.\nLutz, M. (2014). Python Pocket Reference (Fifth Edition). O’Reilly Media, Inc.\nMcKinney, W. (2013). Python for Data Analysis. O’Reilly Media, Inc.\nRogel-Salazar, J. (2023). Statistics and Data Visualisation with Python. Chapman & Hall/CRC.\nVanderPlas, J. (2016). Python Data Science Handbook. O’Reilly Media, Inc.\nVasiliev, Y. (2022). Python for Data Science. No Starch Press.\nVaughan, L. (2023). Python Tools for Scientists: An Introduction to Coding, Anaconda, Jupyterlab, and the Scientific Libraries. No Starch Press\n\n\n\nData Science, Statistics, and Programming in Julia\n\nMcNicholas, P.D. & Tait, P.A. (2019). Data Science with Julia. Chapman and Hall/CRC.\nPhillips, L. (2024). Practical Julia: A Hands-on Introduction for Scientific Minds. No Starch Press.\n\n\n\nDatabases and SQL\n\nDeBarros, A. (2022). Practical SQL: A Beginner’s Guide to Storytelling with Data. (Second Edition). No Starch Press.\nKreibich, J.A. (2010). Using SQLite. O’Reilly Media, Inc.\nObe, R.O. & Hsu, L.S. (2012). PostgreSQL: Up and Running. O’Reilly Media, Inc.\nRobinson, I., Webber, J., & Eifrem, E. (2015). Graph Databases (Second Edition). O’Reilly Media, Inc.\n\n\n\nMachine Learning\n\nBoehmke, B. & Greenwall. B. (2020). Hands-On Machine Learning with R. Chapman & Hall/CRC.\nRhys, H.I. (2020). Machine Learning with R, tidyverse, and mlr. Manning Publications Co.",
    "crumbs": [
      "Preliminaries",
      "Resources"
    ]
  },
  {
    "objectID": "01-module.html",
    "href": "01-module.html",
    "title": "1  Getting Started with R",
    "section": "",
    "text": "1.1 Objectives",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started with ***R***</span>"
    ]
  },
  {
    "objectID": "01-module.html#objectives",
    "href": "01-module.html#objectives",
    "title": "1  Getting Started with R",
    "section": "",
    "text": "The goal of this module is to get everyone’s computers set up with R for the semester and to provide background and an introduction to the R programming language and environment.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started with ***R***</span>"
    ]
  },
  {
    "objectID": "01-module.html#backstory",
    "href": "01-module.html#backstory",
    "title": "1  Getting Started with R",
    "section": "1.2 Backstory",
    "text": "1.2 Backstory\nThe name R is a nod to the statistical programming language S (for “Statistics”) that inspired its creation. S was developed at Bell Laboratories by John Chambers and later sold to a small company that further developed it into S-Plus. R was then developed as an alternative to S by Ross Ihaka and Robert Gentleman in the Department of Statistics at the University of Aukland, New Zealand.\nR is an high-level, interpreted language, like Python or Ruby, where commands are executed directly and sequentially, without previously compiling a program into machine-language instructions. Each statement is translated, on the fly, into a sequence of subroutines that have already been compiled into machine code.\nR is open-source software, meaning that the source code for the program is freely available for anyone to modify, extend, and improve upon. R is also FREE (!) for anyone to use and distribution. The large and active community of users and developers is one of the reasons that R has become very popular in academics, science, engineering, and business - any field that requires data analytics. Developers have also built in the capacity for easily making production-quality graphics, making it a great tool for data visualization. There are thus many good reasons to learn and use R.\nHere are a few of the main ones, in a nutshell:\n\nR is high quality software. It is actively developed by an international community of statisticians and software developers with multiple releases and bug fixes every year.\nR is FREE (as in thought). The source code is openly avaialable under the GNU General Public License, which allows others to easily evaluate the quality of the code, contribute new functionality, and quickly fix bugs.\nR is FREE (as in beer). Whereas licenses for other statistical software such as SAS, SPSS, or Stata may cost thousands of dollars, R is available free of charge.\nR is available for multiple platforms. Installers are available for Windows, MacOS, and other Unix based systems and most package are OS agnostic.\nR is extremely extensible. If there is a procedure you want to run that is not included in one of the standard packages, it is likely available in one of the thousands of extensions packages that have been developed and are also freely available. You can also use R to control or interface with external applications, including other programming languages (like Python, SQL, C++, NetLogo), other analysis tools (like GIS software), and databases (like MySQL, PostgreSQL, SQLite, etc). It is also always possible for you to improve R yourself. You can literally do just about anything in R.\nR has a vibrant, intelligent, and generous user community. LOTS of resources are available online for learning and troubleshooting (see, for example, the section on R at the Stack Overflow website.\n\nR can be run in several ways:\n\nInteractively from a console prompt after launching the program from the command line in either a terminal window or command shell.\nIn batch mode, by sourcing commands from an R script file (which is a simple text file).\nFrom within an R graphical user interface (or GUI) or integrated development envrionment (or IDE), which accommodates both of the above.\n\nWe are going to introduce several of these ways of working with R, but the easiest and most convenient is to use an IDE.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started with ***R***</span>"
    ]
  },
  {
    "objectID": "01-module.html#installing-the-r-software",
    "href": "01-module.html#installing-the-r-software",
    "title": "1  Getting Started with R",
    "section": "1.3 Installing the R Software",
    "text": "1.3 Installing the R Software\n\nDownload and install R from the Compehensive R Archive Network (CRAN) website. Choose the correct version for your operating system.\n\n\n\n\n\n\n\n\n\n\n\nIf you are using MacOS, you should consider also installing XQuartz, which lets you use the X11 X Windows management software.\n\n\n\n\n\n\n\n\n\n\n\nOpen the R program from wherever you installed it (e.g., in MacOS, double-click on the R.app application in your Applications folder; on a PC, search for and open the Rgui.exe application, which should be located somewhere inside your C:\\Program Files\\R\\R-[version] folder… you should see the console window and the &gt; prompt. Note that your screen may look slightly different from the screenshots below.\n\nAlso, note that you can also run R in a terminal shell (MacOS or Unix) or from the Windows command shell after starting it with the command r. Depending on whether you have set your PATH variable to detect the R executable file, you may or may not need to first navigate into the directory containing the executable file.\n\nOn MacOS, the default GUI will look as follows:\n\n\n\n\n\n\n\n\n\n\n\nOn Windows, the default GUI (RGui) looks like this:",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started with ***R***</span>"
    ]
  },
  {
    "objectID": "01-module.html#exploring-the-r-console",
    "href": "01-module.html#exploring-the-r-console",
    "title": "1  Getting Started with R",
    "section": "1.4 Exploring the R Console",
    "text": "1.4 Exploring the R Console\n\nR can be used as an interactive calculator from the console prompt, either in a GUI or in the shell.\n\nStandard mathematical order of operations applies (PEMDAS - parentheses, exponents, multiplication/division, addition/subtraction).\n\nThe assignment operator &lt;- can be used to assign a value, the results of an operation, or specific code to an object (e.g., a variable, a function, a complex data structure).\n\nYou can also use =, but I prefer to use that only to assign values to function arguments (more on this later).\n\nYou can set various settings to customize your interactions with R.\n\nTo change the standard prompt, use the function options() with the prompt argument: options(prompt=\"&lt;prompt text&gt;\"), where you supply, between the quotes, text with what you want the prompt to say.\nTo list the current working directory (the default folder where dialog boxes will open and where files and output will be stored unless otherwise specified), use the function getwd(), which has no arguments.\nTo set the current working directory, use the function setwd(\"&lt;path&gt;\"), where you supply, between the quotes, the path to the desired directory.\n\nOn MacOS, these directory options are also available under the Misc menu.\nOn Windows, you can set the working directory with the Change dir command under the File menu.\nIn RStudio, the working directory can be set under the Session menu.\n\n\nWithin the active workspace, R keeps a log of all executed commands, and you can use the arrow keys to scroll through this history. In RStudio, this list is accessible in the History tab.\nCommands and code can also be written in a text file or script and sent to the console for execution.\n\nIn most GUIs/IDEs, you can choose to create a new script document from the File menu, which opens in a text editor of some kind.\nFrom within the text editor, you can send an individual command to the R interpreter by positioning your cursor somewhere in the line you want to execute and hitting ⌘-RETURN (Mac) or either control-R (for the default GUI that ships with R) or control-ENTER (for the RStudio GUI) (PC).\nTo send a set of commands to the console as a batch, you can highlight the code lines of code you want to execute and then use these same commands.\nYou can include comments in your scripts by prefacing them with #.\nScript files can be saved just like any other type of text file, usually with the “.R” extension by default.\n\nTo view the names of all of the objects in your current workspace, you can use the ls() function. In RStudio, these also all appear in the Environment tab.\nTo clear objects from your workspace, use the rm() function, where an individual object’s name or a list of object names can be included as the argument to rm().\nTo remove all objects from your workspace, you can use rm(list=ls()).\n\nIn this case, you are passing to rm() a list consisting of all the objects in the workspace, provided by the ls() function.\n\n\n\nCHALLENGE\nFire up R in your mode of choice (by typing “R” at the console prompt in the Terminal in MacOS or from a cmd or other shell prompt in Windows) and then practice interacting with the software via the command line and console window.\n\nTry doing some math in R by using it to evaluate the following expressions:\n\n8 + 5\n10 - 6 / 2\n(10 - 6) / 2\n10 * 5\n15 / 5\n10 ^ 5\n3 * pi (where pi is a built-in constant)\n\n\n\n\nShow Code\n8 + 5\n\n\nShow Output\n## [1] 13\n\n\n\nShow Code\n10 - 6/2\n\n\nShow Output\n## [1] 7\n\n\n\nShow Code\n(10 - 6)/2\n\n\nShow Output\n## [1] 2\n\n\n\nShow Code\n10 * 5\n\n\nShow Output\n## [1] 50\n\n\n\nShow Code\n15/5\n\n\nShow Output\n## [1] 3\n\n\n\nShow Code\n10^5\n\n\nShow Output\n## [1] 1e+05\n\n\n\nShow Code\n3 * pi\n\n\nShow Output\n## [1] 9.424778\n\n\n\n\nTry working with assignments:\n\nAssign the number 6 to a variable called x.\nAssign the number 5 to a variable called y.\nAssign x * y to a variable called z.\nAssign x^2 to a variable called x2.\n\n\n\n\nShow Code\nx &lt;- 6\nx\n\n\nShow Output\n## [1] 6\n\n\n\nShow Code\ny &lt;- 5\ny\n\n\nShow Output\n## [1] 5\n\n\n\nShow Code\nz &lt;- x * y\nz\n\n\nShow Output\n## [1] 30\n\n\n\nShow Code\nx2 &lt;- x^2\nx2\n\n\nShow Output\n## [1] 36\n\n\n\n\nTry out some of the built-in functions in R:\n\nAssign the number 10 to a variable called x.\nTake the natural log of x using the log() function.\nFind the factorial of x using the factorial() function.\nAssign the number 81 to a variable called y.\nTake the square root of y using the sqrt() function.\nAssign the number -8.349218 to a variable called z.\nUse ?round or help(round) to view the help file for the function round().\nRound z to the 1000ths place.\nUse ?abs() to view the help file for the function abs().\nTake the absolute value of z * y.\n\n\n\n\nShow Code\nx &lt;- 10\nlog(x)\n\n\nShow Output\n## [1] 2.302585\n\n\n\nShow Code\nfactorial(x)\n\n\nShow Output\n## [1] 3628800\n\n\n\nShow Code\ny &lt;- 81\nsqrt(y)\n\n\nShow Output\n## [1] 9\n\n\n\nShow Code\nz &lt;- -8.349218\nround(z, digits = 3)\n\n\nShow Output\n## [1] -8.349\n\n\n\nShow Code\nabs(z * y)\n\n\nShow Output\n## [1] 676.2867\n\n\n\n\nUse the ls() function to list the variables currently stored in your active session.\n\nHow many variables do you have?\n\n\n\n\nShow Code\nls()\n\n\nShow Output\n## [1] \"x\"  \"x2\" \"y\"  \"z\"\n\n\n\n\nUse the command rm(list=ls()) to clear all the variables you have defined.\nWhat happens if you type a function name without including the parentheses?\nWhat happens if you type a function with an invalid or missing argument?",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started with ***R***</span>"
    ]
  },
  {
    "objectID": "01-module.html#concept-review",
    "href": "01-module.html#concept-review",
    "title": "1  Getting Started with R",
    "section": "Concept Review",
    "text": "Concept Review\n\nInteracting with R from the console prompt\nVariable assignment: &lt;-\nCalling built-in functions: function(&lt;arguments&gt;)\nAccessing R documentation and help files: ?function or help(function)\nWorkspaces and the working directory: getwd(), setwd()\nListing and removing variables from the environment: ls(), rm()\nAccessing the console history",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started with ***R***</span>"
    ]
  },
  {
    "objectID": "02-module.html",
    "href": "02-module.html",
    "title": "2  Getting Started with RStudio",
    "section": "",
    "text": "2.1 Objectives",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started with ***RStudio***</span>"
    ]
  },
  {
    "objectID": "02-module.html#objectives",
    "href": "02-module.html#objectives",
    "title": "2  Getting Started with RStudio",
    "section": "",
    "text": "The goal of this module is to familiar yourself with the RStudio Integrated Development Environment (IDE).",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started with ***RStudio***</span>"
    ]
  },
  {
    "objectID": "02-module.html#installing-the-rstudio-software",
    "href": "02-module.html#installing-the-rstudio-software",
    "title": "2  Getting Started with RStudio",
    "section": "2.2 Installing the RStudio Software",
    "text": "2.2 Installing the RStudio Software\nApart from the GUIs included in the MacOS and Windows installations of R, there are several IDEs that connect to the R interpreter and provide lots of convenient functionality. One of the most versatile and easy to use (and my favorite) is RStudio, created by the company Posit.\n\nDownload and install the RStudio Integrated Development Environment (IDE)\n\n\n\n\n\n\n\n\n\n\n\nOpen the RStudio program\n\nThe workspace that you see is divided into four separate panes (Source and Console panes on the left, two customizable panes on the right). You can modify the layout and appearance of the RStudio IDE to suit your taste by selecting Preferences from the RStudio menu (MacOS) or by selecting Global Options from the Tools menu (both MacOS and Windows).\n\n\n\n\n\n\n\n\n\nThe Source pane is where you work with and edit various file types (e.g., scripts), while the Console pane is where you run commands in the R interpreter and see the results of those commands. The other two customizable panes provide easy access to useful tools and overviews of your interactions with R. For example, the Environment tab can be used to view all of the objects in the different environments in your current workspace, the History tab shows the log of all of the commands you have sent to the interpreter, and the Packages tab provides a convenient interface for installing and loading packages (see below).\nWithin RStudio, you can change the working directory by going to the Session menu and selecting Set Working Directory.\n\nCHALLENGE\nRepeat the basic maths CHALLENGE from Module 01 using the editor and console in RStudio.\n\nNOTE: In both the base GUI that ships with the R application and in RStudio, the console supports code completion. Pressing TAB after starting to type a function or variable name will give you suggestions as to how to complete what you have begun to type. In RStudio, this functionality is present also when you are typing code in the text editor in the Source pane. Also helpful in RStudio are popup windows that accompany code completion that show, for each function, what possible arguments that function can take and their default values.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started with ***RStudio***</span>"
    ]
  },
  {
    "objectID": "02-module.html#using-the-rstudio-posit-server",
    "href": "02-module.html#using-the-rstudio-posit-server",
    "title": "2  Getting Started with RStudio",
    "section": "2.3 Using the RStudio Posit Server",
    "text": "2.3 Using the RStudio Posit Server\nAn alternative (though likely slower!) way to use R and RStudio is to run them through a browser from a cloud computing server adminstered by Posit, the company that developed and continues to refine RStudio. To use this approach, visit the Posit Cloud website, click the Get Started button, and create or sign up for an account.\n\n\n\n\n\n\n\n\n\nYou can use a Facebook or GitHub account to sign up (I recommend the latter. See Module 05 for more info on signing up for and using GitHub) or create a new account that is specifically for Posit Cloud.\n\n\n\n\n\n\n\n\n\nOnce you are signed up, or if you have already done so, you can use your account to log in. Doing so will bring you to the landing page for your Posit Cloud account.\n\n\n\n\n\n\n\n\n\nYour account lets you access a sandboxed environment on Posit’s servers that contains R, RStudio, any packages you install, your own files, etc., which you are connecting via a web browser. You can effectively use it for development without storing anything on your local machine.\nThere, if you create a New Project, you will set up a new R project within your account space. Your account space can include multiple projects, each with its own set of associated files.\n\n\n\n\n\n\n\n\n\nRStudio running through your browser looks virtually the same as running it on your own machine. You can can install packages to your workspace, set your own preferences for the IDE, and even run different versions of R. As a free user, your workspace will have some computational limits, so it’s not going to be useful for production-level computing, but for most things we do in this class, it would be sufficient.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started with ***RStudio***</span>"
    ]
  },
  {
    "objectID": "02-module.html#customizing-the-rstudio-ide",
    "href": "02-module.html#customizing-the-rstudio-ide",
    "title": "2  Getting Started with RStudio",
    "section": "2.4 Customizing the RStudio IDE",
    "text": "2.4 Customizing the RStudio IDE\nIf you open either the Preferences (MacOS) or Global Options (MacOS or PC) dialog box in RStudio you can customize the setup and functionality of your IDE.\nIn the General section, I recommend the settings shown below, particularly about restoring your last workspace into memory upon startup and about asking if you wish to save the contents of your current workspace upon shutdown.\n\n\n\n\n\n\n\n\n\nFeel free to organize the rest of your setup as you would like. For example, you can change lots of options in the Code, Console, Appearance, and Pane Layout sections to set up the IDE as best suits your personal tastes.\n\n\n\n\n\n\n\n\n\nFinally, if you have already installed git on your computer, go to the Git/SVN section…\n\n\n\n\n\n\n\n\n\n… and make sure that the checkbox “Enable version control interface for RStudio projects” is selected and that the path to your git executable is filled. If you have installed git successfully, this should be filled with something like “/usr/bin/git” or “usr/local/bin/git”. If it is not, do not worry… we can set this later after installing and troubleshooting that program. Also make sure that the “Sign git commits” box is unchecked.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started with ***RStudio***</span>"
    ]
  },
  {
    "objectID": "02-module.html#concept-review",
    "href": "02-module.html#concept-review",
    "title": "2  Getting Started with RStudio",
    "section": "Concept Review",
    "text": "Concept Review\n\nInstalling, navigating and customizing the RStudio IDE\nRunning R and RStudio through a browser using RStudio Cloud",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started with ***RStudio***</span>"
    ]
  },
  {
    "objectID": "03-module.html",
    "href": "03-module.html",
    "title": "3  Extending the Functionality of R",
    "section": "",
    "text": "3.1 Objectives",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Extending the Functionality of ***R***</span>"
    ]
  },
  {
    "objectID": "03-module.html#objectives",
    "href": "03-module.html#objectives",
    "title": "3  Extending the Functionality of R",
    "section": "",
    "text": "The goal of this module is to show you how to extend the base functionality of R by installing and loading packages.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Extending the Functionality of ***R***</span>"
    ]
  },
  {
    "objectID": "03-module.html#preliminaries",
    "href": "03-module.html#preliminaries",
    "title": "3  Extending the Functionality of R",
    "section": "3.2 Preliminaries",
    "text": "3.2 Preliminaries\n\nInstall this package in R: {easypackages}",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Extending the Functionality of ***R***</span>"
    ]
  },
  {
    "objectID": "03-module.html#r-packages",
    "href": "03-module.html#r-packages",
    "title": "3  Extending the Functionality of R",
    "section": "3.3 R Packages",
    "text": "3.3 R Packages\nOne of the fantastic things about R, and one of the reasons it is such a flexible tool for so many types of data analysis, is the ability to extend its functionality in a huge variety of ways with packages. Packages are sets of reusable R functions created by the core development team or by users and are akin to libraries in other programming software, like Python. As of January 2020, there are over 15,300 packages that have been contributed to the most common package repository, hosted on the Comprehensive R Archive Network (CRAN) site.\nFrom the console prompt, packages can be installed into R (using the install.packages() function, with the name of the package in parentheses) and then loaded (using the require() or library() functions), which then gives the user access to the functions contained therein. Both RStudio and the base R GUIs for MacOS and Windows have built-in package managers that allow you to search for and install packages.\nEach package includes a namespace associated with the functions contained therein, and those functions are loaded into their own, separate R environments, distinct from the global environment, where the variables we assigned values to in Module 01 were created. An environment can be thought of as a collection of objects (functions, variables etc.) associated either globally, with the R interpreter (the “R_GlobalEnv”) or with a particular package and its namespace. In RStudio, you can see all of the objects associated with a particular environment in the Environment tab.\nIf a package is loaded that has a function with the same name as one in a previously loaded package or in base R, then the older function will be masked and the newer will be used if called by a user. This is because whenever an object is referenced in a command, the R interpreter searches for that object in various loaded environments in a particular order and operates on the first one it encounters. The global environment is searched first, followed by the environments associated with each loaded package in reverse chronological order of when they are loaded and ending with the base R environment.\n\n\n\n\n\n\n\n\n\n\n\nFROM: Wickham (2019). Advanced R, Second Edition. Chapman & Hall/CRC.\n\n\n\nHowever, functions from different packages with the same name can be called explicitly by using the :: operator, using the construct package-name::function to access the desired namespace and environment. A particular function can be called in this way even if the package as a whole has not been loaded into R using library() or require(). The search() function can be used to return a vector of environment names in the order they will be searched.\n\nInstalling Packages\n\nNOTE: A list of packages we will be using in this course is available here.\n\n\nUsing the base MacOS GUI\n\nSelect the Packages & Data menu.\nChoose Package Installer.\nSelect a package repository to install from, e.g., CRAN (binaries).\nThe first time you go to install packages, you will be prompted to select a mirror site (i.e., the remote server from where you can download any of the packages hosted in the selected package repository).\nSelect an install location. I usually install packages to the system rather than user level.\nCheck the box “Install dependencies”.\nSelect a package to install.\n\n\n\nUsing the base Windows GUI\n\nSelect the Packages menu.\nChoose Install package(s)….\nBy default, the package repository that you will install from is CRAN (binaries).\nThe first time you go to install packages, you will be prompted to select a mirror site (a.k.a., the remote server from where you can download any of the packages hosted in the selected package repository).\nSelect a package to install.\nBy default, packages are installed at the system level (inside of the library folder within your R installation), and any missing dependenices are also installed by default.\n\n\n\nUsing the R console prompt\n\nUse the function install.packages(\"&lt;package name&gt;\"), where you include, between the quotes, the name of the package you want to install. This command installs the package, by default, to the user level, though this can be changed by providing a path to install to using the lib=\"&lt;path&gt; argument. Other arguments for this function can be set to specify the repository to download from, etc.\n\n\n\nUsing RStudio\n\nSelect the Packages tab and then click the “Install” button.\nA dialog box will open where you can choose where to install the package from (the central CRAN repository is typically the source you will use) and the install location on your computer.\nYou can install packages either to the user level library (in which case, only the user who is logged in when the package is installed will have access to it) or to the system library (which will make the package available for all users).\nType the name of the package you want to install in the text field in the middle of the dialog box (code completion will list available packages that match what you are typing as you type). You can install multiple packages at the same time by separating them with a space or comma.\nMake sure the “Install dependencies” checkbox is selected… this will automatically check whether other packages that are referenced in the package you want to install are already installed and, if not, will install them as well.\n\n\n\n\n\n\n\n\n\n\n\n\n\nLoading and Attaching Packages\nNote that installing packages simply places them into a standard location on your computer. To actually use the functions they contain in an R session, you need to also load them into your R workspace.\n\nUsing the base MacOS GUI\n\nSelect the Packages & Data menu.\nChoose Package Manager.\nCheck the box next to the package name.\n\n\n\nUsing the base Windows GUI\n\nSelect the Packages menu.\nChoose Load package….\nSelect the package to load.\n\n\n\nUsing RStudio\n\nYou can load a package interactively in RStudio by clicking the checkbox next to the package name in the Packages tab.\n\n\n\n\n\n\n\n\n\n\n\n\nUsing the console prompt or a script\n\nThe most common way that you will load packages is to do so either interactively at the console prompt or in a script using the command library(&lt;package name&gt;) with the package name, not in quotes, as an argument.\nThe require() function is nearly identical to the library() function except that the former is safer to use inside functions because it will not throw an error if a package is not installed. require() also returns a value of TRUE or FALSE depending on whether the package loaded successfully or not. However, I almost always use the library() function in my scripts. Using library() and require() both load a package’s named components (its “namespace”) and attach those to the global environments search list.\n\n\nNOTE: When loading a package with library() or require(), the package name need not be in quotes, although it works if you were to do that.\n\nBe aware that if a named function of a package conflicts with one in an already loaded/attached package, then by default R will warn of the conflict. In that case, it is good form to use explicit function calls, i.e., use the :: operator to specify first the package and then the function (e.g., dplyr::filter()) you wish to call.\n\n\n\nWorking with Packages\n\nYou can use either of the following to list the set of packages you have installed:\n\nlibrary()\ninstalled.packages()\n\nThe command (.packages()) can be used print out the set packages that have been loaded/attached in your current workspace.\n\n\nNOTE: In the example above, the .packages() function is wrapped in parentheses to immediately print the result of the function.\n\nIn RStudio, you can also see a list of all loaded packages by clicking the down arrow next to “Global Environment” in the Environment tab.\n\n\n\n\n\n\n\n\n\n\nThe command detach(package:&lt;package name&gt;), where “package name”, not in quotes, is the name of the package you want to unload, will unload a currently loaded package from memory. You can also do this interactively in RStudio by unchecking the box next to the package name in the Packages tab.\nTo update your installed packages to the latest version, you can use the function update.packages(). Using RStudio, you can also select “Update” from the Packages tab to get an interactive dialog box showing you what updates are available from CRAN and letting you install them.\n\n\n\n\n\n\n\n\n\n\n\nTo remove installed packages from your R installation, you can use the function remove.packages() or click the small “x” to the right of the package name in the RStudio packages tab.\n\n\n\n\n\n\n\n\n\n\n\nTo process several package at once, you can pass a vector of package names to many of these functions as an argument, e.g., remove.packages(c(\"abc\",\"citr\"))\nFinally, the {easypackages} packages makes installing and loading multiple packages “easy” by introducing two helper functions, packages() and libraries(). Both let you specify a vector of package names to either install (e.g., packages(c(\"tidyverse\", \"magrittr\"))) or load (e.g., libraries(c(\"tidyverse\", \"magrittr\"))). For these functions, package names need to be specified in quotation marks.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Extending the Functionality of ***R***</span>"
    ]
  },
  {
    "objectID": "03-module.html#concept-review",
    "href": "03-module.html#concept-review",
    "title": "3  Extending the Functionality of R",
    "section": "Concept Review",
    "text": "Concept Review\n\nWorking with packages: install_packages(), library(), require(), detach(), update.packages(), remove.packages(), and {easypackages}\nEnvironments and namespaces: ::, search()",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Extending the Functionality of ***R***</span>"
    ]
  },
  {
    "objectID": "04-module.html",
    "href": "04-module.html",
    "title": "4  Fundamentals of the R Language",
    "section": "",
    "text": "4.1 Objectives",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Fundamentals of the ***R*** Language</span>"
    ]
  },
  {
    "objectID": "04-module.html#objectives",
    "href": "04-module.html#objectives",
    "title": "4  Fundamentals of the R Language",
    "section": "",
    "text": "The goal of this module is review important conceptual aspects of the R language as well as practices for updating R components of interest.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Fundamentals of the ***R*** Language</span>"
    ]
  },
  {
    "objectID": "04-module.html#r-objects",
    "href": "04-module.html#r-objects",
    "title": "4  Fundamentals of the R Language",
    "section": "4.2 R Objects",
    "text": "4.2 R Objects\nAlmost everything in R can be thought of as an object, including variables, functions, complex data structures, and environments.\n\nClasses, Modes, and Types\nObjects in R fall into different classes. There are a few basic (or atomic) classes that pertain to variables: numeric (real numbers), integer (integer numbers), character (for text), logical (Boolean values, i.e., TRUE or FALSE, represented as 1 and 0, respectively), complex (for imaginary numbers), and factor (for defined levels of categorical variables… we will talk more about factors later on). There are other classes beyond this set of atomic classes relevant to variables. For example, both built-in and user defined functions have the class function. You can ask R to return the class of any object with the class() function, and R objects can have more than one class. You can think of class as being a property of an object that determines how generic functions operate with it.\nExamples:\n\n# class of a variable\nx &lt;- 4\nclass(x)\n\n## [1] \"numeric\"\n\nx &lt;- \"hi there\"\nclass(x)\n\n## [1] \"character\"\n\n# class of a function\nclass(mean)\n\n## [1] \"function\"\n\n\nIn R, environments are objects as well.\n\nWhat is the class of the global environment, where we have been binding values to variable names? To check, use class(globalenv()).\n\n\n\nShow Code\nclass(globalenv())\n\n\nShow Output\n## [1] \"environment\"\n\n\n\nObjects in R also each have a mode and a base type. These are often closely aligned with and similar to the class of an object, but the three terms refer to slightly different things. If an object has no specific class assigned to it, its class is typically the same as its mode. Mode is a mutually exclusive classification of objects, according to their basic structure. When we coerce an object to another basic structure, we are changing its mode but not necessarily the class.\n\n# mode of a variable\nx &lt;- 4\nmode(x)\n\n## [1] \"numeric\"\n\nx &lt;- \"hi there\"\nmode(x)\n\n## [1] \"character\"\n\n# mode of a function\nmode(mean)\n\n## [1] \"function\"\n\n\n\n# type of a variable\nx &lt;- 4\ntypeof(x)\n\n## [1] \"double\"\n\nx &lt;- \"hi there\"\ntypeof(x)\n\n## [1] \"character\"\n\n# type of a function\ntypeof(mean)\n\n## [1] \"closure\"\n\n\n\nNOTE: For more details on the difference between the class, mode, and base type of an object, check out the book Advanced R, Second Edition by Hadley Wickham (RStudio).\n\n\n\nVectors\nR also supports a variety of data structures for variable objects, the most fundamental of which is the vector. Vectors are variables consisting of one or more values of the same type, e.g., student’s grades on an exam. The class of a vector has to be one of the atomic classes described above. A scalar variable, such as a constant, is simply a vector with only one value.\n\nThere are lots of ways to create vectors… one of the most common is to use the c() or “combine” command:\n\n\nx &lt;- c(15, 16, 12, 3, 21, 45, 23)\nx\n\n## [1] 15 16 12  3 21 45 23\n\ny &lt;- c(\"once\", \"upon\", \"a\", \"time\")\ny\n\n## [1] \"once\" \"upon\" \"a\"    \"time\"\n\nz &lt;- \"once upon a time\"\nz\n\n## [1] \"once upon a time\"\n\n\n\nWhat is the class of the vector x? Of z? Use the class() function to check.\n\n\n\nShow Code\nclass(x)\n\n\nShow Output\n## [1] \"numeric\"\n\n\n\nShow Code\nclass(z)\n\n\nShow Output\n## [1] \"character\"\n\n\n\n\nWhat happens if you try the following assignment: x &lt;- c(\"2\", 2, \"zombies\")? What is the class of vector x now?\n\n\n\nShow Code\nx &lt;- c(\"2\", 2, \"zombies\")\nclass(x)\n\n\nShow Output\n## [1] \"character\"\n\n\n\nThis last case is an example of coercion, which happens automatically and often behind the scenes in R. When you attempt to combine different types of elements in the same vector, they are coerced to all be of the same type - the most restrictive type that can accommodate all of the elements. This takes place in a fixed order: logical → integer → double → character. For example, combining a character and an integer yields a character; combining a logical and a double yields a double.\nYou can also deliberately coerce a vector to be represented as a different base type by using an as.*() function, like as.logical(), as.integer(), as.double(), or as.character().\n\nx &lt;- c(3, 4, 5, 6, 7)\nx\n\n## [1] 3 4 5 6 7\n\ny &lt;- as.character(x)\ny\n\n## [1] \"3\" \"4\" \"5\" \"6\" \"7\"\n\n\nAnother way to create vectors is to use the sequence operator, :, which creates a sequence of values from spanning from the left side of the operator to the right, in increments of 1:\n\nx &lt;- 1:10\nx\n\n##  [1]  1  2  3  4  5  6  7  8  9 10\n\nx &lt;- 10:1\nx\n\n##  [1] 10  9  8  7  6  5  4  3  2  1\n\nx &lt;- 1.3:10.5\nx\n\n##  [1]  1.3  2.3  3.3  4.3  5.3  6.3  7.3  8.3  9.3 10.3\n\n\n\nNOTE: Wrapping an assignment in parentheses, as in the code block below, allows simultaneous assignment and printing to the console!\n\n\n(x &lt;- 40:45)\n\n## [1] 40 41 42 43 44 45\n\n\nWe can also create more complex sequences using the seq() function, which takes several arguments:\n\nx &lt;- seq(from = 1, to = 10, by = 2)\n# skips every other value\nx\n\n## [1] 1 3 5 7 9\n\nx &lt;- seq(from = 1, to = 10, length.out = 3)\n# creates 3 evenly spaced values\nx\n\n## [1]  1.0  5.5 10.0\n\n\n\n\nAttributes and Structure\nMany objects in R also have attributes associated with them, which we can think of as metadata, or data describing the object. Some attributes are intrinsic to an object. For example, a useful attribute to know about a vector object is the number of elements in it, which can be queried using the length() command.\n\nlength(x)\n\n## [1] 3\n\n\nWe can also get or assign arbitrary attributes to an object using the function attr(), which takes two arguments: the object whose attributes are being assigned and the name of the attribute.\n\n# we can assign arbitary attributes to the vector x\nattr(x, \"date collected\") &lt;- \"2019-01-22\"\nattr(x, \"collected by\") &lt;- \"Anthony Di Fiore\"\nattributes(x)  # returns a list of attributes of x\n\n## $`date collected`\n## [1] \"2019-01-22\"\n## \n## $`collected by`\n## [1] \"Anthony Di Fiore\"\n\nclass(attributes(x))  # the class of a list is 'list'\n\n## [1] \"list\"\n\n# a 'list' is another R data structure\nattr(x, \"date collected\")  # returns the value of the attribute\n\n## [1] \"2019-01-22\"\n\n\nFinally, every object in R also has a structure, which can be queried using the str() command.\n\nstr(x)  # structure of the variable x\n\n##  num [1:3] 1 5.5 10\n##  - attr(*, \"date collected\")= chr \"2019-01-22\"\n##  - attr(*, \"collected by\")= chr \"Anthony Di Fiore\"\n\nstr(mean)  # struture of the function mean\n\n## function (x, ...)\n\nstr(globalenv())  # structure of the global environment\n\n## &lt;environment: R_GlobalEnv&gt;\n\nstr(attributes(x))  # attribute names are stored as a list\n\n## List of 2\n##  $ date collected: chr \"2019-01-22\"\n##  $ collected by  : chr \"Anthony Di Fiore\"\n\n\n\nNOTE: The glimpse() function from the {dplyr} package also yields information on the structure of an object, sometimes in a more easily-readable format than str().\n\n\n\nCHALLENGE:\nTry some vector math using the console in RStudio:\n\nAssign a sequence of numbers from 15 to 28 to a vector, x.\n\n\nNOTE: There are at least two different ways to do this!\n\n\nThen, assign a sequence of numbers from 1 to 4 to a vector, y.\nFinally, add x and y. What happens?\n\n\n\nShow Code\nx &lt;- 15:28  # or x &lt;- c(15, 16, 17...)\ny &lt;- 1:4\n(x + y)\n\n\nShow Output\n##  [1] 16 18 20 22 20 22 24 26 24 26 28 30 28 30\n\n\n\n\nUse the up arrow to recall the last command from history and modify the command to store the result of the addition to a variable, z. What kind of object is z? Examine it using the class() function. What is the length of z?\n\nNow, think carefully about this output… there are two important things going on.\nFirst, R has used vectorized addition in creating the new variable. The first element of x was added to the first element of y, the second element of x was added to the second element of y, etc.\nSecond, in performing this new variable assignment, the shorter vector has been recycled. Thus, once we get to the fifth element in x we start over with the first element in y.\nThis means we can very easily do things like adding a constant to all of the elements in a vector or multiplying all the elements by a constant.\n\ny &lt;- 2\n# note that we can wrap a command in parentheses for simultaneous\n# assignment/operation and printing\n(z &lt;- x + y)\n\n##  [1] 17 18 19 20 21 22 23 24 25 26 27 28 29 30\n\n(z &lt;- x * y)\n\n##  [1] 30 32 34 36 38 40 42 44 46 48 50 52 54 56\n\n\nMany function operations in R are also vectorized, meaning that if argument of a function is a vector, but the function acts on a single value, then the function will be applied to each value in the vector and will return a vector of the same length where the function has been applied to each element.\n\nx &lt;- 1:20\n(logx &lt;- log(x))\n\n##  [1] 0.0000000 0.6931472 1.0986123 1.3862944 1.6094379 1.7917595 1.9459101\n##  [8] 2.0794415 2.1972246 2.3025851 2.3978953 2.4849066 2.5649494 2.6390573\n## [15] 2.7080502 2.7725887 2.8332133 2.8903718 2.9444390 2.9957323\n\n(x2 &lt;- x^2)\n\n##  [1]   1   4   9  16  25  36  49  64  81 100 121 144 169 196 225 256 289 324 361\n## [20] 400\n\n(y &lt;- 4 * x + 3)\n\n##  [1]  7 11 15 19 23 27 31 35 39 43 47 51 55 59 63 67 71 75 79 83\n\n\nWe can use the {base} R function plot() to do some quick visualizations.\n\n# `plot()` takes values of x and y values as the first two arguments, and the\n# `type='o'` argument superimposes points and lines\nplot(x, logx, type = \"o\")\n\n\n\n\n\n\n\nplot(x, x2, type = \"o\")\n\n\n\n\n\n\n\nplot(x, y, type = \"o\")\n\n\n\n\n\n\n\n\n\n\nCHALLENGE:\n\nUse the rnorm() function to create a vector, s that contains a set of random numbers drawn from a normal distribution with mean 80 and standard deviation 10. Try doing this with n = 10, n = 100, n = 1000, n = 10000.\n\n\nHINT: Use ?rnorm or help(rnorm) to access the help documentation on how to use the rnorm() function.\n\nThen, use the hist() function to plot a histogram showing the distribution of these numbers.\n\n\nShow Code\ns &lt;- rnorm(n = 10000, mean = 80, sd = 10)\nhist(s)  # hist() plots a simple histogram of values for s\n\n\n\n\n\n\n\n\n\n\nUse the mean() and sd() functions to calculate the mean and standard deviation of s. Here, the whole vector is used as the argument of the function, i.e., the function applies to a set of values not a single value. The function thus returns a vector of length 1.\n\n\n\nShow Code\nmean(s)\n\n\nShow Output\n## [1] 80.00466\n\n\n\nShow Code\nsd(s)\n\n\nShow Output\n## [1] 9.911995",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Fundamentals of the ***R*** Language</span>"
    ]
  },
  {
    "objectID": "04-module.html#scripts-and-functions",
    "href": "04-module.html#scripts-and-functions",
    "title": "4  Fundamentals of the R Language",
    "section": "4.3 Scripts and Functions",
    "text": "4.3 Scripts and Functions\nAs mentioned previously, scripts in R are simply text files that store an ordered list of commands, which can be used to link together sets of operations to perform complete analyses and show results.\nFor example, you could enter the lines below into a text editor and then save the script in a file named “my_script.R” in a folder called src/ inside your working directory.\n\nx &lt;- 1:10\ns &lt;- sum(x)\nl &lt;- length(x)\nm &lt;- s/l\nprint(m)\n\n## [1] 5.5\n\n\nIf you save a script, you can then use the source() function (with the path to the script file of interest as an argument) at the console prompt (or in another script) to read and execute the entire contents of the script file. In RStudio you may also go to Code &gt; Source to run an entire script, or you can run select lines from within a script by opening the script text file, highlighting the lines of interest, and sending those lines to the console using the “Run” button or the appropriate keyboard shortcut, ⌘-RETURN (Mac) or control-R (PC).\n\nsource(\"src/my_script.R\")\n\n## [1] 5.5\n\n# assuming the file was saved with the '.R' extension...\n\nIn an R script, you might use several lines of code to accomplish a single analysis, but if you want to be able to flexibly perform that analysis with different input, it is good practice to organize portions of your code within a script into user-defined functions. A function is a bit of code that performs a specific task. It may take arguments or not, and it may return nothing, a single value, or any R object (e.g., a vector or a list, which is another data structure will discuss later on). If care is taken to write functions that work under a wide range of circumstances, then they can be reused in many different places. Novel functions are the basis of the thousands of user-designed packages that are what make R so extensible and powerful.\n\nCHALLENGE:\nTry writing a function!\n\nOpen a new blank document in RStudio\n\nFile &gt; New &gt; R Script\n\nType in the code below to create the say_hi() function, which adds a name to a greeting:\n\n\n# this function takes one argument, `x`, appends the value of that argument to\n# a greeting, and then prints the whole greeting\nsay_hi &lt;- function(x) {\n    hi &lt;- paste(\"Greetings, \", x, \"!\", sep = \"\")\n    return(hi)\n}\n\n\nNOTE: Here, the paste() command allows string concatenation. Alternatively, we could use paste0() and omit the sep= argument.\n\nIn general, the format for a function is as follows: function_name &lt;- function(&lt;arguments&gt;) {&lt;function code&gt;}\nYou can send your new function to the R console by highlighting it in the editor and hitting ⌘-RETURN (Mac) or control-ENTER (PC). This loads the function as an object into the working environment.\n\n\n\n\n\n\n\n\n\n\nNow we can create some test data and call the function. What are the results?\n\n\nname1 &lt;- \"Rick Grimes\"\nname2 &lt;- \"Ruth Bader Ginsburg\"\nsay_hi(name1)\n\n## [1] \"Greetings, Rick Grimes!\"\n\nsay_hi(name2)\n\n## [1] \"Greetings, Ruth Bader Ginsburg!\"\n\n\nYou can also save the function in a file, e.g., in the src/ folder inside your working directory, and then source(\"&lt;path&gt;\") it in code. Save your function script as “say_hi.R” and then run the following:\n\nsource(\"src/say_hi.R\")\nname3 &lt;- \"Charles Darwin\"\nsay_hi(name3)\n\n## [1] \"Greetings, Charles Darwin!\"",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Fundamentals of the ***R*** Language</span>"
    ]
  },
  {
    "objectID": "04-module.html#quitting-r-and-saving-your-work",
    "href": "04-module.html#quitting-r-and-saving-your-work",
    "title": "4  Fundamentals of the R Language",
    "section": "4.4 Quitting R and Saving your Work",
    "text": "4.4 Quitting R and Saving your Work\nWorking in RStudio, you can save script files (which, again, are just plain text files) using standard dialog boxes.\nWhen you go to quit R (by using the q() function or by trying to close RStudio), you may be asked whether you want to…\n“Save workspace image to &lt;path&gt;/.Rdata?”, where &lt;path&gt; is the path to your working directory.\nSaying “Save” will store all of the contents of your workspace in a single hidden file, named “.Rdata”. The leading period (“.”) makes this invisible to most operating systems, unless you deliberately make it possible to see hidden files.\n\nNOTE: I tend to NOT save my workspace images. You can change the default behavior for this by editing RStudio’s preferences and choosing “Always”, “Never”, or “Ask”.\n\n\n\n\n\n\n\n\n\n\nThe next time you start R, the workspace from “.RData” will be loaded again automatically, provided you have not changed your working directory and you have not unchecked “Restore .RData into workspace at startup” in preferences.\nA second hidden file, “.Rhistory”, will also be stored in the same directory, which will contain a log of all commands you sent to the console, provided you have not unchecked “Always save history”.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Fundamentals of the ***R*** Language</span>"
    ]
  },
  {
    "objectID": "04-module.html#updating-r",
    "href": "04-module.html#updating-r",
    "title": "4  Fundamentals of the R Language",
    "section": "4.5 Updating R",
    "text": "4.5 Updating R\nR has been under continuous and active development since its inception in the late 1990s, and several updates are made available each year. These update help to fix bugs, improve speed and computational efficiency, and add new functionality to the software. The following information on how to update R is based on this post from Stack Overflow\n\nStep 1: Get the latest version of R\n\nGo to the R Project website.\nClick on CRAN in the sidebar on the left.\nChoose the CRAN Mirror site that you like.\nClick on Download R for… (choose your operating system).\nFollow the installation procedures for your system.\nRestart RStudio.\n\nStep 2: Relocate your packages\n\nTo ensure that your packages work with your shiny new version of R, you need to:\n\nMove the packages from your old R installation into the new one.\nOn MacOS, this typically means moving all library folders from “/Library/Frameworks/R.framework/Versions/3.5/Resources/library” to “/Library/Frameworks/R.framework/Versions/4.3/Resources/library”\n\n\n\n\nNOTE: You should replace “3.5” and “4.3” with whatever versions you are upgrading from and to, respectively.\n\n\nOn Windows, this typically means moving all library folders from “C:\\Program Files\\R\\R-3.5\\library” to “C:\\Program Files\\R\\R-4.3\\library” (if your packages are installed at the system level) or from “C:\\Users\\&lt;user name&gt;\\R\\win-library\\3.5\\” to “C:\\Users\\&lt;user name&gt;\\R\\win-library\\4.3\\” (if your packages are installed at the user level)\n\n\nNOTE: You only need to copy whatever packages are not already in the destination directory, i.e., you do not need to overwrite your new {base} package, etc., with your old one.\n\n\nIf those paths do not work for you, try using installed.packages() to find the proper path names. These may vary on your system, depending on where you installed R\nNow you can update your packages by typing update.packages() in your RStudio console, and answering “y” to all of the prompts.\nFinally, to reassure yourself that you have done everything correctly, type these two commands in the RStudio console to see what you’ve got in terms of what version of R you are running, the number of packages you have installed, and what packages you have loaded:\n\n\nversion\npackageStatus()\n(.packages())",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Fundamentals of the ***R*** Language</span>"
    ]
  },
  {
    "objectID": "04-module.html#concept-review",
    "href": "04-module.html#concept-review",
    "title": "4  Fundamentals of the R Language",
    "section": "Concept Review",
    "text": "Concept Review\n\nCharacteristics of R objects: class(), mode(), typeof(), str(), attributes(), dplyr::glimpse()\nUsing scripts: source()\n“.RData” and “.Rhistory” files\nUpdating R and packages",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Fundamentals of the ***R*** Language</span>"
    ]
  },
  {
    "objectID": "05-module.html",
    "href": "05-module.html",
    "title": "5  Basics of Version Control",
    "section": "",
    "text": "5.1 Objectives",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basics of Version Control</span>"
    ]
  },
  {
    "objectID": "05-module.html#objectives",
    "href": "05-module.html#objectives",
    "title": "5  Basics of Version Control",
    "section": "",
    "text": "To introduce the basics of working with version control systems in R, using git and GitHub",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basics of Version Control</span>"
    ]
  },
  {
    "objectID": "05-module.html#preliminaries",
    "href": "05-module.html#preliminaries",
    "title": "5  Basics of Version Control",
    "section": "5.2 Preliminaries",
    "text": "5.2 Preliminaries\n\nInstall this package in R: {usethis}",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basics of Version Control</span>"
    ]
  },
  {
    "objectID": "05-module.html#version-control-systems",
    "href": "05-module.html#version-control-systems",
    "title": "5  Basics of Version Control",
    "section": "5.3 Version Control Systems",
    "text": "5.3 Version Control Systems\nFor any of us who work with data files and associated analyses and documents over a long period of time, it can become very complicated to keep track of the “latest version” of what we’re working on. This is especially true if we are collaborating with others on a project and need to share these things back and forth. This is a problem that software developers have been dealing with for a long time, however, and there is a robust ecosystem of “version control systems” (VCSs) out there for dealing with this problem. The basic idea behind these systems is that all of the work on a particular project is stored in a repository (or repo), and as you work on and modify files and data for the project, you “commit” your changes periodically. The VCS keeps track of what changes between commits and allows you to roll back to previous versions if need be.\nYou can also branch a repository - basically, make a duplicate copy of it - and work on the new branch and then, later, merge your changes back into the main branch. Multiple people can each work on different branches simultaneously, and the software will take care of looking for and highlighting changes that occur on different branches so that they can be merged back in appropriately. This module will introduce you to one such system.\n\nNOTE: The source for some of the information covered below, along with a host of other valuable information about using R and git, is provided on the web book Happy Git and GitHub for the useR by Jenny Bryan (RStudio).",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basics of Version Control</span>"
    ]
  },
  {
    "objectID": "05-module.html#first-steps-with-git-and-github",
    "href": "05-module.html#first-steps-with-git-and-github",
    "title": "5  Basics of Version Control",
    "section": "5.4 First Steps with git and GitHub",
    "text": "5.4 First Steps with git and GitHub\n\nInstalling git\nOne of the most popular and frequently used VCSs is git.\n\nDownload and install git for your operating system. This will put the appropriate software on your machine.\n\n\n\n\n\n\n\n\n\n\n\nNOTE: To install on MacOS, binary installers may not be available for the newest version of git. Thus, you may need to first install the package manager software Homebrew and then run brew install git at the command line. Visit https://brew.sh/ for instructions on installing Homebrew.\n\nNote that git is simply a bit of software running on your own computer that watches the contents of particular user-specified folders.\nLet us now check if you have installed correctly…\n\nIn RStudio, go to the Tools menu and select Terminal &gt; New Terminal or, if it is visible, click on the Terminal tab in the lower left pane of RStudio interface. In the terminal window, type, which git. On MacOS you should see something like: /usr/bin/git or /usr/local/bin/git, which is the path to your git executable file. On Windows, you should see something like: /cmd/git\n\n\n(base) ad26693 🐵  $ which git\n/usr/bin/git\n\n\nNOTE: Your terminal prompt may look different than how I have set mine up. My prompt ends with $, which is typical for the bash shell on MacOS. You may see, for example, a prompt that ends in %, which is typical for the zsh shell.\n\n\nIn the same terminal, type git --version to see which version of git you are running. You should see something like git version 2.39.5.\n\n\n(base) ad26693 🐵  $ git --version\ngit version 2.39.5 (Apple Git-154)\n\nRunning the git --version command or the command git config on MacOS may also prompt you to install a set of developer command line tools… this is okay (and good!) to accept. Doing so will install the Xcode Command Line Tools. You can also install these using the command xcode-select --install at the terminal prompt.\n\nNOTE: See also Chapter 6 of the web book, Happy Git and GitHub for the useR\n\n\n\nRegistering a GitHub Account\nThe git software we just installed is strictly a command-line tool and is a bit difficult to pick up from scratch. However, the remote repository hosting service GitHub provides an easy to use web-based graphical interface for repository management and version control. GitHub offers the distributed version control and source code management functionality of git plus some additional features. We will get introduced to the main features of git by using GitHub.\n\nGo to GitHub.com, choose Sign up for GitHub and create your own account. You will be asked to provide an username and email address and to select a password.\n\n\n\n\n\n\n\n\n\n\n\n\nTelling git Who You Are\nThe git VCS keeps track of who makes changes to any files in a watched repository. We thus need to tell git (on your local computer) who you are so that when you make changes to a repository, either locally or remotely, they are associated with a particular user name and email. To do this within RStudio, again select Tools &gt; Terminal &gt; New Terminal (which opens a new terminal window in the bottom left pane of the IDE) or Tools &gt; Shell… (which will open up an external terminal window). Then, type in the following replacing  and  with the name and the email addressed associated with the GitHub account you just set up.\n\ngit config --global user.name \"&lt;your name&gt;\"\ngit config --global user.email \"&lt;your email address&gt;\"\n\n\nNOTE: Your name and email address do not need to be in quotes unless there is a space in one of them.\n\nAlternatively, you can enter these commands directly in a terminal window that you open yourself, rather than one opened from within RStudio (e.g., by opening Applications/Utilities/Terminal.app on MacOS).\n\nNOTE: Under this setup, git is now set to link any commits to the username associated with the email address used to tag your commits, even if you enter a different user name in the --global options here. If you use an email address that is not already associated with a GitHub account, then the username entered here in the local config will appear associated with your commits.\n\nThe command…\n\ngit config --global --list\n\n… can be used to check if you set things up correctly.\nAlternatively, you can set things up from within R, rather than using a terminal, as follows:\n\n# uncomment the following line, if needed, to install the {usethis} package\n# install.packages('usethis')\n\nlibrary(usethis)\nuse_git_config(user.name = \"your name\", user.email = \"your email address\")\ndetach(package:usethis)\n\n\nNOTE: See also Chapter 7 of the web book Happy Git and GitHub for the useR\n\n\n\nAuthenticating Your GitHub Account\nBefore you can access certain resources and functionality on GitHub, you will need to “authenticate” your access to GitHub from each local machine that you might work on. Basically, this process involves providing or confirming your access credentials that prove you are who you say you are when accessing GitHub. Access credentials can include a user name and password (often coupled with 2-factor authentication), a “personal access token”, and/or an SSH key. Depending on whether we are connecting to GitHub via HTTPS (“hypertext transfer protocol secure”) or SSH (“secure shell”), we will need to use different methods of authentication (see further details below).\nTo connect to GitHub via HTTPS (which is the way I typically recommend), we will need to authenticate using a “personal access token”, or PAT. When we then try to connect to GitHub from git and are prompted for a password, we would enter this personal access token (PAT) instead. (Password-based authentication for connecting to GitHub was removed a few years back, and using a PAT is more secure.)\nTo create a personal access token, follow the steps below:\n\nStep 1:\nMake sure your email address is verified in GitHub. You probably will have done this already if you have set up a GitHub account.\n\nIn the upper-right corner of any page on GitHub, click your profile photo, then click “Settings”.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the left sidebar, under the “Access” section, click “Emails”.\nUnder your email address, click “Resend verification email”.\n\nGitHub will then send you an email with a link in it. After clicking that link, you will be taken to your GitHub dashboard and see a confirmation banner.\n\nNOTE: If you have already verified your email, the “Resend verification email” option may not appear.\n\n\n\nStep 2:\nGenerate a new PAT.\n\nAgain, in the upper-right corner of any page on GitHub, click your profile photo, then click “Settings”\nIn the left sidebar of your profile page, scroll to the bottom and click “Developer settings”\n\n\n\n\n\n\n\n\n\n\n\nIn the left sidebar, expand the section on “Personal access tokens”.\n\n\n\n\n\n\n\n\n\n\n\nThere are two types of tokens you can create, “fine-grained” and “classic”, either of which is fine to create. The former offers the opportunity to finely tune control of what actions on GitHub you would be able to access through R, but the latter is easier to set up. Choose “Generate New Token” and select “classic”…\n\n\n\n\n\n\n\n\n\n\n\nGive your token a descriptive name in the “Note” field.\nSet an expiration date for the token.\nSelect scope of permissions you would like to grant this token. To use your token to access repositories from the command line and perform other actions with repositories, select the “repo” checkbox plus any other actions you would like to be able to control.\n\n\n\n\n\n\n\n\n\n\n\nScroll to the bottom of the page and click the green “Generate token” button.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe token will be a string of 40 random letters and digits. Once it has been created, copy and store it in a safe place, like a password manager. You should enter the token instead of your git password when performing operations over HTTPS. Personal access tokens can only be used for HTTPS git operations. If you instead want to use SSH, you will have to set that up using an RSA key (see details below for how to do this within RStudio).\n\n\n\nCaching Your GitHub Credentials\nIf you are not prompted for your username and password, your GitHub credentials may already be cached on your computer. If needed, you can update your credentials in your computer’s keychain to replace your old password with the token.\nIf you are running MacOS, there are several ways you can explicitly cache your credentials if you find that you are asked repeatedly for them when trying to connect to GitHub. Both require that the Homebrew package manager is installed. Visit https://brew.sh/ for instructions on installing Homebrew.\n\nUsing the GitHub CLI (Command Line Interface)\nThe GitHub CLI will automatically store your git credentials for you when you choose HTTPS as your preferred protocol for git operations and answer “yes” to the prompt asking if you would like to authenticate to git with your GitHub credentials.\n\nInstall the GitHub CLI by running brew install gh at a command prompt in a terminal window.\nIn the command line, enter gh auth login, then follow the prompts.\nWhen prompted for your preferred protocol for git operations, select HTTPS\nWhen asked if you would like to authenticate to git with your GitHub credentials, enter “Y”\n\n\n\nUsing the git Credential Manager (GCM)\ngit Credential Manager (GCM) is another way to store your credentials securely and connect to GitHub over HTTPS. With GCM, you do not have to manually create and store a personal access token, as GCM manages authentication on your behalf, including 2FA (two-factor authentication).\n\nInstall GCM by running the following commands at a terminal command prompt:\n\n\nbrew tap microsoft/git\nbrew install --cask git-credential-manager-core\n\nThe next time you clone an HTTPS URL that requires authentication, e.g., for a private repository, git will prompt you to log in using a browser window. You may be asked to authorize an “OAuth app”. If your account or organization requires two-factor authorization, you will also need to complete the 2FA challenge.\nOnce you have authenticated successfully, your credentials will be stored in your macOS keychain and will be used every time you clone an HTTPS URL. git will not require you to type your credentials in the command line again unless you change your credentials.\nIf you are running Windows, GCM is included with the installer for Git for Windows. During installation you will be asked to select a credential helper, with GCM being set as the default.\nThe information above was pulled from the authentication and account security sections of GitHub’s documentation. See those links for more details and recommendations for troubleshooting any problems.\n\n\n\n5.4.1 Creating and Registering GitHub Credentials from RStudio\nUse can also use the {usethis} and {gitcreds} packages to generate new PATs and to register your credentials with git from within RStudio.\n\nCreate a GitHub PAT via R by running: usethis::create_github_token(). This command will access a page in your browser for setting up a new (classic) token, pre-populated with appropriate scopes for working in R\nCopy and store the token in a safe place, such as a password manager\nUse the command gitcreds::gitcreds_set() to register your token with git.\nUse the command usethis::git_sitrep() to get the rundown on your current git credentials.\n\nTroubleshooting: If you are having problems, you can try the following two steps, after generating and saving your PAT:\n\nRun credentials::git_credential_forget() to clear the credentials cache\nRun credentials::set_github_pat() and follow the prompts, which may involving pasting your saved token into one of the prompts.\n\n\n\nCreating a Remote Repository\nNow, we will begin demonstrating how a VCS works by first setting up and working with a remote repository (colloquially, a “repo”) hosted on GitHub. You can do this either by following the instructions laid out in the “Hello World” GitHub guide or as below:\n\nSign in to GitHub in a web browser, navigate to the “Repositories” tab on your user dashboard, and press the green “NEW” button.\n\n\n\n\n\n\n\n\n\n\n\nEnter a name for your repository (e.g., “test-repo”). By default, the repository will be designated as a “public” one. You can create “private” repositories, too, but this typically requires paying a hosting fee.\n\n\nNOTE: Do not include spaces in your repository name!\n\n\nIt is also a good idea to click the “Add a README file” checkbox under “Initialize this repository with:” so that your repository includes at least one file in it. A README file serves as a nice, introductory landing page for your repo. By creating at least one file in your repo, you will also set a default branch, named “main”, for\nWhen you are done, click the large green “Create repository” button.\n\n\nNOTE: It is also good to click the checkbox for “Add .gitignore” and select “.gitignore template R”. This will create a hidden file in which you can specify files within your repo that you DO NOT want or need to keep under version control.\n\n\n\n\n\n\n\n\n\n\nCongratulations! You’ve just set up your first GitHub hosted repository!\n\n\nChanges and Commits\n\nAs a first step to working with version control, let’s make some edits to the remote “README” file in your repository using the browser-based Markdown editor included in GitHub. Clicking the pencil icon at the top right of the “README” file will bring up the editor…\n\n\n\n\n\n\n\n\n\n\n\nYou can then type in edits, using Markdown styling.\n\n\n\n\n\n\n\n\n\n\n\nNOTE: Markdown is essentially a set of rules for how to easily style plain text files in such a way that can be easily converted and rendered in HTML, the structural language of the web. This module, and this entire website, for example, were written in RStudio using a particular version of Markdown called “Quarto Markdown”. GitHub has a nice, short tutorial that you can follow about “Mastering Markdown”. There is also a nice GitHub guide on “Documenting your Projects on GitHub” that provides a useful overview of the benefits of good documentation.\n\n\nWhen you are done editing, commit the changes to your remote “README” file by scrolling to the bottom of the editor window and clicking the “Commit Changes” button. Note that every commit you make to a repository must have some sort of brief, descriptive message associated with it. Here, GitHub has populated the message field with default text (“Update README.md”), but you can change this and, optionally, add a fuller message as well.\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with Branches\nSo far, all of the changes you have made have been to the main branch of your repository, but one of the powerful features of git is that you can create different versions or branches of your repository to try out new things and then merge these back into the main branch. This feature - along with automated checking for merge conflicts created when different modifications have been made on two different branches - is what makes git and other VCSs so valuable for software development.\nHere, we are going to create a new branch of our repository, edit a file in that new branch, and then merge the changes back into the main branch.\n\nCreate a new branch of your repository (e.g., “readme-edits”) by opening the “branch” popdown menu, typing a new branch name, and then clicking on the “Create branch:” text. This will switch you over to the new branch.\n\n\n\n\n\n\n\n\n\n\n\nUsing the same procedure as above, make some edits to the “README” file on your new branch and then commit those changes. E.g., add a new section to your Markdown file…\n\nTo merge changes from the new branch back into the main branch, we now need to [1] compare changes between branches, [2] create what is called a “pull request”, and [3] pull changes from the new branch back into the main branch. Along the way, if there are any differences between the two branches that cannot be merged without “conflicts” (e.g., cases where the same section of a particular file has been modified in both branches), then these will be highlighted and a merge prevented until the conflict is resolved.\n\nStep 1\n\nReturn to the main branch by selecting the link for the repo and then confirm that you are on the main branch by selecting it from the branches popdown menu.\nThen select “Compare & pull request” to initiate a new pull request.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 2\n\nSelect the branches to compare. Choose main as the “base” branch (the branch we are comparing to and merging into) and readme-edits as the “compare” or “head” branch (the branch we are currently working in, where our most recent changes have been made).\n\n\nNOTE: You should notice that when we shifted over to the readme-edits branch, GitHub switched the “head” branch from main to readme-edits.\n\n\n\n\n\n\n\n\n\n\nAfter making these selections, scroll to the bottom of the window… there you should see you a diffs (for “differences”) section that summarizes the differences between the main (base) and compare (head) versions.\n\n\n\n\n\n\n\n\n\n\n\nStep 3\n\nInitialize the pull request by scrolling back up and pressing the green “Create pull request” button.\n\n\n\n\n\n\n\n\n\n\n\n\nStep 4\n\nSet up and then confirm the merge by pressing the green “Merge pull request” and then “Confirm merge” buttons.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 5\n\nOnce the merge has been completed, you can delete the new branch.\n\n\n\n\n\n\n\n\n\n\n\nNOTE: The “Hello World” GitHub guide also covers basics of working with branches and merging.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basics of Version Control</span>"
    ]
  },
  {
    "objectID": "05-module.html#connecting-git-and-github",
    "href": "05-module.html#connecting-git-and-github",
    "title": "5  Basics of Version Control",
    "section": "5.5 Connecting git and GitHub",
    "text": "5.5 Connecting git and GitHub\nNow that we some familiarity with working with a remotely hosted repository, our next step in developing a git/GitHub workflow is to make a clone of that repository on one or more local computer(s). We can then work on and commit changes locally via git and then, when we are ready, we can “push” those changes up to the remotely hosted repository on GitHub.\nSimilarly, once a local repository under version control has been created and connected to a remotely hosted version, we can also make changes remotely and then “pull” those changes down to our local repository to keep them in sync.\nIt is very easy to do this process through the RStudio IDE, as we will see below, but we are going to first do it through the command line to show you all of the steps that RStudio facilitates.\n\nCloning a Remote Repository from the Command Line\nThere are two ways to connect between remote and local git repositories, via either HTTPS (which stands for “hypertext transfer protocol”) or SSH (which stands for “secure shell”). Our first steps will use the first of these approaches and will create a local “cloned” copy of a remote repository we have hosted on GitHub.\n\nOpen a terminal shell. You can either do this directly from your operating system or you can access a shell from within RStudio by choosing Tools &gt; Terminal &gt; New Terminal or Tools &gt; Shell…. Again, the former command opens a new terminal within the RStudio IDE, while the latter opens an external terminal window.\nUsing the UNIX and DOS shell command cd (for “change directories”), navigate into the folder that you want your cloned repository to be created in. I typically create new repos in a dedicated folder called Repos/ that I use for development work, which I keep in my main user folder on MacOS (e.g., “~/Development/Repos”).\n\n\n(base) ad26693 🐵  $ cd ~/Development/Repos\n\n\nNOTE: Here, I am using the shortcut ~ operator to access my user home directory. You can, instead, to use the full path to your home directory, e.g., “/Users/ad26693/Desktop/Repos”.\n\n\nNow open a web browser, go to the landing page for your repository on GitHub.com, click the green button that says “Code”, then select the “HTTPS” tab and copy the web URL that shows up in the text box by clicking on the tiny clipboard icon.\n\n\n\n\n\n\n\n\n\n\nThe web URL should include your GitHub username and the name of your the repository you are cloning and look something like…\nhttps://github.com/&lt;your user name&gt;/&lt;your repository name&gt;.git\ne.g., https://github.com/difiore/test-repo.git\n\nReturn to the terminal window in RStudio or to the external shell prompt and type: git clone followed by the copied URL. For example…\n\n\n(base) ad26693 🐵  $ git clone https://github.com/difiore/test-repo.git\n\n\nNOTE: Here, you may be asked for your GitHub username and password… if so, enter you “personal access token” instead of the password.\n\nThe output should look something like:\n\nCloning into 'test-repo'...\nremote: Enumerating objects: 11, done.\nremote: Counting objects: 100% (11/11), done.\nremote: Compressing objects: 100% (10/10), done.\nremote: Total 11 (delta 1), reused 0 (delta 0), pack-reused 0\nReceiving objects: 100% (11/11), done.\nResolving deltas: 100% (1/1), done.\n\nIf you then navigate into the newly created local directory using the cd command, you can list and look at the cloned files. To do so, at the shell prompt type…\n\ncd test-repo (to change into the correct directory)\nls -a (to list all the files in the directory)\nhead README.md (shows the first few lines of the file “README.md”)\ngit remote show origin (shows information about the remote repository, including the branch that “push” and “fetch” commands will be applied to and which branch is being tracked locally)\n\nThe output should look something like…\n\n(base) ad26693 🐵  $ cd test-repo/\n(base) ad26693 🐵  $ ls -a\n.     ..    .git    .gitignore    README.md\n(base) ad26693 🐵  $ head README.md\n# test-repo\n\nI am making some edits to this README file using Markdown!\n\n## This is a level 2 heading\n\nI can use simple text formating to make **bold** or *italicized* text!\n\n## Added this new section on a branch\n(base) ad26693 🐵  $ git remote show origin\n* remote origin\n  Fetch URL: https://github.com/difiore/test-repo.git\n  Push  URL: https://github.com/difiore/test-repo.git\n  HEAD branch: main\n  Remote branches:\n    main         tracked\n    readme-edits tracked\n  Local branch configured for 'git pull':\n    main merges with remote main\n  Local ref configured for 'git push':\n    main pushes to main (up to date)\n\n\n\nMaking and Pushing Local Changes from the Command Line\nNow, edit one of the local files in your repo. You can do this by opening the “README.md” file in any text editor (e.g., use the shell command open README.md, make some changes, and save) or by using the shell commands echo and &gt;&gt; (which redirects output to a file instead of the command prompt) to add text to the end of the file (e.g., echo \"Here is some new text I am adding from the shell to update the README file.\" &gt;&gt; README.md).\nYou can also do this by navigating to the “README.md” file through the normal Windows or MacOS file manager, opening it up, and editing it with any plain text text editor like Visual Studio Code or BBEdit.\nIf you now type git status at the shell prompt, you will see a message that the “README.md” file has been changed:\n\n(base) ad26693 🐵  $ git status\nOn branch main\nYour branch is up to date with 'origin/main'.\n\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n  modified:   README.md\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n\nTo propagate our changes back up to the remote repository, we now need to do three things: [1] First, we need to “stage” or “add” the files to a queue of local changes. [2] We then need to “commit” those changes to our local repository so that git acknowledges that changes have been made and approved. [3] Finally, we then need to “push” the committed changes up across the internet to the remote repository hosted on GitHub. These are done with the git commands add, commit, and push in the shell as follows:\n\n(base) ad26693 🐵  $ git add -A\n(base) ad26693 🐵  $ git commit -m \"A commit from my local repo\"\n[main e724f84] A commit from my local repo\n 1 file changed, 1 insertion(+)\n \n(base) ad26693 🐵  $ git push\nEnumerating objects: 5, done.\nCounting objects: 100% (5/5), done.\nDelta compression using up to 4 threads\nCompressing objects: 100% (3/3), done.\nWriting objects: 100% (3/3), 387 bytes | 387.00 KiB/s, done.\nTotal 3 (delta 1), reused 0 (delta 0), pack-reused 0\nremote: Resolving deltas: 100% (1/1), completed with 1 local object.\nTo https://github.com/difiore/test-repo.git\n   280fb0a..e724f84  main -&gt; main\n\n\nNOTE: The -A argument following the git add command means to stage all files in the repo that have changed since the last commit (in this case, we have only one, “README.md”). The -m argument following git commit indicates the message we want to include with our commit. It is necessary to include SOME message with each commit, and it is good practice to include a short description of what the commit includes, e.g., “Updating README file”.\n\nTo confirm that your edits have in fact been pushed up successfully, return to the landing page for your repository on GitHub in your web browser and hit refresh… you should see that the “README.md” file has been updated with new text!\n\n\n\n\n\n\n\n\n\nAnd if you click on the “commits” history on the right hand site of message block above the updated README file, you should see one with the message “A commit from my local repo”.\n\n\n\n\n\n\n\n\n\n\nNOTE: See also Chapters 9 to 12 of the web book, Happy Git and GitHub for the useR\n\n\n\nPulling Changes from GitHub from the Command Line\nSimilarly, we can make edits to files in a remote repository on GitHub using the service’s web-based Markdown editor and then then “pull” those down to our local repository.\n\nIn your web browser, navigate to the page for your repository (e.g., “test-repo”) and again click on the pencil icon at the top right of the “README” file in a remote repository to bring up the editor.\n\n\n\n\n\n\n\n\n\n\n\nAdd some new text to the file and then scroll to the bottom of the page and “commit” your changes - remember that you need to enter a commit message in the text box (or accept the default “Update README.md” that is auto-filled).\nReturn to the shell prompt inside the local directory for your repository and enter the command git pull. You should see something like the following, indicating that the main branch of the local repository has been “fast-forwarded” to the state of the remote main branch that is being tracked:\n\n\n(base) ad26693 🐵  $ git pull\nremote: Enumerating objects: 5, done.\nremote: Counting objects: 100% (5/5), done.\nremote: Compressing objects: 100% (3/3), done.\nremote: Total 3 (delta 1), reused 0 (delta 0), pack-reused 0\nUnpacking objects: 100% (3/3), 730 bytes | 182.00 KiB/s, done.\nFrom https://github.com/difiore/test-repo\n   e724f84..8734542  main       -&gt; origin/main\nUpdating e724f84..8734542\nFast-forward\n README.md | 2 ++\n 1 file changed, 2 insertions(+)\n\nIf you run the command git pull and the local repo is in sync with the version hosted on GitHub, you should see something like the following:\n\n(base) ad26693 🐵  $ git pull\nAlready up to date.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basics of Version Control</span>"
    ]
  },
  {
    "objectID": "05-module.html#additional-useful-information",
    "href": "05-module.html#additional-useful-information",
    "title": "5  Basics of Version Control",
    "section": "5.6 Additional Useful Information",
    "text": "5.6 Additional Useful Information\n\nUsing the Shell within RStudio\nRStudio provides an interface to the most common version control operations including managing changelists, “diffing” files, committing, and viewing history. While these features cover basic everyday use of git, as we have seen above, we may also occasionally need to use the command line in either a system shell to access all of the underlying functionality of git.\nRStudio includes functionality to make it very straightforward to use various system shells with projects under version control. This includes:\n\nYou can use the Tools &gt; Terminal &gt; New Terminal and Tools &gt; Shell… commands to open a new terminal within RStudio or a new system shell window with the working directory already initialized to your project’s root directory.\nWhen using git on Windows, these command by default should open the Git Bash shell, which is a port of the Unix bash shell to Windows that has been specially configured for use with a version of git called MSYS Git. Note that you can disable this behavior and use the standard Windows command prompt instead choosing using Tools &gt; Global Options and selecting an alternative terminal in the Terminal section).\n\n\n\nUsing SSH instead of HTTPS\nRemote repositories under version control can be accessed using a variety of internet file transfer protocols, including HTTPS (the protocol used above) and SSH (“secure shell”), a different file transfer protocol that does not require sending a user name and email address for authentication with every information transfer request. Typically, the authentication for an SSH connection is done using what are know as public/private RSA key pairs. This type of authentication requires two steps:\n\nGenerating a public/private key pair\nProviding the public key to the hosting provider (e.g., GitHub or another service)\n\nWhile Linux and Mac OSX both include SSH as part of the base system, Windows does not. As a result the standard Windows distribution of git (MSYS Git, referenced above) also includes an SSH client.\nIf you are interested in using SSH for connecting with GitHub, you can do the following within RStudio:\n\nStep 1\n\nCreate an RSA Key\n\nIn the “Git/SVN” tab of the Tools &gt; Global Options dialog box, press the button Create RSA key, which will create a new code that you will need for using SSH to send and receive data from remote servers as an alternative to the somewhat less secure HTTPS protocol we have already seen.\n\n\n\n\n\n\n\n\n\n\n\nStep 2\n\nView and Copy the RSA Key\n\nFrom the “Git/SVN” tab of the Tools &gt; Global Options dialog box, press the text link View public key, copy the displayed key, and close out of the dialog box.\n\n\n\n\n\n\n\n\n\n\n\nStep 3\n\nSet Up your GitHub Account for SSH\n\nGo to your GitHub account online, open your profile/account settings, and then select the “SSH & GPG keys” tab.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick “New SSH key”, fill out a title for the new key (e.g., “Connect to GitHub from RStudio”), paste in the public key that you copied from RStudio (see above), and then click “Add SSH key” at the bottom of the window.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou should now be set up to use SSH in lieu of HTTPS URLs for cloning repositories and for pushing to/pulling from remote repositories on GitHub. These URLs take the form of git@github.com:&lt;your user name&gt;/&lt;your repository name&gt;.git\n\n\nStep 4\n\nClone the remote repository\n\nThe process is essentially the same as we used above for cloning via HTTPS.\n\nOpen a web browser, go to your repository on GitHub.com, click the green button that says “Clone or download” (but now choose “Clone with SSH”), and copy the URL that shows up in the text box.\n\n\n\n\n\n\n\n\n\n\n\nGo to a shell prompt, cd to the directory that you want the repository to be downloaded into, and type: git clone followed by the copied SSH URL, e.g., git clone git@github.com:&lt;your user name&gt;/&lt;your repository name&gt;.git\n\nThe output should look something like:\n\n(base) ad26693 🐵  $ git clone git@github.com:difiore/test-repo.git\nCloning into 'test-repo'...\nThe authenticity of host 'github.com (140.82.113.3)' can't be established.\nED25519 key fingerprint is SHA256:+DiY3wvvV6TuJJhbpZisF/zLDA0zPMSvHdkr4UvCOqU.\nThis key is not known by any other names\nAre you sure you want to continue connecting (yes/no/[fingerprint])?\n\nHere, you can answer yes and hit .\n\nWarning: Permanently added 'github.com' (ED25519) to the list of known hosts.\nremote: Enumerating objects: 17, done.\nremote: Counting objects: 100% (17/17), done.\nremote: Compressing objects: 100% (15/15), done.\nremote: Total 17 (delta 3), reused 3 (delta 1), pack-reused 0\nReceiving objects: 100% (17/17), done.\nResolving deltas: 100% (3/3), done.\n\n\n\n\nSwitching Transfer Protocols\nWe can check what protocol you are using to connect to a remote repository by cding into the repository and then typing git remote -v. You should then see something like:\n\n(base) ad26693 🐵  $ git remote -v\norigin  git@github.com:difiore/test-repo.git (fetch)\norigin  git@github.com:difiore/test-repo.git (push)\n\nTo change the protocol we use, we simply point git to the URL associated with the desired transfer protocol. The following commands switch us from using SSH to HTTPS, then back.\n\n(base) ad26693 🐵  $ git remote set-url origin https://github.com/difiore/test-repo.git\n(base) ad26693 🐵  $ git remote -v\norigin  https://github.com/difiore/test-repo.git (fetch)\norigin  https://github.com/difiore/test-repo.git (push)\n(base) ad26693 🐵  $ git remote set-url origin git@github.com:difiore/test-repo.git\n(base) ad26693 🐵  $ git remote -v\norigin  git@github.com:difiore/test-repo.git (fetch)\norigin  git@github.com:difiore/test-repo.git (push)\n\n\n\nUsing git with Windows\nSometimes, getting git set up and working properly on Windows machines can be a bit tricky, but I have included information below that might be helpful if you experience difficulties. These notes and procedures were useful for me to install git on a Windows 10 PC and to connect it successfully with RStudio.\n\nSome Notes\n[1] When you install git and you are going through the installation dialog boxes, you can keep the defaults on all of them - just be sure to confirm that…\n\nIn the box about “Adjusting your PATH environment” you check the radio button to say “Git from the command line and also from 3rd party software”\n\n\n\n\n\n\n\n\n\n\n\nIn the box about “Configuring the terminal editor to use with Git Bash” you check the radio box that says “Use MinTTY (the default terminal of MSYS2)”\n\n\n\n\n\n\n\n\n\n\n\nNOTE: As part of its installation on a Windows PC, git will also install another piece of software, Git Bash, which is basically on alternative “shell” program you can use to access your computer’s OS directly. It is analogous (but with somewhat different functionality) to the Windows COMMAND PROMPT and the Windows POWER SHELL.\n\n[2] If you poke around the directory system on your Windows machine, you will find that when you install git it appears to put four, slightly different git.exe files on your computer. Here is a summary of what I think they are and how I think they function, based on searches on StackOverflow\nFile 1. C:\\Program Files\\Git\\bin\\git.exe\nWhen you open RStudio after installing git and then go to Tools &gt; Global Options and select the Git/SVN section, you should see the path above appear in the text box for “Git executable”.\nI am not entirely sure that this is the case, but I THINK this git.exe file simply links an executable file that is actually stored in a different place (see File 2. below), but this is where many programs will expect to find the git executable by default.\nRStudio should find and fill in this path by itself, but if it does not, then click the “Browse” button next to the text box and browse to select this file: C:\\Program Files\\Git\\bin\\git.exe.\nFile 2. C:\\Program Files\\Git\\cmd\\git.exe\nThis is the executable file that is accessible via a PATH environmental variable that should get added to your Windows environment automatically when you installed the git software with the defaults noted above.\nThus, after installing git and restarting your computer, when you then choose Tools &gt; Shell… or Tools &gt; Terminal &gt; New Terminal from RStudio and type which git in the shell that opens up, it should be THIS executable which is called and returned. That is, if you access the shell or terminal from RStudio and type which git you should get /cmd/git.\n\n\n\n\n\n\n\n\n\nFile 3. C:\\Program Files\\Git\\mingw64\\bin\\git.exe\nThis version of the git executable is what opens if you directly open the Git Bash shell, rather than opening it from within RStudio.\nAfter installing git, you can access Git Bash from the Windows start menu (i.e., the menu that pops up from clicking the Windows icon at the bottom left of your Windows desktop). If you select Git &gt; Git Bash from the start menu, a shell window will open up, and if you then type which git there, it will return /mingw64/bin. Honestly, I have no idea why accessing the Git Bash shell this way rather than through RStudio runs a different instance of git!\n\n\n\n\n\n\n\n\n\nFile 4. C:\\Program Files\\Git\\mingw64\\libexec\\git-core\\git.exe\nThis is another version of the executable that is used by the Git Bash shell. I think it probably just points to the one listed in File 3.\nThe fact that multiple versions of slightly different git executables are installed in different places means that we need to be careful that the right one is used when we try to access git from RStudio. Below, I have tried to distill what we need to know to get things to work for Windows users.\nThus, with the above in mind, if you are having issues with your git installation for Windows, try the following…\n\n\nPreliminaries\nConfirm that your “HOME”” environmental variable is set to the root of your user folder. To do this, open the Environmental Variables Control Panel, look in the top panel (“User variables for USERNAME”) and confirm that there is a “HOME” variable and it has the path to your user folder. If not, create a new variable called “HOME” and enter the path or modify the path.\n\nNOTE: When I set up my Windows machine, there was no “HOME” variable yet specified… I had to create one. Setting this is important because the git config --global commands you will run below set up and then look for an invisible file, .gitconfig, at the root of your home folder.\n\n\n\nStep 1\nDownload git for Windows from the git website.\nInstall it using the default settings in the installation dialog boxes. This should add C:\\Program Files\\Git\\cmd to your Windows PATH (which contains a list of directories that is loaded into your Windows environment when you either start up Windows or log in, which tells Windows where to search for any installed executable software it is asked to run).\n\n\nStep 2\nLog out of Windows and log in again or completely restart Windows. This is needed to have the PATH environmental variable updated.\n\n\nStep 3\nStart up RStudio and check that the path to the git executable in Tools &gt; Global Options &gt; Git/SVN is set to C:\\Program Files\\Git\\bin\\git.exe I think it should be set automatically, but if not, click the “Browse” button next to the text box and find this path and then restart RStudio.\n\n\n\n\n\n\n\n\n\n\nNOTE: The git icon and tab will likely NOT YET APPEAR in your RStudio IDE.\n\n\n\nStep 4\nIn Tools &gt; Global Options &gt; Terminal, make sure that new terminals are set to open with Git Bash rather than some other shell. You can set this in the pop down menu “New terminals open with…” There seemed to be an issue that some folks had where new terminal windows were opening at the COMMAND PROMPT, which is not what we want but is easily corrected.\n\n\n\n\n\n\n\n\n\n\n\nStep 5\nChoose Tools &gt; Shell… or Tools &gt; Terminal &gt; New Terminal and confirm that you are opening in the Git Bash shell. This should be the case if you have done step 4 above. The command prompt will read something like…\n\nIn green: YOUR.USERNAME@YOUR.COMPUTER NAME\nIn purple: MSYS (the default name for the MinTTY Git Bash shell)\nIn orange: The name of the directory/repository you are currently in (i.e., your current working directory)\nThe typical bash command prompt: $\n\n\n\n\n\n\n\n\n\n\n\nNOTE: If your current directory is under version control (e.g., if you are opening a shell for a project that you are already managing with git) then the name of the branch you are on will be listed in in blue parentheses.\n\n\n\n\n\n\n\n\n\n\n\n\nStep 6\nConfirm that git is available by typing which git. It should return /cmd/git\n\n\n\n\n\n\n\n\n\n\n\nStep 7\nSet your git identity as instructed above by typing the following at the command prompt:\n\ngit config --global user.name \"&lt;your name&gt;\"\ngit config --global user.email \"&lt;your email address&gt;\"\n\n\nNOTE: Your name and email address do not need to be in quotes unless there is a space in one of them.\n\n\n\nStep 8\nCheck your configuration by typing the following at the command prompt:\n\ngit config --list\n\nYou should get a list of settings that includes at least two settings, user.name and user.email, that match what you just entered.\n\n\n\n\n\n\n\n\n\n\n\nStep 9\nReturn to RStudio and start a new project (File &gt; New Project) from a “New Directory” (e.g., “test-repo”).\nCheck the box indicating that you want to “Create a git repository”, RStudio should close and reopen and the git icon and git tab should now appear in the IDE. Also, if you now open a project that is already under version control (e.g., that you clone from GitHub), then these should also appear.\n\n\n\n\n\n\n\n\n\nVoíla!!! You should now be ready to go! Try creating a new file in your project and then committing it, as described above.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basics of Version Control</span>"
    ]
  },
  {
    "objectID": "05-module.html#concept-review",
    "href": "05-module.html#concept-review",
    "title": "5  Basics of Version Control",
    "section": "Concept Review",
    "text": "Concept Review\n\nSetting up git\nSetting up GitHub\nWorking in the Terminal/Shell\nCloning a remote repository\ngit basics\n\nStaging and committing changes to a repository\nPushing to and pulling from a remote repository",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basics of Version Control</span>"
    ]
  },
  {
    "objectID": "06-module.html",
    "href": "06-module.html",
    "title": "6  Reproducible Research",
    "section": "",
    "text": "6.1 Objectives",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducible Research</span>"
    ]
  },
  {
    "objectID": "06-module.html#objectives",
    "href": "06-module.html#objectives",
    "title": "6  Reproducible Research",
    "section": "",
    "text": "In the last module, we introduced the concept of version control and looked at tools for interfacing between between a repository maintained remotely on GitHub and a local repository, using the version control system, git. Now, we are going to learn how we can use RStudio to manage the git/GitHub version control workflow. An additional objective of this module is to promote the ideas of reproducible research practice and literate programming by introducing you to Quarto and RMarkdown, which are both plain-text document formats that allow us to mix text and code in a flexible way and to generate a variety of nicely rendered outputs (e.g., websites, books, PDFs, presentations, and other media formats).",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducible Research</span>"
    ]
  },
  {
    "objectID": "06-module.html#preliminaries",
    "href": "06-module.html#preliminaries",
    "title": "6  Reproducible Research",
    "section": "6.2 Preliminaries",
    "text": "6.2 Preliminaries\nAs a first step, if you haven’t done so already in Module 02, open the Preferences pane in RStudio (MacOS), go to the Git/SVN section, and make sure that the checkbox “Enable version control interface for RStudio projects” is selected. Alternatively, you can access the same dialog box by choosing Global Options from the Tools menu (MacOS and PC).\n\n\n\n\n\n\n\n\n\nIn this dialog box, also confirm that the path to your git executable is filled in and correct. If you have already successfully installed git, this should be filled with something like “/usr/bin/git” (on MacOS or Linux) or “C:\\Program Files\\Git\\bin\\git.exe” on Windows). If it is not, then you can try troubleshooting by following the recommendations in Chapters 13 and 14 of the web book Happy Git and GitHub for the useR.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducible Research</span>"
    ]
  },
  {
    "objectID": "06-module.html#backstory",
    "href": "06-module.html#backstory",
    "title": "6  Reproducible Research",
    "section": "6.3 Backstory",
    "text": "6.3 Backstory\nReproducible research refers to the practice of conducting and disseminating scientific research in a way that makes data analysis (and scientific claims more generally) more transparent and repeatable Academics already have means of sharing methods and results generally, through publications (although perhaps typically in less than complete detail), and we can share the data on which those our analyses are based by depositing them in some form of online repository (e.g., via “supplementary information” that accompanies an article or by posting datasets to repositories like the Dryad Digital Repository or Figshare.\nBut how do we share the details of exactly how we did an analysis? And how can we ensure that it is possible for us to go back, ourselves, and replicate a particular analysis or data transformation? One solution is to integrate detailed text describing a workflow and analytical source code (such as R scripts) together in the same document.\nThis idea of tying together narrative, logic, specific code, and data (or references to them) in a single document stems from the principle of literate programming developed by Donald Knuth. Applied to scientific practice, the concept of literate programming means documenting both the logic behind and analysis and the code used to implement that analysis using computer software. This documentation allows researchers to return to and re-examine their own thought processes at any later time, and also allows them to share their thought processes so that others can understand how an analysis was performed. The upshot is that our scholarship can be better understood, recreated, and independently verified.\nThis is exactly the point of a Quarto or RMarkdown document and of other, similar document formats (e.g., R Notebooks, iPython Notebooks, or Julia Notebooks).\nSo, how does this work?\nFirst, as we saw in the last module, Markdown, is simply a formal way of styling a plain text document so that it can be easily rendered into HTML or PDF files for sharing or publishing. It is based on using some simple formatting and special characters to tag pieces of text such that a parser knows how to convert a plain text document into HTML or PDF. This link takes you to a classic description of Markdown, its syntax, and the philosophy behind it written by John Gruber, Markdown’s creator. There are now several different “dialects” of Markdown that have been developed, derived from Gruber’s original suggestions, including a specific one used on GitHub called “GitHub Flavored Markdown”, or GFM. A guide to this dialect is provided as a PDF here and is available online at this link.\nRMarkdown and Quarto documents are very similar extensions of standard Markdown that allows you to embed chunks of R code (or code blocks of other programming languages, e.g., Python, Latex), along with additional parsing instructions and options for running code, in a plain text file. During the parsing and rendering (or “knitting”) stage, when the text file is being converted to HTML or PDF format, the output of running the embedded code can also be included.\nRMarkdown uses the package {knitr} to produce intermediate files that can then be translated into a variety of formats, including HTML, traditional Markdown, PDFs, MS Word documents, web presentations, and others. A cheatsheet on RMarkdown syntax (which, again, is very similar to Markdown) can be found here. Quarto documents use a separate piece of independent software, quarto (along with the R package {knitr}, in some cases), to do the same thing. An overview and useful definitive guide to all you can do with Quarto can be found here\nIt is important to stress that Markdown (“.md”), Quarto (“.qmd”), and RMarkdown (“.Rmd”) documents ARE JUST PLAIN TEXT files! They are easy to work with, easy to share, easy to edit, easy to put under version control, and robust to changes in proprietary file formats!\nAs a demonstration of reproducible research workflow and best practices, we are going to create a new R project and corresponding repository that we will track with version control using git. Within that project, we then are going to create an Quarto document in which you can take notes or practice coding during class today.\nAs we have seen already, git, at its heart, is a command-line tool, but both local (on your computer) and hosted (e.g., on GitHub) git repositories can be managed using a dedicated git client GUI, such as GitHub Desktop, GitUp, GitKraken, SourceTree, or a host of others. Importantly for us, though, RStudio is designed to also function as a powerful git client. We will explore several ways of setting up RStudio to manage both local and remote copies of a git repository and of keep them in sync.\n\nNOTE: See also Chapters 8 and 12 of the web book Happy Git and GitHub for the useR for more information on managing git and GitHub through RStudio.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducible Research</span>"
    ]
  },
  {
    "objectID": "06-module.html#organizing-work-with-r-projects",
    "href": "06-module.html#organizing-work-with-r-projects",
    "title": "6  Reproducible Research",
    "section": "6.4 Organizing Work with R Projects",
    "text": "6.4 Organizing Work with R Projects\nThe easiest way to get RStudio to play nicely with git and GitHub for version control - and a recommended best practice - is to organize your work in RStudio using projects. You can think of an R project as a convenient workspace that is associated with its own working directory, data files, scripts, images, history log, etc. We might, for example, create a separate R project for each manuscript that we are working on.\n\nNOTE: Not surprisingly, the idea of an RStudio “project” (i.e., an organizing workspace and its associated files) and a “repo” (a directory that is under version control) go together nicely. It is, however, totally possible to create RStudio projects without having them be under version control, and it is also quite possible to use git or another VCS to track changes to files in a directory without there being an associated RStudio project!\n\nBasically, creating an RStudio project means creating a special text file (“.Rproj”) that stores settings for particular RStudio setup and session. When we open an “.Rproj” file, the working directory is automatically set to the root directory for the project (i.e., the directory in which the actual “.Rproj” file is stored), which makes organizing and navigating around the computer’s filesystem, either from the command line or in R scripts within the project, much easier.\n\nWorkflows for Creating Projects\nFor the sake of security, reproducibility, and collaboration, it makes a lot of sense for us to have all of our data science/data analysis projects both be under version control (e.g., using git) and hosted remotely on a secure and reliable platform (e.g., GitHub) that we and collaborators can access from different locations. There are multiple ways we can accomplish this.\nFor example, we could begin by setting up a new repository remotely, as we did in Module 05, and then “clone” it to our local computer and establish it as a version-controlled RStudio project. Alternatively, we could first create a new version-controlled RStudio project in a local directory and then push it up to GitHub or some other hosting site. [For either of these scenarios, we could also begin with either a new (empty) repository or with one that already has files in it.] We will go through all of these methods below, but I personally think the first process - beginning with a remote repository - is the easiest and most intuitive, but I describe all of these approaches further below.\n\nMethod 1: Create a new RStudio project by cloning from a remote repository\nPerhaps the easiest way to get RStudio working nicely with git and GitHub is by creating a new repository on GitHub and then “cloning” this remote repository to a local computer and placing it under version control.\n\nNOTE: Before completing the following, you should make sure to delete any existing local version of the remote repository that you might have cloned previously, e.g., by moving it to the Trash on MacOS or the Recycle Bin on Windows. This is because if you try to clone a remote repository into an existing directory, you will get an error!\n\n\nStep 1\n\nFirst, as described in Module 05, go to your user profile in GitHub and create a new repository.\n\nYou will need to specify a repository name (with no spaces), whether it is a public or private repository, and whether to initialize the repository with one or more files already in it. I recommend initializing with both a “README” file and with a “.gitignore” file. You can also choose to use an R template to follow for the “.gitignore” file. The “.gitignore” file is simply a text document the provides git with a list of files that you ask it NOT to track changes to. These are typically either very large files (e.g., data files) or various hidden files that are either unnecessary or undesirable to track every single change to.\n\nOnce your repository is created, go to the green “Code” popdown menu and click the clipboard icon to copy the repository’s HTTPS URL. This is the same process we used in Module 05 for cloning a remote repository, and the URL is likely to be https://github.com/ followed by &lt;your user name&gt;/&lt;your repository name&gt;).\n\n\n\nStep 2\n\nFrom the File menu in RStudio, choose New Project and select to “Checkout a project from a version control repository”…\n\n\nNOTE: This step can also be done from the popdown menu located at the top right of the RStudio IDE.\n\n\n\n\n\n\n\n\n\n\n\nChoose the top item, “Clone a project from a Git repository”…\n\n\n\n\n\n\n\n\n\n\n\nIn the subsequent dialog box, paste in the “Repository URL:” you copied in Step 1.\n\nYou can choose what parent folder to clone the repository into using the “Create project as a subdirectory of:” field (e.g., ~/Development/Repos). In most cases, the “Project directory name:” field will be filled in automatically, as the name of the remote repository you are cloning.\n\n\n\n\n\n\n\n\n\nHitting the “Create Project” button will download the repository from GitHub into a new directory on your local computer. RStudio will close and reopen, after which the working directory will be set to the new local repository directory, which you can confirm by typing getwd() at the R console prompt. The local directory is now set up to be tracked by git on your local computer and should be connected to GitHub.\nSelecting the Files tab in RStudio will then show you all of the files in the newly created local repository, which should now contain a “.gitignore” file and an “.Rproj” file with the name of your project (e.g., “test-repo.Rproj”), in addition to the “README.md” file you created on GitHub.\nYou should now also see a “Git” tab in the upper right pane of the RStudio IDE, and that tab should list the “.Rproj” file associated with the status ?, indicating that it has not yet been added, committed, or synced with the remote repository. The other two files (“.gitignore” and “README.md”) are already in sync.\n\n\n\n\n\n\n\n\n\n\n\nOther Notes\nThis process works identically if you want to clone a repo from GitHub that already has other files in it besides “.gitignore” and “README.md”. In that case, RStudio will simply just create the “.Rproj” file and modify any existing “.gitignore” file to also include one or more other files that it recommends that git not track, e.g., “.Rproj.user”. The new “.Rproj” file and modified “.gitignore” file, which were created locally, should appear under the “Git” tab with with the status ? until you add, commit, and push them.\nThe process also works the same if you set up an empty repo on GitHub, in which case you will have seen the window below immediately upon creating the repository:\n\n\n\n\n\n\n\n\n\nHere, grabbing the URL from the “Quick setup” section and using that to set up a new project is RStudio is exactly the same process as we followed above.\n\nNOTE: You could also, in a terminal window, navigate to where you want the remote project to be cloned and follow either the “… or create” or “… or push” instructions listed. Doing so will link a local directory to the remote repo you created on GitHub BUT you will then not have created that as an RStudio project and would need to follow up with other instructions below about creating a new project from an existing local directory.\n\n\n\n\nMethod 2: Creating a new RStudio project under version control in a new directory\n\nIn RStudio, select File &gt; New project and click New Directory.\n\n\n\n\n\n\n\n\n\n\n\nChoose the project type as “New Project”, then name the directory and choose where you would like it to be stored, checking the box marked Create a git repository, and pressing “Create Project”.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRStudio will create a new directory for you with an “.Rproj” file (with the name of your project) and a “.gitignore” file inside of it. This directory is now being tracked by git and RStudio. You can now create and edit files in this new directory and stage and commit them locally directly from RStudio. See the section below on “Modifying Files Locally” in RStudio.\n\n\nNOTE: It is important to remember that this project is still only under local version control… you will not yet be able to push changes up to GitHub. To do that, see the section below on “Connecting a Local Repo to GitHub”.\n\n\n\nMethod 3: Creating a new RStudio project in an existing directory\nIf you have an existing directory on your local computer that is already under git version control, then you can simply create a new RStudio project for that directory, and version control features will be automatically enabled. To do this:\n\nExecute the New Project command (from the File menu)\nChoose to create a new project from an Existing Directory\n\n\n\n\n\n\n\n\n\n\n\nSelect the appropriate directory and then click Create Project\n\nNew “.Rproj” and “.gitignore” files will be created inside of that directory, and RStudio’s version control features will then be available for that directory. Now, you can edit files currently in the directory or create new ones, as well as stage and commit them to the local repository directly from RStudio. See the section below on “Modifying Files Locally” in RStudio.\nIf you create a new project for a directory that was not already under version control, you can enable version control within RStudio by choosing Tools &gt; Project Options to open the Project Options dialog box. Once there, go to the Git/SVN section and choose “Git” from the “Version control system” popdown menu, and then confirm that you want to set up version control for the project. You can also enable version control for a new project in a directory not formerly under version control by running the use_git() command from the {usethis} package. In either case, again, new “.Rproj” and “.gitignore” files will be created inside that directory, and RStudio’s version control features will then be available.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNOTE: Again, it is important to remember that local projects created via Method 2 and Method 3 are still only under local version control… you will not yet be able to push any additional files you add to the project directory or changes to local files up to GitHub. To do that, see the section below on “Connecting a Local Repo to GitHub”.\n\n\n\n\nConnecting a Local Repo to GitHub\nTo create a new remote repository on GitHub into which you can push the contents of an existing local repository, e.g., one created by Method 2 or Method 3 above, so that it is also backed-up off site and accessible to you or collaborators working at different locations, you have a couple of different options (see also Chapter 17 of the web book Happy Git and GitHub for the userR).\n\nOption 1: Use the {usethis} package to create a new remote repository on GitHub from within RStudio\n\nStep 1\n\nMake sure you have configured a GitHub Personal Access Token (PAT) using the process outlined in Module 5. Briefly…\n\nLogin to your GitHub account and go to the Settings tab. Then select Developer Settings and then Personal Access Tokens.\nSelect Generate new token, create a new “classic” token, and give it a nickname that reminds you of the intended purpose (e.g., “GitHub Access from R”).\nPick a scope that confers the privileges you need, e.g., repo, and then press the green “Generate Token” button.\nCopy this token to a safe place, and register this credential with git using one of the methods outlined in Module 5\n\n\n\n\nStep 2\n\nCreate a remote repository on GitHub and push a local repository to it:\n\nOpen the RStudio project that you want to create a remote repository for. This project needs to be under version control already and include at least one commit.\nAt the R console prompt within the project’s working directory, type usethis::use_github(protocol=\"https\")\nAnswer the question about whether the suggested name for the remote repository is adequate.\nThis should create a new repository on GitHub, add it as a remote origin/main, set up a tracking branch, and open it in your browser.\n\n\nFrom within RStudio, you can now add or edit files to the project, locally commit any changes you make to those files, and push them up to GitHub as described above.\n\n\n\nOption 2: Set up a dummy remote repository on GitHub and push to it from within RStudio\n\nGo to the GitHub website and create a new, empty repository. You can use the same name as that of the RStudio project you want to push up, or you can create a different one.\n\n\n\n\n\n\n\n\n\n\n\nNOTE: In setting up the repository on GitHub, you should choose not to initialize the remote repository you are going to be pushing to with either a “README” or “.gitignore” file!\n\n\nFrom the Git tab in RStudio, select the New Branch icon.\n\n\n\n\n\n\n\n\n\n\n\nClick “Add remote”, paste in the URL for your dummy remote repository in the text field, and type in “origin” for the name of the remote branch.\nClick “Add”. We should be back in the “New Branch” dialog box. Enter main as the branch name (to push from the local main branch to a remote main), and make sure “Sync branch with remote” button is checked.\nClick “Create” and in the next dialog box, choose “Overwrite”. This should push your local repository main branch up to the remote main branch.\n\n\n\nOption 3: Set up a dummy remote repository on GitHub and push to it from a terminal shell\n\nAs in Option 2, go to the GitHub website and create a new, empty repository. You can use the same name as that of the RStudio project you want to push up, or you can create a different one.\n\n\n\n\n\n\n\n\n\n\n\nNOTE: Here, again, you should choose not to initialize the remote repository you are going to be pushing to with either a “README” or “.gitignore” file!\n\n\nFrom the next screen, select the desired transfer protocol you want to use under the Quick setup... option at the top (HTTPS or SSH). Then, scroll down to the option: ...or push an existing repository from the command line.\n\n\n\n\n\n\n\n\n\n\n\nCopy the lines of code listed there and then return to RStudio.\nIn RStudio, open the project that you want to push to GitHub and click Tools &gt; Terminal &gt; New Terminal or Tools &gt; Shell… to open a terminal window. Alternatively, open a separate terminal window and navigate to the root of the directory of the repository you wish to push.\n\n\nNOTE: It is important that you run these lines of code from within the directory that you wish to push to GitHub. When you open a new terminal or shell from within RStudio, you should be in the correct directory, as those processes open the shell in the current working directory. If not, though, use shell commands (i.e., cd) to navigate into the correct directory.\n\nAt the shell prompt, enter the lines of code you copied.\n\ngit remote add origin https://github.com/&lt;your user name&gt;/&lt;your repository name&gt;.git\ngit branch -M main\ngit push -u origin main\n\nThe first line tells git the remote URL that you are going to push to, the second makes sure you are on the main branch of the repository, and the third pushes your local repository main branch up to the remote main branch.\nCongratulations! You have now pushed commits from a local repository to GitHub, and you should be able to see those files in your remote GitHub repository online. The “Pull” (blue down arrow) and “Push” (green up arrow) buttons in RStudio should now also work.\n\nIMPORTANT: Remember, after each commit you do via RStudio (or via the command line), you will have to push to GitHub manually. This does not happen automatically!\n\n\nNOTE: Additional information on using projects in RStudio is available here.\n\n\n\n\nTroubleshooting\nMost of the time, the installation and setup instructions provided in Module 05 and this module for getting git and RStudio to work together work just fine, but sometimes you may have issues. The most common problem I have seen is when RStudio is unable to find your proper git installation, either because the path to the correct git executable did not get written into your shell profile when you installed git or because the correct path to git has not be set properly in RStudio.\nChapters 13 and 14 of the web book Happy Git and GitHub R offer a number of suggestions for how to troubleshoot these problems, but some things to check are:\n\nIs git installed correctly? An easy way to test this is to enter git in a terminal shell. If you get a complaint about git not being found, it means either installation was unsuccessful or that the path to the git program is not on your PATH in your shell profile. Try reinstalling git and then either logging out of your computer and logging in again or restarting.\nIs the proper path to git set in RStudio? InRStudio, go to Tools &gt; Global Options and select the “Git/SVN” tab and make sure that the box “Git executable” points to the executably file for your git installation. On macOS and Linux operating systems, the path usually is “/usr/bin/git” (on MacOS or Linux) and on Windows operating systems it is usually “C:\\Program Files\\Git\\bin\\git.exe”. Sometimes, git will be installed in a slightly different place, most commonly (on MacOS) in “/usr/local/bin/git”, in which case you will need to change the path in the dialog box. To find the correct path, type which git in a terminal shell and then enter that in the ““Git executable” box.\n\n\nNOTE: If you make any changes, e.g., if you reinstall git, you will likely need to open a new shell window (so that your updated PATH gets read and your computer can find git). You may also need to also either log out of your commputer and log in again or restart your computer. If you make changes in the RStudio IDE, e.g., if you change the path to your git executable via the Tools &gt; Global Options &gt; Git/SVN tab, you will need to completely RESTART RStudio, and you may need to also log out and back in or restart your computer.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducible Research</span>"
    ]
  },
  {
    "objectID": "06-module.html#working-with-projects-in-rstudio",
    "href": "06-module.html#working-with-projects-in-rstudio",
    "title": "6  Reproducible Research",
    "section": "6.5 Working with Projects in RStudio",
    "text": "6.5 Working with Projects in RStudio\nOnce you have a project set up in RStudio and under version control, git will be watching your local project directory for any changes that you might make to what is contained in that directory or to the files therein. For example, if you add a file to the directory, it will show up in the Files tab, and if you delete a file, it will disappear from that list. Thus, you can drag and drop files and directories into and out of the project repo and git will keep track of them.\nNow, you can edit existing files in the repository (e.g., the “README.md” file), create one or more new scripts (“.R”), Quarto (“.qmd”) documents, RMarkdown (“.Rmd”) documents, or add other files and folders (e.g., data files) to your repository and git will keept track of them.\n\nModifying Files Locally\nWithin RStudio, click on the “README.md” file to open it in the text editor panel of the IDE. Make some changes to this file (e.g., add the line, “And here is some new text I am adding in the RStudio IDE.”) and then save your changes. When you do, the “README.md” file should show up in the Git tab with a blue “M” next to it, indicating that it has been “modified”.\nWe can commit these changes to the local repository and push them up to GitHub as follows:\n\nClick the Git tab in upper right pane.\nCheck the Staged box next to the “README.md” file, which will get the file ready to commit to your local repository. If you have not already done so, you should also stage the “.gitignore” and “.Rproj” files.\n\n\nNOTE: The “.gitignore” file that was created when you activated version control is basically a list of files that you want git to ignore and not worry about tracking changes to. By default, it includes the names of several files, e.g., “.Rhistory” and “.RData”, which can be quite large and are not necessarily that important to track all changes to. You can also add to the “.gitignore” document the names of any other files in your working directory that you do not want git to track. These files can sit in your local working directory, unstaged and uncommitted, with no problems.\n\nAfter being staged, the status of the files should turn to a green A (for “added”).\n\nClick the “Commit” button. You will see a new, “Review Changes” window open up with the names of the files in your directory in the upper left pane. Selecting any of these will show you, in the lower pane, an overview of the differences between current version of the file and the version that was most recently previously committed.\n\n\n\n\n\n\n\n\n\n\n\nEnter an identifying “Commit message” in the box in the upper right pane, e.g., “First commit from RStudio” and then click “Commit”.\n\nA window will pop up confirming what you have just done, which you can then close.\n\n\n\n\n\n\n\n\n\nIf you now select the History tab in the “Review Changes” window, you can see the history of commits. Selecting any of the nodes in the commit history will show you (in the lower pane) the files involved in the commit and how the content of those files has changed since your last commit. For example, the node associated with your initial commit will show you the initial file contents, while subsequent nodes highlight where a new version differs from the previous one.\n\n\nPushing Changes to a Remote Repo\n\nWith your commits completed, the files disappear from the “Review Changes” window and from the Git tab in RStudio window (meaning they have all been committed locally). You should now be able to pass all of your local changes up to GitHub by clicking the green “Push” (up arrow) button. This updates the remote copy of your repository and makes it available to collaborators or to you, working on a different local computer.\n\n\n\n\n\n\n\n\n\n\nYou should see a dialog box like the following indicating success:\n\n\n\n\n\n\n\n\n\n\n\nConfirming Changes\nFinally, you can confirm that the local changes were indeed sent up to the remote repository on GitHub by going back to the page for the repository in your web browser and hitting refresh… again, you should see that the “README.md” file has been updated with new text and, if you click on the “commits” tab, you should see one with the message “Commit from RStudio”.\n\n\n\n\n\n\n\n\n\n\n\nCHALLENGE\nCreate and save a completely new Quarto (“.qmd”), RMarkdown (“.Rmd”), markdown, or plain text document in your current working directory/test repository by selecting File &gt; New File and then picking the file type of your choice. If you create a markdown or plain text file, a blank document will open, while if you create a Quarto or RMarkdown (“.Rmd”) document, a dialog box will open that allows you set such things as the title and author for your document and a choice of output format for when you choose to “render” (for Quarto documents) or “knit” (for RMarkdown documents) the file you create.\n\nNOTE: The process of rendering/knitting converts your file, with notes and code, into nicely formatted output (e.g., a web page, PDF document, or MS Word document) that can include the output of running your code.\n\nThe image below shows the dialog box for creating a new Quarto document. Minimally, provide a title for your document. I also always uncheck the box for “Use visual markdown editor” because I prefer to work on my documents in “Source” mode (you can toggle between these options using the buttons at the top left of the document pane in RStudio).\n\n\n\n\n\n\n\n\n\nOnce you have created a document, keep the header section (the piece at the top of the file between the --- lines) but feel free to erase the rest of the pre-populated content that RStudio provides as a template and to fill it with your own notes and/or code blocks. The content does not matter… be creative and try some markdown formatting! Code blocks begin and end with three backticks, and the the programming language for the block (R, python, bash) is indicated in braces immediately following the opening st of backticks. Anything outside of code blocks can be formatted using markdown styling.\nFor example…\n---\ntitle: \"My document\"\n---\n\n# This is **markdown** formatted content.\n\nThe block below is an **R** code block.\n\n```{r}\n\n```\n\nHere is some more **markdown** content\nAfter you have made some edits to your file, save it, commit it to the local clone of your repository, and push it from your local repository up to the main branch of your repository on GitHub.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducible Research</span>"
    ]
  },
  {
    "objectID": "06-module.html#deleting-repositories",
    "href": "06-module.html#deleting-repositories",
    "title": "6  Reproducible Research",
    "section": "6.6 Deleting Repositories",
    "text": "6.6 Deleting Repositories\nIf you want to get rid of a local repository, you can simply send it to the Trash (on MacOS) or Recycle Bin (on Windows) and throw it away. It is just a regular, self-contained directory, and your local git executable will no longer track it.\nIf you want to get rid of a remote repository on GitHub, navigate to the repository’s web page in a browser, click on “Settings” tab, scroll down to bottom of the page in the “Danger Zone” section, and select the option “Delete this repository”. You will be asked to type in the repository name to confirm that you want to delete it and will likely have to enter your GitHub password.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducible Research</span>"
    ]
  },
  {
    "objectID": "06-module.html#additional-resources",
    "href": "06-module.html#additional-resources",
    "title": "6  Reproducible Research",
    "section": "6.7 Additional Resources",
    "text": "6.7 Additional Resources\nThe web book Happy Git and GitHub for the useR by Dr. Jenny Bryan (an important contributor to a number of key R packages) is an excellent source of information about how to set up and troubleshoot your RStudio/git/GitHub workflow… I encourage you to check it out!",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducible Research</span>"
    ]
  },
  {
    "objectID": "06-module.html#customizing-execution-of-quarto-and-rmardown-documents",
    "href": "06-module.html#customizing-execution-of-quarto-and-rmardown-documents",
    "title": "6  Reproducible Research",
    "section": "6.8 Customizing Execution of Quarto and RMardown Documents",
    "text": "6.8 Customizing Execution of Quarto and RMardown Documents\nIt is possible to set various options for how Quarto and RMardown documents are rendered, either for an entire document or on a code chunk-by-code chunk basis. For example, for Quarto documents, we can set the following options to either TRUE or FALSE by including these either in the header of the document or at the start of a chunk:\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\neval\nEvaluate the code (TRUE) or just echo code (FALSE) in the output\n\n\necho\nInclude (TRUE) or omit (FALSE) the code in output\n\n\noutput\nInclude (TRUE) or omit (FALSE) the results of executing the code in the output\n\n\nwarnings\nInclude (TRUE) or suppress warnings (FALSE) in the output\n\n\nerror\nInclude (TRUE) or suppress (FALSE) errors in the output (If TRUE, errors executing code will not halt rendering of the document)\n\n\ninclude\nFALSE prevents any output (code or results) from being included\n\n\n\nFor example, to apply to the whole document, we place the option under execute in the header…\n---\ntitle: \"My Document\"\nexecute:\n  echo: false\n---\n… and to apply to a specific code chunk, we preface the line with #|…\n```{r}\n#| echo: false\n```\nWe can also set options in either document header to or in a given code chunk to include code folding. For example, to produce an HTML document that shows code as folded, we can use…\n---\ntitle: \"My Document\"\nformat:\n  html:\n    code-fold: TRUE\n---\n… and for a specific chunk, we can use…\n```{r}\n#| code-fold: TRUE\n```\nTypically, when rendering to HTML, Quarto will produce a separate folder of output images that is linked to from the HTML file that is produced, but it is possible to create a self-contained HTML file that embeds those (and any other) resources by specifying embed-resource: TRUE in the header. Note that this can result in very large files being created!\n---\ntitle: \"My Document\"\nformat:\n  html:\n    embed-resources: TRUE\n---\nAdditional information on Quarto HTML options can be found at https://quarto.org/docs/reference/formats/html.html#rendering\nDetails on rendering and chunk options for knitting RMarkdown documents can be found at https://yihui.org/knitr/options/.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducible Research</span>"
    ]
  },
  {
    "objectID": "06-module.html#concept-review",
    "href": "06-module.html#concept-review",
    "title": "6  Reproducible Research",
    "section": "Concept Review",
    "text": "Concept Review\n\nQuarto and RMarkdown documents and execution options\nCreating R Projects (3 ways)\n\nCreating a project from a remote repository under version control (Method 1)\nCreating a brand new project locally (Method 2)\nCreating a project from an existing local repository under version control (Method 3)\n\nConnecting a local repository and a remote repository on GitHub (2 ways)\n\nLESS COMPLICATED (Corresponding to Method 1): Start by setting up a repository on GitHub and then clone the repository locally (see above). Once created, you can copy files into it, stage and commit those files, and push them to GitHub\nMORE COMPLICATED (Corresponding to Method 2 and Method 3): Start by setting up a local repository under version control and then set up and connect to a remote repository on GitHub. See Chapter 17 of the web book Happy Git and GitHub for the useR for additional instructions to use for following this workflow.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducible Research</span>"
    ]
  },
  {
    "objectID": "07-module.html",
    "href": "07-module.html",
    "title": "7  Additional Data Structures in R",
    "section": "",
    "text": "7.1 Objectives",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Additional Data Structures in ***R***</span>"
    ]
  },
  {
    "objectID": "07-module.html#objectives",
    "href": "07-module.html#objectives",
    "title": "7  Additional Data Structures in R",
    "section": "",
    "text": "The objective of this module is to introduce additional fundamental data structures in R (matrices, arrays, lists, data frames, and the like) and to learn how to extract, filter, and subset data from them.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Additional Data Structures in ***R***</span>"
    ]
  },
  {
    "objectID": "07-module.html#preliminaries",
    "href": "07-module.html#preliminaries",
    "title": "7  Additional Data Structures in R",
    "section": "7.2 Preliminaries",
    "text": "7.2 Preliminaries\n\nGO TO: https://github.com/difiore/ada-datasets, select the “random-people.csv” file, then press the “Download” button and save the file to your local computer (e.g., on your desktop).\n\n\n\n\n\n\n\n\n\n\nAlternatively, you can press the “Raw” button, highlight, and copy the text to a text editor, and save it. RStudio, as we have seen, has a powerful built-in text editor. There are also a number of other excellent text editors that you download for FREE (e.g., BBEdit for MacOS, Notepad++ for Windows, or Visual Studio Code for either operating system). - Install and load these packages in R: {tidyverse} (which includes {ggplot2}, {dplyr}, {readr}, {tibble}, and {tidyr}, plus others, so they do not need to be installed separately) and {data.table}",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Additional Data Structures in ***R***</span>"
    ]
  },
  {
    "objectID": "07-module.html#matrices-and-arrays",
    "href": "07-module.html#matrices-and-arrays",
    "title": "7  Additional Data Structures in R",
    "section": "7.3 Matrices and Arrays",
    "text": "7.3 Matrices and Arrays\nSo far, we have seen several way of creating vectors, which are the most fundamental data structures in R. Today, we will explore and learn how to manipulate other fundamental data structures, including matrices, arrays, lists, and data frames, as well as variants on data frames (e.g., data tables and “tibbles”.)\n\nNOTE: The kind of vectors we have been talking about so far are also sometimes referred to as atomic vectors, and all of the elements of a vector have to have the same data type. We can think of lists (see below) as a different kind of vector, where the elements can have different types, but I prefer to consider lists as a different kind of data structure. Wickham (2019) Advanced R, Second Edition discusses the nuances of various R data structures in more detail.\n\nMatrices and arrays are extensions of the basic vector data structure, and like vectors, all of the elements in an array or matrix have to be of the same atomic type.\nWe can think of a matrix as a two-dimensional structure consisting of several atomic vectors stored together, but, more accurately, a matrix is essentially a single atomic vector that is split either into multiple columns or multiple rows of the same length. Matrices are useful constructs for performing many mathematical and statistical operations. Again, like 1-dimensional atomic vectors, matrices can only store data of one atomic class (e.g., numerical or character). Matrices are created using the matrix() function.\n\nm &lt;- matrix(data = c(1, 2, 3, 4), nrow = 2, ncol = 2)\nm\n\n##      [,1] [,2]\n## [1,]    1    3\n## [2,]    2    4\n\n\nMatrices are typically filled column-wise, with the argument, byrow=, set to FALSE by default (note that FALSE is not in quotation marks). This means that the first column of the matrix will be filled first, the second column second, etc.\n\nm &lt;- matrix(data = c(1, 2, 3, 4, 5, 6), nrow = 2, ncol = 3, byrow = FALSE)\nm\n\n##      [,1] [,2] [,3]\n## [1,]    1    3    5\n## [2,]    2    4    6\n\n\nThis pattern can be changed by specifying the byrow= argument as TRUE.\n\nm &lt;- matrix(data = c(1, 2, 3, 4, 5, 6), nrow = 2, ncol = 3, byrow = TRUE)\nm\n\n##      [,1] [,2] [,3]\n## [1,]    1    2    3\n## [2,]    4    5    6\n\n\nYou can also create matrices by binding vectors of the same length together either row-wise (with the function rbind()) or column-wise (with the function cbind()).\n\nv1 &lt;- c(1, 2, 3, 4)\nv2 &lt;- c(6, 7, 8, 9)\nm1 &lt;- rbind(v1, v2)\nm1\n\n##    [,1] [,2] [,3] [,4]\n## v1    1    2    3    4\n## v2    6    7    8    9\n\n\n\nm2 &lt;- cbind(v1, v2)\nm2\n\n##      v1 v2\n## [1,]  1  6\n## [2,]  2  7\n## [3,]  3  8\n## [4,]  4  9\n\n\nStandard metadata about a matrix can be extracted using the class(), dim(), names(), rownames(), colnames() and other commands. The dim() command returns an vector containing the number of rows at index position 1 and the number of columns at index position 2.\n\nclass(m1)\n\n## [1] \"matrix\" \"array\"\n\nclass(m2)\n\n## [1] \"matrix\" \"array\"\n\n\n\ndim(m1)\n\n## [1] 2 4\n\ndim(m2)\n\n## [1] 4 2\n\n\n\ncolnames(m1)\n\n## NULL\n\nrownames(m1)\n\n## [1] \"v1\" \"v2\"\n\n\n\nNOTE: In this example, colnames are not defined for m1 since rbind() was used to create the matrix.\n\n\ncolnames(m2)\n\n## [1] \"v1\" \"v2\"\n\nrownames(m2)\n\n## NULL\n\n\n\nNOTE: Similarly, in this example, rownames are not defined for m2, since cbind() was used to create the matrix.\n\nAs we saw with vectors, the structure (str()) and glimpse (dplyr::glimpse()) commands can be applied to any data structure to provide details about that object. These are incredibly useful functions that you will find yourself using over and over again.\n\nstr(m1)\n\n##  num [1:2, 1:4] 1 6 2 7 3 8 4 9\n##  - attr(*, \"dimnames\")=List of 2\n##   ..$ : chr [1:2] \"v1\" \"v2\"\n##   ..$ : NULL\n\nstr(m2)\n\n##  num [1:4, 1:2] 1 2 3 4 6 7 8 9\n##  - attr(*, \"dimnames\")=List of 2\n##   ..$ : NULL\n##   ..$ : chr [1:2] \"v1\" \"v2\"\n\n\nThe attributes (attributes()) command can be used to list the attributes of a data structure.\n\nattributes(m1)\n\n## $dim\n## [1] 2 4\n## \n## $dimnames\n## $dimnames[[1]]\n## [1] \"v1\" \"v2\"\n## \n## $dimnames[[2]]\n## NULL\n\nattr(m1, which = \"dim\")\n\n## [1] 2 4\n\nattr(m1, which = \"dimnames\")[[1]]\n\n## [1] \"v1\" \"v2\"\n\nattr(m1, which = \"dimnames\")[[2]]\n\n## NULL\n\n\nAn array is a more general atomic data structure, of which a vector (with 1 implicit dimension) and a matrix (with 2 defined dimensions) are but examples. Arrays can include additional dimensions, but (like vectors and matrices) they can only include elements that are all of the same atomic data class (e.g., numeric, character). The example below shows the construction of a 3 dimensional array with 5 rows, 6 columns, and 3 “levels”). Visualizing higher and higher dimension arrays, obviously, becomes challenging!\n\na &lt;- array(data = 1:90, dim = c(5, 6, 3))\na\n\n## , , 1\n## \n##      [,1] [,2] [,3] [,4] [,5] [,6]\n## [1,]    1    6   11   16   21   26\n## [2,]    2    7   12   17   22   27\n## [3,]    3    8   13   18   23   28\n## [4,]    4    9   14   19   24   29\n## [5,]    5   10   15   20   25   30\n## \n## , , 2\n## \n##      [,1] [,2] [,3] [,4] [,5] [,6]\n## [1,]   31   36   41   46   51   56\n## [2,]   32   37   42   47   52   57\n## [3,]   33   38   43   48   53   58\n## [4,]   34   39   44   49   54   59\n## [5,]   35   40   45   50   55   60\n## \n## , , 3\n## \n##      [,1] [,2] [,3] [,4] [,5] [,6]\n## [1,]   61   66   71   76   81   86\n## [2,]   62   67   72   77   82   87\n## [3,]   63   68   73   78   83   88\n## [4,]   64   69   74   79   84   89\n## [5,]   65   70   75   80   85   90\n\n\n\nSubsetting\nYou can reference or extract select elements from vectors, matrices, and arrays by subsetting them using their index position(s) in what is knows as bracket notation ([ ]). For vectors, you would specify an index value in one dimension. For matrices, you would give the index values in two dimensions. For arrays generally, you would give index values for each dimension in the array.\nFor example, suppose you have the following vector:\n\nv &lt;- 1:100\nv\n\n##   [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n##  [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n##  [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n##  [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n##  [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n##  [91]  91  92  93  94  95  96  97  98  99 100\n\n\nYou can select the first 15 elements using bracket notation as follows:\n\nv[1:15]\n\n##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15\n\n\nYou can also supply a vector of index values as the argument to [ ] to use for subsetting:\n\nv[c(2, 4, 6, 8, 10)]\n\n## [1]  2  4  6  8 10\n\n\nSimilarly, you can also use a function or a calculation to subset a vector. What does the following return?\n\nv &lt;- 101:200\nv[seq(from = 1, to = 100, by = 2)]\n\n##  [1] 101 103 105 107 109 111 113 115 117 119 121 123 125 127 129 131 133 135 137\n## [20] 139 141 143 145 147 149 151 153 155 157 159 161 163 165 167 169 171 173 175\n## [39] 177 179 181 183 185 187 189 191 193 195 197 199\n\n\nAs an example for a matrix, suppose you have the following:\n\nm &lt;- matrix(data = 1:80, nrow = 8, ncol = 10, byrow = FALSE)\nm\n\n##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n## [1,]    1    9   17   25   33   41   49   57   65    73\n## [2,]    2   10   18   26   34   42   50   58   66    74\n## [3,]    3   11   19   27   35   43   51   59   67    75\n## [4,]    4   12   20   28   36   44   52   60   68    76\n## [5,]    5   13   21   29   37   45   53   61   69    77\n## [6,]    6   14   22   30   38   46   54   62   70    78\n## [7,]    7   15   23   31   39   47   55   63   71    79\n## [8,]    8   16   24   32   40   48   56   64   72    80\n\n\nYou can extract the element in row 4, column 5 and assign it to a new variable, x, as follows:\n\nx &lt;- m[4, 5]\nx\n\n## [1] 36\n\n\nYou can also extract an entire row or an entire column (or set of rows or set of columns) from a matrix by specifying the desired row or column number(s) and leaving the other value blank.\n\nx &lt;- m[4, ]  # extracts 4th row\nx\n\n##  [1]  4 12 20 28 36 44 52 60 68 76\n\n\n\n\nCHALLENGE\n\nGiven the matrix, m, above, extract the 2nd, 3rd, and 6th columns and assign them to the variable x\n\n\n\nShow Code\nx &lt;- m[, c(2, 3, 6)]\nx\n\n\nShow Output\n##      [,1] [,2] [,3]\n## [1,]    9   17   41\n## [2,]   10   18   42\n## [3,]   11   19   43\n## [4,]   12   20   44\n## [5,]   13   21   45\n## [6,]   14   22   46\n## [7,]   15   23   47\n## [8,]   16   24   48\n\n\n\n\nGiven the matrix, m, above, extract the 6th to 8th row and assign them to the variable x\n\n\n\nShow Code\nx &lt;- m[6:8, ]\nx\n\n\nShow Output\n##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n## [1,]    6   14   22   30   38   46   54   62   70    78\n## [2,]    7   15   23   31   39   47   55   63   71    79\n## [3,]    8   16   24   32   40   48   56   64   72    80\n\n\n\n\nGiven the matrix, m, above, extract the elements from row 2, column 2 to row 6, column 9 and assign them to the variable x\n\n\n\nShow Code\nx &lt;- m[2:6, 2:9]\nx\n\n\nShow Output\n##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n## [1,]   10   18   26   34   42   50   58   66\n## [2,]   11   19   27   35   43   51   59   67\n## [3,]   12   20   28   36   44   52   60   68\n## [4,]   13   21   29   37   45   53   61   69\n## [5,]   14   22   30   38   46   54   62   70\n\n\n\n\n\nOverwriting Elements\nYou can replace elements in a vector or matrix, or even entire rows or columns, by identifying the elements to be replaced and then assigning them new values.\nStarting with the matrix, m, defined above, explore what will be the effects of operations below. Pay careful attention to row and column index values, vector recycling, and automated conversion/recasting among data classes.\n\nm[7, 1] &lt;- 564\nm[, 8] &lt;- 2\nm[2:5, 4:8] &lt;- 1\nm[2:5, 4:8] &lt;- c(20, 19, 18, 17)\nm[2:5, 4:8] &lt;- matrix(data = c(20:1), nrow = 4, ncol = 5, byrow = TRUE)\nm[, 8] &lt;- c(\"a\", \"b\")",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Additional Data Structures in ***R***</span>"
    ]
  },
  {
    "objectID": "07-module.html#lists-and-data-frames",
    "href": "07-module.html#lists-and-data-frames",
    "title": "7  Additional Data Structures in R",
    "section": "7.4 Lists and Data Frames",
    "text": "7.4 Lists and Data Frames\nUnlike vectors, matrices, and arrays, two other data structures - lists and data frames - can be used to group together a heterogeneous mix of R structures and objects. A single list, for example, could contain a matrix, vector of character strings, vector of factors, an array, even another list.\nLists are created using the list() function where the elements to add to the list are given as arguments to the function, separated by commas. Type in the following example:\n\ns &lt;- c(\"this\", \"is\", \"a\", \"vector\", \"of\", \"strings\")\n# this is a vector of character strings\nm &lt;- matrix(data = 1:40, nrow = 5, ncol = 8)  # this is a matrix\nb &lt;- FALSE  # this is a boolean variable\nl &lt;- list(s, m, b)\nl\n\n## [[1]]\n## [1] \"this\"    \"is\"      \"a\"       \"vector\"  \"of\"      \"strings\"\n## \n## [[2]]\n##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n## [1,]    1    6   11   16   21   26   31   36\n## [2,]    2    7   12   17   22   27   32   37\n## [3,]    3    8   13   18   23   28   33   38\n## [4,]    4    9   14   19   24   29   34   39\n## [5,]    5   10   15   20   25   30   35   40\n## \n## [[3]]\n## [1] FALSE\n\n\n\nSubsetting Lists\nYou can reference or extract elements from a list similarly to how you would from other data structure, except that you use double brackets ([[ ]]) to reference a single element in the list.\n\nl[[2]]\n\n##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n## [1,]    1    6   11   16   21   26   31   36\n## [2,]    2    7   12   17   22   27   32   37\n## [3,]    3    8   13   18   23   28   33   38\n## [4,]    4    9   14   19   24   29   34   39\n## [5,]    5   10   15   20   25   30   35   40\n\n\nAn extension of this notation can be used to access elements contained within an element in the list. For example:\n\nl[[2]][2, 6]\n\n## [1] 27\n\nl[[2]][2, ]\n\n## [1]  2  7 12 17 22 27 32 37\n\nl[[2]][, 6]\n\n## [1] 26 27 28 29 30\n\n\nTo reference or extract multiple elements from a list, you would use single bracket ([ ]) notation, which would itself return a list. This is called “list slicing”.\n\nl[2:3]\n\n## [[1]]\n##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n## [1,]    1    6   11   16   21   26   31   36\n## [2,]    2    7   12   17   22   27   32   37\n## [3,]    3    8   13   18   23   28   33   38\n## [4,]    4    9   14   19   24   29   34   39\n## [5,]    5   10   15   20   25   30   35   40\n## \n## [[2]]\n## [1] FALSE\n\n\n\nl[c(1, 3)]\n\n## [[1]]\n## [1] \"this\"    \"is\"      \"a\"       \"vector\"  \"of\"      \"strings\"\n## \n## [[2]]\n## [1] FALSE\n\n\nUsing class() and str() (or dplyr::glimpse()) provides details about the our list and its three elements:\n\nclass(l)\n\n## [1] \"list\"\n\nstr(l)\n\n## List of 3\n##  $ : chr [1:6] \"this\" \"is\" \"a\" \"vector\" ...\n##  $ : int [1:5, 1:8] 1 2 3 4 5 6 7 8 9 10 ...\n##  $ : logi FALSE\n\ndplyr::glimpse(l)\n\n## List of 3\n##  $ : chr [1:6] \"this\" \"is\" \"a\" \"vector\" ...\n##  $ : int [1:5, 1:8] 1 2 3 4 5 6 7 8 9 10 ...\n##  $ : logi FALSE\n\n\nYou can name the elements in a list using the names() function, which adds a name attribute to each list item.\n\nnames(l) &lt;- c(\"string\", \"matrix\", \"logical\")\nnames(l)\n\n## [1] \"string\"  \"matrix\"  \"logical\"\n\n\nYou can also use the name of an item in the list to refer to it using the shortcut $ notation. This is the equivalent of using [[ ]] with either the column number or the name of the column in quotation marks as the argument inside of the double bracket.\n\n# all of the following are equivalent!\nl$string\n\n## [1] \"this\"    \"is\"      \"a\"       \"vector\"  \"of\"      \"strings\"\n\nl[[1]]\n\n## [1] \"this\"    \"is\"      \"a\"       \"vector\"  \"of\"      \"strings\"\n\nl[[\"string\"]]\n\n## [1] \"this\"    \"is\"      \"a\"       \"vector\"  \"of\"      \"strings\"\n\n\n\n# all of the following are equivalent\nl$matrix[3, 5]\n\n## [1] 23\n\nl[[2]][3, 5]\n\n## [1] 23\n\nl[[\"matrix\"]][3, 5]\n\n## [1] 23\n\n\nThe data frame is the perhaps the most useful (and most familiar) data structure that we can operate with in R as it most closely aligns with how we tend to represent tabular data, with rows as cases or observations and columns as variables describing those observations (e.g., a measurement of a particular type). Variables tend to be measured using the same units and thus fall into the same data class and can be thought of as analogous to vectors, so a data frame is essentially a list of atomic vectors that all have the same length.\nThe data.frame() command can be used to create data frames from scratch.\n\ndf &lt;- data.frame(firstName = c(\"Rick\", \"Negan\", \"Dwight\", \"Maggie\", \"Michonne\"),\n    community = c(\"Alexandria\", \"Saviors\", \"Saviors\", \"Hiltop\", \"Alexandria\"), sex = c(\"M\",\n        \"M\", \"M\", \"F\", \"F\"), age = c(42, 40, 33, 28, 31))\ndf\n\n##   firstName  community sex age\n## 1      Rick Alexandria   M  42\n## 2     Negan    Saviors   M  40\n## 3    Dwight    Saviors   M  33\n## 4    Maggie     Hiltop   F  28\n## 5  Michonne Alexandria   F  31\n\n\nMore commonly we read tabular data into R from some external data source (see Module 08, which typically results in the table being represented as a data frame. The following code, for example, will read from the file “random-people.csv” stored in a folder called “data” (a data/ directory) located inside a user’s working directory.\n\ndf &lt;- read.csv(file = \"data/random-people.csv\", sep = \",\", header = TRUE, stringsAsFactors = FALSE)\n# only print select columns of this data frame head() means we will also only\n# print the first several rows\nhead(df[, c(1, 3, 4, 11, 12)])\n\n##   gender name.first name.last login.password           dob\n## 1   male        ted    wright          rolex  11/8/73 1:33\n## 2   male    quentin   schmitt         norton  5/24/51 3:16\n## 3 female      laura  johansen        stevens 5/22/77 21:03\n## 4   male     ismael   herrero         303030   8/1/58 9:13\n## 5 female     susana    blanco          aloha 12/18/55 3:21\n## 6   male      mason    wilson         topdog  6/23/60 9:19\n\n\n\nNOTE: To run the example code above, you may need to replace the string in file=\"&lt;string&gt;\" with the path to where you stored the file on your local computer.\n\n\nstr(df)\n\n## 'data.frame':    20 obs. of  17 variables:\n##  $ gender           : chr  \"male\" \"male\" \"female\" \"male\" ...\n##  $ name.title       : chr  \"mr\" \"mr\" \"ms\" \"mr\" ...\n##  $ name.first       : chr  \"ted\" \"quentin\" \"laura\" \"ismael\" ...\n##  $ name.last        : chr  \"wright\" \"schmitt\" \"johansen\" \"herrero\" ...\n##  $ location.street  : chr  \"2020 royal ln\" \"2433 rue dubois\" \"2142 elmelunden\" \"3897 calle del barquillo\" ...\n##  $ location.city    : chr  \"coffs harbour\" \"vitry-sur-seine\" \"silkeboeg\" \"gandia\" ...\n##  $ location.state   : chr  \"tasmania\" \"indre-et-loire\" \"hovedstaden\" \"ceuta\" ...\n##  $ location.postcode: chr  \"4126\" \"99856\" \"16264\" \"61349\" ...\n##  $ email            : chr  \"ted.wright@example.com\" \"quentin.schmitt@example.com\" \"laura.johansen@example.com\" \"ismael.herrero@example.com\" ...\n##  $ login.username   : chr  \"organicleopard402\" \"bluegoose191\" \"orangebird528\" \"heavyswan518\" ...\n##  $ login.password   : chr  \"rolex\" \"norton\" \"stevens\" \"303030\" ...\n##  $ dob              : chr  \"11/8/73 1:33\" \"5/24/51 3:16\" \"5/22/77 21:03\" \"8/1/58 9:13\" ...\n##  $ date.registered  : chr  \"5/5/07 20:26\" \"4/11/11 7:05\" \"5/16/14 15:53\" \"2/17/06 16:53\" ...\n##  $ phone            : chr  \"01-0349-5128\" \"05-72-65-32-21\" \"81616775\" \"974-117-403\" ...\n##  $ cell             : chr  \"0449-989-455\" \"06-83-24-92-41\" \"697-993-20\" \"665-791-673\" ...\n##  $ picture.large    : chr  \"https://randomuser.me/api/portraits/men/48.jpg\" \"https://randomuser.me/api/portraits/men/53.jpg\" \"https://randomuser.me/api/portraits/women/70.jpg\" \"https://randomuser.me/api/portraits/men/79.jpg\" ...\n##  $ nat              : chr  \"AU\" \"FR\" \"DK\" \"ES\" ...\n\ndplyr::glimpse(df)\n\n## Rows: 20\n## Columns: 17\n## $ gender            &lt;chr&gt; \"male\", \"male\", \"female\", \"male\", \"female\", \"male\", …\n## $ name.title        &lt;chr&gt; \"mr\", \"mr\", \"ms\", \"mr\", \"ms\", \"mr\", \"mr\", \"miss\", \"m…\n## $ name.first        &lt;chr&gt; \"ted\", \"quentin\", \"laura\", \"ismael\", \"susana\", \"maso…\n## $ name.last         &lt;chr&gt; \"wright\", \"schmitt\", \"johansen\", \"herrero\", \"blanco\"…\n## $ location.street   &lt;chr&gt; \"2020 royal ln\", \"2433 rue dubois\", \"2142 elmelunden…\n## $ location.city     &lt;chr&gt; \"coffs harbour\", \"vitry-sur-seine\", \"silkeboeg\", \"ga…\n## $ location.state    &lt;chr&gt; \"tasmania\", \"indre-et-loire\", \"hovedstaden\", \"ceuta\"…\n## $ location.postcode &lt;chr&gt; \"4126\", \"99856\", \"16264\", \"61349\", \"29445\", \"91479\",…\n## $ email             &lt;chr&gt; \"ted.wright@example.com\", \"quentin.schmitt@example.c…\n## $ login.username    &lt;chr&gt; \"organicleopard402\", \"bluegoose191\", \"orangebird528\"…\n## $ login.password    &lt;chr&gt; \"rolex\", \"norton\", \"stevens\", \"303030\", \"aloha\", \"to…\n## $ dob               &lt;chr&gt; \"11/8/73 1:33\", \"5/24/51 3:16\", \"5/22/77 21:03\", \"8/…\n## $ date.registered   &lt;chr&gt; \"5/5/07 20:26\", \"4/11/11 7:05\", \"5/16/14 15:53\", \"2/…\n## $ phone             &lt;chr&gt; \"01-0349-5128\", \"05-72-65-32-21\", \"81616775\", \"974-1…\n## $ cell              &lt;chr&gt; \"0449-989-455\", \"06-83-24-92-41\", \"697-993-20\", \"665…\n## $ picture.large     &lt;chr&gt; \"https://randomuser.me/api/portraits/men/48.jpg\", \"h…\n## $ nat               &lt;chr&gt; \"AU\", \"FR\", \"DK\", \"ES\", \"ES\", \"NZ\", \"DE\", \"US\", \"TR\"…\n\n\nAs for other data structures, you can select and subset data frames using single bracket notation ([ ]). You can also select named columns from a data frame using the $ operator or the equivalent double bracket notation ([[ ]]).\n\n# single bracket notation\ndf[, 4]\n\n##  [1] \"wright\"     \"schmitt\"    \"johansen\"   \"herrero\"    \"blanco\"    \n##  [6] \"wilson\"     \"strauio\"    \"gordon\"     \"limoncuocu\" \"perrin\"    \n## [11] \"lopez\"      \"waisanen\"   \"brewer\"     \"brown\"      \"baettner\"  \n## [16] \"wallace\"    \"gonzalez\"   \"neva\"       \"barnaby\"    \"moser\"\n\nstr(df[, 4])\n\n##  chr [1:20] \"wright\" \"schmitt\" \"johansen\" \"herrero\" \"blanco\" \"wilson\" ...\n\n# returns the vector of data stored in column 4\n\nThe following are all equivalent…\n\n# using the $ operator with the column name\ndf$name.last\n\n##  [1] \"wright\"     \"schmitt\"    \"johansen\"   \"herrero\"    \"blanco\"    \n##  [6] \"wilson\"     \"strauio\"    \"gordon\"     \"limoncuocu\" \"perrin\"    \n## [11] \"lopez\"      \"waisanen\"   \"brewer\"     \"brown\"      \"baettner\"  \n## [16] \"wallace\"    \"gonzalez\"   \"neva\"       \"barnaby\"    \"moser\"\n\nstr(df$name.last)\n\n##  chr [1:20] \"wright\" \"schmitt\" \"johansen\" \"herrero\" \"blanco\" \"wilson\" ...\n\n# returns the vector of data stored in column `name.last`\n\n\n# using double bracket notation and a column index\ndf[[4]]\n\n##  [1] \"wright\"     \"schmitt\"    \"johansen\"   \"herrero\"    \"blanco\"    \n##  [6] \"wilson\"     \"strauio\"    \"gordon\"     \"limoncuocu\" \"perrin\"    \n## [11] \"lopez\"      \"waisanen\"   \"brewer\"     \"brown\"      \"baettner\"  \n## [16] \"wallace\"    \"gonzalez\"   \"neva\"       \"barnaby\"    \"moser\"\n\nstr(df[[4]])\n\n##  chr [1:20] \"wright\" \"schmitt\" \"johansen\" \"herrero\" \"blanco\" \"wilson\" ...\n\n# returns the vector of data stored in column 4\n\n\n# using double bracket notation with the column name\ndf[[\"name.last\"]]\n\n##  [1] \"wright\"     \"schmitt\"    \"johansen\"   \"herrero\"    \"blanco\"    \n##  [6] \"wilson\"     \"strauio\"    \"gordon\"     \"limoncuocu\" \"perrin\"    \n## [11] \"lopez\"      \"waisanen\"   \"brewer\"     \"brown\"      \"baettner\"  \n## [16] \"wallace\"    \"gonzalez\"   \"neva\"       \"barnaby\"    \"moser\"\n\nstr(df[[\"name.last\"]])\n\n##  chr [1:20] \"wright\" \"schmitt\" \"johansen\" \"herrero\" \"blanco\" \"wilson\" ...\n\n# returns the vector of data stored in column `name.last`\n\nNote that the following return data structures that are not quite the same as those returned above. Instead, these return data frames rather than vectors!\n\n# using single bracket notation with a column index and no row index\nhead(df[4])\n\n##   name.last\n## 1    wright\n## 2   schmitt\n## 3  johansen\n## 4   herrero\n## 5    blanco\n## 6    wilson\n\nstr(df[4])\n\n## 'data.frame':    20 obs. of  1 variable:\n##  $ name.last: chr  \"wright\" \"schmitt\" \"johansen\" \"herrero\" ...\n\n# returns a data frame of the data from column 4\n\n\n# using single bracket notation with a column name\nhead(df[\"name.last\"])\n\n##   name.last\n## 1    wright\n## 2   schmitt\n## 3  johansen\n## 4   herrero\n## 5    blanco\n## 6    wilson\n\nstr(df[\"name.last\"])\n\n## 'data.frame':    20 obs. of  1 variable:\n##  $ name.last: chr  \"wright\" \"schmitt\" \"johansen\" \"herrero\" ...\n\n# returns a data frame of the data from column `name.last`\n\nAs with matrixes, you can add rows (additional cases) or columns (additional variables) to a data frame using rbind() and cbind().\n\ndf &lt;- cbind(df, id = c(1:20))\ndf &lt;- cbind(df, school = c(\"UT\", \"UT\", \"A&M\", \"A&M\", \"UT\", \"Rice\", \"Texas Tech\",\n    \"UT\", \"UT\", \"Texas State\", \"A&M\", \"UT\", \"Rice\", \"UT\", \"A&M\", \"Texas Tech\", \"A&M\",\n    \"UT\", \"Texas State\", \"A&M\"))\nhead(df)\n\n##   gender name.title name.first name.last          location.street\n## 1   male         mr        ted    wright            2020 royal ln\n## 2   male         mr    quentin   schmitt          2433 rue dubois\n## 3 female         ms      laura  johansen          2142 elmelunden\n## 4   male         mr     ismael   herrero 3897 calle del barquillo\n## 5 female         ms     susana    blanco  2208 avenida de america\n## 6   male         mr      mason    wilson         4576 wilson road\n##     location.city location.state location.postcode                       email\n## 1   coffs harbour       tasmania              4126      ted.wright@example.com\n## 2 vitry-sur-seine indre-et-loire             99856 quentin.schmitt@example.com\n## 3       silkeboeg    hovedstaden             16264  laura.johansen@example.com\n## 4          gandia          ceuta             61349  ismael.herrero@example.com\n## 5        mastoles    extremadura             29445   susana.blanco@example.com\n## 6         dunedin       taranaki             91479    mason.wilson@example.com\n##      login.username login.password           dob date.registered          phone\n## 1 organicleopard402          rolex  11/8/73 1:33    5/5/07 20:26   01-0349-5128\n## 2      bluegoose191         norton  5/24/51 3:16    4/11/11 7:05 05-72-65-32-21\n## 3     orangebird528        stevens 5/22/77 21:03   5/16/14 15:53       81616775\n## 4      heavyswan518         303030   8/1/58 9:13   2/17/06 16:53    974-117-403\n## 5    silverkoala701          aloha 12/18/55 3:21   10/3/02 17:55    917-199-202\n## 6    organicduck470         topdog  6/23/60 9:19    12/1/08 8:31 (137)-326-5772\n##             cell                                    picture.large nat id school\n## 1   0449-989-455   https://randomuser.me/api/portraits/men/48.jpg  AU  1     UT\n## 2 06-83-24-92-41   https://randomuser.me/api/portraits/men/53.jpg  FR  2     UT\n## 3     697-993-20 https://randomuser.me/api/portraits/women/70.jpg  DK  3    A&M\n## 4    665-791-673   https://randomuser.me/api/portraits/men/79.jpg  ES  4    A&M\n## 5    612-612-929 https://randomuser.me/api/portraits/women/18.jpg  ES  5     UT\n## 6 (700)-060-1523   https://randomuser.me/api/portraits/men/60.jpg  NZ  6   Rice\n\n\nAlternatively, you can extend a data frame by adding a new variable directly using the $ operator, like this:\n\ndf$school &lt;- c(\"UT\", \"UT\", \"A&M\", \"A&M\", \"UT\", \"Rice\", \"Texas Tech\", \"UT\", \"UT\",\n    \"Texas State\", \"A&M\", \"UT\", \"Rice\", \"UT\", \"A&M\", \"Texas Tech\", \"A&M\", \"UT\", \"Texas State\",\n    \"A&M\")\nhead(df)\n\n##   gender name.title name.first name.last          location.street\n## 1   male         mr        ted    wright            2020 royal ln\n## 2   male         mr    quentin   schmitt          2433 rue dubois\n## 3 female         ms      laura  johansen          2142 elmelunden\n## 4   male         mr     ismael   herrero 3897 calle del barquillo\n## 5 female         ms     susana    blanco  2208 avenida de america\n## 6   male         mr      mason    wilson         4576 wilson road\n##     location.city location.state location.postcode                       email\n## 1   coffs harbour       tasmania              4126      ted.wright@example.com\n## 2 vitry-sur-seine indre-et-loire             99856 quentin.schmitt@example.com\n## 3       silkeboeg    hovedstaden             16264  laura.johansen@example.com\n## 4          gandia          ceuta             61349  ismael.herrero@example.com\n## 5        mastoles    extremadura             29445   susana.blanco@example.com\n## 6         dunedin       taranaki             91479    mason.wilson@example.com\n##      login.username login.password           dob date.registered          phone\n## 1 organicleopard402          rolex  11/8/73 1:33    5/5/07 20:26   01-0349-5128\n## 2      bluegoose191         norton  5/24/51 3:16    4/11/11 7:05 05-72-65-32-21\n## 3     orangebird528        stevens 5/22/77 21:03   5/16/14 15:53       81616775\n## 4      heavyswan518         303030   8/1/58 9:13   2/17/06 16:53    974-117-403\n## 5    silverkoala701          aloha 12/18/55 3:21   10/3/02 17:55    917-199-202\n## 6    organicduck470         topdog  6/23/60 9:19    12/1/08 8:31 (137)-326-5772\n##             cell                                    picture.large nat id school\n## 1   0449-989-455   https://randomuser.me/api/portraits/men/48.jpg  AU  1     UT\n## 2 06-83-24-92-41   https://randomuser.me/api/portraits/men/53.jpg  FR  2     UT\n## 3     697-993-20 https://randomuser.me/api/portraits/women/70.jpg  DK  3    A&M\n## 4    665-791-673   https://randomuser.me/api/portraits/men/79.jpg  ES  4    A&M\n## 5    612-612-929 https://randomuser.me/api/portraits/women/18.jpg  ES  5     UT\n## 6 (700)-060-1523   https://randomuser.me/api/portraits/men/60.jpg  NZ  6   Rice\n\n\nUsing the [[ ]] operator with a new variable name in quotation marks works, too:\n\ndf[[\"school\"]] &lt;- c(\"UT\", \"UT\", \"A&M\", \"A&M\", \"UT\", \"Rice\", \"Texas Tech\", \"UT\", \"UT\",\n    \"Texas State\", \"A&M\", \"UT\", \"Rice\", \"UT\", \"A&M\", \"Texas Tech\", \"A&M\", \"UT\", \"Texas State\",\n    \"A&M\")\nhead(df)\n\n##   gender name.title name.first name.last          location.street\n## 1   male         mr        ted    wright            2020 royal ln\n## 2   male         mr    quentin   schmitt          2433 rue dubois\n## 3 female         ms      laura  johansen          2142 elmelunden\n## 4   male         mr     ismael   herrero 3897 calle del barquillo\n## 5 female         ms     susana    blanco  2208 avenida de america\n## 6   male         mr      mason    wilson         4576 wilson road\n##     location.city location.state location.postcode                       email\n## 1   coffs harbour       tasmania              4126      ted.wright@example.com\n## 2 vitry-sur-seine indre-et-loire             99856 quentin.schmitt@example.com\n## 3       silkeboeg    hovedstaden             16264  laura.johansen@example.com\n## 4          gandia          ceuta             61349  ismael.herrero@example.com\n## 5        mastoles    extremadura             29445   susana.blanco@example.com\n## 6         dunedin       taranaki             91479    mason.wilson@example.com\n##      login.username login.password           dob date.registered          phone\n## 1 organicleopard402          rolex  11/8/73 1:33    5/5/07 20:26   01-0349-5128\n## 2      bluegoose191         norton  5/24/51 3:16    4/11/11 7:05 05-72-65-32-21\n## 3     orangebird528        stevens 5/22/77 21:03   5/16/14 15:53       81616775\n## 4      heavyswan518         303030   8/1/58 9:13   2/17/06 16:53    974-117-403\n## 5    silverkoala701          aloha 12/18/55 3:21   10/3/02 17:55    917-199-202\n## 6    organicduck470         topdog  6/23/60 9:19    12/1/08 8:31 (137)-326-5772\n##             cell                                    picture.large nat id school\n## 1   0449-989-455   https://randomuser.me/api/portraits/men/48.jpg  AU  1     UT\n## 2 06-83-24-92-41   https://randomuser.me/api/portraits/men/53.jpg  FR  2     UT\n## 3     697-993-20 https://randomuser.me/api/portraits/women/70.jpg  DK  3    A&M\n## 4    665-791-673   https://randomuser.me/api/portraits/men/79.jpg  ES  4    A&M\n## 5    612-612-929 https://randomuser.me/api/portraits/women/18.jpg  ES  5     UT\n## 6 (700)-060-1523   https://randomuser.me/api/portraits/men/60.jpg  NZ  6   Rice\n\n\n\nNOTE: In the above examples, cbind() results in school being added as a factor while using the $ operator results in school being added as a character vector. You can see this by using the str() command.\n\nA factor is another atomic data class for R for dealing efficiently with nominal variables, usually character strings. Internally, R assigns integer values to each unique string (e.g., 1 for “female”, 2 for “male”, etc.).\n\n\nFiltering Rows of a Data Frame\nAn expression that evaluates to a logical vector also be used to subset data frames. Here, we filter the data frame for only those rows where the variable school is “UT”.\n\nnew_df &lt;- df[df$school == \"UT\", ]\nnew_df\n\n##    gender name.title name.first  name.last         location.street\n## 1    male         mr        ted     wright           2020 royal ln\n## 2    male         mr    quentin    schmitt         2433 rue dubois\n## 5  female         ms     susana     blanco 2208 avenida de america\n## 8  female       miss     kaylee     gordon         5475 camden ave\n## 9    male         mr     baraek limoncuocu          2664 baedat cd\n## 12   male         mr   valtteri   waisanen          9850 hemeentie\n## 14 female       miss   kimberly      brown         8654 manor road\n## 18 female         ms       ella       neva          4620 visiokatu\n##      location.city location.state location.postcode\n## 1    coffs harbour       tasmania              4126\n## 2  vitry-sur-seine indre-et-loire             99856\n## 5         mastoles    extremadura             29445\n## 8            flint         oregon             84509\n## 9            siirt          tokat             86146\n## 12          halsua  south karelia             58124\n## 14          bangor        borders          HI92 8RY\n## 18          kerava finland proper             26385\n##                              email    login.username login.password\n## 1           ted.wright@example.com organicleopard402          rolex\n## 2      quentin.schmitt@example.com      bluegoose191         norton\n## 5        susana.blanco@example.com    silverkoala701          aloha\n## 8        kaylee.gordon@example.com beautifulgoose794       atlantis\n## 9  baraek.limoncuoculu@example.com whitebutterfly599         tobias\n## 12   valtteri.waisanen@example.com        redswan919       nocturne\n## 14      kimberly.brown@example.com  crazyelephant996       nebraska\n## 18           ella.neva@example.com  orangegorilla786       f00tball\n##               dob date.registered          phone           cell\n## 1    11/8/73 1:33    5/5/07 20:26   01-0349-5128   0449-989-455\n## 2    5/24/51 3:16    4/11/11 7:05 05-72-65-32-21 06-83-24-92-41\n## 5   12/18/55 3:21   10/3/02 17:55    917-199-202    612-612-929\n## 8   3/24/48 12:22     5/5/13 8:14 (817)-962-1275 (831)-325-1142\n## 9    5/8/92 22:01    9/12/04 0:56 (023)-879-4331 (837)-014-1113\n## 12 12/24/80 10:40   9/22/03 20:47     02-227-661  042-153-83-79\n## 14    1/9/86 8:54    12/3/11 0:41   017684 80873   0799-553-944\n## 18  7/18/91 14:30    3/17/14 7:13     02-351-279  043-436-42-30\n##                                       picture.large nat id school\n## 1    https://randomuser.me/api/portraits/men/48.jpg  AU  1     UT\n## 2    https://randomuser.me/api/portraits/men/53.jpg  FR  2     UT\n## 5  https://randomuser.me/api/portraits/women/18.jpg  ES  5     UT\n## 8  https://randomuser.me/api/portraits/women/65.jpg  US  8     UT\n## 9    https://randomuser.me/api/portraits/men/94.jpg  TR  9     UT\n## 12   https://randomuser.me/api/portraits/men/80.jpg  FI 12     UT\n## 14 https://randomuser.me/api/portraits/women/49.jpg  GB 14     UT\n## 18 https://randomuser.me/api/portraits/women/68.jpg  FI 18     UT\n\n\nIn this case, R evaluates the expression df$school == \"UT\" and returns a logical vector equal in length to the number of rows in df.\n\ndf$school == \"UT\"\n\n##  [1]  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE  TRUE\n## [13] FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE\n\n\nIt then subsets the original df based on that vector, returning only rows that evaluate to “TRUE”.\nThe Boolean operators & (for “AND”) and | (for “OR”) can be used to create more complex filtering criteria. Here, we filter the data frame for only those rows where the variable school is “UT” AND the variable gender is “female”.\n\nnew_df &lt;- df[df$school == \"UT\" & df$gender == \"female\", ]\nnew_df\n\n##    gender name.title name.first name.last         location.street location.city\n## 5  female         ms     susana    blanco 2208 avenida de america      mastoles\n## 8  female       miss     kaylee    gordon         5475 camden ave         flint\n## 14 female       miss   kimberly     brown         8654 manor road        bangor\n## 18 female         ms       ella      neva          4620 visiokatu        kerava\n##    location.state location.postcode                      email\n## 5     extremadura             29445  susana.blanco@example.com\n## 8          oregon             84509  kaylee.gordon@example.com\n## 14        borders          HI92 8RY kimberly.brown@example.com\n## 18 finland proper             26385      ella.neva@example.com\n##       login.username login.password           dob date.registered\n## 5     silverkoala701          aloha 12/18/55 3:21   10/3/02 17:55\n## 8  beautifulgoose794       atlantis 3/24/48 12:22     5/5/13 8:14\n## 14  crazyelephant996       nebraska   1/9/86 8:54    12/3/11 0:41\n## 18  orangegorilla786       f00tball 7/18/91 14:30    3/17/14 7:13\n##             phone           cell\n## 5     917-199-202    612-612-929\n## 8  (817)-962-1275 (831)-325-1142\n## 14   017684 80873   0799-553-944\n## 18     02-351-279  043-436-42-30\n##                                       picture.large nat id school\n## 5  https://randomuser.me/api/portraits/women/18.jpg  ES  5     UT\n## 8  https://randomuser.me/api/portraits/women/65.jpg  US  8     UT\n## 14 https://randomuser.me/api/portraits/women/49.jpg  GB 14     UT\n## 18 https://randomuser.me/api/portraits/women/68.jpg  FI 18     UT\n\n\nHere, we filter the data frame for only rows where either the school is “UT” OR the variable gender is “female”, using the | operator. We also select only the columns gender, name.first, and name.last by passing a vector to the second argument of the [ ] function.\n\nnew_df &lt;- df[df$school == \"UT\" | df$gender == \"female\", c(\"gender\", \"name.first\",\n    \"name.last\")]\nnew_df\n\n##    gender name.first  name.last\n## 1    male        ted     wright\n## 2    male    quentin    schmitt\n## 3  female      laura   johansen\n## 5  female     susana     blanco\n## 8  female     kaylee     gordon\n## 9    male     baraek limoncuocu\n## 12   male   valtteri   waisanen\n## 13 female    vanessa     brewer\n## 14 female   kimberly      brown\n## 15 female     loreen   baettner\n## 16 female      becky    wallace\n## 18 female       ella       neva\n\n\n\n\nSelecting Columns of a Data Frame\nWe can also select to only return particular columns when we filter. Here, we return only the columns name.last, name.first, and school.\n\nnew_df &lt;- df[df$school == \"UT\", c(\"name.last\", \"name.first\", \"school\")]\nnew_df\n\n##     name.last name.first school\n## 1      wright        ted     UT\n## 2     schmitt    quentin     UT\n## 5      blanco     susana     UT\n## 8      gordon     kaylee     UT\n## 9  limoncuocu     baraek     UT\n## 12   waisanen   valtteri     UT\n## 14      brown   kimberly     UT\n## 18       neva       ella     UT\n\n\nHere, we return all rows from the data frame, but only the “name.last”, “name.first”, and “school” columns.\n\nnew_df &lt;- df[, c(\"name.last\", \"name.first\", \"school\")]\nnew_df\n\n##     name.last name.first      school\n## 1      wright        ted          UT\n## 2     schmitt    quentin          UT\n## 3    johansen      laura         A&M\n## 4     herrero     ismael         A&M\n## 5      blanco     susana          UT\n## 6      wilson      mason        Rice\n## 7     strauio       lutz  Texas Tech\n## 8      gordon     kaylee          UT\n## 9  limoncuocu     baraek          UT\n## 10     perrin     basile Texas State\n## 11      lopez      ruben         A&M\n## 12   waisanen   valtteri          UT\n## 13     brewer    vanessa        Rice\n## 14      brown   kimberly          UT\n## 15   baettner     loreen         A&M\n## 16    wallace      becky  Texas Tech\n## 17   gonzalez     hector         A&M\n## 18       neva       ella          UT\n## 19    barnaby      simon Texas State\n## 20      moser        max         A&M\n\n\nWe can also refer to columns by their positions and return them in a select order, thereby restructuring the data frame. Here, we return all rows from the data frame, but include only columns 1, 3, and 4, flipping the order of the latter two columns:\n\nnew_df &lt;- df[, c(1, 4, 3)]\nnew_df\n\n##    gender  name.last name.first\n## 1    male     wright        ted\n## 2    male    schmitt    quentin\n## 3  female   johansen      laura\n## 4    male    herrero     ismael\n## 5  female     blanco     susana\n## 6    male     wilson      mason\n## 7    male    strauio       lutz\n## 8  female     gordon     kaylee\n## 9    male limoncuocu     baraek\n## 10   male     perrin     basile\n## 11   male      lopez      ruben\n## 12   male   waisanen   valtteri\n## 13 female     brewer    vanessa\n## 14 female      brown   kimberly\n## 15 female   baettner     loreen\n## 16 female    wallace      becky\n## 17   male   gonzalez     hector\n## 18 female       neva       ella\n## 19   male    barnaby      simon\n## 20   male      moser        max\n\n\nWe can use a minus sign - in front of a vector of indices to instead indicate columns we do not want to return:\n\nnew_df &lt;- df[, -c(1, 2, 5:18)]\nnew_df\n\n##    name.first  name.last      school\n## 1         ted     wright          UT\n## 2     quentin    schmitt          UT\n## 3       laura   johansen         A&M\n## 4      ismael    herrero         A&M\n## 5      susana     blanco          UT\n## 6       mason     wilson        Rice\n## 7        lutz    strauio  Texas Tech\n## 8      kaylee     gordon          UT\n## 9      baraek limoncuocu          UT\n## 10     basile     perrin Texas State\n## 11      ruben      lopez         A&M\n## 12   valtteri   waisanen          UT\n## 13    vanessa     brewer        Rice\n## 14   kimberly      brown          UT\n## 15     loreen   baettner         A&M\n## 16      becky    wallace  Texas Tech\n## 17     hector   gonzalez         A&M\n## 18       ella       neva          UT\n## 19      simon    barnaby Texas State\n## 20        max      moser         A&M",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Additional Data Structures in ***R***</span>"
    ]
  },
  {
    "objectID": "07-module.html#factors",
    "href": "07-module.html#factors",
    "title": "7  Additional Data Structures in R",
    "section": "7.5 Factors",
    "text": "7.5 Factors\nWe were introduced to the factor data class above. Again, factors are numeric codes that R can use internally that correspond to character value “levels”.\nWhen we load in data from an external source (as we do in Module 08), {base} R tends to import character string data as factors, assigning to each unique string to an integer numeric code and assigning the string as a “label” for that code. Using factors can make some code run much more quickly (e.g., ANOVA, ANCOVA, and other forms of regression using categorical variables).",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Additional Data Structures in ***R***</span>"
    ]
  },
  {
    "objectID": "07-module.html#variable-conversion-and-coercion",
    "href": "07-module.html#variable-conversion-and-coercion",
    "title": "7  Additional Data Structures in R",
    "section": "7.6 Variable Conversion and Coercion",
    "text": "7.6 Variable Conversion and Coercion\nYou can convert factor to character data (and vice versa) using the as.character() or as.factor() commands. You can also convert/coerce any vector to a different class using similar constructs (e.g., as.numeric()), although not all such conversions are really meaningful. Converting factor data to numeric results in the the converted data having the value of R’s internal numeric code for the factor level, while converting character data to numeric results in the data being coerced into the special data value of NA (see below) for missing data.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Additional Data Structures in ***R***</span>"
    ]
  },
  {
    "objectID": "07-module.html#special-data-values",
    "href": "07-module.html#special-data-values",
    "title": "7  Additional Data Structures in R",
    "section": "7.7 Special Data Values",
    "text": "7.7 Special Data Values\nFinally, R has three special data values that it uses in a variety of situations.\n\nNA (for not available) is used for missing data. Many statistical functions offer the possibility to include as an argument na.rm=TRUE (“remove NAs”) so that NAs are excluded from a calculation.\nInf (and -Inf) is used when the result of a numerical calculation is too extreme for R to express\nNaN (for not a number) is used when R cannot express the results of a calculation , e.g., when you try to take the square root of a negative number\n\n\nCHALLENGE\n\nStore the following vector of numbers as a 5 x 3 matrix: 3, 0, 1 ,23, 1, 2, 33, 1, 1, 42, 0, 1, 41, 0, 2\nBe sure to fill the matrix ROWWISE\n\n\n\nShow Code\nm &lt;- matrix(c(3, 0, 1, 23, 1, 2, 33, 1, 1, 42, 0, 1, 41, 0, 2), nrow = 5, ncol = 3,\n    byrow = TRUE)\nm\n\n\nShow Output\n##      [,1] [,2] [,3]\n## [1,]    3    0    1\n## [2,]   23    1    2\n## [3,]   33    1    1\n## [4,]   42    0    1\n## [5,]   41    0    2\n\n\n\n\nThen, coerce the matrix to a data frame\n\n\n\nShow Code\n(d &lt;- as.data.frame(m))\n\n\nShow Output\n##   V1 V2 V3\n## 1  3  0  1\n## 2 23  1  2\n## 3 33  1  1\n## 4 42  0  1\n## 5 41  0  2\n\n\n\n\nAs a data frame, coerce the second column to be logical (i.e., Boolean)\n\n\n\nShow Code\n(d[, 2] &lt;- as.logical(d[, 2]))\n\n\nShow Output\n## [1] FALSE  TRUE  TRUE FALSE FALSE\n\n\n\n\nAs a data frame, coerce the third column to be a factor\n\n\n\nShow Code\n(d[, 3] &lt;- as.factor(d[, 3]))\n\n\nShow Output\n## [1] 1 2 1 1 2\n## Levels: 1 2\n\n\n\n\nWhen you are done, use the str() command to show the data type for each variable in your dataframe.\n\n\nstr(d)\n\n## 'data.frame':    5 obs. of  3 variables:\n##  $ V1: num  3 23 33 42 41\n##  $ V2: logi  FALSE TRUE TRUE FALSE FALSE\n##  $ V3: Factor w/ 2 levels \"1\",\"2\": 1 2 1 1 2",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Additional Data Structures in ***R***</span>"
    ]
  },
  {
    "objectID": "07-module.html#other-data-frame-like-structures",
    "href": "07-module.html#other-data-frame-like-structures",
    "title": "7  Additional Data Structures in R",
    "section": "7.8 Other Data Frame-Like Structures",
    "text": "7.8 Other Data Frame-Like Structures\n\nData Tables\nA “data table” is a structure introduced in the package {data.table} that provides an enhancements to the data frame structure, which is the standard data structure for storing tabular data in {base} R. We use the same syntax for creating a data table…\n\ndt &lt;- data.table(firstName = c(\"Rick\", \"Negan\", \"Dwight\", \"Maggie\", \"Michonne\"),\n    community = c(\"Alexandria\", \"Saviors\", \"Saviors\", \"Hiltop\", \"Alexandria\"), sex = c(\"M\",\n        \"M\", \"M\", \"F\", \"F\"), age = c(42, 40, 33, 28, 31))\ndt\n\n##    firstName  community    sex   age\n##       &lt;char&gt;     &lt;char&gt; &lt;char&gt; &lt;num&gt;\n## 1:      Rick Alexandria      M    42\n## 2:     Negan    Saviors      M    40\n## 3:    Dwight    Saviors      M    33\n## 4:    Maggie     Hiltop      F    28\n## 5:  Michonne Alexandria      F    31\n\nstr(dt)\n\n## Classes 'data.table' and 'data.frame':   5 obs. of  4 variables:\n##  $ firstName: chr  \"Rick\" \"Negan\" \"Dwight\" \"Maggie\" ...\n##  $ community: chr  \"Alexandria\" \"Saviors\" \"Saviors\" \"Hiltop\" ...\n##  $ sex      : chr  \"M\" \"M\" \"M\" \"F\" ...\n##  $ age      : num  42 40 33 28 31\n##  - attr(*, \".internal.selfref\")=&lt;externalptr&gt;\n\n# versus...\n\ndf &lt;- data.frame(firstName = c(\"Rick\", \"Negan\", \"Dwight\", \"Maggie\", \"Michonne\"),\n    community = c(\"Alexandria\", \"Saviors\", \"Saviors\", \"Hiltop\", \"Alexandria\"), sex = c(\"M\",\n        \"M\", \"M\", \"F\", \"F\"), age = c(42, 40, 33, 28, 31))\ndf\n\n##   firstName  community sex age\n## 1      Rick Alexandria   M  42\n## 2     Negan    Saviors   M  40\n## 3    Dwight    Saviors   M  33\n## 4    Maggie     Hiltop   F  28\n## 5  Michonne Alexandria   F  31\n\nstr(df)\n\n## 'data.frame':    5 obs. of  4 variables:\n##  $ firstName: chr  \"Rick\" \"Negan\" \"Dwight\" \"Maggie\" ...\n##  $ community: chr  \"Alexandria\" \"Saviors\" \"Saviors\" \"Hiltop\" ...\n##  $ sex      : chr  \"M\" \"M\" \"M\" \"F\" ...\n##  $ age      : num  42 40 33 28 31\n\n\nNote that printing a data table results in a slightly different output than printing a data frame (e.g., row numbers are printed followed by a “:”) and the structure (str()) looks a bit different. Also, different from data frames, when we read in data, columns of character type are never converted to factors by default (i.e., we do not need to specify anything like stringsAsFactors=FALSE when we read in data… that’s the opposite default as we see for data frames).\nThe big advantage of using data tables over data frames is that they support a different, easier syntax for filtering rows and selecting columns and for grouping output.\n\ndt[sex == \"M\"]  # filter for sex = 'M' in a data table\n\n##    firstName  community    sex   age\n##       &lt;char&gt;     &lt;char&gt; &lt;char&gt; &lt;num&gt;\n## 1:      Rick Alexandria      M    42\n## 2:     Negan    Saviors      M    40\n## 3:    Dwight    Saviors      M    33\n\ndf[df$sex == \"M\", ]  # filter for sex = 'M' in a data frame\n\n##   firstName  community sex age\n## 1      Rick Alexandria   M  42\n## 2     Negan    Saviors   M  40\n## 3    Dwight    Saviors   M  33\n\ndt[1:2]  # return the first two rows of the data table\n\n##    firstName  community    sex   age\n##       &lt;char&gt;     &lt;char&gt; &lt;char&gt; &lt;num&gt;\n## 1:      Rick Alexandria      M    42\n## 2:     Negan    Saviors      M    40\n\ndf[1:2, ]  # return the first two rows of the data table\n\n##   firstName  community sex age\n## 1      Rick Alexandria   M  42\n## 2     Negan    Saviors   M  40\n\ndt[, sex]  # returns the variable 'sex'\n\n## [1] \"M\" \"M\" \"M\" \"F\" \"F\"\n\nstr(dt[, sex])  # sex is a CHARACTER vector\n\n##  chr [1:5] \"M\" \"M\" \"M\" \"F\" \"F\"\n\ndf[, c(\"sex\")]  # returns the variable 'sex'\n\n## [1] \"M\" \"M\" \"M\" \"F\" \"F\"\n\nstr(df[, c(\"sex\")])  # sex is a FACTOR with 2 levels\n\n##  chr [1:5] \"M\" \"M\" \"M\" \"F\" \"F\"\n\n\nThe data table structure also allows us more straightforward syntax - and implements much faster algorithms - for computations on columns and for perform aggregations of data by a grouping variable.\n\n\n“Tibbles”\nA “tibble” is another newer take on the data frame structure, implemented in the package {tibble} (which is loaded as part of {tidyverse}). The structure was created to keep the best features of data frames and correct some of the more frustrating aspects associated with the older structure. For example, like data tables, tibbles do not by default change the input type of a variable from character to factor when the tibble is created.\n\nt &lt;- tibble(firstName = c(\"Rick\", \"Negan\", \"Dwight\", \"Maggie\", \"Michonne\"), community = c(\"Alexandria\",\n    \"Saviors\", \"Saviors\", \"Hiltop\", \"Alexandria\"), sex = c(\"M\", \"M\", \"M\", \"F\", \"F\"),\n    age = c(42, 40, 33, 28, 31))\nt\n\n## # A tibble: 5 × 4\n##   firstName community  sex     age\n##   &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt; &lt;dbl&gt;\n## 1 Rick      Alexandria M        42\n## 2 Negan     Saviors    M        40\n## 3 Dwight    Saviors    M        33\n## 4 Maggie    Hiltop     F        28\n## 5 Michonne  Alexandria F        31\n\nstr(t)\n\n## tibble [5 × 4] (S3: tbl_df/tbl/data.frame)\n##  $ firstName: chr [1:5] \"Rick\" \"Negan\" \"Dwight\" \"Maggie\" ...\n##  $ community: chr [1:5] \"Alexandria\" \"Saviors\" \"Saviors\" \"Hiltop\" ...\n##  $ sex      : chr [1:5] \"M\" \"M\" \"M\" \"F\" ...\n##  $ age      : num [1:5] 42 40 33 28 31\n\n\nNote that the output of printing a tibble again looks slightly different than that for data frames or data tables… e.g., the data type of each column is included in the header row, for example. str() also shows us that characters were not converted to factors.\nAdditionally, subsetting tibbles with the single bracket operator ([ ]) always returns a tibble, whereas doing the same with a data frame can return either a data frame or a vector.\n\nt[, \"age\"]\n\n## # A tibble: 5 × 1\n##     age\n##   &lt;dbl&gt;\n## 1    42\n## 2    40\n## 3    33\n## 4    28\n## 5    31\n\nclass(t[, \"age\"])\n\n## [1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\ndf[, \"age\"]\n\n## [1] 42 40 33 28 31\n\nclass(df[, \"age\"])\n\n## [1] \"numeric\"\n\n\nThere are some other subtle differences regarding the behavior of tibbles versus data frames that are also worthwhile to note. Data frames support “partial matching” in variable names when the $ operator is used, thus df$a will return the variable df$age. Tibbles are stricter and will never do partial matching.\n\ndf$a  # returns df$age\n\n## [1] 42 40 33 28 31\n\nt$a  # returns NULL and gives a warning\n\n## Warning: Unknown or uninitialised column: `a`.\n\n\n## NULL\n\n\nFinally, tibbles are careful about recycling. When creating a tibble, columns have to have consistent lengths and only values of length 1 are recycled. Thus, in the following…\n\nt &lt;- tibble(a = 1:4, c = 1)\n# this works fine... c is recycled\nt\n\n## # A tibble: 4 × 2\n##       a     c\n##   &lt;int&gt; &lt;dbl&gt;\n## 1     1     1\n## 2     2     1\n## 3     3     1\n## 4     4     1\n\n# t &lt;- tibble(a=1:4, c=1:2) but this would throw an error... c is not recycled\n# even though it could fit evenly into the number of rows\n\ndf &lt;- data.frame(a = 1:4, c = 1:2)\ndf\n\n##   a c\n## 1 1 1\n## 2 2 2\n## 3 3 1\n## 4 4 2\n\n\n\n# | include: false\ndetach(package:tidyverse)\ndetach(package:data.table)",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Additional Data Structures in ***R***</span>"
    ]
  },
  {
    "objectID": "07-module.html#concept-review",
    "href": "07-module.html#concept-review",
    "title": "7  Additional Data Structures in R",
    "section": "Concept Review",
    "text": "Concept Review\n\nCreating matrices and arrays: matrix(data=, nrow=, ncol=, byrow=), array(data=, dim=), rbind(), cbind()\nCreating lists: list()\nCreating data frames: data.frame()\nSubsetting: single bracket ([ ]), double bracket ([[ ]]), and $ notation\nVariable coercion: as.numeric(), as.character(), as.data.frame()\nReading .csv data: read.csv(file=, header=, stringsAsFactors=)\nFiltering rows of a data frame using {base} R: df[df$&lt;variable name&gt; == \"&lt;criterion&gt;\", ], df[df[[\"&lt;variable name&gt;\"]] == \"&lt;criterion&gt;\", ]\nSelecting/excluding columns of a data frame: df[ , c(\"&lt;variable name 1&gt;\", \"&lt;variable name 2&gt;\",...)],df[ , c(&lt;column index 1&gt;, &lt;column index 2&gt;,...)]",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Additional Data Structures in ***R***</span>"
    ]
  },
  {
    "objectID": "08-module.html",
    "href": "08-module.html",
    "title": "8  Getting Data into R",
    "section": "",
    "text": "8.1 Objectives",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Getting Data into ***R***</span>"
    ]
  },
  {
    "objectID": "08-module.html#objectives",
    "href": "08-module.html#objectives",
    "title": "8  Getting Data into R",
    "section": "",
    "text": "The objective of this module to learn how to download data sets from various local and online sources.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Getting Data into ***R***</span>"
    ]
  },
  {
    "objectID": "08-module.html#preliminaries",
    "href": "08-module.html#preliminaries",
    "title": "8  Getting Data into R",
    "section": "8.2 Preliminaries",
    "text": "8.2 Preliminaries\n\nGO TO: https://github.com/difiore/ada-datasets, download the “.txt” and “.csv” versions of “Country-Data-2016”, and save them locally.\n\nThis data set consists of basic statistics (area, current population size, birth rate, death rate, life expectancy, and form of government) for 249 countries taken from WorldData.info that I have combined with data from the International Union for the Conservation of Nature (IUCN)’s Red List Summary Statistics about the number of threatened animal species by country.\n\nFrom the same page, download each of the “.txt”, “.csv”, and “.xlsx” versions of “CPDS-1960-2014-reduced”.\n\nThese files contain a version of the Comparative Political Data Set (CPDS), which is “a collection of political and institutional country-level data provided by Prof. Dr. Klaus Armingeon and collaborators at the University of Berne. The dataset consists of annual data for 36 democratic countries for the period of 1960 to 2014 or since their transition to democracy” (Armingeon et al. 2016). The full dataset consists of 300 variables, which I have pared down to a smaller set of economical and population size variables.\n\nCITATION: Armingeon K, Isler C, Knöpfel L, Weisstanner D, and Engler S. 2016. Comparative Political Data Set 1960-2014. Bern: Institute of Political Science, University of Berne.\n\n\nInstall these packages in R: {readxl}, {XLConnect}, {gdata}, {xlsx}, {curl}, {rdrop2}, {repmis}, {googlesheets4}, and {googledrive}\nLoad {tidyverse}\n\n\nNOTE: Some of these packages (e.g., {XLConnect}, {xlsx}) require that your computer has a Java Runtime Environment (JRE) installed. A JRE is a bundle of software contaiing an interpreter and compiler for Java code, which is used to implement some of the functionality in the package. If your computer does not already have a JRE installed, you will need to also install one before being able to use these packages. You can download a JRE from Oracle’s Java website.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Getting Data into ***R***</span>"
    ]
  },
  {
    "objectID": "08-module.html#the-tao-of-text",
    "href": "08-module.html#the-tao-of-text",
    "title": "8  Getting Data into R",
    "section": "8.3 The Tao of Text",
    "text": "8.3 The Tao of Text\nSo far, we have seen how to create a variety of data structures by hand (e.g., using the c() function), but for larger data sets we will need mechanisms to import data into R. There are many methods for importing tabular data, stored in various formats (like text files, spreadsheets, and databases).\nPlain text files are, arguably, the very best way to store data (and scripts and other documents) as they are a standard format that has been around longer than most operating systems and are unlikely to change anytime soon. Some of the benefits of using text files are listed below:\n\nPlain text does not have a version and does not age.\nPlain text files are platform and software agnostic.\nPlain text files can be opened by a wide variety of programs.\nPlain text can easily be copied and pasted into a wide range of software.\nPlain text files tend to be smaller and quicker to open then proprietary formats.\nPlain text files are easy to transmit over the web.\nMany mature and efficient software tools exist for indexing, parsing, searching, and modifying text.\nThe content of plain text files looks the same on any system.\nVarious flavors of Markdown can be used for styling plain text files, if needed.\nPlain text remains itself outside of the digital context.\nCAVEAT: With text, we do have to think about the sometimes gnarly issue of text encoding. See this article or this pdf for a nice overview of the issues.\n\n\nTL/DR: Work with UTF-8 encoding whenever you can!",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Getting Data into ***R***</span>"
    ]
  },
  {
    "objectID": "08-module.html#working-with-local-files",
    "href": "08-module.html#working-with-local-files",
    "title": "8  Getting Data into R",
    "section": "8.4 Working with Local Files",
    "text": "8.4 Working with Local Files\n\nSetting the Path to a File\nThe file.choose() command is a useful command for interactive engagement with R. It gives you a familiar operating system-specific dialog box and allows you to select a file. You can use this to specify the path to a locally-stored file. The code below will assign the variable f to the full path to the file you choose.\n\nf &lt;- file.choose()\n\nAlternatively, you can directly assign a variable, e.g., f, to be the path to a locally-stored file. The file paths below refer to where I have saved the data files I downloaded - in a folder called data/ within my working directory. You may need to change this path if you have saved downloaded data to a different location on your computer.\n\n\nLoading Data from Text Files\nIn R, we can load a data set from a several types of plain text file stored on a local computer using the read.table() function from the {base} package, with the path to the file as the first (file=\"&lt;path&gt;\") argument for the function. An additional argument (header=&lt;boolean&gt;) can be used to specify whether the first row of the data file consists of column/field names.\nThe generic read.table() function can be used to read data files where columns are separated by tabs, commas, white space, or some other delimiter. The sep= argument tells R what character is used as a delimiter. The skip= argument can be used to start reading a file after a set number of rows.\nThere are format-specific variants of read.table() (e.g., read.csv()) that have different defaults and may be quicker for certain file types. Note that, as mentioned in Module 07, when using this function from the {base} package, the argument stringsAsFactors= is set to be TRUE by default, and we need to set it as FALSE if we want character strings to be loaded as actual strings.\nAs an example, we will read in some of the data sets that you have copied and stored locally in the files “CPDS-1960-2014-reduced.csv” and “CPDS-1960-2014-reduced.txt”.\n\nReading comma-separated (“.csv”) text files with {base} R\n\nf &lt;- \"data/CPDS-1960-2014-reduced.csv\"\nd &lt;- read.table(f, header = TRUE, sep = \",\", stringsAsFactors = FALSE)\nhead(d)  # shows the first 6 lines of data\n\n##   year   country gov_right1 gov_cent1 gov_left1 gov_party      elect vturn\n## 1 1960 Australia        100         0         0         1             95.5\n## 2 1961 Australia        100         0         0         1 09/12/1961  95.3\n## 3 1962 Australia        100         0         0         1             95.3\n## 4 1963 Australia        100         0         0         1 30/11/1963  95.7\n## 5 1964 Australia        100         0         0         1             95.7\n## 6 1965 Australia        100         0         0         1             95.7\n##   womenpar realgdpgr inflation debt_hist deficit ttl_labf labfopar unemp\n## 1        0        NA      3.73     40.15    0.46  4215.00       NA  1.25\n## 2        0     -0.64      2.29     38.62   -0.36  4286.00       NA  2.46\n## 3        0      5.77     -0.32     38.75   -0.79  4382.00       NA  2.32\n## 4        0      6.01      0.64     37.34   -0.51  4484.00       NA  1.87\n## 5        0      6.26      2.87     35.31   -0.08  4610.80    67.24  1.45\n## 6        0      4.99      3.41     53.99   -0.73  4745.95    67.66  1.36\n##       pop pop15_64 pop65 elderly\n## 1 10275.0   6296.5 874.9    8.51\n## 2 10508.2   6428.6 894.6    8.51\n## 3 10700.5   6571.5 913.6    8.54\n## 4 10906.9   6710.9 933.0    8.55\n## 5 11121.6   6857.3 948.1    8.52\n## 6 11340.9   7014.6 966.3    8.52\n\n\n\nNOTE: You can use a second argument to the head() function to return a specified number of lines, e.g., head(d, 10). You can also use bracket notation to display a certain range of lines, e.g., head(d)[11:20].\n\nOr, alternatively…\n\nd &lt;- read.csv(f, header = TRUE, stringsAsFactors = FALSE)\nhead(d)\n\n##   year   country gov_right1 gov_cent1 gov_left1 gov_party      elect vturn\n## 1 1960 Australia        100         0         0         1             95.5\n## 2 1961 Australia        100         0         0         1 09/12/1961  95.3\n## 3 1962 Australia        100         0         0         1             95.3\n## 4 1963 Australia        100         0         0         1 30/11/1963  95.7\n## 5 1964 Australia        100         0         0         1             95.7\n## 6 1965 Australia        100         0         0         1             95.7\n##   womenpar realgdpgr inflation debt_hist deficit ttl_labf labfopar unemp\n## 1        0        NA      3.73     40.15    0.46  4215.00       NA  1.25\n## 2        0     -0.64      2.29     38.62   -0.36  4286.00       NA  2.46\n## 3        0      5.77     -0.32     38.75   -0.79  4382.00       NA  2.32\n## 4        0      6.01      0.64     37.34   -0.51  4484.00       NA  1.87\n## 5        0      6.26      2.87     35.31   -0.08  4610.80    67.24  1.45\n## 6        0      4.99      3.41     53.99   -0.73  4745.95    67.66  1.36\n##       pop pop15_64 pop65 elderly\n## 1 10275.0   6296.5 874.9    8.51\n## 2 10508.2   6428.6 894.6    8.51\n## 3 10700.5   6571.5 913.6    8.54\n## 4 10906.9   6710.9 933.0    8.55\n## 5 11121.6   6857.3 948.1    8.52\n## 6 11340.9   7014.6 966.3    8.52\n\n\n\ntail(d)  # shows the last 6 lines of data\n\n##      year country gov_right1 gov_cent1 gov_left1 gov_party      elect vturn\n## 1573 2009     USA      16.36     83.64         0         1             53.2\n## 1574 2010     USA      11.76     88.24         0         1 02/11/2010  39.8\n## 1575 2011     USA       8.80     91.20         0         1             39.8\n## 1576 2012     USA       5.88     94.12         0         1 06/11/2012  50.9\n## 1577 2013     USA       5.88     94.12         0         1             50.9\n## 1578 2014     USA       8.40     91.60         0         1 04/11/2014  35.6\n##      womenpar realgdpgr inflation debt_hist deficit ttl_labf labfopar unemp\n## 1573     16.8     -2.78     -0.36     93.47  -12.83 155454.0    75.49   9.3\n## 1574     16.8      2.53      1.64    102.66  -12.18 155220.3    74.80   9.6\n## 1575     16.8      1.60      3.16    108.25  -10.75 154949.3    74.12   8.9\n## 1576     18.0      2.22      2.07    111.48   -9.00 156368.6    74.53   8.1\n## 1577     17.8      1.49      1.46    111.45   -5.49 156761.2    74.41   7.4\n## 1578     19.3      2.43      1.62    111.70   -5.13 157268.8       NA   6.2\n##           pop pop15_64    pop65 elderly\n## 1573 306771.5 206060.8 39623.18   12.92\n## 1574 309347.1 207665.3 40479.35   13.09\n## 1575 311721.6 209179.2 41366.63   13.27\n## 1576 314112.1 209823.0 43164.91   13.74\n## 1577 316498.0 210673.5 44723.04   14.13\n## 1578 318857.0 211545.9 46243.21   14.50\n\nclass(d)  # shows that tables are typically loaded in as data frames\n\n## [1] \"data.frame\"\n\n\n\n\nReading tab-separated (“.tsv”, “.txt”) text files with {base} R\n\nNOTE: In the following snippet, you can change the sep= argument as needed to use other delimiters\n\n\nf &lt;- \"data/CPDS-1960-2014-reduced.txt\"  # specfies a local path\nd &lt;- read.table(f, header = TRUE, sep = \"\\t\", stringsAsFactors = FALSE, fill = TRUE)\n# if fill is left as the default (FALSE) then this will throw an error...  if\n# TRUE then if the rows have unequal length, blank fields are implicitly added\nhead(d)  # shows the first 6 lines of data\n\n##   year   country gov_right1 gov_cent1 gov_left1 gov_party      elect vturn\n## 1 1960 Australia        100         0         0         1             95.5\n## 2 1961 Australia        100         0         0         1 09/12/1961  95.3\n## 3 1962 Australia        100         0         0         1             95.3\n## 4 1963 Australia        100         0         0         1 30/11/1963  95.7\n## 5 1964 Australia        100         0         0         1             95.7\n## 6 1965 Australia        100         0         0         1             95.7\n##   womenpar realgdpgr inflation debt_hist deficit ttl_labf labfopar unemp\n## 1        0        NA      3.73     40.15    0.46  4215.00       NA  1.25\n## 2        0     -0.64      2.29     38.62   -0.36  4286.00       NA  2.46\n## 3        0      5.77     -0.32     38.75   -0.79  4382.00       NA  2.32\n## 4        0      6.01      0.64     37.34   -0.51  4484.00       NA  1.87\n## 5        0      6.26      2.87     35.31   -0.08  4610.80    67.24  1.45\n## 6        0      4.99      3.41     53.99   -0.73  4745.95    67.66  1.36\n##       pop pop15_64 pop65 elderly\n## 1 10275.0   6296.5 874.9    8.51\n## 2 10508.2   6428.6 894.6    8.51\n## 3 10700.5   6571.5 913.6    8.54\n## 4 10906.9   6710.9 933.0    8.55\n## 5 11121.6   6857.3 948.1    8.52\n## 6 11340.9   7014.6 966.3    8.52\n\n\nOr, alternatively…\n\nd &lt;- read.delim(f, header = TRUE, stringsAsFactors = FALSE)\n# for the `read.delim()` function, fill=TRUE by default\nhead(d)\n\n##   year   country gov_right1 gov_cent1 gov_left1 gov_party      elect vturn\n## 1 1960 Australia        100         0         0         1             95.5\n## 2 1961 Australia        100         0         0         1 09/12/1961  95.3\n## 3 1962 Australia        100         0         0         1             95.3\n## 4 1963 Australia        100         0         0         1 30/11/1963  95.7\n## 5 1964 Australia        100         0         0         1             95.7\n## 6 1965 Australia        100         0         0         1             95.7\n##   womenpar realgdpgr inflation debt_hist deficit ttl_labf labfopar unemp\n## 1        0        NA      3.73     40.15    0.46  4215.00       NA  1.25\n## 2        0     -0.64      2.29     38.62   -0.36  4286.00       NA  2.46\n## 3        0      5.77     -0.32     38.75   -0.79  4382.00       NA  2.32\n## 4        0      6.01      0.64     37.34   -0.51  4484.00       NA  1.87\n## 5        0      6.26      2.87     35.31   -0.08  4610.80    67.24  1.45\n## 6        0      4.99      3.41     53.99   -0.73  4745.95    67.66  1.36\n##       pop pop15_64 pop65 elderly\n## 1 10275.0   6296.5 874.9    8.51\n## 2 10508.2   6428.6 894.6    8.51\n## 3 10700.5   6571.5 913.6    8.54\n## 4 10906.9   6710.9 933.0    8.55\n## 5 11121.6   6857.3 948.1    8.52\n## 6 11340.9   7014.6 966.3    8.52\n\n\n\n\nReading text files with the {readr} package\nThe {readr} package, which is loaded as part of {tidyverse} provides easy alternative functions to read in delimited text files. It runs faster than the {base} package functions. It begins by reading in an initial set of rows (a default number of 1000) from the table and then tries to impute the data class of each column. If you want, you can also directly specify the data class of each column with the col_types() function. The col_names=&lt;boolean&gt; argument is used to specify if your data has a header row.\n\nNOTE: There are variants of the main read_&lt;type&gt;() function for different types of files, e.g., tab-separated values (read_tsv()), comma-separated values (read_csv()), those with some other delimiter (read_delim()). A few common delimiters that might be used in text files are commas (,), tabs (\\t), semicolons (;), and end-of-line characters, e.g., “new lines” (\\n) or “carriage returns” (\\r).\n\n\nf &lt;- \"data/CPDS-1960-2014-reduced.txt\"\nd &lt;- read_tsv(f, col_names = TRUE)  # for tab-separated value files\nhead(d)\n\n## # A tibble: 6 × 20\n##    year country   gov_right1 gov_cent1 gov_left1 gov_party elect  vturn womenpar\n##   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n## 1  1960 Australia        100         0         0         1 &lt;NA&gt;    95.5        0\n## 2  1961 Australia        100         0         0         1 09/12…  95.3        0\n## 3  1962 Australia        100         0         0         1 &lt;NA&gt;    95.3        0\n## 4  1963 Australia        100         0         0         1 30/11…  95.7        0\n## 5  1964 Australia        100         0         0         1 &lt;NA&gt;    95.7        0\n## 6  1965 Australia        100         0         0         1 &lt;NA&gt;    95.7        0\n## # ℹ 11 more variables: realgdpgr &lt;dbl&gt;, inflation &lt;dbl&gt;, debt_hist &lt;dbl&gt;,\n## #   deficit &lt;dbl&gt;, ttl_labf &lt;dbl&gt;, labfopar &lt;dbl&gt;, unemp &lt;dbl&gt;, pop &lt;dbl&gt;,\n## #   pop15_64 &lt;dbl&gt;, pop65 &lt;dbl&gt;, elderly &lt;dbl&gt;\n\nclass(d)\n\n## [1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\"\n\n# returns d as a data frame, but also as a 'tibble' note the output is more\n# verbose and the problems() function highlights where there might have been\n# parsing errors\n\nOr, alternatively…\n\nd &lt;- read_delim(f, delim = \"\\n\", col_names = TRUE)\n# for generic delimited files, where the delimiter is a tab ('\\t')\nhead(d)\n\n## # A tibble: 6 × 1\n##   year\\tcountry\\tgov_right1\\tgov_cent1\\tgov_left1\\tgov_party\\telect\\tvturn\\two…¹\n##   &lt;chr&gt;                                                                         \n## 1 \"1960\\tAustralia\\t100.00\\t0.00\\t0.00\\t1\\t\\t95.5\\t0.0\\t\\t3.73\\t40.15\\t0.46\\t42…\n## 2 \"1961\\tAustralia\\t100.00\\t0.00\\t0.00\\t1\\t09/12/1961\\t95.3\\t0.0\\t-0.64\\t2.29\\t…\n## 3 \"1962\\tAustralia\\t100.00\\t0.00\\t0.00\\t1\\t\\t95.3\\t0.0\\t5.77\\t-0.32\\t38.75\\t-0.…\n## 4 \"1963\\tAustralia\\t100.00\\t0.00\\t0.00\\t1\\t30/11/1963\\t95.7\\t0.0\\t6.01\\t0.64\\t3…\n## 5 \"1964\\tAustralia\\t100.00\\t0.00\\t0.00\\t1\\t\\t95.7\\t0.0\\t6.26\\t2.87\\t35.31\\t-0.0…\n## 6 \"1965\\tAustralia\\t100.00\\t0.00\\t0.00\\t1\\t\\t95.7\\t0.0\\t4.99\\t3.41\\t53.99\\t-0.7…\n## # ℹ abbreviated name:\n## #   ¹​`year\\tcountry\\tgov_right1\\tgov_cent1\\tgov_left1\\tgov_party\\telect\\tvturn\\twomenpar\\trealgdpgr\\tinflation\\tdebt_hist\\tdeficit\\tttl_labf\\tlabfopar\\tunemp\\tpop\\tpop15_64\\tpop65\\telderly`\n\n\n\nf &lt;- \"data/CPDS-1960-2014-reduced.csv\"\nd &lt;- read_csv(f, col_names = TRUE)  # for comma-separated value files\n\n## Rows: 1578 Columns: 20\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr  (2): country, elect\n## dbl (18): year, gov_right1, gov_cent1, gov_left1, gov_party, vturn, womenpar...\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(d)\n\n## # A tibble: 6 × 20\n##    year country   gov_right1 gov_cent1 gov_left1 gov_party elect  vturn womenpar\n##   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n## 1  1960 Australia        100         0         0         1 &lt;NA&gt;    95.5        0\n## 2  1961 Australia        100         0         0         1 09/12…  95.3        0\n## 3  1962 Australia        100         0         0         1 &lt;NA&gt;    95.3        0\n## 4  1963 Australia        100         0         0         1 30/11…  95.7        0\n## 5  1964 Australia        100         0         0         1 &lt;NA&gt;    95.7        0\n## 6  1965 Australia        100         0         0         1 &lt;NA&gt;    95.7        0\n## # ℹ 11 more variables: realgdpgr &lt;dbl&gt;, inflation &lt;dbl&gt;, debt_hist &lt;dbl&gt;,\n## #   deficit &lt;dbl&gt;, ttl_labf &lt;dbl&gt;, labfopar &lt;dbl&gt;, unemp &lt;dbl&gt;, pop &lt;dbl&gt;,\n## #   pop15_64 &lt;dbl&gt;, pop65 &lt;dbl&gt;, elderly &lt;dbl&gt;\n\n\nOr, alternatively…\n\nd &lt;- read_delim(f, delim = \",\", col_names = TRUE)\n\n## Rows: 1578 Columns: 20\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr  (2): country, elect\n## dbl (18): year, gov_right1, gov_cent1, gov_left1, gov_party, vturn, womenpar...\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# for generic delimited files, where the delimiter is a comma\nhead(d)\n\n## # A tibble: 6 × 20\n##    year country   gov_right1 gov_cent1 gov_left1 gov_party elect  vturn womenpar\n##   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n## 1  1960 Australia        100         0         0         1 &lt;NA&gt;    95.5        0\n## 2  1961 Australia        100         0         0         1 09/12…  95.3        0\n## 3  1962 Australia        100         0         0         1 &lt;NA&gt;    95.3        0\n## 4  1963 Australia        100         0         0         1 30/11…  95.7        0\n## 5  1964 Australia        100         0         0         1 &lt;NA&gt;    95.7        0\n## 6  1965 Australia        100         0         0         1 &lt;NA&gt;    95.7        0\n## # ℹ 11 more variables: realgdpgr &lt;dbl&gt;, inflation &lt;dbl&gt;, debt_hist &lt;dbl&gt;,\n## #   deficit &lt;dbl&gt;, ttl_labf &lt;dbl&gt;, labfopar &lt;dbl&gt;, unemp &lt;dbl&gt;, pop &lt;dbl&gt;,\n## #   pop15_64 &lt;dbl&gt;, pop65 &lt;dbl&gt;, elderly &lt;dbl&gt;\n\n\n\n\n\nLoading Data from Excel Files\nWhile you should never need to use Excel, sometimes you will no doubt be given a spreadsheet file with some data in it that you want to read in R. There are several packages available that provide functions for loading data into R from Excel spreadsheet files: {readxl}, {XLConnect}, and {xlsx}. The first two of these are fast, easy to use, and work well. Both require that you have successfully installed {rJava} on your computer, which both packages import as a dependency. Be warned, sometimes installing {rJava} can be tricky! {xlsx} also may be slower than the other methods, but its read.xlsx2() function may improve the speed.\n\nNOTE: You shoud always use str() to check if your variables come in as the correct data class!\n\n\nUsing the {readxl} package\n\nlibrary(readxl)\nf &lt;- \"data/CPDS-1960-2014-reduced.xlsx\"\nd &lt;- read_excel(f, sheet = 1, col_names = TRUE)\nhead(d)\n\n## # A tibble: 6 × 20\n##    year country   gov_right1 gov_cent1 gov_left1 gov_party elect              \n##   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dttm&gt;             \n## 1  1960 Australia        100         0         0         1 NA                 \n## 2  1961 Australia        100         0         0         1 1961-12-09 00:00:00\n## 3  1962 Australia        100         0         0         1 NA                 \n## 4  1963 Australia        100         0         0         1 1963-11-30 00:00:00\n## 5  1964 Australia        100         0         0         1 NA                 \n## 6  1965 Australia        100         0         0         1 NA                 \n## # ℹ 13 more variables: vturn &lt;dbl&gt;, womenpar &lt;dbl&gt;, realgdpgr &lt;dbl&gt;,\n## #   inflation &lt;dbl&gt;, debt_hist &lt;dbl&gt;, deficit &lt;dbl&gt;, ttl_labf &lt;dbl&gt;,\n## #   labfopar &lt;dbl&gt;, unemp &lt;dbl&gt;, pop &lt;dbl&gt;, pop15_64 &lt;dbl&gt;, pop65 &lt;dbl&gt;,\n## #   elderly &lt;dbl&gt;\n\nstr(d)  # `read_excel()` yields a 'tibble'\n\n## tibble [1,578 × 20] (S3: tbl_df/tbl/data.frame)\n##  $ year      : num [1:1578] 1960 1961 1962 1963 1964 ...\n##  $ country   : chr [1:1578] \"Australia\" \"Australia\" \"Australia\" \"Australia\" ...\n##  $ gov_right1: num [1:1578] 100 100 100 100 100 100 100 100 100 100 ...\n##  $ gov_cent1 : num [1:1578] 0 0 0 0 0 0 0 0 0 0 ...\n##  $ gov_left1 : num [1:1578] 0 0 0 0 0 0 0 0 0 0 ...\n##  $ gov_party : num [1:1578] 1 1 1 1 1 1 1 1 1 1 ...\n##  $ elect     : POSIXct[1:1578], format: NA \"1961-12-09\" ...\n##  $ vturn     : num [1:1578] 95.5 95.3 95.3 95.7 95.7 95.7 95.1 95.1 95.1 95 ...\n##  $ womenpar  : num [1:1578] 0 0 0 0 0 0 0.8 0.8 0.8 0 ...\n##  $ realgdpgr : num [1:1578] NA -0.643 5.767 6.009 6.258 ...\n##  $ inflation : num [1:1578] 3.729 2.288 -0.319 0.641 2.866 ...\n##  $ debt_hist : num [1:1578] 40.1 38.6 38.7 37.3 35.3 ...\n##  $ deficit   : num [1:1578] 0.4582 -0.3576 -0.7938 -0.5062 -0.0804 ...\n##  $ ttl_labf  : num [1:1578] 4215 4286 4382 4484 4611 ...\n##  $ labfopar  : num [1:1578] NA NA NA NA 67.2 ...\n##  $ unemp     : num [1:1578] 1.25 2.46 2.32 1.87 1.45 ...\n##  $ pop       : num [1:1578] 10275 10508 10700 10907 11122 ...\n##  $ pop15_64  : num [1:1578] 6296 6429 6572 6711 6857 ...\n##  $ pop65     : num [1:1578] 875 895 914 933 948 ...\n##  $ elderly   : num [1:1578] 8.51 8.51 8.54 8.55 8.52 ...\n\ndetach(package:readxl)\n\n\n\nUsing the {XLConnect} package\n\nlibrary(XLConnect)\nf &lt;- \"data/CPDS-1960-2014-reduced.xlsx\"\nd &lt;- readWorksheetFromFile(f, sheet = 1, header = TRUE)\nhead(d)\nstr(d)\n\nThe {XLConnect} package can also write data frames back out to Excel worksheets. If the file does not exist, it is created. If it does exist, data is cleared and overwritten. The second process is MUCH slower. In the following, I have included a conditional statement (if(){}) which will implement the file.remove() command here, if needed.\n\nf &lt;- \"output.xlsx\"\nif (file.exists(f)) {\n    file.remove(f)\n}\nwriteWorksheetToFile(f, d, sheet = \"myData\", clearSheets = TRUE)\ndetach(package:XLConnect)\n\nFor futher information on using {XLConnect} check out this blog post.\n\n\nUsing the {xlsx} package\n\nlibrary(xlsx)\nf &lt;- \"data/CPDS-1960-2014-reduced.xlsx\"\nd &lt;- read.xlsx(f, sheetIndex = 1)\n# or pass a named sheet using the argument `sheetIndex=` the function\n# `read.xlsx2()` is an updated alternative from the same package and may run\n# faster\nhead(d)\ndetach(package:xlsx)\n\n\nTL/DR: {readxl} seems to be the best package for reading Excel data, but you might need other packages and functions to write native Excel files (“.xls” or “.xlsx”).",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Getting Data into ***R***</span>"
    ]
  },
  {
    "objectID": "08-module.html#working-with-remote-files",
    "href": "08-module.html#working-with-remote-files",
    "title": "8  Getting Data into R",
    "section": "8.5 Working with Remote Files",
    "text": "8.5 Working with Remote Files\nWe can also use R as an interface to work with data and files stored on a server elsewhere on the web, e.g., Dropbox, GitHub, or Google Drive.\nTo read “.csv” or “.txt” files directly from GitHub, use the {curl} or {readr} packages.\nGO TO: https://github.com/difiore/ada-datasets, select the “.csv” version of the “CPDS-1960-2014-reduced” file, then press “RAW” and copy the URL from the address box of your browser window… this is what you need to use as an argument for the functions below (you will repeat this for the “.txt” version later on)\n\nUsing the {curl} Package\nThe {curl} package lets us open connection across the internet to read data from a URL, which we can then couple with one of the {base} read.table() functions.\nFor a comma-separated value (“.csv”) text file…\n\nlibrary(curl)\nf &lt;- curl(\"https://raw.githubusercontent.com/difiore/ada-datasets/main/CPDS-1960-2014-reduced.csv\")\nd &lt;- read.csv(f, header = TRUE, sep = \",\", stringsAsFactors = FALSE)\nhead(d)\n\n##   year   country gov_right1 gov_cent1 gov_left1 gov_party      elect vturn\n## 1 1960 Australia        100         0         0         1             95.5\n## 2 1961 Australia        100         0         0         1 09/12/1961  95.3\n## 3 1962 Australia        100         0         0         1             95.3\n## 4 1963 Australia        100         0         0         1 30/11/1963  95.7\n## 5 1964 Australia        100         0         0         1             95.7\n## 6 1965 Australia        100         0         0         1             95.7\n##   womenpar realgdpgr inflation debt_hist deficit ttl_labf labfopar unemp\n## 1        0        NA      3.73     40.15    0.46  4215.00       NA  1.25\n## 2        0     -0.64      2.29     38.62   -0.36  4286.00       NA  2.46\n## 3        0      5.77     -0.32     38.75   -0.79  4382.00       NA  2.32\n## 4        0      6.01      0.64     37.34   -0.51  4484.00       NA  1.87\n## 5        0      6.26      2.87     35.31   -0.08  4610.80    67.24  1.45\n## 6        0      4.99      3.41     53.99   -0.73  4745.95    67.66  1.36\n##       pop pop15_64 pop65 elderly\n## 1 10275.0   6296.5 874.9    8.51\n## 2 10508.2   6428.6 894.6    8.51\n## 3 10700.5   6571.5 913.6    8.54\n## 4 10906.9   6710.9 933.0    8.55\n## 5 11121.6   6857.3 948.1    8.52\n## 6 11340.9   7014.6 966.3    8.52\n\n# returns a data frame\n\nFor a tab-delimited (“.tsv” or .”txt”) text file…\n\nf &lt;- curl(\"https://raw.githubusercontent.com/difiore/ada-datasets/main/CPDS-1960-2014-reduced.txt\")\nd &lt;- read.table(f, header = TRUE, sep = \"\\t\", stringsAsFactors = FALSE)\nhead(d)\n\n##   year   country gov_right1 gov_cent1 gov_left1 gov_party      elect vturn\n## 1 1960 Australia        100         0         0         1             95.5\n## 2 1961 Australia        100         0         0         1 09/12/1961  95.3\n## 3 1962 Australia        100         0         0         1             95.3\n## 4 1963 Australia        100         0         0         1 30/11/1963  95.7\n## 5 1964 Australia        100         0         0         1             95.7\n## 6 1965 Australia        100         0         0         1             95.7\n##   womenpar realgdpgr inflation debt_hist deficit ttl_labf labfopar unemp\n## 1        0        NA      3.73     40.15    0.46  4215.00       NA  1.25\n## 2        0     -0.64      2.29     38.62   -0.36  4286.00       NA  2.46\n## 3        0      5.77     -0.32     38.75   -0.79  4382.00       NA  2.32\n## 4        0      6.01      0.64     37.34   -0.51  4484.00       NA  1.87\n## 5        0      6.26      2.87     35.31   -0.08  4610.80    67.24  1.45\n## 6        0      4.99      3.41     53.99   -0.73  4745.95    67.66  1.36\n##       pop pop15_64 pop65 elderly\n## 1 10275.0   6296.5 874.9    8.51\n## 2 10508.2   6428.6 894.6    8.51\n## 3 10700.5   6571.5 913.6    8.54\n## 4 10906.9   6710.9 933.0    8.55\n## 5 11121.6   6857.3 948.1    8.52\n## 6 11340.9   7014.6 966.3    8.52\n\n# returns a data frame\ndetach(package:curl)\n\n\n\nUsing the {readr} Package\nUsing {readr}, filenames beginning with “http://”, “https://”, “ftp://”, or “fttps://” can be read without having to set up a curl connection interface.\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/CPDS-1960-2014-reduced.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\n\n## Rows: 1578 Columns: 20\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr  (2): country, elect\n## dbl (18): year, gov_right1, gov_cent1, gov_left1, gov_party, vturn, womenpar...\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(d)\n\n## # A tibble: 6 × 20\n##    year country   gov_right1 gov_cent1 gov_left1 gov_party elect  vturn womenpar\n##   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n## 1  1960 Australia        100         0         0         1 &lt;NA&gt;    95.5        0\n## 2  1961 Australia        100         0         0         1 09/12…  95.3        0\n## 3  1962 Australia        100         0         0         1 &lt;NA&gt;    95.3        0\n## 4  1963 Australia        100         0         0         1 30/11…  95.7        0\n## 5  1964 Australia        100         0         0         1 &lt;NA&gt;    95.7        0\n## 6  1965 Australia        100         0         0         1 &lt;NA&gt;    95.7        0\n## # ℹ 11 more variables: realgdpgr &lt;dbl&gt;, inflation &lt;dbl&gt;, debt_hist &lt;dbl&gt;,\n## #   deficit &lt;dbl&gt;, ttl_labf &lt;dbl&gt;, labfopar &lt;dbl&gt;, unemp &lt;dbl&gt;, pop &lt;dbl&gt;,\n## #   pop15_64 &lt;dbl&gt;, pop65 &lt;dbl&gt;, elderly &lt;dbl&gt;\n\n# returns a 'tibble', a new version of a data frame\n\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/CPDS-1960-2014-reduced.txt\"\nd &lt;- read_tsv(f, col_names = TRUE)\n\n## Rows: 1578 Columns: 20\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \"\\t\"\n## chr  (2): country, elect\n## dbl (18): year, gov_right1, gov_cent1, gov_left1, gov_party, vturn, womenpar...\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(d)\n\n## # A tibble: 6 × 20\n##    year country   gov_right1 gov_cent1 gov_left1 gov_party elect  vturn womenpar\n##   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n## 1  1960 Australia        100         0         0         1 &lt;NA&gt;    95.5        0\n## 2  1961 Australia        100         0         0         1 09/12…  95.3        0\n## 3  1962 Australia        100         0         0         1 &lt;NA&gt;    95.3        0\n## 4  1963 Australia        100         0         0         1 30/11…  95.7        0\n## 5  1964 Australia        100         0         0         1 &lt;NA&gt;    95.7        0\n## 6  1965 Australia        100         0         0         1 &lt;NA&gt;    95.7        0\n## # ℹ 11 more variables: realgdpgr &lt;dbl&gt;, inflation &lt;dbl&gt;, debt_hist &lt;dbl&gt;,\n## #   deficit &lt;dbl&gt;, ttl_labf &lt;dbl&gt;, labfopar &lt;dbl&gt;, unemp &lt;dbl&gt;, pop &lt;dbl&gt;,\n## #   pop15_64 &lt;dbl&gt;, pop65 &lt;dbl&gt;, elderly &lt;dbl&gt;\n\n# returns a 'tibble', a new version of a data frame\n\n\n\nAccessing Files on Dropbox\nTo load data from a “.csv” file located in a personal Dropbox account you can use the {rdrop2} package.\n\nNOTE: The {rdrop2} package has been deprecated because one of the dependencies upon which it relies - {assertive} - and, in turn, many of the dependencies upon which {assertive} relies, have not been maintained for some time. Still, it is possible with some care to install an older versions of {rdrop2} and all of its recursive dependencies from Package Archive Files downloaded from CRAN and thus get it working. It takes some time, but may be worthwhile.\n\n\nAlso note that the following code block cannot be rendered to show you the output because it requires an interactive R environment for drop_auth(), drop_search(), etc.\n\n\nlibrary(rdrop2)\ndrop_auth(new_user = TRUE)  # opens a browser dialog box to ask for authorization...\ndrop_dir()  # lists the contents of your dropbox folder\nf &lt;- \"CPDS-1960-2014-reduced.csv\"  # name of the file to read from\nf &lt;- drop_search(query = f, mode = \"filename\")\n# searches your dropbox directory for file or directory names... this can be\n# slow!\nfilenames &lt;- vector()\nfor (i in 1:length(f$matches)) {\n    filenames &lt;- c(filenames, f$matches[[i]]$metadata$path_display)\n    # this is the location of the results returned above the [[i]] returns each\n    # encountered file with a matching filename and puts them into a vector\n}\nd &lt;- drop_read_csv(filenames[1], header = TRUE, sep = \",\", stringsAsFactors = FALSE)\n# here the [1] reads only the first file from filenames, but this can be\n# modified this to read more than one file\ndetach(package:rdrop2)\n\nThis same process can be done to load data from other types of delimited files in Dropbox by setting the appropriate sep= argument.\nYou can also read text files from a Dropbox account (e.g., your own or someone else’s) using a direct link that can be created for any file. To create this link, you will need to choose to share the file (via the dropdown menu activated by clicking the three dots icon to the right of the filename) and then create and copy the direct link to the file.\n\nlink &lt;- \"https://www.dropbox.com/s/hes2loy1x4tikh9/CPDS-1960-2014-reduced.csv?dl=0\"\n\nYou can read text files from a Dropbox account (e.g., your own or someone else’s) using direct link for the file. To find this link, hover over the filename on Dropbox and then select “Copy link” and grab the direct link to the file.\n\n\n\n\n\n\n\n\n\nYou can then assign a variable to hold the link, which likely will end in dl=0.\n\nlink &lt;- \"https://www.dropbox.com/scl/fi/xyhwpfzhdo42mj840pv2e/CPDS-1960-2014-reduced.csv?rlkey=3r4ck0thhgb3p8t3zwmonund5&dl=0\"\n# NOTE: enter your link between quotation marks in lieu of this dummy link!\n\n\nNOTE: Following a shared Dropbox link like this one will take you to a webpage that has the data embedded… to get R to access the raw data, you will need to change the characters at end of the link from dl=0 to dl=1 or to raw=1. That is what the gsub() command in the first line of code below does. Then, you can load data from that file with read.csv() or read_csv(). This\n\n\nlink &lt;- gsub(pattern = \"dl=0\", replacement = \"dl=1\", x = link)\nd &lt;- read.csv(link, header = TRUE, sep = \",\", stringsAsFactors = FALSE)\n# or, using {tidyverse}, `d &lt;-read_csv(link, col_names = TRUE)`\nhead(d)\nstr(d)\n\nYou can also use the source_data() function from the {repmis} package (“Miscellaneous Tools for Reproducible Research”) to load data from a file on Dropbox. This function detects column types and gives a few more warnings than others if it encounters somthing odd.\n\nlibrary(repmis)\nd &lt;- source_data(link, header = TRUE, sep = \",\")\n# use the same updated link to the raw data as above\nhead(d)\nstr(d)\n\n\n\nAccessing Files on Box\nYou can load tabular data from Box sites (e.g., UT Box) with {base} R read.table() functions using a direct link that someone has shared with you. To get such a link, hover over the filename on Box and click the link button to “Copy Shared Link”.\n\n\n\n\n\n\n\n\n\nIn the dialog box that opens, select “Link Settings”…\n\n\n\n\n\n\n\n\n\n… and then copy the direct link, which will include the file type extension for the file. It is important to get the direct link to the file rather than just copying the shared link without the file type extension!\n\n\n\n\n\n\n\n\n\nYou can then assign this link to a variable and read data from it using read.csv() or read_csv().\n\nlink &lt;- \"https://utexas.box.com/shared/static/gnu54b6qt9e6t1b93ydyevanus7m7frj.csv\"\n# NOTE: enter the direct link between quotation marks in lieu of this dummy\n# link!\nd &lt;- read.csv(link, sep = \",\", header = TRUE, stringsAsFactors = FALSE)\n# or, using {tidyverse}, `d &lt;-read_csv(link, col_names = TRUE)`\n\nOr, alternatively, using {repmis}…\n\nd &lt;- source_data(link, header = TRUE, sep = \",\")\ndetach(package:repmis)\n\n\nNOTE: The {boxr} package also provides lots of functionality to interact with files on Box directly, but setup requires an authentication process that I have not yet been able to get working!\n\n\n\nImporting Data from Google Sheets\nFinally, you can also load data directly from a Google Sheets spreadsheet into R using the {googledrive} and {googlesheets4} packages. Try saving one of the “CPDS-1960-2014-reduced” file as a Google Sheet and then extracting it into R using the code below.\n\nNOTE: The following code block cannot be rendered to show you the output because it requires an interactive R environment for several functions (e.g., drive_auth()).\n\n\nlibrary(googledrive)\nlibrary(googlesheets4)\ndrive_auth()  # authenticate access to Google Drive\n# usually only needed once opens a web browser and has you sign in\ngs4_auth(token = drive_token())  # apply that authentication to Google Sheets\nf &lt;- gs4_find() %&gt;%\n    filter(name == \"CPDS-1960-2014-reduced\")\n# first find all Google Sheets in your drive and then filter for one of\n# interest\ngs4_get(f)\n# get info on the Google Sheets file selected, including the number and names\n# of the different worksheets within the spreadsheet\nd &lt;- read_sheet(f)  # read data from the first sheet in the spreadsheet\ndetach(package:googlesheets4)\ndetach(package:googledrive)\n\nFollow the links below for more information on the basics of using {googledrive} and {googlesheets4}.\n\n\nDownloading Remote Files\n\nDropbox\nThe {rdrop2} package, if you are able to install it, can also be used to download a file from a personal Dropbox account to your local computer, rather than just connecting to a Dropbox file to read the data stored there. This should work with any file type. The {rdrop2} package may not be available to install via CRAN, so you may need to install the package directly from the developer’s GitHub site https://github.com/karthik/rDrop2. To do so, install the package {devtools} and then run devtools::install_github(\"karthik/rdrop2\")\n\nNOTE: Again, the following code block cannot be rendered to show you the output because it requires an interactive R environment for drop_search(), etc.\n\n\nlibrary(rdrop2)\nf &lt;- \"CPDS-1960-2014-reduced.csv\"  # name of file to download\nf &lt;- drop_search(query = f, mode = \"filename\")\n# searches your dropbox directory for that file or directory name\nfilenames &lt;- vector()\nfor (i in 1:length(f$matches)) {\n    filenames &lt;- c(filenames, f$matches[[i]]$metadata$path_display)\n    # this is the location of the results returned above the [[i]] returns each\n    # encountered file with a matching filename and puts them into a vector\n}\n\ndrop_download(filenames[1], local_path = paste0(\"data\", filenames), overwrite = TRUE,\n    progress = TRUE)\n# here the [1] reads only the first encountered file... this can be modified\n# this to read more than one file\n\n# this function will save the file to a folder called 'data' inside the current\n# working directory\n\nThe progress=TRUE argument gives you a reassuring progress bar. By default, this argument is set to FALSE.\n\nNOTE: This process also works for other file types, e.g., Excel files:\n\n\nfilename &lt;- \"CPDS-1960-2014-reduced.xlsx\"  # name of file to download\nf &lt;- drop_search(filename)\n# searches your dropbox directory for that file or directory name\nfilenames &lt;- vector()\nfor (i in 1:length(f$matches)) {\n    filenames &lt;- c(filenames, f$matches[[i]]$metadata$path_display)\n    # this is the location of the results returned above the [[i]] returns each\n    # encountered file with a matching filename and puts them into a vector\n}\ndrop_download(filenames[1], local_path = paste0(\"data\", filenames), overwrite = TRUE,\n    progress = TRUE)\n# here the [1] reads only the first file...  need to modify this to read more\n# than one file\n\n# this will save the file to a folder called 'data' inside the current working\n# directory\ndetach(package:rdrop2)\n\n\n\nGoogle Drive\nThe {googledrive} package allows you to interact with a Google Drive account to search for, download, upload, and manipulate files.\n\nlibrary(googledrive)\nf &lt;- \"CPDS-1960-2014-reduced.xlsx\"  # name of the file to download\ndrive_auth()  # authenticate access to Google Drive\n\n# to download the file...\ndrive_download(f, path = paste0(\"data/\", f), overwrite = TRUE)\n\n# this will save the file to a folder called 'data' inside the current working\n# directory\n\n# to search for a file and get info about it\nfiles &lt;- drive_find(pattern = f, n_max = 1)\n# this example finds a single file, but this might return a tibble\n\nprint(files)  # prints a list of files matching the pattern\n\nfiles$drive_resource  # shows metadata about file\n\nid &lt;- files$id  # get the Google file id for the file\nid\n\n# to remove a file\ndrive_rm(files)\n\n# to upload a file...\ndrive_upload(paste0(\"data/\", f), name = \"CPDS-1960-2014-reduced.csv\", overwrite = TRUE)\ndetach(package:googledrive)\n\nMore on the basics of using {googledrive} and its functionality can be found here.\n\n# | include: false\ndetach(package:tidyverse)",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Getting Data into ***R***</span>"
    ]
  },
  {
    "objectID": "08-module.html#concept-review",
    "href": "08-module.html#concept-review",
    "title": "8  Getting Data into R",
    "section": "Concept Review",
    "text": "Concept Review\n\nThere are lots and lots of ways to get data into R from a variety of sources!\nThe file.choose() command will allow you to browse the directory structure on your local machine\nThe {readr} and {readxl} packages contain probably the most useful functions for reading in most types of delimited data (“.csv”, “.txt”, “.tsv”, “.xlsx”)\nWe can read in or download data from remote sites on the web with {curl} or specific packages designed to work with particular hosting sites (e.g., GitHub, Dropbox, Box, Google Sheets, Google Drive)",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Getting Data into ***R***</span>"
    ]
  },
  {
    "objectID": "09-module.html",
    "href": "09-module.html",
    "title": "9  Exploratory Data Analysis",
    "section": "",
    "text": "9.1 Objectives",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "09-module.html#objectives",
    "href": "09-module.html#objectives",
    "title": "9  Exploratory Data Analysis",
    "section": "",
    "text": "The objective of this module to begin exploring data using the summary functions and graphing abilities of R.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "09-module.html#preliminaries",
    "href": "09-module.html#preliminaries",
    "title": "9  Exploratory Data Analysis",
    "section": "9.2 Preliminaries",
    "text": "9.2 Preliminaries\n\nGO TO: https://github.com/difiore/ada-datasets, select the “.csv” version of the “Country-Data-2016” file, then press the “RAW” button, highlight, and copy the text to a text editor and save it locally. Do the same for the “KamilarAndCooperData” file.\nInstall these packages in R: {skimr}, {summarytools}, {dataMaid}, {psych}, {pastecs}, {Hmisc}, {ggExtra}, {car}, {GGally}, {corrplot}, {patchwork}, {cowplot}, and {gridExtra}\nLoad {curl} and {tidyverse}",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "09-module.html#backstory",
    "href": "09-module.html#backstory",
    "title": "9  Exploratory Data Analysis",
    "section": "9.3 Backstory",
    "text": "9.3 Backstory\nExploratory data analysis (EDA) is typically the first step in any kind of data analysis in which you examine your dataset, often using visual methods, to summarize the main characteristics of your variables (central tendency, distributions) and check for various issues that may complicate further analysis (e.g., missing data). The influential statistician, John Tukey, wrote a classic book on the subject, Exploratory Data Analysis (Addison-Wesley) in 1977, in which he advocated the use of the five-number summary to quickly understand the distributions of numeric variables. These include:\n\nthe sample minimum (i.e., the value of the smallest observation)\nthe lower or first quartile\nthe median (i.e., the middle value in the distribution of observation)\nthe upper quartile or third quartile\nthe sample maximum (i.e., the value of the largest observation)\n\nR has some very easy to use functions for taking a quick tour of your data. We have seen some of these already (e.g., head(), tail(), and str()), and you should always use these right after loading in a dataset to work with. Also useful are dim() to return the number of rows and columns in a data frame, names(), colnames(), and sometimes rownames().\n\nNOTE: You can use the attach() function to make variables within data frames accessible in R with fewer keystrokes. The attach() function binds the variables from data frame named as an argument to the local namespace so that as long as the data frame is attached, variables can be called by their names without explicitly referring to the data frame. That is, if you attach() a data frame, then you do not need to use the $ operator or double bracket notation to refer to a particular variable or column vector from your data frame.\nIt is important to remember to detach() data frames when finished. It is also possible to attach multiple data frames (or the same data frame multiple times), and, if these share variable names, then the more recently attached one will mask the other. Thus, it is best to attach only one data frame at a time. You can also attach() and detach() packages from the namespace. In general, I do not recommend using attach() and detach() for dataframes.\n\nEXAMPLE:\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/Country-Data-2016.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\nnames(d)\n\n##  [1] \"country\"        \"population\"     \"area\"           \"govt_form\"     \n##  [5] \"birthrate\"      \"deathrate\"      \"life_expect\"    \"mammals\"       \n##  [9] \"birds\"          \"reptiles\"       \"amphibians\"     \"fishes\"        \n## [13] \"mollucs\"        \"other_inverts\"  \"plants\"         \"fungi_protists\"\n\nattach(d)\nmean(life_expect, na.rm = TRUE)\n\n## [1] 72.19083\n\ndetach(d)\n# mean(life_expect, na.rm=TRUE) # this throws an error!\nmean(d$life_expect, na.rm = TRUE)\n\n## [1] 72.19083\n\n\n\nNOTE: The with() function accomplishes much the same thing as attach() but is self-contained and cleaner, especially for use in functions. If you use with(), however, all code to be run should be included as an argument of the function.\n\nEXAMPLE:\n\nwith(d, mean(life_expect, na.rm = TRUE))\n\n## [1] 72.19083",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "09-module.html#tidy-data",
    "href": "09-module.html#tidy-data",
    "title": "9  Exploratory Data Analysis",
    "section": "9.4 “Tidy” Data",
    "text": "9.4 “Tidy” Data\nThe tabular data we have worked with thus far has all been presented in a format where each row represents a single case or observation or record, and each column contains a different variable that is scored for each of these observations. Data in this format is referred to as “tidy”, and the {tidyverse} set of R packages (which we have used a bit already) provides many tools for manipulating tabular data in this format.\n\n\n\n\n\n\n\n\n\nHaving data in a “tidy” format is useful for many kinds of analyses (although “tidy” data is not the ONLY useful data format). Often, then, the first manipulations we need to do with data in order to work with it efficiently are to reformat it to make it “tidy”.\nThere are multiple ways that data can be “non-tidy”, but one common one way is because data is organized in such a way to make data entry in a spreadsheet easier. For example, consider the following table of data representing body weight in grams for three individual titi monkeys, a species of South American primate, collected in each of two years:\n\n\n\nIndividual\nYear_1\nYear_2\n\n\n\n\nLily\n580\n600\n\n\nLeia\n550\n575\n\n\nLoki\n600\n620\n\n\n\nThis data is easy to ENTER, but it is non-tidy… Why? Because the latter two columns actually represent a combinations of two variables (“Year” and “Weight”) that pertain to a given individual. Each row thus represents two observations.\nBelow is the same data in tidy format, where each row represents a single observation:\n\n\n\nIndividual\nYear\nWeight\n\n\n\n\nLily\nYear_1\n580\n\n\nLily\nYear_2\n600\n\n\nLeia\nYear_1\n550\n\n\nLeia\nYear_2\n575\n\n\nLoki\nYear_1\n600\n\n\nLoki\nYear_2\n620\n\n\n\nWe can convert data from the former format to the latter using the function pivot_longer(), where the first argument is the tabular data of interest, the next argument is a vector of columns to collect data from, the names_to= argument is the name we will assign to a new variable that indicates from which column values are being collected from in the non-tidy dataset, and the final argument, values_to=, is the name for the new variable into which we are collecting values. This process has the effect, often, of making “wide” tables narrower and longer, thus is sometime referred to as converting data to “long” format, thus the name for the function, pivot_longer(). Note that this function is essentially an update of an older {tidyr} function, gather(), which had a less intuitive name and set of argument names.\n\nd &lt;- data.frame(Individual = c(\"Lily\", \"Leia\", \"Loki\"), Year_1 = c(580, 550, 600),\n    Year_2 = c(600, 575, 620))\nd\n\n##   Individual Year_1 Year_2\n## 1       Lily    580    600\n## 2       Leia    550    575\n## 3       Loki    600    620\n\nd &lt;- pivot_longer(d, c(\"Year_1\", \"Year_2\"), names_to = \"Year\", values_to = \"Weight\")\nd\n\n## # A tibble: 6 × 3\n##   Individual Year   Weight\n##   &lt;chr&gt;      &lt;chr&gt;   &lt;dbl&gt;\n## 1 Lily       Year_1    580\n## 2 Lily       Year_2    600\n## 3 Leia       Year_1    550\n## 4 Leia       Year_2    575\n## 5 Loki       Year_1    600\n## 6 Loki       Year_2    620\n\n\nAlternatively, sometimes we will have data that is non-tidy because variables for the same observation are presented in different rows. Consider the following example of “non-tidy” data:\n\n\n\nSpecies\nVariable\nValue\n\n\n\n\nOrangutans\nBody Size (kg)\n37\n\n\nOrangutans\nBrain Size (cc)\n340\n\n\nChimpanzees\nBody Size (kg)\n38\n\n\nChimpanzees\nBrain Size (cc)\n350\n\n\nGorillas\nBody Size (kg)\n80\n\n\nGorillas\nBrain Size (cc)\n470\n\n\n\nThe same data could be represented in “tidy” format as:\n\n\n\nSpecies\nBody Size (kg)\nBrain Size (cc)\n\n\n\n\nOrangutans\n37\n340\n\n\nChimpanzees\n38\n350\n\n\nGorillas\n80\n470\n\n\n\nThe pivot_wider() function lets us easily reformat. Here, names_from= is the column containing the different variables, and values_from= is the column containing the measured values of those variables. Note that this function is essentially an update of an older {tidyr} function, spread().\n\nd &lt;- data.frame(Species = c(\"Orangutans\", \"Orangutans\", \"Chimpanzees\", \"Chimpanzees\",\n    \"Gorillas\", \"Gorillas\"), Variable = rep(c(\"Body Size (kg)\", \"Brain Size (cc)\"),\n    3), Value = c(37, 340, 38, 350, 80, 470))\nd\n\n##       Species        Variable Value\n## 1  Orangutans  Body Size (kg)    37\n## 2  Orangutans Brain Size (cc)   340\n## 3 Chimpanzees  Body Size (kg)    38\n## 4 Chimpanzees Brain Size (cc)   350\n## 5    Gorillas  Body Size (kg)    80\n## 6    Gorillas Brain Size (cc)   470\n\nd &lt;- pivot_wider(d, names_from = \"Variable\", values_from = \"Value\")\nd\n\n## # A tibble: 3 × 3\n##   Species     `Body Size (kg)` `Brain Size (cc)`\n##   &lt;chr&gt;                  &lt;dbl&gt;             &lt;dbl&gt;\n## 1 Orangutans                37               340\n## 2 Chimpanzees               38               350\n## 3 Gorillas                  80               470\n\n\nThis process has the effect, often, of making long and narrow tables wider, thus is sometime referred to as converting data to “wide” format.\nThe image below, pulled from a cheatsheet on Data Wrangling with {dplyr} and {tidyr}, summarizes the basics of these processes for “reshaping” data.\n\n\n\n\n\n\n\n\n\nFor the remainder of this module, all of the example datasets will comprise data already in the “tidy” format.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "09-module.html#exploring-single-variables",
    "href": "09-module.html#exploring-single-variables",
    "title": "9  Exploratory Data Analysis",
    "section": "9.5 Exploring Single Variables",
    "text": "9.5 Exploring Single Variables\n\nVariable Summaries\nThe summary() function from {base} R provides a quick overview of each variable in a data frame. For numeric variables, this includes the five-number summary and the mean, as well as a count of NA (missing values). For factors, it includes a count of each factor.\n\n\nCHALLENGE\n\nLoad the “Country-Data-2016” dataset into a data frame variable, d, and summarize the variables in that data frame. You can load the file any way you want, e.g., load from a local file or access the data straight from GitHub, as in the code below.\n\n\n# using {base} R\nf &lt;- curl(\"https://raw.githubusercontent.com/difiore/ada-datasets/main/Country-Data-2016.csv\")\nd &lt;- read.csv(f, header = TRUE, sep = \",\", stringsAsFactors = FALSE)\nd &lt;- as_tibble(d)  # I like tibbles!\nhead(d)\n\n## # A tibble: 6 × 16\n##   country    population   area govt_form birthrate deathrate life_expect mammals\n##   &lt;chr&gt;           &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;   &lt;int&gt;\n## 1 Afghanist…   32564342 6.52e5 islamic …      38.6      13.9        50.9      11\n## 2 Albania       3029278 2.87e4 republic       12.9       6.6        78.1       3\n## 3 Algeria      39542166 2.38e6 republic       23.7       4.3        76.6      14\n## 4 American …      54343 1.99e2 territor…      22.9       4.8        75.1       1\n## 5 Andorra         85580 4.68e2 constitu…       8.1       7          82.7       2\n## 6 Angola       19625353 1.25e6 republic       38.8      11.5        55.6      17\n## # ℹ 8 more variables: birds &lt;int&gt;, reptiles &lt;int&gt;, amphibians &lt;int&gt;,\n## #   fishes &lt;int&gt;, mollucs &lt;int&gt;, other_inverts &lt;int&gt;, plants &lt;int&gt;,\n## #   fungi_protists &lt;int&gt;\n\nnames(d)\n\n##  [1] \"country\"        \"population\"     \"area\"           \"govt_form\"     \n##  [5] \"birthrate\"      \"deathrate\"      \"life_expect\"    \"mammals\"       \n##  [9] \"birds\"          \"reptiles\"       \"amphibians\"     \"fishes\"        \n## [13] \"mollucs\"        \"other_inverts\"  \"plants\"         \"fungi_protists\"\n\n# or, using {readr}\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/Country-Data-2016.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\n\n## Rows: 248 Columns: 16\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr  (2): country, govt_form\n## dbl (14): population, area, birthrate, deathrate, life_expect, mammals, bird...\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(d)\n\n## # A tibble: 6 × 16\n##   country    population   area govt_form birthrate deathrate life_expect mammals\n##   &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;   &lt;dbl&gt;\n## 1 Afghanist…   32564342 6.52e5 islamic …      38.6      13.9        50.9      11\n## 2 Albania       3029278 2.87e4 republic       12.9       6.6        78.1       3\n## 3 Algeria      39542166 2.38e6 republic       23.7       4.3        76.6      14\n## 4 American …      54343 1.99e2 territor…      22.9       4.8        75.1       1\n## 5 Andorra         85580 4.68e2 constitu…       8.1       7          82.7       2\n## 6 Angola       19625353 1.25e6 republic       38.8      11.5        55.6      17\n## # ℹ 8 more variables: birds &lt;dbl&gt;, reptiles &lt;dbl&gt;, amphibians &lt;dbl&gt;,\n## #   fishes &lt;dbl&gt;, mollucs &lt;dbl&gt;, other_inverts &lt;dbl&gt;, plants &lt;dbl&gt;,\n## #   fungi_protists &lt;dbl&gt;\n\nnames(d)\n\n##  [1] \"country\"        \"population\"     \"area\"           \"govt_form\"     \n##  [5] \"birthrate\"      \"deathrate\"      \"life_expect\"    \"mammals\"       \n##  [9] \"birds\"          \"reptiles\"       \"amphibians\"     \"fishes\"        \n## [13] \"mollucs\"        \"other_inverts\"  \"plants\"         \"fungi_protists\"\n\n\n\nWhat are the median area and population size of all countries in the dataset?\n\n\nHINT: There are a couple of ways to do this… try summary() and median() (for the latter, you’ll need to use the na.rm=TRUE argument).\n\n\n\nShow Code\nsummary(d)\n\n\nShow Output\n##    country            population             area           govt_form        \n##  Length:248         Min.   :3.000e+01   Min.   :       0   Length:248        \n##  Class :character   1st Qu.:2.991e+05   1st Qu.:    1769   Class :character  \n##  Mode  :character   Median :4.912e+06   Median :   69700   Mode  :character  \n##                     Mean   :2.999e+07   Mean   :  610952                     \n##                     3rd Qu.:1.803e+07   3rd Qu.:  398754                     \n##                     Max.   :1.367e+09   Max.   :17098242                     \n##                     NA's   :6           NA's   :1                            \n##    birthrate       deathrate      life_expect       mammals      \n##  Min.   : 0.00   Min.   : 0.00   Min.   :49.80   Min.   :  0.00  \n##  1st Qu.:11.40   1st Qu.: 5.65   1st Qu.:67.40   1st Qu.:  3.00  \n##  Median :16.40   Median : 7.40   Median :74.70   Median :  8.00  \n##  Mean   :18.95   Mean   : 7.61   Mean   :72.19   Mean   : 13.85  \n##  3rd Qu.:24.35   3rd Qu.: 9.40   3rd Qu.:78.40   3rd Qu.: 15.00  \n##  Max.   :45.50   Max.   :14.90   Max.   :89.50   Max.   :188.00  \n##  NA's   :17      NA's   :17      NA's   :19      NA's   :3       \n##      birds           reptiles         amphibians          fishes      \n##  Min.   :  0.00   Min.   :  0.000   Min.   :  0.000   Min.   :  0.00  \n##  1st Qu.:  6.00   1st Qu.:  2.000   1st Qu.:  0.000   1st Qu.: 11.00  \n##  Median : 12.00   Median :  5.000   Median :  0.000   Median : 25.00  \n##  Mean   : 17.82   Mean   :  8.331   Mean   :  9.849   Mean   : 32.84  \n##  3rd Qu.: 19.00   3rd Qu.:  8.000   3rd Qu.:  4.000   3rd Qu.: 43.00  \n##  Max.   :165.00   Max.   :139.000   Max.   :215.000   Max.   :249.00  \n##  NA's   :3        NA's   :3         NA's   :3         NA's   :3       \n##     mollucs       other_inverts        plants        fungi_protists   \n##  Min.   :  0.00   Min.   :  0.00   Min.   :   0.00   Min.   : 0.0000  \n##  1st Qu.:  0.00   1st Qu.:  3.00   1st Qu.:   2.00   1st Qu.: 0.0000  \n##  Median :  1.00   Median : 11.00   Median :  10.00   Median : 0.0000  \n##  Mean   :  9.62   Mean   : 32.57   Mean   :  60.78   Mean   : 0.6082  \n##  3rd Qu.:  6.00   3rd Qu.: 33.00   3rd Qu.:  44.00   3rd Qu.: 0.0000  \n##  Max.   :301.00   Max.   :340.00   Max.   :1856.00   Max.   :12.0000  \n##  NA's   :3        NA's   :3        NA's   :3         NA's   :3\n\n\n\nShow Code\nmedian(d$area, na.rm = TRUE)\n\n\nShow Output\n## [1] 69700\n\n\n\nShow Code\nmedian(d$population, na.rm = TRUE)\n\n\nShow Output\n## [1] 4911766\n\n\n\n\nCreate a new density variable in your data frame which is population / area. What are the 10 most dense countries? The 10 least dense?\n\n\nHINT: Check out the order() function.\n\n\n\nShow Code\nd$density &lt;- d$population/d$area\nd &lt;- d[order(d$density, decreasing = TRUE), ]  # or\nd &lt;- d[order(-d$density), ]\nd[1:10, ]\n\n\nShow Output\n## # A tibble: 10 × 17\n##    country   population   area govt_form birthrate deathrate life_expect mammals\n##    &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;   &lt;dbl&gt;\n##  1 Macau         592731   28   special …       8.9       4.2        84.5       0\n##  2 Monaco         30535    2   constitu…       6.7       9.2        89.5       3\n##  3 Holy See…        842    0.1 monarchy       NA        NA          NA         1\n##  4 Singapore    5674472  697   republic        8.3       3.4        84.7      13\n##  5 Hong Kong    7141106 1108   special …       9.2       7.1        82.9       3\n##  6 Gibraltar      29258    7   British …      14.1       8.4        79.3       4\n##  7 Bahrain      1346613  760   constitu…      13.7       2.7        78.7       3\n##  8 Maldives      393253  298   republic       15.8       3.9        75.4       2\n##  9 Malta         413965  316   republic       10.2       9.1        80.2       2\n## 10 Bermuda        70196   54   British …      11.3       8.2        81.2       4\n## # ℹ 9 more variables: birds &lt;dbl&gt;, reptiles &lt;dbl&gt;, amphibians &lt;dbl&gt;,\n## #   fishes &lt;dbl&gt;, mollucs &lt;dbl&gt;, other_inverts &lt;dbl&gt;, plants &lt;dbl&gt;,\n## #   fungi_protists &lt;dbl&gt;, density &lt;dbl&gt;\n\n\n\nShow Code\nd &lt;- d[order(d$density), ]\nd[1:10, ]\n\n\nShow Output\n## # A tibble: 10 × 17\n##    country   population   area govt_form birthrate deathrate life_expect mammals\n##    &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;   &lt;dbl&gt;\n##  1 South Ge…         30 3.90e3 British …      NA        NA          NA         3\n##  2 Greenland      57733 2.17e6 autonomo…      14.5       8.5        72.1       9\n##  3 Falkland…       3361 1.22e4 British …      NA        NA          NA         4\n##  4 Pitcairn…         48 4.7 e1 British …      NA        NA          NA         1\n##  5 Mongolia     2992908 1.56e6 republic       20.3       6.4        69.3      11\n##  6 Western …     570866 2.66e5 autonomo…      30.2       8.3        62.6      10\n##  7 French G…     181000 8.35e4 overseas…       0         0          76.1       8\n##  8 Namibia      2212307 8.24e5 republic       19.8      13.9        51.6      14\n##  9 Australia   22751014 7.74e6 parliame…      12.2       7.1        82.2      63\n## 10 Iceland       331918 1.03e5 republic       13.9       6.3        83         6\n## # ℹ 9 more variables: birds &lt;dbl&gt;, reptiles &lt;dbl&gt;, amphibians &lt;dbl&gt;,\n## #   fishes &lt;dbl&gt;, mollucs &lt;dbl&gt;, other_inverts &lt;dbl&gt;, plants &lt;dbl&gt;,\n## #   fungi_protists &lt;dbl&gt;, density &lt;dbl&gt;\n\n\n\n\nUse subsetting or filtering to extract data from the 20 largest countries into a new variable, s. What are the median area and population size of these countries?\n\n\n\nShow Code\ns &lt;- d[order(-d$population), ]\ns &lt;- s[1:20, ]\nmed_area &lt;- median(s$area, na.rm = TRUE)\nmed_area\n\n\nShow Output\n## [1] 1052875\n\n\n\nShow Code\nmed_pop &lt;- median(s$population, na.rm = TRUE)\nmed_pop\n\n\nShow Output\n## [1] 124328234\n\n\n\n\nExtract data from all countries beginning with the letters “A” through “F”. What are the mean area and population size of these countries?\n\n\nNOTE: Single bracket notation subsetting is used here to return the rows of the d data frame where the country name (d$country) begins with the capital letters “A” through “F”. The grep() function is a pattern recognition function that uses a regular expression to pull out the country names of interest.\n\n\n\nShow Code\ns &lt;- d[grep(pattern = \"^[A-F]\", d$country), ]\nsummary(s)\n\n\nShow Output\n##    country            population             area           govt_form        \n##  Length:78          Min.   :5.960e+02   Min.   :      14   Length:78         \n##  Class :character   1st Qu.:2.991e+05   1st Qu.:    4066   Class :character  \n##  Mode  :character   Median :4.785e+06   Median :   51148   Mode  :character  \n##                     Mean   :3.507e+07   Mean   :  918248                     \n##                     3rd Qu.:1.469e+07   3rd Qu.:  466498                     \n##                     Max.   :1.367e+09   Max.   :14000000                     \n##                     NA's   :4                                                \n##    birthrate       deathrate       life_expect       mammals    \n##  Min.   : 0.00   Min.   : 0.000   Min.   :49.80   Min.   : 0.0  \n##  1st Qu.:11.65   1st Qu.: 5.850   1st Qu.:68.75   1st Qu.: 3.0  \n##  Median :15.90   Median : 7.700   Median :75.50   Median : 7.0  \n##  Mean   :18.77   Mean   : 7.861   Mean   :72.25   Mean   :13.4  \n##  3rd Qu.:23.30   3rd Qu.: 9.500   3rd Qu.:78.40   3rd Qu.:14.0  \n##  Max.   :42.00   Max.   :14.400   Max.   :82.70   Max.   :81.0  \n##  NA's   :7       NA's   :7        NA's   :7                     \n##      birds           reptiles        amphibians         fishes      \n##  Min.   :  0.00   Min.   : 0.000   Min.   :  0.00   Min.   :  0.00  \n##  1st Qu.:  6.00   1st Qu.: 2.000   1st Qu.:  0.00   1st Qu.: 10.00  \n##  Median : 11.00   Median : 5.000   Median :  0.00   Median : 24.50  \n##  Mean   : 18.62   Mean   : 7.397   Mean   : 11.86   Mean   : 29.54  \n##  3rd Qu.: 18.00   3rd Qu.: 8.000   3rd Qu.:  3.00   3rd Qu.: 41.50  \n##  Max.   :165.00   Max.   :43.000   Max.   :215.00   Max.   :133.00  \n##                                                                     \n##     mollucs       other_inverts        plants        fungi_protists  \n##  Min.   :  0.00   Min.   :  0.00   Min.   :   0.00   Min.   :0.0000  \n##  1st Qu.:  0.00   1st Qu.:  4.00   1st Qu.:   2.25   1st Qu.:0.0000  \n##  Median :  1.00   Median : 11.00   Median :  10.00   Median :0.0000  \n##  Mean   : 10.17   Mean   : 23.63   Mean   :  70.64   Mean   :0.6026  \n##  3rd Qu.:  5.00   3rd Qu.: 25.25   3rd Qu.:  41.75   3rd Qu.:0.0000  \n##  Max.   :174.00   Max.   :340.00   Max.   :1856.00   Max.   :8.0000  \n##                                                                      \n##     density         \n##  Min.   :   0.2761  \n##  1st Qu.:  24.5932  \n##  Median :  75.0297  \n##  Mean   : 162.4785  \n##  3rd Qu.: 140.7140  \n##  Max.   :1771.8592  \n##  NA's   :4\n\n\n\nDoing the same thing with {dplyr} verbs (see Module 10) is easier…\n\n\nShow Code\ns &lt;- filter(d, grepl(pattern = \"^[A-F]\", country))\nsummary(s)\n\n\nShow Output\n##    country            population             area           govt_form        \n##  Length:78          Min.   :5.960e+02   Min.   :      14   Length:78         \n##  Class :character   1st Qu.:2.991e+05   1st Qu.:    4066   Class :character  \n##  Mode  :character   Median :4.785e+06   Median :   51148   Mode  :character  \n##                     Mean   :3.507e+07   Mean   :  918248                     \n##                     3rd Qu.:1.469e+07   3rd Qu.:  466498                     \n##                     Max.   :1.367e+09   Max.   :14000000                     \n##                     NA's   :4                                                \n##    birthrate       deathrate       life_expect       mammals    \n##  Min.   : 0.00   Min.   : 0.000   Min.   :49.80   Min.   : 0.0  \n##  1st Qu.:11.65   1st Qu.: 5.850   1st Qu.:68.75   1st Qu.: 3.0  \n##  Median :15.90   Median : 7.700   Median :75.50   Median : 7.0  \n##  Mean   :18.77   Mean   : 7.861   Mean   :72.25   Mean   :13.4  \n##  3rd Qu.:23.30   3rd Qu.: 9.500   3rd Qu.:78.40   3rd Qu.:14.0  \n##  Max.   :42.00   Max.   :14.400   Max.   :82.70   Max.   :81.0  \n##  NA's   :7       NA's   :7        NA's   :7                     \n##      birds           reptiles        amphibians         fishes      \n##  Min.   :  0.00   Min.   : 0.000   Min.   :  0.00   Min.   :  0.00  \n##  1st Qu.:  6.00   1st Qu.: 2.000   1st Qu.:  0.00   1st Qu.: 10.00  \n##  Median : 11.00   Median : 5.000   Median :  0.00   Median : 24.50  \n##  Mean   : 18.62   Mean   : 7.397   Mean   : 11.86   Mean   : 29.54  \n##  3rd Qu.: 18.00   3rd Qu.: 8.000   3rd Qu.:  3.00   3rd Qu.: 41.50  \n##  Max.   :165.00   Max.   :43.000   Max.   :215.00   Max.   :133.00  \n##                                                                     \n##     mollucs       other_inverts        plants        fungi_protists  \n##  Min.   :  0.00   Min.   :  0.00   Min.   :   0.00   Min.   :0.0000  \n##  1st Qu.:  0.00   1st Qu.:  4.00   1st Qu.:   2.25   1st Qu.:0.0000  \n##  Median :  1.00   Median : 11.00   Median :  10.00   Median :0.0000  \n##  Mean   : 10.17   Mean   : 23.63   Mean   :  70.64   Mean   :0.6026  \n##  3rd Qu.:  5.00   3rd Qu.: 25.25   3rd Qu.:  41.75   3rd Qu.:0.0000  \n##  Max.   :174.00   Max.   :340.00   Max.   :1856.00   Max.   :8.0000  \n##                                                                      \n##     density         \n##  Min.   :   0.2761  \n##  1st Qu.:  24.5932  \n##  Median :  75.0297  \n##  Mean   : 162.4785  \n##  3rd Qu.: 140.7140  \n##  Max.   :1771.8592  \n##  NA's   :4\n\n\n\n\nNOTE: Here, we use grepl() instead of grep() because we want to return a logical vector for all of the elements of “country” to filter() on the basis of. grep() returns a vector of index positions.\n\nOr, instead of summary(), we can use mean() with the na.rm=TRUE argument…\n\n\nShow Code\nmean(s$population, na.rm = TRUE)\n\n\nShow Output\n## [1] 35065172\n\n\n\nShow Code\nmean(s$area, na.rm = TRUE)\n\n\nShow Output\n## [1] 918247.7\n\n\n\nThere are a number of other packages and associated functions we might also use to generate nice summaries of our data.\nFor example, with the skim() function from {skimr}, you can get for numeric variables a count of observations, the number of missing observations, the mean and standard deviation, and the five-number summary. The select(), kable() and kable_styling() functions will format the output nicely!\n\nlibrary(skimr)\nlibrary(kableExtra)\n\n\ns &lt;- skim(d)  # the main `skimr()` function\ns %&gt;%\n    filter(skim_type == \"numeric\") %&gt;%\n    rename(variable = skim_variable, missing = n_missing, mean = numeric.mean, sd = numeric.sd,\n        min = numeric.p0, p25 = numeric.p25, median = numeric.p50, p75 = numeric.p75,\n        max = numeric.p100, hist = numeric.hist) %&gt;%\n    select(variable, missing, mean, sd, min, median, max, hist) %&gt;%\n    # drop p25 and p75 for purposes of display\nkable() %&gt;%\n    kable_styling(font_size = 10, full_width = FALSE)\n\n\n\n\nvariable\nmissing\nmean\nsd\nmin\nmedian\nmax\nhist\n\n\n\n\npopulation\n6\n2.998647e+07\n1.241345e+08\n30.0000000\n4.911766e+06\n1.367485e+09\n▇▁▁▁▁\n\n\narea\n1\n6.109518e+05\n1.927077e+06\n0.1000000\n6.970000e+04\n1.709824e+07\n▇▁▁▁▁\n\n\nbirthrate\n17\n1.894892e+01\n9.925912e+00\n0.0000000\n1.640000e+01\n4.550000e+01\n▂▇▅▂▁\n\n\ndeathrate\n17\n7.610390e+00\n3.135197e+00\n0.0000000\n7.400000e+00\n1.490000e+01\n▁▅▇▃▂\n\n\nlife_expect\n19\n7.219083e+01\n8.587024e+00\n49.8000000\n7.470000e+01\n8.950000e+01\n▂▂▃▇▂\n\n\nmammals\n3\n1.385306e+01\n2.058178e+01\n0.0000000\n8.000000e+00\n1.880000e+02\n▇▁▁▁▁\n\n\nbirds\n3\n1.781633e+01\n2.213342e+01\n0.0000000\n1.200000e+01\n1.650000e+02\n▇▁▁▁▁\n\n\nreptiles\n3\n8.330612e+00\n1.408903e+01\n0.0000000\n5.000000e+00\n1.390000e+02\n▇▁▁▁▁\n\n\namphibians\n3\n9.848980e+00\n2.869408e+01\n0.0000000\n0.000000e+00\n2.150000e+02\n▇▁▁▁▁\n\n\nfishes\n3\n3.283673e+01\n3.552576e+01\n0.0000000\n2.500000e+01\n2.490000e+02\n▇▂▁▁▁\n\n\nmollucs\n3\n9.620408e+00\n2.734627e+01\n0.0000000\n1.000000e+00\n3.010000e+02\n▇▁▁▁▁\n\n\nother_inverts\n3\n3.256735e+01\n5.362966e+01\n0.0000000\n1.100000e+01\n3.400000e+02\n▇▁▁▁▁\n\n\nplants\n3\n6.077551e+01\n1.618624e+02\n0.0000000\n1.000000e+01\n1.856000e+03\n▇▁▁▁▁\n\n\nfungi_protists\n3\n6.081633e-01\n1.847000e+00\n0.0000000\n0.000000e+00\n1.200000e+01\n▇▁▁▁▁\n\n\ndensity\n6\n4.230267e+02\n1.882632e+03\n0.0076864\n8.389234e+01\n2.116896e+04\n▇▁▁▁▁\n\n\n\n\n\n\n\n\ndetach(package:kableExtra)\ndetach(package:skimr)\n\nWith descr(), dfSummary(), and view() from the {summarytools} package, you can also return nicely formatted tables of summary statistics. Note that the output of the code below is not shown.\n\nCAUTION: Q1 and Q3 (the equivalent of “numeric.p25” and “numeric.p75”) are calculated slightly differently for descr() and dfSummary() than for summary() and skim()!\n\n\nlibrary(summarytools)\ns &lt;- descr(d, style = \"rmarkdown\", transpose = TRUE)\n# %&gt;% to view() print nicely formatted table to viewer\ns %&gt;%\n    summarytools::view()\ns &lt;- dfSummary(d, style = \"grid\", plain.ascii = FALSE)\ns %&gt;%\n    summarytools::view()\ndetach(package:summarytools)\n\nWith makeDataReport() from the {dataMaid} package, you can generate a nicely formated report that gets written out to a location of your choice. Again, the output of this code is not shown as it generates a report in an external file.\n\nlibrary(dataMaid)\n# this code below produces a formated report, with the type of report specified\n# by `output=`\nmakeDataReport(d, output = \"html\", file = \"~/Desktop/dataMaid-output.Rmd\", replace = TRUE)\ndetach(package:dataMaid)\n\nAdditionally, the packages {psych}, {pastecs}, and {Hmisc} also all contain functions for producing variable summaries:\npsych::describe(d)\npastecs::stat.desc(d)\nHmisc::describe(d)\n\n\nBoxplots, Barplots, and Dotcharts\nThe boxplot() function from {base} R provides a box-and-whiskers visual representation of the five-number summary, sometimes noting outliers that go beyond the bulk of the data. The function balks if you pass it nonnumeric data, so you will want to reference columns of interest specifically using either double bracket notation or the $ operator.\n\nNOTE: Actually, it is technically not true that boxplot() always shows you the five-number summary. One of the default arguments for boxplot() includes range=1.5, which means that the “whiskers” of the boxplot extend to 1.5 times the interquartile range, and then values more extreme than that are indicated as single points. If range=0, then, the “whiskers” extend to the minimum and maximum values of the data.\n\nThe barplot() function is also useful taking a quick look at crude data, with bar height proportional to the value of the variable. The function dotchart() provides a similar graphical summary.\n\n\nCHALLENGE\n\nMake boxplots, barplots, and dotcharts of the raw population and area data, then log() transform the variables and repeat the boxplots.\n\n\nNOTE: The par() command in the code below lets you set up a grid of panels in which to plot. Here, we set up a 1 row x 2 column grid.\n\n\n\nShow Code\npar(mfrow = c(1, 2))\nd$log_population &lt;- log(d$population)\nd$log_area &lt;- log(d$area)\nboxplot(d$population, ylab = \"Population\")\nboxplot(d$area, ylab = \"Area\")\n\n\n\n\n\n\n\n\n\nShow Code\nbarplot(d$population, xlab = \"Case\", ylab = \"Population\")\ndotchart(d$population, xlab = \"Population\", ylab = \"Case\")\n\n\n\n\n\n\n\n\n\nShow Code\nbarplot(d$area, xlab = \"Case\", ylab = \"Area\")\ndotchart(d$area, xlab = \"Area\", ylab = \"Case\")\n\n\n\n\n\n\n\n\n\nShow Code\nboxplot(d$log_population, ylab = \"log(Population)\")\nboxplot(d$log_area, ylab = \"log(Area)\")\n\n\n\n\n\n\n\n\n\n\n\nPlotting with {ggplot2}\nAs an alternative for plotting, we can use the “grammar of graphics” {ggplot2} package:\n\n# first, we use `tidyr::pivot_longer()` to convert our data from wide to long\n# format this is so we can use `facet.grid()`\n\n# uncomment this to look at a different set of variables d_long &lt;-\n# pivot_longer(d, c('log_population','log_area'), names_to='Variable',\n# values_to='Value')\n\n# uncomment this to look at a different set of variables d_long &lt;-\n# pivot_longer(d, c('mammals','birds','reptiles','fishes'),\n# names_to='Variable', values_to='Value')\n\nd_long &lt;- pivot_longer(d, c(\"birthrate\", \"deathrate\", \"life_expect\"), names_to = \"Variable\",\n    values_to = \"Value\")\n\np &lt;- ggplot(data = d_long, aes(x = factor(0), y = Value)) + geom_boxplot(na.rm = TRUE,\n    outlier.shape = NA) + theme(axis.title.x = element_blank(), axis.text.x = element_blank(),\n    axis.ticks.x = element_blank()) + geom_dotplot(binaxis = \"y\", stackdir = \"center\",\n    stackratio = 0.2, alpha = 0.3, dotsize = 0.5, color = NA, fill = \"red\", na.rm = TRUE) +\n    facet_grid(. ~ Variable) + geom_rug(sides = \"l\")\np\n\n\n\n\n\n\n\np &lt;- ggplot(data = d_long, aes(x = factor(0), y = Value)) + geom_violin(na.rm = TRUE,\n    draw_quantiles = c(0.25, 0.5, 0.75)) + theme(axis.title.x = element_blank(),\n    axis.text.x = element_blank(), axis.ticks.x = element_blank()) + geom_dotplot(binaxis = \"y\",\n    stackdir = \"center\", stackratio = 0.2, alpha = 0.3, dotsize = 0.5, color = NA,\n    fill = \"red\", na.rm = TRUE) + facet_grid(. ~ Variable) + geom_rug(sides = \"l\")\np\n\n\n\n\n\n\n\n\n\n\nHistograms\nThe hist() function returns a histogram showing the complete empirical distribution of the data in binned categories, which is useful for checking skewwness of the data, symmetry, multi-modality, etc. Setting the argument freq=FALSE will scale the Y axis to represent the proportion of observations falling into each bin rather than the count.\n\n\nCHALLENGE\n\nMake histograms of the log() transformed population and area data from the “Country-Data-2016” file. Explore what happens if you set freq=FALSE versus the default of freq=TRUE. Try looking at other variables as well.\n\n\n\nShow Code\npar(mfrow = c(1, 2))  # sets up two panels side by side\nattach(d)  # lets us use variable names without specifying the data frame!\nhist(log(population), freq = FALSE, col = \"red\", main = \"Plot 1\", xlab = \"log(population size)\",\n    ylab = \"density\", ylim = c(0, 0.2))\nhist(log(area), freq = FALSE, col = \"red\", main = \"Plot 2\", xlab = \"log(area)\", ylab = \"density\",\n    ylim = c(0, 0.2))\n\n\n\n\n\n\n\n\n\n\nNOTE: You can add lines to your histograms (e.g., to show the mean value for a variable) using the abline() command, with arguments. For example, to show a single vertical line representing the mean log(population size), you would add the argument v=mean(log(area), na.rm=TRUE). We need to set na.rm to be TRUE otherwise the mean() function will not run the way we expect.\n\n\n\nShow Code\nhist(log(area), freq = FALSE, col = \"red\", main = \"Plot 2\", xlab = \"log(area)\", ylab = \"density\",\n    ylim = c(0, 0.2))\nabline(v = mean(log(area), na.rm = TRUE))\n\n\n\n\n\n\n\n\n\n\n\nDensity Plots\nThe density() function computes a non-parametric estimate of the distribution of a variable, which can be combined with other plots to also yield a graphical view of the distribution of the data. If your data have missing values, then you need to add the argument na.rm=TRUE to the density() function. To superimpose a density() curve on a histogram, you can use the lines(density()) function.\n\npar(mfrow = c(1, 1))  # set up one panel and redraw the log(population) histogram\nhist(log(population), freq = FALSE, col = \"white\", main = \"Density Plot with Mean\",\n    xlab = \"log(population size)\", ylab = \"density\", ylim = c(0, 0.2))\nabline(v = mean(log(population), na.rm = TRUE), col = \"blue\")\nlines(density(log(population), na.rm = TRUE), col = \"green\")\n\n\n\n\n\n\n\ndetach(d)\n\n\n\nTables\nThe table() function can be used to summarize counts and proportions for categorical variables in your dataset.\n\n\nCHALLENGE\n\nUsing the table() function, find what is the most common form of government in the “Country-Data-2016” dataset. How many countries have that form?\n\n\nHINT: We can combine table() with sort() and the argument decreasing=TRUE to get the desired answered straight away.\n\n\n\nShow Code\nt &lt;- sort(table(d$govt_form), decreasing = TRUE)\nt\n\n\nShow Output\n## \n##                                republic                 constitutional monarchy \n##                                     127                                      33 \n##              British overseas territory            overseas territory of France \n##                                      12                                       7 \n##           presidential federal republic                                monarchy \n##                                       7                                       6 \n##                  parliamentary monarchy          territory of the United States \n##                                       5                                       5 \n##                 parliamentary democracy                  territory of Australia \n##                                       4                                       4 \n## autonomous territory of the Netherlands                        federal republic \n##                                       3                                       3 \n##                        islamic republic                      socialist republic \n##                                       3                                       3 \n##                       absolute monarchy            autonomous region of Denmark \n##                                       2                                       2 \n##     autonomous territory of New Zealand            overseas community of France \n##                                       2                                       2 \n##          parliamentary federal republic  special administrative region of China \n##                                       2                                       2 \n##                     territory of Norway                       autonomous region \n##                                       2                                       1 \n##             autonomous region of France            autonomous region of Morocco \n##                                       1                                       1 \n##                              federation         foreign-administrated territory \n##                                       1                                       1 \n##      islamic-socialist peoples republic                  parliamentary republic \n##                                       1                                       1 \n##                        peoples republic                     territory of France \n##                                       1                                       1 \n##                territory of New Zealand            territory of the Netherlands \n##                                       1                                       1\n\n\n\nDoing the same thing with {dplyr} verbs (see Module 10) provides nicer output…\n\nt &lt;- group_by(d, govt_form) %&gt;%\n    summarize(count = n()) %&gt;%\n    arrange(desc(count))\nt\n\n## # A tibble: 33 × 2\n##    govt_form                      count\n##    &lt;chr&gt;                          &lt;int&gt;\n##  1 republic                         127\n##  2 constitutional monarchy           33\n##  3 British overseas territory        12\n##  4 overseas territory of France       7\n##  5 presidential federal republic      7\n##  6 monarchy                           6\n##  7 parliamentary monarchy             5\n##  8 territory of the United States     5\n##  9 parliamentary democracy            4\n## 10 territory of Australia             4\n## # ℹ 23 more rows",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "09-module.html#exploring-multiple-variables",
    "href": "09-module.html#exploring-multiple-variables",
    "title": "9  Exploratory Data Analysis",
    "section": "9.6 Exploring Multiple Variables",
    "text": "9.6 Exploring Multiple Variables\n\nPlotting Several Variables\n\nMultiple boxplots or histograms can be laid out side-by-side or overlaid.\n\n\n\nCHALLENGE\nRead in the dataset “KamilarAndCooperData”, which contains a host of summary information about 213 primate species.\nSpend some time exploring the data on your own and then make boxplots of log(female body mass) ~ family. Try doing this with {base} graphics and then look at how we might do in in {ggplot2}, which provides a standard “grammar of graphics” (see the {ggplot2} documentation)\n\n\nShow Code\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/KamilarAndCooperData.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\nhead(d)\n\n\nShow Output\n## # A tibble: 6 × 44\n##   Scientific_Name             Family        Genus Species Brain_Size_Species_M…¹\n##   &lt;chr&gt;                       &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;                    &lt;dbl&gt;\n## 1 Allenopithecus_nigroviridis Cercopitheci… Alle… nigrov…                   58.0\n## 2 Allocebus_trichotis         Cercopitheci… Allo… tricho…                   NA  \n## 3 Alouatta_belzebul           Atelidae      Alou… belzeb…                   52.8\n## 4 Alouatta_caraya             Atelidae      Alou… caraya                    52.6\n## 5 Alouatta_guariba            Atelidae      Alou… guariba                   51.7\n## 6 Alouatta_palliata           Atelidae      Alou… pallia…                   49.9\n## # ℹ abbreviated name: ¹​Brain_Size_Species_Mean\n## # ℹ 39 more variables: Brain_Size_Female_Mean &lt;dbl&gt;, Brain_size_Ref &lt;chr&gt;,\n## #   Body_mass_male_mean &lt;dbl&gt;, Body_mass_female_mean &lt;dbl&gt;,\n## #   Mass_Dimorphism &lt;dbl&gt;, Mass_Ref &lt;chr&gt;, MeanGroupSize &lt;dbl&gt;,\n## #   AdultMales &lt;dbl&gt;, AdultFemale &lt;dbl&gt;, AdultSexRatio &lt;dbl&gt;,\n## #   Social_Organization_Ref &lt;chr&gt;, InterbirthInterval_d &lt;dbl&gt;, Gestation &lt;dbl&gt;,\n## #   WeaningAge_d &lt;dbl&gt;, MaxLongevity_m &lt;dbl&gt;, LitterSz &lt;dbl&gt;, …\n\n\n\n\nlibrary(skimr)\nlibrary(kableExtra)\n\n\n\nShow Code\ns &lt;- skim(d)  # formats results to a wide table\n# here we make use of the `%&gt;%` operator and {dplyr} verbs... see below\ns %&gt;%\n    filter(skim_variable == \"Scientific_Name\" | skim_type == \"numeric\") %&gt;%\n    rename(variable = skim_variable, missing = n_missing, mean = numeric.mean, sd = numeric.sd,\n        min = numeric.p0, p25 = numeric.p25, median = numeric.p50, p75 = numeric.p75,\n        max = numeric.p100, hist = numeric.hist) %&gt;%\n    select(variable, missing, mean, sd, min, median, max, hist) %&gt;%\n    # drop p25 and p75 for purposes of display\nkable() %&gt;%\n    kable_styling(font_size = 10, full_width = FALSE)\n\n\n\n\n\nvariable\nmissing\nmean\nsd\nmin\nmedian\nmax\nhist\n\n\n\n\nScientific_Name\n0\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nBrain_Size_Species_Mean\n42\n68.109649\n7.768403e+01\n1.630\n61.45000\n491.2700\n▇▂▁▁▁\n\n\nBrain_Size_Female_Mean\n48\n65.235879\n7.371151e+01\n1.660\n57.20000\n480.1500\n▇▂▁▁▁\n\n\nBody_mass_male_mean\n18\n8111.800513\n1.920858e+04\n31.000\n4290.00000\n170400.0000\n▇▁▁▁▁\n\n\nBody_mass_female_mean\n18\n5396.473846\n1.008896e+04\n30.000\n3039.00000\n97500.0000\n▇▁▁▁▁\n\n\nMass_Dimorphism\n18\n1.246005\n3.212141e-01\n0.841\n1.10900\n2.6880\n▇▃▂▁▁\n\n\nMeanGroupSize\n60\n15.055065\n1.759245e+01\n1.000\n7.80000\n90.0000\n▇▂▁▁▁\n\n\nAdultMales\n68\n2.515621\n2.373255e+00\n0.900\n1.50000\n16.0000\n▇▁▁▁▁\n\n\nAdultFemale\n68\n5.048655\n5.428615e+00\n1.000\n2.90000\n25.2000\n▇▂▁▁▁\n\n\nAdultSexRatio\n84\n2.304729\n2.192575e+00\n0.500\n1.45000\n15.6000\n▇▁▁▁▁\n\n\nInterbirthInterval_d\n103\n572.058546\n3.549477e+02\n144.470\n476.93000\n2007.5000\n▇▅▁▁▁\n\n\nGestation\n75\n163.465362\n3.740063e+01\n59.990\n165.04000\n256.0000\n▁▃▇▃▂\n\n\nWeaningAge_d\n95\n310.042881\n2.546776e+02\n40.000\n237.73500\n1260.8100\n▇▅▂▁▁\n\n\nMaxLongevity_m\n66\n327.331701\n1.259496e+02\n103.000\n303.60000\n720.0000\n▅▇▆▂▁\n\n\nLitterSz\n47\n1.180542\n3.601098e-01\n0.990\n1.01000\n2.5200\n▇▁▁▁▁\n\n\nGR_MidRangeLat_dd\n38\n-1.796457\n1.362270e+01\n-24.500\n-0.76000\n35.8800\n▆▆▇▂▁\n\n\nPrecip_Mean_mm\n38\n1543.134286\n5.158597e+02\n419.000\n1541.90000\n2794.3000\n▂▅▇▅▂\n\n\nTemp_Mean_degC\n38\n23.132000\n3.405405e+00\n2.600\n24.30000\n27.4000\n▁▁▁▂▇\n\n\nAET_Mean_mm\n38\n1253.112000\n2.635696e+02\n453.100\n1291.10000\n1828.3000\n▁▃▆▇▂\n\n\nPET_Mean_mm\n38\n1553.158857\n1.567096e+02\n842.500\n1566.90000\n1927.3000\n▁▁▂▇▂\n\n\nHomeRange_km2\n65\n1.937905\n5.210604e+00\n0.002\n0.27500\n28.2400\n▇▁▁▁▁\n\n\nDayLength_km\n104\n1.551449\n1.439277e+00\n0.250\n1.21200\n11.0000\n▇▁▁▁▁\n\n\nTerritoriality\n109\n2.228885\n2.201259e+00\n0.225\n1.59235\n15.5976\n▇▁▁▁▁\n\n\nFruit\n98\n47.736956\n2.408200e+01\n1.000\n48.00000\n97.0000\n▅▇▇▇▃\n\n\nCanine_Dimorphism\n92\n1.617438\n6.722173e-01\n0.880\n1.56000\n5.2630\n▇▃▁▁▁\n\n\nFeed\n141\n33.075833\n1.366899e+01\n9.000\n33.30000\n63.9000\n▇▇▇▇▂\n\n\nMove\n143\n21.668857\n1.059124e+01\n3.000\n21.00000\n70.6000\n▅▇▂▁▁\n\n\nRest\n143\n34.259571\n1.967175e+01\n4.000\n30.48000\n78.5000\n▇▇▅▆▂\n\n\nSocial\n136\n7.368701\n5.242077e+00\n0.900\n5.40000\n23.5000\n▇▃▃▁▁\n\n\n\n\n\n\n\n\ndetach(package:kableExtra)\ndetach(package:skimr)\n\nWe can also make boxplots summarizing a numerical variable in relation to another categorical variable.\nPlotting using {base} graphics… the ~ operator can be read as “by”.\n\nboxplot(log(d$Body_mass_female_mean) ~ d$Family)\n\n\n\n\n\n\n\n\nAlternatively, plotting using {ggplot2}…\n\np &lt;- ggplot(data = d, aes(x = Family, y = log(Body_mass_female_mean)))\np &lt;- p + geom_boxplot(na.rm = TRUE)\np &lt;- p + theme(axis.text.x = element_text(angle = 90))\np &lt;- p + ylab(\"log(Female Body Mass)\")\np\n\n\n\n\n\n\n\n\n\n\nBivariate Scatterplots\nScatterplots are a natural tool for visualizing two continuous variables and can be made easily with the plot(x=&lt;variable 1&gt;, y=&lt;variable 2&gt;) function in {base} graphics (where &lt;variable 1&gt; and &lt;variable 2&gt; denote the names of the two variables you wish to plot). Transformations of the variables, e.g., log or square root (sqrt()), may be necessary for effective visualization.\n\n\nCHALLENGE\n\nAgain using data from the “KamilarAndCooperData” dataset, plot the relationship between female body size and female brain size. Then, play with log transforming the data and plot again.\n\n\n\nShow Code\npar(mfrow = c(1, 2))  # sets up two panels side by side\nplot(x = d$Body_mass_female_mean, y = d$Brain_Size_Female_Mean)\nplot(x = log(d$Body_mass_female_mean), y = log(d$Brain_Size_Female_Mean))\n\n\n\n\n\n\n\n\n\nThe grammar for {ggplot2} is a bit more complicated… see if you can follow it in the example below.\n\np &lt;- ggplot(data = d, aes(x = log(Body_mass_female_mean), y = log(Brain_Size_Female_Mean),\n    color = factor(Family)))  # first, we build a plot object and color points by Family\n# then we modify the axis labels\np &lt;- p + xlab(\"log(Female Body Mass)\") + ylab(\"log(Female Brain Size)\")\n# then we make a scatterplot\np &lt;- p + geom_point(na.rm = TRUE)\n# then we modify the legend\np &lt;- p + theme(legend.position = \"bottom\", legend.title = element_blank())\n# and, finally, we plot the object\np\n\n\n\n\n\n\n\n\nWe can use the cool package {ggExtra} to add marginal univariate plots to our bivariate scatterplots, too.\n\nlibrary(ggExtra)\nggMarginal(p, type = \"densigram\")  # try with other types, too!\n\n## Warning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\n## ℹ Please use `after_stat(density)` instead.\n## ℹ The deprecated feature was likely used in the ggExtra package.\n##   Please report the issue at &lt;https://github.com/daattali/ggExtra/issues&gt;.\n\n\n\n\n\n\n\n\ndetach(package:ggExtra)\n\nUsing {ggplot2}, we can also easily set up a grid for “faceting”” by a grouping variable…\n\np &lt;- p + facet_wrap(~Family, ncol = 4)  # wrap data 'by' family into 4 columns\np &lt;- p + theme(legend.position = \"none\")\np\n\n\n\n\n\n\n\n\nIn {ggplot2} can easily add regression lines to our plot. Here, we add a linear model to each facet.\n\np &lt;- p + geom_smooth(method = \"lm\", fullrange = FALSE, na.rm = TRUE)\np\n\n\n\n\n\n\n\n\nThe scatterplot() function from the {car} package also produces nice bivariate plots and includes barplots along the margins.\n\nlibrary(car)\n\n\nscatterplot(data = d, log(Brain_Size_Female_Mean) ~ log(Body_mass_female_mean), xlab = \"log(Female Body Mass\",\n    ylab = \"log(Female Brain Size\", boxplots = \"xy\", regLine = list(method = lm,\n        lty = 1, lwd = 2, col = \"red\"))\n\n\n\n\n\n\n\ndetach(package:car)\n\n\n\nCHALLENGE\n\nBuild your own bivariate scatterplot of log(MaxLongevity_m) by log(Body_mass_female_mean) using the “KamilarAndCooperData” dataset.\n\n\n\nShow Code\np &lt;- ggplot(data = d, aes(x = log(Body_mass_female_mean), y = log(MaxLongevity_m)))\np &lt;- p + geom_point(na.rm = TRUE)\np &lt;- p + geom_smooth(method = \"lm\", na.rm = TRUE)\np\n\n\n## `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe can also use either the pairs() function from {base} R, the scatterplotMatrix() function from the {car} package, or the pairs.panel() function from the {psych} package to do multiple scatterplots simulaneously. The ggpairs() function from the {GGally} package can also be used, but it is slower.\n\n\nCHALLENGE\nSelect the variables Brain_Size_Female_Mean, Body_mass_female_mean, MeanGroupSize, WeaningAge_d, MaxLongevity_m, HomeRange_km2, and DayLength_km from data frame d and plot scatterplots of all pairs of variables.\n\nNOTE: Here we are using the select() function from the {dplyr} package, which we cover in more detail in Module 10.\n\n\n\nShow Code\ns &lt;- select(d, c(\"Brain_Size_Female_Mean\", \"Body_mass_female_mean\", \"MeanGroupSize\",\n    \"WeaningAge_d\", \"MaxLongevity_m\", \"HomeRange_km2\", \"DayLength_km\"))\npairs(s[, 1:ncol(s)])  # or\n\n\n\n\n\n\n\n\n\nShow Code\npairs(data = s, ~.)  # NAs are ignored by default\n# adding argument `upper.panel=NULL` will suppress plots above the diagonal\n\n\nThe {car} package function scatterplotMatrix() provides a more customizable interface to the pairs() function from {base} R. It includes a neat rug plot for each variable.\n\nlibrary(car)\n\n\nscatterplotMatrix(s, smooth = TRUE, regLine = list(method = lm, lty = 1, lwd = 2),\n    ellipse = TRUE, upper.panel = NULL)\n\n\n\n\n\n\n\ndetach(package:car)\n\nThe {psych} package also makes it easy to construct a scatterplot matrix with customizable output using the pairs.panel() function. Here, method= specifies the correlation method, density= specifies whether to superimpose a density curve on the histogram, and elipses= specifies whether to show correlation elipses.\n\nlibrary(psych)\npairs.panels(s[], smooth = FALSE, lm = TRUE, method = \"pearson\", hist.col = \"#00AFBB\",\n    density = TRUE, ellipses = TRUE)\n\n\n\n\n\n\n\n# NAs are ignored by default\ndetach(package:psych)\n\nThe {GGally} package lets us do something similar…\n\nlibrary(GGally)\nggpairs(s, columns = 1:ncol(s))  # NAs produce a warning\n\n\n\n\n\n\n\ndetach(package:GGally)\n\nFinally, we can do a nice correlation plot using the {corrplot} package, where we get a matrix of correlations among variables and a graphical representation of the strength and direction of the bivariate correlation coefficients among them.\n\nlibrary(corrplot)\ncor = cor(s, use = \"pairwise.complete.obs\")\ncorrplot.mixed(cor, lower.col = \"black\", number.cex = 0.7)\n\n\n\n\n\n\n\ndetach(package:corrplot)",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "09-module.html#custom-layouts-for-multiple-plots",
    "href": "09-module.html#custom-layouts-for-multiple-plots",
    "title": "9  Exploratory Data Analysis",
    "section": "9.7 Custom Layouts for Multiple Plots",
    "text": "9.7 Custom Layouts for Multiple Plots\nAbove, we saw the typical {base} R method for laying out plots, using the par() command with the mfrow= argument to set up a grid of panels in which to plot to the current graphics output device. There are, however, several easier ways for combining plots into custom layouts. Three alternatives are provided in the {patchwork}, {cowplot}, and {gridExtra} packages. Each starts by generating plot objects, typically using {ggplot2}. First, we create 3 plot objects using the KamilarAndCooper dataset:\n\np1 &lt;- ggplot(data = d, aes(x = log(Body_mass_female_mean), y = log(MaxLongevity_m)))\np1 &lt;- p1 + geom_point(na.rm = TRUE)\np1 &lt;- p1 + geom_smooth(method = \"lm\", na.rm = TRUE)\np1 &lt;- p1 + xlab(\"log(Female Body Size)\") + ylab(\"log(Lifespan)\")\n\np2 &lt;- ggplot(data = d, aes(x = log(Body_mass_male_mean), y = log(MaxLongevity_m)))\np2 &lt;- p2 + geom_point(na.rm = TRUE)\np2 &lt;- p2 + geom_smooth(method = \"lm\", na.rm = TRUE)\np2 &lt;- p2 + xlab(\"log(Male Body Size)\") + ylab(\"log(Lifespan)\")\n\np3 &lt;- ggplot(data = d, aes(x = Family, y = log(Body_mass_female_mean)))\np3 &lt;- p3 + geom_boxplot(na.rm = TRUE)\np3 &lt;- p3 + theme(axis.text.x = element_text(angle = 90))\np3 &lt;- p3 + xlab(\"Family\") + ylab(\"log(Female Body Mass)\")\n\nOnce we have these plot objects, we can arrange them in custom ways using these different packages. Some alternative ways of arranging these three plots in 2 rows, with 2 panels in the top row and 1 in the bottom row, are given below…\nThe {patchwork} package is perhaps the easiest to use!\n\nlibrary(patchwork)\n(p1 | p2)/p3 + plot_annotation(tag_levels = \"A\")\n\n\n\n\n\n\n\ndetach(package:patchwork)\n\nWe can also use {cowplot}…\n\nlibrary(cowplot)\nplot_grid(plot_grid(p1, p2, labels = c(\"A\", \"B\"), label_size = 12, nrow = 1), p3,\n    labels = c(\"\", \"C\"), label_size = 12, nrow = 2)\n\n\n\n\n\n\n\ndetach(package:cowplot)\n\n… or {gridExtra}\n\nlibrary(gridExtra)\ngrid.arrange(grid.arrange(p1, p2, nrow = 1), p3, nrow = 2)\n\n\n\n\n\n\n\ndetach(package:gridExtra)",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "09-module.html#aggregate-statistics",
    "href": "09-module.html#aggregate-statistics",
    "title": "9  Exploratory Data Analysis",
    "section": "9.8 Aggregate Statistics",
    "text": "9.8 Aggregate Statistics\nTo calculate summary statistics for groups of observations in a data frame, there are many different approaches. One is to use the aggregate() function from the {stats} package (a {base} R standard package), which provides a quick way to look at summary statistics for sets of observations, though it requires a bit of clunky code. Here, we apply a particular function (FUN = \"mean\") to mean female body mass, grouped by Family.\n\naggregate(d$Body_mass_female_mean ~ d$Family, FUN = \"mean\", na.rm = TRUE)\n\n##           d$Family d$Body_mass_female_mean\n## 1         Atelidae               6616.2000\n## 2          Cebidae                876.3323\n## 3  Cercopithecidae               6327.8247\n## 4    Cheirogalidae                186.0286\n## 5    Daubentonidae               2490.0000\n## 6        Galagidae                371.6143\n## 7        Hominidae              53443.7167\n## 8      Hylobatidae               6682.1200\n## 9        Indriidae               3886.5333\n## 10       Lemuridae               1991.1200\n## 11   Lepilemuridae                813.5000\n## 12       Lorisidae                489.8625\n## 13     Pitheciidae               1768.5000\n## 14       Tarsiidae                120.0000\n\n\nOr, alternatively…\n\naggregate(x = d[\"Body_mass_female_mean\"], by = d[\"Family\"], FUN = \"mean\", na.rm = TRUE)\n\n##             Family Body_mass_female_mean\n## 1         Atelidae             6616.2000\n## 2          Cebidae              876.3323\n## 3  Cercopithecidae             6327.8247\n## 4    Cheirogalidae              186.0286\n## 5    Daubentonidae             2490.0000\n## 6        Galagidae              371.6143\n## 7        Hominidae            53443.7167\n## 8      Hylobatidae             6682.1200\n## 9        Indriidae             3886.5333\n## 10       Lemuridae             1991.1200\n## 11   Lepilemuridae              813.5000\n## 12       Lorisidae              489.8625\n## 13     Pitheciidae             1768.5000\n## 14       Tarsiidae              120.0000\n\n\n\n# | include: false\ndetach(package:curl)\ndetach(package:tidyverse)",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "09-module.html#concept-review",
    "href": "09-module.html#concept-review",
    "title": "9  Exploratory Data Analysis",
    "section": "Concept Review",
    "text": "Concept Review\n\nSummary statistics: summary(), skim()\nBasic plotting: boxplot(), barplot(), histogram()\nPlotting with {ggplot2}: ggplot(data = , mapping = aes()) + geom() + theme() + ...\nSummarizing by groups: aggregrate()\nArranging plots with {patchwork}, {cowplot}, and {gridExtra}",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "10-module.html",
    "href": "10-module.html",
    "title": "10  Data Wrangling",
    "section": "",
    "text": "10.1 Objectives",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "10-module.html#objectives",
    "href": "10-module.html#objectives",
    "title": "10  Data Wrangling",
    "section": "",
    "text": "The objective of this module to introduce you to data wrangling and transformation using the {dplyr} and {tidyr} packages, which are part of the {tidyverse} family.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "10-module.html#preliminaries",
    "href": "10-module.html#preliminaries",
    "title": "10  Data Wrangling",
    "section": "10.2 Preliminaries",
    "text": "10.2 Preliminaries\n\nInstall (but do not load yet) this package in R: {tidylog}\nInstall and load this package: {magrittr}\nLoad {tidyverse}\nLoad in the KamilarAndCooper dataset we used in Module 09 as a “tibble” named d\n\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/KamilarAndCooperData.csv\"\nd &lt;- read_csv(f, col_names = TRUE)  # creates a 'tibble'\nhead(d)\n\n## # A tibble: 6 × 44\n##   Scientific_Name             Family        Genus Species Brain_Size_Species_M…¹\n##   &lt;chr&gt;                       &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;                    &lt;dbl&gt;\n## 1 Allenopithecus_nigroviridis Cercopitheci… Alle… nigrov…                   58.0\n## 2 Allocebus_trichotis         Cercopitheci… Allo… tricho…                   NA  \n## 3 Alouatta_belzebul           Atelidae      Alou… belzeb…                   52.8\n## 4 Alouatta_caraya             Atelidae      Alou… caraya                    52.6\n## 5 Alouatta_guariba            Atelidae      Alou… guariba                   51.7\n## 6 Alouatta_palliata           Atelidae      Alou… pallia…                   49.9\n## # ℹ abbreviated name: ¹​Brain_Size_Species_Mean\n## # ℹ 39 more variables: Brain_Size_Female_Mean &lt;dbl&gt;, Brain_size_Ref &lt;chr&gt;,\n## #   Body_mass_male_mean &lt;dbl&gt;, Body_mass_female_mean &lt;dbl&gt;,\n## #   Mass_Dimorphism &lt;dbl&gt;, Mass_Ref &lt;chr&gt;, MeanGroupSize &lt;dbl&gt;,\n## #   AdultMales &lt;dbl&gt;, AdultFemale &lt;dbl&gt;, AdultSexRatio &lt;dbl&gt;,\n## #   Social_Organization_Ref &lt;chr&gt;, InterbirthInterval_d &lt;dbl&gt;, Gestation &lt;dbl&gt;,\n## #   WeaningAge_d &lt;dbl&gt;, MaxLongevity_m &lt;dbl&gt;, LitterSz &lt;dbl&gt;, …",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "10-module.html#data-wrangling-using-dplyr",
    "href": "10-module.html#data-wrangling-using-dplyr",
    "title": "10  Data Wrangling",
    "section": "10.3 Data Wrangling Using {dplyr}",
    "text": "10.3 Data Wrangling Using {dplyr}\nThe {dplyr} package, included in the {tidyverse}, provides “a flexible grammar of data manipulation” that makes many of the manipulations that we explore in Module 07 and Module 09 much easier and much more intuitive!\n\n\n\n\n\n\n\n\n\nAmong other functions, {dplyr} introduces a set of verbs (filter(), select(), arrange(), rename(), mutate(), summarize(), and group_by()) that can be used to perform useful operations on “tibbles” and related tabular data structures (e.g., normal data frames and data tables). Before using {dplyr} for summarizing data and producing aggregate statistics, let’s look in general at what we can do with these verbs…\n\nfilter()\nThe filter() function lets us pull out rows from a data frame that meet a particular criterion or set of criteria:\n\n\n\n\n\n\n\n\n\n\n\nFROM: Baumer et al. (2017). Modern Data Science with R. Chapman and Hall/CRC.\n\n\n\n\n# selecting rows..\ns &lt;- filter(d, Family == \"Hominidae\" & Mass_Dimorphism &gt; 2)\nhead(s)\n\n## # A tibble: 3 × 44\n##   Scientific_Name Family    Genus   Species  Brain_Size_Species_Mean\n##   &lt;chr&gt;           &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;                      &lt;dbl&gt;\n## 1 Gorilla_gorilla Hominidae Gorilla gorilla                     490.\n## 2 Pongo_abelii    Hominidae Pongo   abelii                      390.\n## 3 Pongo_pygmaeus  Hominidae Pongo   pygmaeus                    377.\n## # ℹ 39 more variables: Brain_Size_Female_Mean &lt;dbl&gt;, Brain_size_Ref &lt;chr&gt;,\n## #   Body_mass_male_mean &lt;dbl&gt;, Body_mass_female_mean &lt;dbl&gt;,\n## #   Mass_Dimorphism &lt;dbl&gt;, Mass_Ref &lt;chr&gt;, MeanGroupSize &lt;dbl&gt;,\n## #   AdultMales &lt;dbl&gt;, AdultFemale &lt;dbl&gt;, AdultSexRatio &lt;dbl&gt;,\n## #   Social_Organization_Ref &lt;chr&gt;, InterbirthInterval_d &lt;dbl&gt;, Gestation &lt;dbl&gt;,\n## #   WeaningAge_d &lt;dbl&gt;, MaxLongevity_m &lt;dbl&gt;, LitterSz &lt;dbl&gt;,\n## #   Life_History_Ref &lt;chr&gt;, GR_MidRangeLat_dd &lt;dbl&gt;, Precip_Mean_mm &lt;dbl&gt;, …\n\n\n\nNOTE: The first argument of any of the {dplyr} verbs is the .data argument. That is the line of code above is equivalent to s &lt;- filter(.data = d, Family == \"Hominidae\" & Mass_Dimorphism &gt; 2)\n\n\n\nselect()\nThe select() function lets us pull out only particular columns from a data frame:\n\n\n\n\n\n\n\n\n\n\n\nFROM: Baumer et al. (2017). Modern Data Science with R. Chapman and Hall/CRC.\n\n\n\n\n# selecting specific columns...\ns &lt;- select(d, Family, Genus, Body_mass_male_mean)\nhead(s)\n\n## # A tibble: 6 × 3\n##   Family          Genus          Body_mass_male_mean\n##   &lt;chr&gt;           &lt;chr&gt;                        &lt;dbl&gt;\n## 1 Cercopithecidae Allenopithecus                6130\n## 2 Cercopithecidae Allocebus                       92\n## 3 Atelidae        Alouatta                      7270\n## 4 Atelidae        Alouatta                      6525\n## 5 Atelidae        Alouatta                      5800\n## 6 Atelidae        Alouatta                      7150\n\n\n\n\narrange()\nThe arrange() function lets us sort a data frame based on a select variable or set of variables:\n\n\n\n\n\n\n\n\n\n\n\nFROM: Baumer et al. (2017). Modern Data Science with R. Chapman and Hall/CRC.\n\n\n\n\n# reordering a data frame by a set of variables...\ns &lt;- arrange(d, Family, Genus, Body_mass_male_mean)\nhead(s)\n\n## # A tibble: 6 × 44\n##   Scientific_Name    Family   Genus    Species   Brain_Size_Species_Mean\n##   &lt;chr&gt;              &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;                       &lt;dbl&gt;\n## 1 Alouatta_guariba   Atelidae Alouatta guariba                      51.7\n## 2 Alouatta_caraya    Atelidae Alouatta caraya                       52.6\n## 3 Alouatta_seniculus Atelidae Alouatta seniculus                    55.2\n## 4 Alouatta_palliata  Atelidae Alouatta palliata                     49.9\n## 5 Alouatta_belzebul  Atelidae Alouatta belzebul                     52.8\n## 6 Alouatta_pigra     Atelidae Alouatta pigra                        51.1\n## # ℹ 39 more variables: Brain_Size_Female_Mean &lt;dbl&gt;, Brain_size_Ref &lt;chr&gt;,\n## #   Body_mass_male_mean &lt;dbl&gt;, Body_mass_female_mean &lt;dbl&gt;,\n## #   Mass_Dimorphism &lt;dbl&gt;, Mass_Ref &lt;chr&gt;, MeanGroupSize &lt;dbl&gt;,\n## #   AdultMales &lt;dbl&gt;, AdultFemale &lt;dbl&gt;, AdultSexRatio &lt;dbl&gt;,\n## #   Social_Organization_Ref &lt;chr&gt;, InterbirthInterval_d &lt;dbl&gt;, Gestation &lt;dbl&gt;,\n## #   WeaningAge_d &lt;dbl&gt;, MaxLongevity_m &lt;dbl&gt;, LitterSz &lt;dbl&gt;,\n## #   Life_History_Ref &lt;chr&gt;, GR_MidRangeLat_dd &lt;dbl&gt;, Precip_Mean_mm &lt;dbl&gt;, …\n\n\nWe can also specify the direction in which we want the data frame to be sorted:\n\n# `desc()` can be used to reverse the order\ns &lt;- arrange(d, desc(Family), Genus, Species, desc(Body_mass_male_mean))\nhead(s)\n\n## # A tibble: 6 × 44\n##   Scientific_Name         Family      Genus      Species  Brain_Size_Species_M…¹\n##   &lt;chr&gt;                   &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt;                     &lt;dbl&gt;\n## 1 Tarsius_bancanus        Tarsiidae   Tarsius    bancanus                   3.16\n## 2 Tarsius_dentatus        Tarsiidae   Tarsius    dentatus                  NA   \n## 3 Tarsius_syrichta        Tarsiidae   Tarsius    syrichta                   3.36\n## 4 Cacajao_calvus          Pitheciidae Cacajao    calvus                    76   \n## 5 Cacajao_melanocephalus  Pitheciidae Cacajao    melanoc…                  68.8 \n## 6 Callicebus_donacophilus Pitheciidae Callicebus donacop…                  NA   \n## # ℹ abbreviated name: ¹​Brain_Size_Species_Mean\n## # ℹ 39 more variables: Brain_Size_Female_Mean &lt;dbl&gt;, Brain_size_Ref &lt;chr&gt;,\n## #   Body_mass_male_mean &lt;dbl&gt;, Body_mass_female_mean &lt;dbl&gt;,\n## #   Mass_Dimorphism &lt;dbl&gt;, Mass_Ref &lt;chr&gt;, MeanGroupSize &lt;dbl&gt;,\n## #   AdultMales &lt;dbl&gt;, AdultFemale &lt;dbl&gt;, AdultSexRatio &lt;dbl&gt;,\n## #   Social_Organization_Ref &lt;chr&gt;, InterbirthInterval_d &lt;dbl&gt;, Gestation &lt;dbl&gt;,\n## #   WeaningAge_d &lt;dbl&gt;, MaxLongevity_m &lt;dbl&gt;, LitterSz &lt;dbl&gt;, …\n\n\n\n\nrename()\nThe rename() function allows us to change the names of particular columns in a data frame:\n\n# renaming columns...\ns &lt;- rename(d, Female_Mass = Body_mass_female_mean)\nhead(s$Female_Mass)\n\n## [1] 3180   84 5520 4240 4550 5350\n\n\n\n\nmutate()\nThe mutate() function allows us to add new columns to a data frame:\n\n\n\n\n\n\n\n\n\n\n\nFROM: Baumer et al. (2017). Modern Data Science with R. Chapman and Hall/CRC.\n\n\n\n\n# and adding new columns...\ns &lt;- mutate(d, Binomial = paste(Genus, Species, sep = \" \"))\nhead(s$Binomial)  # or head(s[['Binomial']])\n\n## [1] \"Allenopithecus nigroviridis\" \"Allocebus trichotis\"        \n## [3] \"Alouatta belzebul\"           \"Alouatta caraya\"            \n## [5] \"Alouatta guariba\"            \"Alouatta palliata\"\n\n\n\n\nsummarize() and group_by()\nThe {dplyr} package makes it easy to summarize data using more convenient functions than the {base} function aggregate(), which we looked at in Module 09. The summarize() function specifies a list of summary variables that will appear in the output, along with the operations that will be performed on vectors in the data frame to produce those summary variables:\n\n\n\n\n\n\n\n\n\n\n\nFROM: Baumer et al. (2017). Modern Data Science with R. Chapman and Hall/CRC.\n\n\n\n\n# the n() function returns the number of rows in the data frame\ns &lt;- summarize(d, n_cases = n(), avgF = mean(Body_mass_female_mean, na.rm = TRUE),\n    avgM = mean(Body_mass_male_mean, na.rm = TRUE))\ns\n\n## # A tibble: 1 × 3\n##   n_cases  avgF  avgM\n##     &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1     213 5396. 8112.\n\n\nAdditionally, the group_by() function allows us to construct these summary variables for sets of observations defined by a particular categorical variable, as we did above with aggregate().\n\nby_family &lt;- group_by(d, Family)\n# here, n() returns the number of rows in the group being considered\ns &lt;- summarise(by_family, n_cases = n(), avgF = mean(Body_mass_female_mean, na.rm = TRUE),\n    avgM = mean(Body_mass_male_mean, na.rm = TRUE))\ns\n\n## # A tibble: 14 × 4\n##    Family          n_cases   avgF   avgM\n##    &lt;chr&gt;             &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n##  1 Atelidae             12  6616.  7895.\n##  2 Cebidae              37   876.  1012.\n##  3 Cercopithecidae      79  6328.  9543.\n##  4 Cheirogalidae         7   186.   193.\n##  5 Daubentonidae         1  2490   2620 \n##  6 Galagidae             7   372.   395.\n##  7 Hominidae             6 53444. 98681.\n##  8 Hylobatidae          11  6682.  6926.\n##  9 Indriidae             9  3887.  3638.\n## 10 Lemuridae            17  1991.  2077.\n## 11 Lepilemuridae         6   814.   792 \n## 12 Lorisidae             8   490.   512.\n## 13 Pitheciidae          10  1768.  1955.\n## 14 Tarsiidae             3   120    131",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "10-module.html#other-useful-dplyr-functions",
    "href": "10-module.html#other-useful-dplyr-functions",
    "title": "10  Data Wrangling",
    "section": "10.4 Other Useful {dplyr} Functions",
    "text": "10.4 Other Useful {dplyr} Functions\n\nungroup() - clears group metadata from a table put in place group_by()\nbind_rows() and bind_cols() - adds rows and columns, respectively, to a dataframe or tibble; when binding rows, if the column names do not match, the column will still be added and missing values filled with NA; when binding columns, the number of rows in each dataframe needs to be the same\npull() - pulls a single variable out of a dataframe as a vector\nsample_n() - randomly samples a set of “size=” rows from a dataframe with (“replace=TRUE”) or without (“replace=FALSE”) replacement; this function is being superceded in favor of slice_sample(), where an additional argument (n= or prop=) allows you to specify the number or proportion of rows, respectively, to sample randomly\ndrop_na() - drops rows from a dataframe that have NA values for any variable names passed as arguments to the function\nrowwise() - allows you to explicitly perform functions on a data frame on a row-at-a-time basis, which is useful if a vectorized function does not exit\n\nA full list of {dplyr} functions and their descriptions is available here.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "10-module.html#joining-tables",
    "href": "10-module.html#joining-tables",
    "title": "10  Data Wrangling",
    "section": "10.5 Joining Tables",
    "text": "10.5 Joining Tables\nOne of the other major forms of data wrangling that we often need to do is to combine variables from different tabular data structures into a new table. This process is often referred to as performing a “mutating join” or simply a “join”.\n\nNOTE: For those with experience with other database systems, it is related to the “JOIN” commands in SQL.\n\nThe process works by matching observations in two different tables by a common key variable and then selecting additional variables of interest to pull from each of the tables. A simple example is the following… suppose we have two tables, one that contains average brain sizes for particular species of primates and one that contains individual body sizes for some of the same species, plus others. In the latter table, too, we may have data from multiple individuals of the same species represented.\n\ntable1 &lt;- tibble(Taxon = c(\"Gorilla\", \"Human\", \"Chimpanzee\", \"Orangutan\", \"Baboon\"),\n    Avg_Brain_Size = c(470, 1100, 350, 340, 140))\ntable1\n\n## # A tibble: 5 × 2\n##   Taxon      Avg_Brain_Size\n##   &lt;chr&gt;               &lt;dbl&gt;\n## 1 Gorilla               470\n## 2 Human                1100\n## 3 Chimpanzee            350\n## 4 Orangutan             340\n## 5 Baboon                140\n\ntable2 &lt;- tibble(Taxon = c(\"Gorilla\", \"Gorilla\", \"Gorilla\", \"Human\", \"Human\", \"Chimpanzee\",\n    \"Orangutan\", \"Orangutan\", \"Macaque\", \"Macaque\", \"Macaque\"), Body_Weight = c(80,\n    81, 77, 48, 49, 38, 37, 36, 6, 7, 6))\ntable2\n\n## # A tibble: 11 × 2\n##    Taxon      Body_Weight\n##    &lt;chr&gt;            &lt;dbl&gt;\n##  1 Gorilla             80\n##  2 Gorilla             81\n##  3 Gorilla             77\n##  4 Human               48\n##  5 Human               49\n##  6 Chimpanzee          38\n##  7 Orangutan           37\n##  8 Orangutan           36\n##  9 Macaque              6\n## 10 Macaque              7\n## 11 Macaque              6\n\n\n\nInner Joins\nAn inner join or equijoin matches up sets of observations between two tables whenever their keys are equal. The output of an inner join is a new data frame that contains all rows from the left-hand (x) and right-hand (y) tables where there are matching values in the key column, plus all columns from x and y. If there are multiple matches between the tables, all combination of the matches are returned. This is represented schematically below:\n\n\n\n\n\n\n\n\n\n\n\nFROM: Wickham & Grolemund (2017). R for Data Science. O’Reilly Media, Inc.\n\n\n\n\ninner_join(table1, table2, by = \"Taxon\")\n\n## # A tibble: 8 × 3\n##   Taxon      Avg_Brain_Size Body_Weight\n##   &lt;chr&gt;               &lt;dbl&gt;       &lt;dbl&gt;\n## 1 Gorilla               470          80\n## 2 Gorilla               470          81\n## 3 Gorilla               470          77\n## 4 Human                1100          48\n## 5 Human                1100          49\n## 6 Chimpanzee            350          38\n## 7 Orangutan             340          37\n## 8 Orangutan             340          36\n\n\n\n\nOuter Joins\nWhile an inner join keeps only observations that appear in both tables, different flavors of outer joins keep observations that appear in at least one of the tables. There are three types of outer joins:\n\nA left join returns all rows from the left-hand table, x, and all columns from x and y. Rows in x with no match in y will have NA values in the new columns from the y table. If there are multiple matches between x and *y, all combinations of the matches are returned.\nA right join returns all rows from the right-hand table, y, and all columns from x and y. Rows in y with no match in x will have NA values in the new columns from the x table. If there are multiple matches between x and y, all combinations of the matches are returned.\nA full join returns all rows and all columns in both the left-hand (x) and right-hand (y) tables, joining them where there are matches. Where there are not matching values, the join returns NA for the columns from table where they are missing.\n\nThe following figure shows a schematic representation of these various types of outer joins:\n\n\n\n\n\n\n\n\n\n\n\nFROM: Wickham & Grolemund (2017). R for Data Science. O’Reilly Media, Inc.\n\n\n\n\nleft_join(table1, table2, by = \"Taxon\")\n\n## # A tibble: 9 × 3\n##   Taxon      Avg_Brain_Size Body_Weight\n##   &lt;chr&gt;               &lt;dbl&gt;       &lt;dbl&gt;\n## 1 Gorilla               470          80\n## 2 Gorilla               470          81\n## 3 Gorilla               470          77\n## 4 Human                1100          48\n## 5 Human                1100          49\n## 6 Chimpanzee            350          38\n## 7 Orangutan             340          37\n## 8 Orangutan             340          36\n## 9 Baboon                140          NA\n\nright_join(table1, table2, by = \"Taxon\")\n\n## # A tibble: 11 × 3\n##    Taxon      Avg_Brain_Size Body_Weight\n##    &lt;chr&gt;               &lt;dbl&gt;       &lt;dbl&gt;\n##  1 Gorilla               470          80\n##  2 Gorilla               470          81\n##  3 Gorilla               470          77\n##  4 Human                1100          48\n##  5 Human                1100          49\n##  6 Chimpanzee            350          38\n##  7 Orangutan             340          37\n##  8 Orangutan             340          36\n##  9 Macaque                NA           6\n## 10 Macaque                NA           7\n## 11 Macaque                NA           6\n\nfull_join(table1, table2, by = \"Taxon\")\n\n## # A tibble: 12 × 3\n##    Taxon      Avg_Brain_Size Body_Weight\n##    &lt;chr&gt;               &lt;dbl&gt;       &lt;dbl&gt;\n##  1 Gorilla               470          80\n##  2 Gorilla               470          81\n##  3 Gorilla               470          77\n##  4 Human                1100          48\n##  5 Human                1100          49\n##  6 Chimpanzee            350          38\n##  7 Orangutan             340          37\n##  8 Orangutan             340          36\n##  9 Baboon                140          NA\n## 10 Macaque                NA           6\n## 11 Macaque                NA           7\n## 12 Macaque                NA           6\n\n\n\n\nOther Joins\nThere are also two additional join types that may be sometimes be useful… note that these joins only return columns from the left-hand table, x.\n\nA semi_join returns rows from the left-hand table, x, where there are matching values in y, but keeping just the columns from x. A semi_join differs from an inner_join because an inner_join will return a row of x for every matching row of y (so some x rows can be duplicated), whereas a semi_join will never duplicate rows of x.\nAn anti_join returns all rows from the left-hand table,x where there are not matching values in y, keeping just the columns from x.\n\n\nsemi_join(table1, table2, by = \"Taxon\")\n\n## # A tibble: 4 × 2\n##   Taxon      Avg_Brain_Size\n##   &lt;chr&gt;               &lt;dbl&gt;\n## 1 Gorilla               470\n## 2 Human                1100\n## 3 Chimpanzee            350\n## 4 Orangutan             340\n\nanti_join(table1, table2, by = \"Taxon\")\n\n## # A tibble: 1 × 2\n##   Taxon  Avg_Brain_Size\n##   &lt;chr&gt;           &lt;dbl&gt;\n## 1 Baboon            140\n\n\nThe cheatsheet on Data Transformation with {dplyr} provides a nice overview of these and additional data wrangling functions included the {dplyr} package.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "10-module.html#chaining-and-piping",
    "href": "10-module.html#chaining-and-piping",
    "title": "10  Data Wrangling",
    "section": "10.6 Chaining and Piping",
    "text": "10.6 Chaining and Piping\nOne other cool thing about the {dplyr} package is that it provides a convenient way to chain together operations on a data frame using the “forward pipe” operator (%&gt;%). The %&gt;% operator basically takes what is on the left-hand side (LHS) of the operator and directly applies the function call on the right-hand side (RHS) of the operator to it. That is, it “pipes” what is on the LHS of the operator directly to the first argument of the function on the right. This process allows us to build of chains of successive operations, each one being applied to the outcome of the previous operation in the chain.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEXAMPLE:\n\n# this...\nd %&gt;%\n    select(Scientific_Name, Body_mass_female_mean) %&gt;%\n    head()\n\n## # A tibble: 6 × 2\n##   Scientific_Name             Body_mass_female_mean\n##   &lt;chr&gt;                                       &lt;dbl&gt;\n## 1 Allenopithecus_nigroviridis                  3180\n## 2 Allocebus_trichotis                            84\n## 3 Alouatta_belzebul                            5520\n## 4 Alouatta_caraya                              4240\n## 5 Alouatta_guariba                             4550\n## 6 Alouatta_palliata                            5350\n\n# is equivalent to...\nhead(select(d, Scientific_Name, Body_mass_female_mean))\n\n## # A tibble: 6 × 2\n##   Scientific_Name             Body_mass_female_mean\n##   &lt;chr&gt;                                       &lt;dbl&gt;\n## 1 Allenopithecus_nigroviridis                  3180\n## 2 Allocebus_trichotis                            84\n## 3 Alouatta_belzebul                            5520\n## 4 Alouatta_caraya                              4240\n## 5 Alouatta_guariba                             4550\n## 6 Alouatta_palliata                            5350\n\n\nThe forward pipe is useful because it allows us to write and follow code from left to right (as when writing in English), instead of right to left with many nested parentheses.\n\nCHALLENGE\n\nIn one line of code, do the following:\n\nAdd a variable, Binomial to our data frame d, which is a concatenation of the Genus and Species…\nTrim the data frame to only include the variables Binomial, Family, Body_mass_female_mean, Body_mass_male_mean and Mass_Dimorphism…\nGroup these variables by Family…\nCalculate the average value for female body mass, male body mass, and mass dimorphism (remember, you will need to specify na.rm = TRUE…)\nAnd arrange by decreasing average mass dimorphism.\n\n\n\n\nShow Code\ns &lt;- mutate(d, Binomial = paste(Genus, Species, sep = \" \")) %&gt;%\n    select(Binomial, Family, Body_mass_female_mean, Body_mass_male_mean, Mass_Dimorphism) %&gt;%\n    group_by(Family) %&gt;%\n    summarise(avgF = mean(Body_mass_female_mean, na.rm = TRUE), avgM = mean(Body_mass_male_mean,\n        na.rm = TRUE), avgBMD = mean(Mass_Dimorphism, na.rm = TRUE)) %&gt;%\n    arrange(desc(avgBMD))\ns\n\n\nShow Output\n## # A tibble: 14 × 4\n##    Family            avgF   avgM avgBMD\n##    &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n##  1 Hominidae       53444. 98681.  1.81 \n##  2 Cercopithecidae  6328.  9543.  1.49 \n##  3 Atelidae         6616.  7895.  1.23 \n##  4 Tarsiidae         120    131   1.09 \n##  5 Pitheciidae      1768.  1955.  1.09 \n##  6 Cebidae           876.  1012.  1.07 \n##  7 Lemuridae        1991.  2077.  1.06 \n##  8 Daubentonidae    2490   2620   1.05 \n##  9 Lorisidae         490.   512.  1.05 \n## 10 Galagidae         372.   395.  1.05 \n## 11 Hylobatidae      6682.  6926.  1.03 \n## 12 Cheirogalidae     186.   193.  1.02 \n## 13 Lepilemuridae     814.   792   0.980\n## 14 Indriidae        3887.  3638.  0.950\n\n\n\nThere are several other, very cool, “special case” pipe operators that are useful in particular situations. These are available from the {magrittr} package. [Actually, the functionality of the forward pipe operator also comes from the {magrittr} package, but it is replicated in {dplyr}.]\n\nThe “tee” pipe (%T&gt;%) allows you to pipe the outcome of a process into a new expression (just like the forward pipe operator does) and to simultaneously return the original value instead of the forward-piped result to an intermediate expression. This is useful, for example, for printing out or plotting intermediate results. In the example below, we filter our data frame for just observations of the genus Alouatta, print those to the screen as an intermediate side effect, and pass the filtered data to the summarise() function.\n\n\ns &lt;- filter(d, Genus == \"Alouatta\") %T&gt;%\n    print() %&gt;%\n    summarise(avgF = mean(Body_mass_female_mean, na.rm = TRUE))\n\n## # A tibble: 6 × 44\n##   Scientific_Name    Family   Genus    Species   Brain_Size_Species_Mean\n##   &lt;chr&gt;              &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;                       &lt;dbl&gt;\n## 1 Alouatta_belzebul  Atelidae Alouatta belzebul                     52.8\n## 2 Alouatta_caraya    Atelidae Alouatta caraya                       52.6\n## 3 Alouatta_guariba   Atelidae Alouatta guariba                      51.7\n## 4 Alouatta_palliata  Atelidae Alouatta palliata                     49.9\n## 5 Alouatta_pigra     Atelidae Alouatta pigra                        51.1\n## 6 Alouatta_seniculus Atelidae Alouatta seniculus                    55.2\n## # ℹ 39 more variables: Brain_Size_Female_Mean &lt;dbl&gt;, Brain_size_Ref &lt;chr&gt;,\n## #   Body_mass_male_mean &lt;dbl&gt;, Body_mass_female_mean &lt;dbl&gt;,\n## #   Mass_Dimorphism &lt;dbl&gt;, Mass_Ref &lt;chr&gt;, MeanGroupSize &lt;dbl&gt;,\n## #   AdultMales &lt;dbl&gt;, AdultFemale &lt;dbl&gt;, AdultSexRatio &lt;dbl&gt;,\n## #   Social_Organization_Ref &lt;chr&gt;, InterbirthInterval_d &lt;dbl&gt;, Gestation &lt;dbl&gt;,\n## #   WeaningAge_d &lt;dbl&gt;, MaxLongevity_m &lt;dbl&gt;, LitterSz &lt;dbl&gt;,\n## #   Life_History_Ref &lt;chr&gt;, GR_MidRangeLat_dd &lt;dbl&gt;, Precip_Mean_mm &lt;dbl&gt;, …\n\ns\n\n## # A tibble: 1 × 1\n##    avgF\n##   &lt;dbl&gt;\n## 1 5217.\n\n\n\nThe “assignment” pipe (%&lt;&gt;%) evaluates the expression on the right-hand side of the pipe operator and reassigns the resultant value to the left-hand side.\n\n\ns &lt;- filter(d, Genus == \"Alouatta\")\ns %&lt;&gt;%\n    select(Genus, Species)\ns\n\n## # A tibble: 6 × 2\n##   Genus    Species  \n##   &lt;chr&gt;    &lt;chr&gt;    \n## 1 Alouatta belzebul \n## 2 Alouatta caraya   \n## 3 Alouatta guariba  \n## 4 Alouatta palliata \n## 5 Alouatta pigra    \n## 6 Alouatta seniculus\n\n\n\nFinally, the “exposition” pipe (%$%) exposes the names within the object on the left-hand side of the pipe to the right-hand side expression.\n\n\ns &lt;- filter(d, Genus == \"Alouatta\") %$%\n    paste0(Genus, \" \", Species)\ns\n\n## [1] \"Alouatta belzebul\"  \"Alouatta caraya\"    \"Alouatta guariba\"  \n## [4] \"Alouatta palliata\"  \"Alouatta pigra\"     \"Alouatta seniculus\"",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "10-module.html#dot-syntax-and-the-pipe",
    "href": "10-module.html#dot-syntax-and-the-pipe",
    "title": "10  Data Wrangling",
    "section": "10.7 Dot Syntax and the Pipe",
    "text": "10.7 Dot Syntax and the Pipe\nNormally, when we use the forward pipe operator, the LHS of the operator is passed to the first argument of the function on the RHS. Thus, the following are all equivalent:\n\ns &lt;- filter(d, Genus == \"Alouatta\")\ns &lt;- d %&gt;%\n    filter(Genus == \"Alouatta\")\nd %&gt;%\n    filter(Genus == \"Alouatta\") -&gt; s\n\nThe behavior of the forward pipe operator means we can use it do something like the following, where d is implicitly piped into the data argument for ggplot()\n\nd %&gt;%\n    ggplot(aes(x = log(Body_mass_female_mean), y = log(Brain_Size_Species_Mean))) +\n    geom_point()\n\n\n\n\n\n\n\n\nWe can also use dot (.) syntax with the {magrittr} forward pipe operator to pass the LHS of a statement to somewhere other than the first argument of the function on the RHS. Thus…\ny %&gt;% function(x, .) is equivalent to function(x, y)\n… which means we can do something like this to pipe d into a function such as lm() (“linear model”), where the data frame that the function is run on is not the first argument:\n\nd %&gt;%\n    lm(log(Body_mass_female_mean) ~ log(Brain_Size_Species_Mean), data = .)\n\n## \n## Call:\n## lm(formula = log(Body_mass_female_mean) ~ log(Brain_Size_Species_Mean), \n##     data = .)\n## \n## Coefficients:\n##                  (Intercept)  log(Brain_Size_Species_Mean)  \n##                        3.713                         1.135\n\n\nWe can also use the {magrittr} pipe’s curly brace ({}) syntax to wrap the RHS of a statement and pass the LHS into several places:\n\nd %&gt;%\n    {\n        plot(log(.$Brain_Size_Species_Mean), log(.$Body_mass_female_mean))\n    }\n\n\n\n\n\n\n\n\n\nNOTE: R recently introduced a “native” pipe operator |&gt; into its {base} syntax. It behaves very similarly to the {magrittr} forward pipe, but does not support dot syntax the same way. It also requires an explicit functions call on the RHS, which means appending () to the end of the function name, rather than just using the name. The first version of the line below using %&gt;% could be used to take the log of all Brain_Size_Female_Mean values in d, while the second version, using |&gt;, needs to have () appended to the functions calls to work properly.\n\n\nd %&gt;%\n    select(Brain_Size_Female_Mean) %&gt;%\n    log %&gt;%\n    head\n\n## # A tibble: 6 × 1\n##   Brain_Size_Female_Mean\n##                    &lt;dbl&gt;\n## 1                   3.98\n## 2                  NA   \n## 3                   3.94\n## 4                   3.87\n## 5                   3.89\n## 6                   3.87\n\nd |&gt;\n    select(Brain_Size_Female_Mean) |&gt;\n    log() |&gt;\n    head()\n\n## # A tibble: 6 × 1\n##   Brain_Size_Female_Mean\n##                    &lt;dbl&gt;\n## 1                   3.98\n## 2                  NA   \n## 3                   3.94\n## 4                   3.87\n## 5                   3.89\n## 6                   3.87",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "10-module.html#useful-related-packages",
    "href": "10-module.html#useful-related-packages",
    "title": "10  Data Wrangling",
    "section": "10.8 Useful Related Packages",
    "text": "10.8 Useful Related Packages\nThe {tidylog} package provides wrappers around many {dplyr} and {tidyr} package functions that provide logged feedback on the outcome of those functions, which can be useful for understanding the effects of whatever data wrangling processes we run. For example, running filter() will provide feedback on the number of runs removed and kept as part of a filtering operation…\n\nlibrary(tidylog)\n\n## \n## Attaching package: 'tidylog'\n\n\n## The following objects are masked from 'package:dplyr':\n## \n##     add_count, add_tally, anti_join, count, distinct, distinct_all,\n##     distinct_at, distinct_if, filter, filter_all, filter_at, filter_if,\n##     full_join, group_by, group_by_all, group_by_at, group_by_if,\n##     inner_join, left_join, mutate, mutate_all, mutate_at, mutate_if,\n##     relocate, rename, rename_all, rename_at, rename_if, rename_with,\n##     right_join, sample_frac, sample_n, select, select_all, select_at,\n##     select_if, semi_join, slice, slice_head, slice_max, slice_min,\n##     slice_sample, slice_tail, summarise, summarise_all, summarise_at,\n##     summarise_if, summarize, summarize_all, summarize_at, summarize_if,\n##     tally, top_frac, top_n, transmute, transmute_all, transmute_at,\n##     transmute_if, ungroup\n\n\n## The following objects are masked from 'package:tidyr':\n## \n##     drop_na, fill, gather, pivot_longer, pivot_wider, replace_na,\n##     separate_wider_delim, separate_wider_position,\n##     separate_wider_regex, spread, uncount\n\n\n## The following object is masked from 'package:stats':\n## \n##     filter\n\n# compare...\ns &lt;- dplyr::filter(d, Family == \"Hominidae\" & Mass_Dimorphism &gt; 2)\n# to...\ns &lt;- filter(d, Family == \"Hominidae\" & Mass_Dimorphism &gt; 2)\n\n## filter: removed 210 rows (99%), 3 rows remaining\n\n\nSimilarly, running sample_n() will give us logged output on that random record sampling process…\n\n# compare...\ns &lt;- dplyr::sample_n(d, size = 100, replace = FALSE)\n# to...\ns &lt;- sample_n(d, size = 100, replace = FALSE)\n\n## sample_n: removed 113 rows (53%), 100 rows remaining\n\ndetach(package:tidylog)\n\n\nNOTE: Loading in {tidylog} function will conflict with or “mask” corresponding function names from {dplyr} and {tidyr}. In the examples above, then, to run the {tidyverse} versions of filter() and select_n(), it was necessary to use the :: notation to specifically call the {dplyr} version of the functions directly. The {tidylog} versions of these functions run a bit more slowly, so if speed is important, you may not want to use {tidylog}, you may want to call the {dplyr} or {tidyr} functions explicitly after loading {tidylog}, or you may want to simply call the {tidylog} version of a function explicitly.\n\n\ns &lt;- tidylog::filter(d, Family == \"Hominidae\" & Mass_Dimorphism &gt; 2)\n\n## filter: removed 210 rows (99%), 3 rows remaining\n\n\n\n# | include: false\ndetach(package:magrittr)\ndetach(package:tidyverse)",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "10-module.html#concept-review",
    "href": "10-module.html#concept-review",
    "title": "10  Data Wrangling",
    "section": "Concept Review",
    "text": "Concept Review\n\nUsing {dplyr}: select(), filter(), arrange(), rename(), mutate(), summarise(), group_by()\nChaining and piping with %&gt;% and other pipe operators\nJoining tables: inner_join(), left_join(), right_join(), full_join()\nUsing the {tidylog} package",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "11-module.html",
    "href": "11-module.html",
    "title": "11  Functions and Flow Control",
    "section": "",
    "text": "11.1 Objectives",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Functions and Flow Control</span>"
    ]
  },
  {
    "objectID": "11-module.html#objectives",
    "href": "11-module.html#objectives",
    "title": "11  Functions and Flow Control",
    "section": "",
    "text": "The objective of this module to become familiar with how some additional basic programming concepts are implemented in R.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Functions and Flow Control</span>"
    ]
  },
  {
    "objectID": "11-module.html#preliminaries",
    "href": "11-module.html#preliminaries",
    "title": "11  Functions and Flow Control",
    "section": "11.2 Preliminaries",
    "text": "11.2 Preliminaries\n\nInstall this package in R: {sjmisc}\nLoad {tidyverse}",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Functions and Flow Control</span>"
    ]
  },
  {
    "objectID": "11-module.html#functions",
    "href": "11-module.html#functions",
    "title": "11  Functions and Flow Control",
    "section": "11.3 Functions",
    "text": "11.3 Functions\nOne of the strengths of using a programming language like R for data manipulation and analysis is that we can write our own functions to things we need to do in a particular way, e.g., to create a custom analysis or visualization. In Module 04 we practiced writing our own simple function, and we will revisit that here.\nRecall that the general format for a function is as follows:\nfunction_name &lt;- function(&lt;argument list&gt;) {\n  &lt;function body&gt;\n}\nFunctions that we define ourselves can have multiple arguments, and each argument can have a default value. Arguments are separated by commas and default values are specified in the list of arguments. For example, suppose we wanted to make a function that added a user-specified prefix to every entry in a particular named variable in a data frame, we could write the following:\n\nadd_prefix &lt;- function(df, prefix = \"\", variable) {\n    df[[variable]] &lt;- paste0(prefix, df[[variable]])\n    return(df)\n}\n\nmy_data &lt;- data.frame(name = c(\"Ned\", \"Sansa\", \"Cersei\", \"Tyrion\", \"Jon\", \"Daenerys\",\n    \"Aria\", \"Brienne\", \"Rickon\", \"Edmure\", \"Petyr\", \"Jamie\", \"Robert\", \"Stannis\",\n    \"Theon\"), house = c(\"Stark\", \"Stark\", \"Lannister\", \"Lannister\", \"Stark\", \"Targaryen\",\n    \"Stark\", \"Tarth\", \"Stark\", \"Tully\", \"Baelish\", \"Lannister\", \"Baratheon\", \"Baratheon\",\n    \"Greyjoy\"), code = sample(1e+05:999999, 15, replace = FALSE))\n\ndf &lt;- add_prefix(my_data, variable = \"house\")  # uses default prefix\nhead(df)\n\n##       name     house   code\n## 1      Ned     Stark 513382\n## 2    Sansa     Stark 230251\n## 3   Cersei Lannister 287114\n## 4   Tyrion Lannister 775662\n## 5      Jon     Stark 672267\n## 6 Daenerys Targaryen 792721\n\ndf &lt;- add_prefix(my_data, prefix = \"House \", variable = \"house\")\nhead(df)\n\n##       name           house   code\n## 1      Ned     House Stark 513382\n## 2    Sansa     House Stark 230251\n## 3   Cersei House Lannister 287114\n## 4   Tyrion House Lannister 775662\n## 5      Jon     House Stark 672267\n## 6 Daenerys House Targaryen 792721\n\ndf &lt;- add_prefix(my_data, prefix = \"00001-\", variable = \"code\")\nhead(df)\n\n##       name     house         code\n## 1      Ned     Stark 00001-513382\n## 2    Sansa     Stark 00001-230251\n## 3   Cersei Lannister 00001-287114\n## 4   Tyrion Lannister 00001-775662\n## 5      Jon     Stark 00001-672267\n## 6 Daenerys Targaryen 00001-792721\n\n\n\nNOTE: Arguments can be passed to a function in any order, as long as the argument name is included. For example, for the add_prefix() function above, the following are equivalent:\n\n\nhead(add_prefix(df = my_data, variable = \"house\"))\n\n##       name     house   code\n## 1      Ned     Stark 513382\n## 2    Sansa     Stark 230251\n## 3   Cersei Lannister 287114\n## 4   Tyrion Lannister 775662\n## 5      Jon     Stark 672267\n## 6 Daenerys Targaryen 792721\n\n# versus...\nhead(add_prefix(variable = \"house\", df = my_data))\n\n##       name     house   code\n## 1      Ned     Stark 513382\n## 2    Sansa     Stark 230251\n## 3   Cersei Lannister 287114\n## 4   Tyrion Lannister 775662\n## 5      Jon     Stark 672267\n## 6 Daenerys Targaryen 792721\n\n\nNote that in the example above, because the argument name prefix was excluded, the default value was used.\nR also uses positional matching to assign values to arguments when argument names are excluded. Note the difference in the results of these lines:\n\nhead(add_prefix(my_data, \"00001-\", \"code\"))\n\n##       name     house         code\n## 1      Ned     Stark 00001-513382\n## 2    Sansa     Stark 00001-230251\n## 3   Cersei Lannister 00001-287114\n## 4   Tyrion Lannister 00001-775662\n## 5      Jon     Stark 00001-672267\n## 6 Daenerys Targaryen 00001-792721\n\n# versus...\nhead(add_prefix(my_data, \"House \", \"house\"))\n\n##       name           house   code\n## 1      Ned     House Stark 513382\n## 2    Sansa     House Stark 230251\n## 3   Cersei House Lannister 287114\n## 4   Tyrion House Lannister 775662\n## 5      Jon     House Stark 672267\n## 6 Daenerys House Targaryen 792721\n\n# versus...\nhead(add_prefix(my_data, \"\", \"house\"))\n\n##       name     house   code\n## 1      Ned     Stark 513382\n## 2    Sansa     Stark 230251\n## 3   Cersei Lannister 287114\n## 4   Tyrion Lannister 775662\n## 5      Jon     Stark 672267\n## 6 Daenerys Targaryen 792721\n\n\nIf we try to run the line below, however, it throws an error because too few (unnamed) arguments are passed to the function for it to be able to disambiguate them:\n\nhead(add_prefix(my_data, \"00001-\"))\n\nUnless otherwise specified, functions return the result of the last expression evaluated in the function body. However, is good programming practice to explicitly specify the object or value you want returned from the function with return(&lt;value&gt;) or return(&lt;object&gt;).",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Functions and Flow Control</span>"
    ]
  },
  {
    "objectID": "11-module.html#conditional-expressions",
    "href": "11-module.html#conditional-expressions",
    "title": "11  Functions and Flow Control",
    "section": "11.4 Conditional Expressions",
    "text": "11.4 Conditional Expressions\nConditional expressions are a basic feature of any programming language. They are used for “flow control”, i.e., to structure what your program does when. The most common conditional expression is the “if… else…” statement, which is used to direct flow of a program between two paths. The general form is:\nif (&lt;test&gt;) {\n  &lt;action 1&gt;\n} else {\n  &lt;action 2&gt;\n}\nAs an example…\n\ni &lt;- TRUE\n\nif (i == TRUE) {\n    print(\"Yes\")\n} else {\n    print(\"No\")\n}\n\n## [1] \"Yes\"\n\ni &lt;- FALSE\n\nif (i == TRUE) {\n    print(\"Yes\")\n} else {\n    print(\"No\")\n}\n\n## [1] \"No\"\n\n\nA related form is the ifelse() function, which has three arguments: the test condition, the value to be returned or expression to be run if the test condition is true, and the value to be returned or expression to be run if the test condition is false. Unlike the “if… else…” formulation, the ifelse() function can work on a vector, too, and returns a vector.\n\ni &lt;- 9\nifelse(i &lt;= 10, \"Yes\", \"No\")\n\n## [1] \"Yes\"\n\ni &lt;- 11\nifelse(i &lt;= 10, \"Yes\", \"No\")\n\n## [1] \"No\"\n\ni &lt;- c(9, 10, 11)\nifelse(i &lt;= 10, \"Yes\", \"No\")\n\n## [1] \"Yes\" \"Yes\" \"No\"\n\n\n\nNOTE: There is also a {dplyr} version of the “if… else…” conditional: if_else(). I tend to use this one much more than the {base} R version.\n\nThe function case_when() is the equivalent of mixing several “if… else…” statements.\n\ni &lt;- 1:10\noutput &lt;- case_when(i &lt;= 3 ~ \"small\", i &lt;= 7 ~ \"medium\", i &lt;= 10 ~ \"large\")\noutput\n\n##  [1] \"small\"  \"small\"  \"small\"  \"medium\" \"medium\" \"medium\" \"medium\" \"large\" \n##  [9] \"large\"  \"large\"\n\n\nFor conditional statements, there are two additional functions that are often useful: any() and all(). The any() function takes a vector of logical values and returns TRUE if any of the elements is TRUE, while the all() function takes a vector of logical values and returns TRUE if all elements are TRUE.\n\ni &lt;- c(9, 10, 11)\nany(i &lt;= 10)\n\n## [1] TRUE\n\nall(i &lt;= 10)\n\n## [1] FALSE",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Functions and Flow Control</span>"
    ]
  },
  {
    "objectID": "11-module.html#relational-operators",
    "href": "11-module.html#relational-operators",
    "title": "11  Functions and Flow Control",
    "section": "11.5 Relational Operators",
    "text": "11.5 Relational Operators\nThe following relational operators are often used in conditional expressions:\n\nless than, greater than: &lt;, &gt;\nless than or equal to, greater than or equal to: &lt;=, &gt;=\nequal to: == NOTE: This uses a double equal sign!\nnot equal to: !=",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Functions and Flow Control</span>"
    ]
  },
  {
    "objectID": "11-module.html#logical-and-other-operators",
    "href": "11-module.html#logical-and-other-operators",
    "title": "11  Functions and Flow Control",
    "section": "11.6 Logical and Other Operators",
    "text": "11.6 Logical and Other Operators\nThe following additional operators are also useful and important:\n\n!: logical NOT Note that this operator can be applied to values and to functions\n&: element-wise logical AND (applies to element in a vector)\n&&: logical AND (applies to single conditions or first element in vector)\n|: element-wise logical OR (applies to element in a vector)\n||: logical OR (applies to single conditions or first element in vector)\n%in%: tests for membership in a vector\nthe {sjmisc} package adds a “not in” operator to test for membership in a vector: %nin%\n\nExamples:\n\ni &lt;- c(9, 10, 11)\n!any(i &lt;= 10)  # logical NOT\n\n## [1] FALSE\n\n!all(i &lt;= 10)  # logical NOT\n\n## [1] TRUE\n\n\n\ni &lt;- 1:20\ni &lt; 12 & (i%%3) == 0  # element-wise logical AND\n\n##  [1] FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE\n## [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\n\ni[1] &lt; 12 && (i[1]%%3) == 0  # logical AND\n\n## [1] FALSE\n\n\n\ni &gt; 10 | (i%%2) == 0  # element-wise logical OR\n\n##  [1] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE\n## [13]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n\n\n\ni[1] &gt; 10 || (i[1]%%2) == 0  # logical OR\n\n## [1] FALSE\n\n\n\na &lt;- c(\"There\", \"is\", \"grandeur\", \"in\", \"this\", \"view\", \"of\", \"life\")\nb &lt;- \"grandeur\"\nb %in% a  # membership\n\n## [1] TRUE\n\nb &lt;- c(\"selection\", \"life\")\nb %in% a\n\n## [1] FALSE  TRUE\n\n\n\nlibrary(sjmisc)\nb %nin% a  # not membership\n\n## [1]  TRUE FALSE\n\ndetach(package:sjmisc)",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Functions and Flow Control</span>"
    ]
  },
  {
    "objectID": "11-module.html#iterating-with-loops",
    "href": "11-module.html#iterating-with-loops",
    "title": "11  Functions and Flow Control",
    "section": "11.7 Iterating with Loops",
    "text": "11.7 Iterating with Loops\n\nfor() Loops\nWhen we want to execute a particular piece of code multiple times, for example to iterate over a set of values or to apply the same function to a set of elements in a vector, one way (but not the only way!) to do this is with a loop. There are several different constructions we can use for looping, one of the most common of which is a for() loop. The basic contruction for a for() loop is:\nfor (&lt;index&gt; in &lt;range&gt;){\n  &lt;code to execute&gt;\n}\nThe following examples print out each element in a vector, v:\n\nv &lt;- seq(from = 100, to = 120, by = 2)\nfor (i in 1:length(v)) {\n    # here, we are looping over the indices of v\n    print(v[i])\n}\n\n## [1] 100\n## [1] 102\n## [1] 104\n## [1] 106\n## [1] 108\n## [1] 110\n## [1] 112\n## [1] 114\n## [1] 116\n## [1] 118\n## [1] 120\n\nfor (i in seq_along(v)) {\n    # seq_along() also loops over the indices of v\n    print(v[i])\n}\n\n## [1] 100\n## [1] 102\n## [1] 104\n## [1] 106\n## [1] 108\n## [1] 110\n## [1] 112\n## [1] 114\n## [1] 116\n## [1] 118\n## [1] 120\n\nfor (i in v) {\n    # here we loop over the elements of v\n    print(i)\n}\n\n## [1] 100\n## [1] 102\n## [1] 104\n## [1] 106\n## [1] 108\n## [1] 110\n## [1] 112\n## [1] 114\n## [1] 116\n## [1] 118\n## [1] 120\n\n\nIt is good form and improves efficiency if we allocate memory to whatever output we may want to generate inside of a loop beforehand. For example, if we want to use a loop to calculate the median() across all Platyrrhine primate genera for the female brain size, female body size, and canine dimorphism variables in the Kamilar and Cooper dataset we have used previously, we could do the following:\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/KamilarAndCooperData.csv\"\nd &lt;- read_csv(f, col_names = TRUE)  # creates a 'tibble'\ns &lt;- filter(d, Family %in% c(\"Atelidae\", \"Pitheciidae\", \"Cebidae\")) %&gt;%\n    select(Brain_Size_Female_Mean, Body_mass_female_mean, Canine_Dimorphism)\n# good practice\noutput &lt;- vector(\"double\", ncol(s))\nfor (i in seq_along(s)) {\n    output[[i]] &lt;- median(s[[i]], na.rm = TRUE)\n}\noutput\n\n## [1]   29.0500 1030.0000    1.2955\n\n\nThe following is another very common way to do this, but is less efficient as we are rewriting the object output (and making it one element longer) in every iteration of the loop.\n\n# not so good practice\noutput &lt;- vector()\nfor (i in seq_along(s)) {\n    output &lt;- c(output, median(s[[i]], na.rm = TRUE))\n}\noutput\n\n## [1]   29.0500 1030.0000    1.2955\n\n\n\n\nCHALLENGE\nWrite a for() loop to print out each row in the data frame my_data.\n\n\nShow Code\nfor (i in 1:nrow(my_data)) {\n    print(my_data[i, ])\n}\n\n\nShow Output\n##   name house   code\n## 1  Ned Stark 513382\n##    name house   code\n## 2 Sansa Stark 230251\n##     name     house   code\n## 3 Cersei Lannister 287114\n##     name     house   code\n## 4 Tyrion Lannister 775662\n##   name house   code\n## 5  Jon Stark 672267\n##       name     house   code\n## 6 Daenerys Targaryen 792721\n##   name house   code\n## 7 Aria Stark 313742\n##      name house   code\n## 8 Brienne Tarth 504327\n##     name house   code\n## 9 Rickon Stark 995848\n##      name house   code\n## 10 Edmure Tully 139285\n##     name   house   code\n## 11 Petyr Baelish 145695\n##     name     house   code\n## 12 Jamie Lannister 416322\n##      name     house   code\n## 13 Robert Baratheon 273858\n##       name     house   code\n## 14 Stannis Baratheon 594831\n##     name   house   code\n## 15 Theon Greyjoy 606938\n\n\n\nWrite a for() loop to print out the reverse of each element in the code vector in the data frame my_data.\n\nHINT: Check out the function stri_reverse() from the {stringi} package.\n\n\n\nShow Code\nfor (i in my_data$code) {\n    print(stringi::stri_reverse(i))\n}\n\n\nShow Output\n## [1] \"283315\"\n## [1] \"152032\"\n## [1] \"411782\"\n## [1] \"266577\"\n## [1] \"762276\"\n## [1] \"127297\"\n## [1] \"247313\"\n## [1] \"723405\"\n## [1] \"848599\"\n## [1] \"582931\"\n## [1] \"596541\"\n## [1] \"223614\"\n## [1] \"858372\"\n## [1] \"138495\"\n## [1] \"839606\"\n\n\n\n\n\nwhile() Loops\nAn alternative to using a for() loop for repeating a particular block of code is to use a while() loop. The general construction for a while() loop is:\nwhile (&lt;test expression&gt;) {\n  &lt;code to execute&gt;\n}\nHere, the test expression is evaluated at the start of the loop, and the body of the loop is only entered if the result is TRUE. Once the statements inside the loop are executed, flow returns to the top of the loop to evaluate the test expression again. That process is repeated until the test expression is FALSE, and then the loop is exited and control moves on to subsequent parts of the program.\nAs above, the following example prints out each element in the vector, v:\n\nv &lt;- seq(from = 100, to = 120, by = 2)\ni &lt;- 1\nwhile (i &lt;= length(v)) {\n    print(v[i])\n    i &lt;- i + 1\n}\n\n## [1] 100\n## [1] 102\n## [1] 104\n## [1] 106\n## [1] 108\n## [1] 110\n## [1] 112\n## [1] 114\n## [1] 116\n## [1] 118\n## [1] 120",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Functions and Flow Control</span>"
    ]
  },
  {
    "objectID": "11-module.html#vectorization-and-functionals",
    "href": "11-module.html#vectorization-and-functionals",
    "title": "11  Functions and Flow Control",
    "section": "11.8 Vectorization and Functionals",
    "text": "11.8 Vectorization and Functionals\nIn many cases what we want to do in a loop is apply the same function or operation to each of the elements in a vector, matrix, data frame, list or part thereof… and often we do not actually need a loop to do that. Rather, we can often use what is called a vectorized function, or functional. The function sapply(), and related functions (e.g., apply(), lapply(), mapply(), vapply()) are examples of functionals: they allow us to perform element-wise operations on the entries in a data object.\nThe function sapply() takes two arguments, a data object (a vector, list, or data frame) and a function (FUN=) to apply to its elements. Each element of the data object is passed on to the function, and the result is returned and concatenated into a vector of the same length as the original data object. lapply() is similar except that the output is a list rather than a vector.\nThe following examples replicate what we did with for() loops above. Here, s is a data frame, and the function median() is being applied to each element, i.e., each variable, in that data frame:\n\noutput &lt;- sapply(s, FUN = median, na.rm = TRUE)\n# Here we are passing on an extra argument to the `median()`function, i.e.,\n# `na.rm=TRUE`. This is an example of 'dot-dot-dot' (`...`) being an extra\n# argument of the `sapply()` function where those arguments are 'passed\n# through' as arguments of the `FUN=` function. Basically, this means that we\n# can pass on an arbitrary set and number of arguments into `sapply()` which,\n# in this case, are then being used in the `median()` function.\noutput\n\n## Brain_Size_Female_Mean  Body_mass_female_mean      Canine_Dimorphism \n##                29.0500              1030.0000                 1.2955\n\nclass(output)\n\n## [1] \"numeric\"\n\noutput &lt;- lapply(s, FUN = median, na.rm = TRUE)\noutput\n\n## $Brain_Size_Female_Mean\n## [1] 29.05\n## \n## $Body_mass_female_mean\n## [1] 1030\n## \n## $Canine_Dimorphism\n## [1] 1.2955\n\nclass(output)\n\n## [1] \"list\"\n\n\nThe map() family of functions from the {purrr} package (which is part of the {tidyverse} set of packages) works very similarly to the apply() functions:\n\noutput &lt;- map_dbl(s, .f = median, na.rm = TRUE)\n# note the argument `.f=` instead of `FUN=` `map_dbl()` returns an atomic\n# vector of type 'double'\noutput\n\n## Brain_Size_Female_Mean  Body_mass_female_mean      Canine_Dimorphism \n##                29.0500              1030.0000                 1.2955\n\nclass(output)  # returns a vector, like `sapply()`\n\n## [1] \"numeric\"\n\noutput &lt;- map(s, .f = median, na.rm = TRUE)\n# `map()` returns a list\noutput\n\n## $Brain_Size_Female_Mean\n## [1] 29.05\n## \n## $Body_mass_female_mean\n## [1] 1030\n## \n## $Canine_Dimorphism\n## [1] 1.2955\n\nclass(output)  # returns a list, like `lapply()`\n\n## [1] \"list\"\n\n# `map_dfr()` returns a data frame\noutput &lt;- map_dfr(s, .f = median, na.rm = TRUE)\nclass(output)  # returns a data frame, unlike any of the `apply()` functions\n\n## [1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\noutput\n\n## # A tibble: 1 × 3\n##   Brain_Size_Female_Mean Body_mass_female_mean Canine_Dimorphism\n##                    &lt;dbl&gt;                 &lt;dbl&gt;             &lt;dbl&gt;\n## 1                   29.0                  1030              1.30\n\n\n\n# | include: false\ndetach(package:tidyverse)",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Functions and Flow Control</span>"
    ]
  },
  {
    "objectID": "11-module.html#concept-review",
    "href": "11-module.html#concept-review",
    "title": "11  Functions and Flow Control",
    "section": "Concept Review",
    "text": "Concept Review\n\nFunctions: arguments, default values, and “dot-dot-dot” (...)\nConditional expressions: if... else..., ifelse(), if_else(), case_when()\nIterating with loops: for() loops, while() loops\nFunctionals for vectorizing data manipulations: apply() and map() families of function",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Functions and Flow Control</span>"
    ]
  },
  {
    "objectID": "12-module.html",
    "href": "12-module.html",
    "title": "12  Descriptive Statisics and Sampling",
    "section": "",
    "text": "12.1 Objectives",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Descriptive Statisics and Sampling</span>"
    ]
  },
  {
    "objectID": "12-module.html#objectives",
    "href": "12-module.html#objectives",
    "title": "12  Descriptive Statisics and Sampling",
    "section": "",
    "text": "The objective of this module is to review some key terms and ideas that form the foundation of statistics and statistical inference. In particular, this module considers ways for describing distributions of data, particularly measures of central tendency, spread (i.e., variation), and shape in our observations of a variable, which play an important role in both descriptive statistics and in various approaches to statistical hypothesis testing.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Descriptive Statisics and Sampling</span>"
    ]
  },
  {
    "objectID": "12-module.html#preliminaries",
    "href": "12-module.html#preliminaries",
    "title": "12  Descriptive Statisics and Sampling",
    "section": "12.2 Preliminaries",
    "text": "12.2 Preliminaries\n\nInstall these packages in R: {mosiac}, {radiant}, {moments}, {sciplot}, {infer}\nLoad {tidyverse}",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Descriptive Statisics and Sampling</span>"
    ]
  },
  {
    "objectID": "12-module.html#important-terms",
    "href": "12-module.html#important-terms",
    "title": "12  Descriptive Statisics and Sampling",
    "section": "12.3 Important Terms",
    "text": "12.3 Important Terms\n\nPopulation - all of the elements from a set of data (e.g., all of the gorillas in the world) = N\nSample - one or more observations drawn from a population by some kind of sampling process (e.g., the set of gorillas living in Rwanda, the set of gorilla skeletons found in a museum) = n\n\n\nNOTE: We often assume that a sampling process is random, but there are lots of ways in which sampling might be biased, thus the samples we work with may not be (and often are not) random samples!\n\n\nParameter - a measurable characteristic of a population that summarizes data (e.g., the mean value of the femur length of all gorillas)\n\n\nNOTE: Population means for a given variable, x, are often indicated as \\(\\mu_x\\)\n\n\nStatistic - a measurable characteristic about a sample that summarizes data (e.g., the mean femur length of gorilla femurs found at the American Museum of Natural History)\n\n\nNOTE: Sample means for a given variable, x, are often indicated as \\(\\bar{x}\\)\n\nWhen we use statistical methods or attempt statistical inference - whether using a null hypothesis significance testing (NHST) framework or using Bayesian approaches - we are basically trying to estimate and draw conclusions about population-level parameters and processes and their distributions based on observations or measurements we take from a sample. Sometimes, we are simply trying evaluate whether it is reasonable to assume that our sample is drawn from a population with particular characteristics. Other times, we may be trying to understand what variables explain variation we see in a response measure of interest or be trying to evaluate which among a set of alternative models best predicts a given response.\nRegardless, we should always keep in mind that the process of trying to draw conclusions about a population based on a sample can be complicated by the fact that…\n\nour sample may be biased, non-random, or non-representative in some way\nthere may be unknown or unobserved variables that impact how the sample is related to the population\nthe assumptions we make about the population that our sample is drawn from might not be correct",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Descriptive Statisics and Sampling</span>"
    ]
  },
  {
    "objectID": "12-module.html#describing-sets-of-observations",
    "href": "12-module.html#describing-sets-of-observations",
    "title": "12  Descriptive Statisics and Sampling",
    "section": "12.4 Describing Sets of Observations",
    "text": "12.4 Describing Sets of Observations\nIt is important for us to be able to describe the general characteristics of the distribution of a set of observations or measurements about a population or a sample, and we often do this by calculating some measure(s) of central tendency, some measure(s) of spread around that statistic, and some measure of the shape of a distribution. The “five-number summary” that we have talked about previously provides some such statistics.\n\nMeasures of Central Tendency\n\nMedian - the middle value in a rank ordered series of values\nMean - the sum of measured values divided by \\(n\\), a.k.a., the average or the arithimetic mean\nMode - the most common measurement of values observed\nHarmonic mean - the reciprocal of the average of the reciprocals of a set of values\n\nThe measures above are relevant to summarizing observations about processes that are additive.\n\nGeometric mean - a measure of central tendency for processes that are exponential (e.g., some phases of population growth in natural populations) or multiplicative (e.g., increases in area or volume that accompany increases in linear dimension) in nature, rather than additive = the \\(n^{th}\\) root of the product of the values, taken across a set of \\(n\\) values; for the mathematically inclined, it also equals the antilog of the averaged log values\n\n\n\nCHALLENGE\n\nGiven a vector, x &lt;- c(1,2,3,4,5,6,7,8,9,10,25,50,100,200,1000), write your own function to determine the geometric mean of the values in a vector. Remember the general form for functions is: &lt;function name&gt; &lt;- function(&lt;arguments&gt;) {&lt;code&gt;}\n\n\nHINT: Taking the \\(n^{th}\\) root of a number is equivalent to raising the number to the power of \\(\\frac{1}{n}\\)\n\n\n\nShow Code\nx &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 25, 50, 100, 200, 1000)\ngeometric_mean &lt;- function(x) {\n    prod(x)^(1/length(x))\n}\ngeometric_mean(x)\n\n\nShow Output\n## [1] 13.50559\n\n\n\nShow Code\ngeometric_mean &lt;- function(x) {\n    exp(mean(log(x)))\n}\ngeometric_mean(x)\n\n\nShow Output\n## [1] 13.50559\n\n\n\nWhat happens if you have NAs or zeros or negative numbers in your vector?\n\nHINT: Including an na.rm=TRUE argument and the function na.omit() in your function code to ignore data points with  values may help you write more generic functions!\n\n\n\nMeasures of Spread\nIn addition to measures of central tendency, measures of spread or variability in the distribution of variables of interest are some of the most important summary statistics to calculate. The total range (min to max) is one measure of spread, as is the interquartile range (25th to 75th quartile), which as we have seen are both part of the “five-number summary”.\nWe more commonly characterize spread, however, in terms of some measure of the deviation of a set of values from the mean of those values. One such measure is the sum of squares…\n\nSum of Squares = the sum of the squared deviations of a set of values from the mean of that set\n\n\nNOTE: Why do we use the sum of the squared deviations of values from the mean rather than just the sum of deviations? Because the latter would simply be ZERO!\n\n\n\nCHALLENGE\n\nWrite a function to calculate the sum of squares for a vector.\n\n\n\nShow Code\nsum_of_squares &lt;- function(x) {\n    sum((x - mean(x))^2)\n}\nsum_of_squares(x)\n\n\nShow Output\n## [1] 917183.3\n\n\n\nShow Code\n# This is equivalent to...\n\nsum_of_squares &lt;- function(x) {\n    sum(x^2) - length(x) * mean(x)^2\n}\nsum_of_squares(x)\n\n\nShow Output\n## [1] 917183.3\n\n\n\nA shortcut to calculate the sum of squares for a vector, x, that does not actually require calculating mean(x) is the (sum of the squared values in the dataset) minus the (square of the summed values) / n, or…\n\\[\\sum\\limits_{i=1}^{k}(x^2) - \\frac{(\\sum\\limits_{i=1}^{k} x)^2}{n}\\]\nThus, another formula for the sum of squares is the following:\n\nsum_of_squares &lt;- function(x) {\n    sum(x^2) - (sum(x))^2/length(x)\n}\nsum_of_squares(x)\n\n## [1] 917183.3\n\n\nNote that the sum of squares always increases with sample size… you can see this by adding more data points to your vector. Thus, to be able to compare across data sets of different size, we are often more interested in the average deviation of values from the mean rather than the straight sum of squares, i.e., a mean squared deviation.\nThis is the definition of the variability or variance in a dataset. If we are simply interested in describing the mean squared deviation in a population, where we have a value or measurement for every case (e.g., the femur length of all of the gorillas in a museum population), we could then just divide the sum of squares by the number of cases.\n\nPopulation Variance (\\(\\sigma^2\\)) = \\(\\frac{SS}{N}\\)\n\nIn R parlance, we can write this as:\n\npop_var &lt;- function(x) {\n    sum((x - mean(x))^2)/(length(x))\n}\npop_var(x)\n\n## [1] 61145.56\n\n\nIf, however, we have not measured all of the individual cases in population - i.e., if we are, instead, dealing with a sample from the population and are trying to use that sample to say something about the population from which it is drawn (e.g., to say something about gorilla femur lengths in general based on those that appear in a museum sample) - then we need to use a slightly different formula to get an unbiased estimate of the population variance. Such an estimate for a population parameter, based on data from a sample, is calculated as:\n\nSample Variance (an estimator of the population variance) = \\(\\frac{SS}{n-1}\\)\n\nIn this formula, \\(n - 1\\) is the number of degrees of freedom implied by the sample. The degrees of freedom is the number of values used to calculate a sample statistic that are “free to vary”. For example, we used n observations to calculate the mean of our sample, which implies \\(n - 1\\) degrees of freedom (i.e., if we know the mean and \\(n - 1\\) values, then we also know the last value… it is not “free to vary”). We then use that statistic about our sample (i.e., the sample mean) as an estimate of the population mean, which is then used to derive an estimate of the population variance based on the sample variance.\n\n\nCHALLENGE\n\nWrite a function to calculate the variance for a vector of values representing a sample of measurements. Remember this means dividing the sample sum of squares by \\(n-1\\).\n\n\n\nShow Code\nsample_var &lt;- function(x) {\n    sum((x - mean(x))^2)/(length(x) - 1)\n}\nsample_var(x)\n\n\nShow Output\n## [1] 65513.1\n\n\n\nCompare the results of your function to the built-in R function, var(), which calculates sample variance.\n\n\nShow Code\nvar(x)\n\n\nShow Output\n## [1] 65513.1\n\n\n\n\n\nMeasures of Shape\nTwo common measures of the shape of a distribution include its skewness and kurtosis.\nSkewness measures the asymmetry of a distribution. Symmetrical distributions have zero skewness. Those with a longer or fatter tail on the right-hand side are called “right-skewed” (and have positive skewness), while those with a longer or fatter tail on the left-hand side are called “left-skewed” (and have negative skewness). One measure of skewness is\n\\[\\frac{\\sum\\limits_{i=1}^{N}(x_i-\\bar{x})^3}{(N-1)\\sigma^3}\\] … where \\(N\\) is the number of observations, \\(\\bar{x}\\) is the mean, and \\(\\sigma\\) is the standard deviation. That is, skewness is based on the cube of deviations of each observation from the mean, whereas variance is based on squared deviations from the mean. Generally, a distribution with a skewness value of between -0.5 and +0.5 is not considered to be far from symmetrical.\nEXAMPLES\n\nx &lt;- rnorm(1e+05, 0, 1)  # draw a random sample from a normal distribution with mean = 0 and stdev = 1\nhist(x, main = \"Normal: Symmetrical\", freq = FALSE)  # plot as a histogram\n\n\n\n\n\n\n\n(skewness &lt;- sum((x - mean(x))^3)/((length(x) - 1) * sd(x)^3))\n\n## [1] 0.001250904\n\nx &lt;- rbeta(1e+05, 9, 3)  # draw a random sample from a beta distribution with alpha = 9 and beta = 2\nhist(x, main = \"Beta: Left-Skewed\", freq = FALSE)  # plot as a histogram\n\n\n\n\n\n\n\n(skewness &lt;- sum((x - mean(x))^3)/((length(x) - 1) * sd(x)^3))\n\n## [1] -0.5960675\n\n\nKurtosis measures the peakedness or flatness of a distribution compared to a normal distribution. Distributions with high kurtosis have a sharper peak and fatter tails, while those with low kurtosis have a flatter, wider peak and thinner tails. The formula for kurtosis is…\n\\[\\frac{\\sum\\limits_{i=1}^{N}(x_i-\\bar{x})^4}{(N-1)\\sigma^4}\\]\nThat is, kurtosis is based on deviations of each observation raised to the fourth power. Data drawn from a normal distribution have an expected kurtosis of three, while those with “negative” kurtosis have a value of greater than three and those with “postive” kurtosis have values of less than three.\n\nx &lt;- rnorm(1e+05, 0, 1)  # draw a random sample from a normal distribution with mean = 0 and stdev = 1\n(kurtosis &lt;- sum((x - mean(x))^4)/((length(x) - 1) * sd(x)^4))\n\n## [1] 3.006718\n\n\nThe {moments} package has functions for skewness() and kurtosis().\n\nlibrary(moments)\nx &lt;- rnorm(1e+05, 0, 1)\nskewness(x)\n\n## [1] -0.002264951\n\nkurtosis(x)\n\n## [1] 2.989331\n\ndetach(package:moments)\n\n\n\nQuestions to Explore\n\nFor a random variable, how is variance related to sample size?\n\nWe will explore this, and at the same time practice a bit about loops in R programming, via simulation, where we repeatedly draw samples of random variables from a specific distribution. As an example, we will we draw from a normal distribution with a mean of 10 and a standard deviation of 2.\nTo visualize this distribution, we can use the plotDist() function from the {mosaic} package…\n\nlibrary(mosaic)\nmu &lt;- 10\nsigma &lt;- 2\nplotDist(\"norm\", mean = mu, sd = sigma, xlab = \"x\", ylab = \"Frequency\")\n\n\n\n\n\n\n\n# `plotDist()` comes from the {mosaic} package and makes it easy to plot a\n# distribution\n\nIn the following code block, we first set up a plot window to hold the results of our simulations and plot a line for the known population variance (i.e., the square of \\(\\sigma\\), the standard deviation). We then use nested for loops to iterate the process of drawing samples of a specific size, \\(n\\), from the specified distribution. We do this for samples of size \\(n\\) = 5, 10, 15… up to 100, and we draw out 50 replicates of each sample size. Recall from Module 11 that the structure for for loops is:\nfor (&lt;index&gt; in &lt;range&gt;){\n  &lt;code to execute&gt;\n}\n\n# set up plot window\nplot(c(0, 100), c(0, 15), type = \"n\", xlab = \"Sample Size\", ylab = \"Variance\")\n\n# add the population variance (= square of population standard deviation) to\n# the plot\nabline(h = 4, col = \"red\", lwd = 2, lty = 2)\n\n# run simulations and add results to plot\nmu &lt;- 10\nsigma &lt;- 2\n# samples of 5, 10, 15...\nfor (n in seq(from = 5, to = 100, by = 5)) {\n    # set up a variable, reps, to hold the set of variances calculated for each\n    # replicate\n    reps &lt;- vector(\"double\", 50)\n    # 50 replicates\n    for (i in 1:50) {\n        x &lt;- rnorm(n, mean = mu, sd = sigma)\n        points(n, var(x))\n        reps[[i]] = var(x)\n        # this is a common programming motif in R and is more memory and time\n        # efficient than another common motif, `reps &lt;- c(reps, var(x))`\n    }\n    points(n, mean(reps), bg = \"red\", pch = 23, cex = 2)  # plots average\n}\n\n\n\n\n\n\n\n\n\nHow does sample variance compare to population variance? What happens to the sample variances as sample size increases?\n\nAnother measure of spread around a mean that we often see reported is the standard deviation. The standard deviation is simply the square root of the variance (\\(\\sqrt{\\sigma^2} = \\sigma\\)). The advantage of using the standard deviation as a statistic or parameter is that the units of standard deviation are the same as the units of our original measurement (rather than being units squared, which are our units for variance).\nIn R we can write…\n\nx &lt;- rnorm(1000, mean = 10, sd = 2)\npop_sd &lt;- function(x) {\n    sqrt(pop_var(x))\n}\npop_sd(x)\n\n## [1] 2.000879\n\nsample_sd &lt;- function(x) {\n    sqrt(sample_var(x))\n}\nsample_sd(x)\n\n## [1] 2.00188\n\n\nThe sdpop() function from the {radiant} package can be used to calculate the standard deviation for a completely sampled population.\n\nlibrary(radiant)\nsdpop(x)\n\n## [1] 2.000879\n\ndetach(package:radiant)\n\nThe built-in R function sd() can be used to calculate the standard deviation of a sample.\n\nsd(x)\n\n## [1] 2.00188",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Descriptive Statisics and Sampling</span>"
    ]
  },
  {
    "objectID": "12-module.html#using-measures-of-spread",
    "href": "12-module.html#using-measures-of-spread",
    "title": "12  Descriptive Statisics and Sampling",
    "section": "12.5 Using Measures of Spread",
    "text": "12.5 Using Measures of Spread\n\nSampling Distributions\nSince one of the goals of statistics is to estimate and make inferences about population-level parameters based on characteristics of a sample, it is important that we be able to judge and report just how reliable or unreliable our statistical estimates those population-level parameters are.\nTo explore how we do this, let’s start with a population where we KNOW every data point. We will use that first to describe our population empirically. We will then draw samples out of that population and see how well that the samples we draw can be used to describe the population.\nLoad the “IMDB-movies.csv” dataset from the ada-datasets repository on GitHub as a “tibble”, d. This dataset contains data on close to 29,000 movies scraped in early 2020 from the online Internet Movie Database, including their year of title, director, year of production, running time, average viewer rating on a 10 point scale, and the number of votes that rating is based on. [Note that in collating this data, I excluded movies with fewer than 1000 votes.] Once we load the data, we will filter the dataset to keep only movies with a startYear from 1999 to 2019, which should leave us with 17,628 movies. We are going to use this dataset as our POPULATION.\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/IMDB-movies.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\nd &lt;- filter(d, startYear %in% 1999:2019)  # the %in% operator is VERY useful!\nhead(d)\n\n## # A tibble: 6 × 10\n##   tconst    titleType primaryTitle startYear runtimeMinutes genres averageRating\n##   &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;            &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;\n## 1 tt0035423 movie     Kate & Leop…      2001            118 Comed…           6.4\n## 2 tt0069049 movie     The Other S…      2018            122 Drama            6.8\n## 3 tt0111068 movie     Sangharsh         1999            127 Actio…           6.8\n## 4 tt0112444 movie     My Teacher'…      1999             89 Comed…           5.5\n## 5 tt0113026 movie     The Fantast…      2000             86 Music…           5.6\n## 6 tt0118589 movie     Glitter           2001            104 Drama…           2.2\n## # ℹ 3 more variables: numVotes &lt;dbl&gt;, nconst &lt;chr&gt;, director &lt;chr&gt;\n\n\nFirst, we will do some exploratory data analysis on this dataset using, in part, some functions from the {mosaic} package. {mosaic} makes some kinds of quick exploratory data analysis very fast and easy to do (though we already know how to do lots of what we can do with {mosaic} in other ways)!\nFunctions in the {mosaic} package have a data= argument as well as a ~ argument that specifies which variable of interest from the data= argument the function is to be applied to.\n\nboxplot(averageRating ~ startYear, data = d, xlab = \"Year\", ylab = \"Average Rating\")\n\n\n\n\n\n\n\n# the `histogram()` function from {mosaic} plots neat 'augmented' histograms\nhistogram(~averageRating, data = d, xlab = \"Average Rating\")\n\n\n\n\n\n\n\n# the `favstats()` function from {mosiac} calculates a variant of the 5-number\n# summary\n(pop_stats &lt;- favstats(~averageRating, data = d))\n\n##  min  Q1 median  Q3 max     mean       sd     n missing\n##    1 5.6    6.4 7.1 9.9 6.238972 1.160221 17628       0\n\n\nNow, let’s draw a single SAMPLE of 100 movies randomly from this population and visualize its average viewer rating. After setting the random number seed (analogous to setting what position we start at in a traditional table of random numbers), run the subsequent lines of code several times and look at how the results differ…\n\nNOTE: We use set.seed() here so that each time this function is run, it returns the same sequence of random numbers until the seed is reset.\n\n\nset.seed(1)\n\n\nn &lt;- 100\ns &lt;- sample_n(d, size = n, replace = FALSE)\n# `sample_n()` from {dplyr} selects rows at random from a data frame it's\n# another SUPER useful function\ntable(s$startYear)\n\n## \n## 1999 2000 2001 2002 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 \n##    2    4    2    2    4    3    1    5    6    4   10    7    4    6    6    7 \n## 2016 2017 2018 2019 \n##   10    7    8    2\n\nboxplot(averageRating ~ startYear, data = s, xlab = \"Year\", ylab = \"Average Rating\")\n\n\n\n\n\n\n\nhistogram(~averageRating, data = s, xlab = \"Average Rating\")\n\n\n\n\n\n\n\n(samp_stats &lt;- favstats(~averageRating, data = s))\n\n##  min  Q1 median Q3 max  mean       sd   n missing\n##  3.3 5.5   6.45  7 8.5 6.325 1.004673 100       0\n\n\nNote that each time we select a sample and calculate summary statistics, such as the mean and standard deviation of a sample, we get slightly different results. If we repeat this sampling process multiple times, we can use the results to generate a distribution for a particular summary statistic of interest, e.g., for the mean or the median. This process generates what is called a sampling distribution for the statistic.\nThe code below allows us to generate a sampling distribution virtually. The do() * construction from {mosaic}, in combination with one of the {mosaic} package’s aggregating functions (e.g., mean(), median(), etc.), can be used to repeat sampling from the population a user-specified number of times, calculate a summary statistic, and then bundle the results into a vector all in the same line. [Of course, we could also write a loop to do the same thing… and there are other ways we could do this as well (see examples below).]\nThis process of simulating samples drawn from a population and then generating statistics on the basis of each of our virtual samples is a very powerful tool that we will apply over and over again as we talk about statistical inference. This process is the basis for bootstrapping confidence intervals (see Module 14) and for conducting randomization/permutation tests (see Module 16).\n\nUsing the {mosaic} Package\n\n# using `do(reps) *` from {mosaic} to generate a sampling distribution\nreps &lt;- 1000\nsamp_dist_mean &lt;- do(reps) * mean(~averageRating, data = sample_n(d, size = n, replace = FALSE))\n# generates a sampling distribution\nmean_plot &lt;- histogram(~mean, data = samp_dist_mean, xlab = \"Sampling Distribution for the\\nMean of Average Rating\")\nsamp_dist_median &lt;- do(reps) * median(~averageRating, data = sample_n(d, size = n,\n    replace = FALSE))\n# generates a sampling distribution\nmedian_plot &lt;- histogram(~median, data = samp_dist_median, xlab = \"Sampling Distribution for the\\nMedian of Average Rating\")\n\n\n\nUsing the {purrr} Package\n\nlibrary(purrr)\n# using `map()` from {purrr} to generate a sampling distribution\nsamp_dist_mean_alt1 &lt;- map(1:reps, ~mean(~averageRating, data = sample_n(d, size = n,\n    replace = FALSE))) |&gt;\n    unlist()\nmean_plot_alt1 &lt;- histogram(samp_dist_mean_alt1, xlab = \"Sampling Distribution for the\\nMean of Average Rating\")\ndetach(package:purrr)\n\n\n\nUsing the {infer} Package\n\n# using `rep_sample_n()` from {infer} to generate a sampling distribution\nlibrary(infer)\nsamp_dist_mean_alt2 &lt;- d |&gt;\n    rep_sample_n(size = n, reps = reps, replace = FALSE) |&gt;\n    group_by(replicate) |&gt;\n    summarize(mean = mean(averageRating)) |&gt;\n    pull(mean)\ndetach(package:infer)\n\nmean_plot_alt2 &lt;- histogram(samp_dist_mean_alt2, xlab = \"Sampling Distribution for the\\nMean of Average Rating\")\n\n\nlibrary(cowplot)\nplot_grid(mean_plot, median_plot, ncol = 2)\n\n\n\n\n\n\n\ndetach(package:cowplot)\n\n\nIMPORTANT NOTE: The histograms we plot in the code above are for sampling distributions of the statistics in question. They do not represent the distribution of values in any particular sample! This is SUPER IMPORTANT to recognize and keep in mind.\n\nThe mean of the sampling distribution (i.e., the mean of \\(\\bar{x}\\)) for a particular statistic should be a really good point estimate of the population value for that statistic (i.e., \\(\\mu\\)). Compare the following to confirm this:\n\nmean(~mean, data = samp_dist_mean)\n\n## [1] 6.239391\n\n# this is the estimated population mean calculated as the mean of the sampling\n# distribution of sample means\npop_stats$mean  # true population mean\n\n## [1] 6.238972\n\nmean(~median, data = samp_dist_median)\n\n## [1] 6.38125\n\n# this is the estimated population median calculated as the mean of the\n# sampling distribution of sample medians\npop_stats$median  # true population median\n\n## [1] 6.4\n\n\n\n\n\nStandard Errors\nSo, just how reliable or unreliable are these estimates of a population parameter based on the mean of the sampling distribution for a statistic of interest? That is, how far off is a statistic that we calculate based on a sampling distribution likely to be from the true POPULATION value of the parameter of interest?\nOne way to quantify the uncertainty is by calculating the variability of the summary statistic of interest across replicate samples drawn from the population. For example, we could calculate the variance of the sampling distribution. More commonly, because variance is expressed in units squared, we take the square root of this variance to calculate the standard deviation of the sampling distribution and thus express our uncertainly in units of the original measurement. Formally, this value is referred to as the standard error (SE) of measurement for any given summary statistic of interest. [Typically, that statistic is the mean, and we thus are often calculating the standard error of the mean.]\nThe SE is, in effect, the average deviation between statistic values calculated from different and incomplete sets of samples drawn from a population and the average statistic value calculated across that set of samples (which should converge on the true population value for that statistic). The SE of the mean is thus a measure of how dispersed sample means (\\(\\bar{x}\\)) are expected to be around the estimated population mean (\\(\\mu\\)) (i.e., how far off from the true population mean an estimate based on a sample of size \\(n\\) is likely to be). Similarly, the SE of the median would be a measure of how dispersed sample medians are expected to be, on average, around the estimated population median, and so forth.\n\nEstimating a SE from a Sampling Distribution\nTo estimate the SE from a sampling distribution, we simply take the standard standard deviation of the set of values comprising that distribution. Above, we used the do() * &lt;function&gt; construction to generate sampling distributions for the mean and median of average viewer ratings in the “movies.csv” dataset, so all we need to do is pull out the standard deviation of those distributions to estimate the standard error.\n\nse_mean &lt;- favstats(~mean, data = samp_dist_mean)$sd\n# or, se_mean &lt;- sd(samp_dist_mean$mean)\n(paste0(\"Estimated population mean = \", round(favstats(~mean, data = samp_dist_mean)$mean,\n    3), \" ± \", round(se_mean, 3), \" SE based on \", reps, \" samples of size \", n))\n\n## [1] \"Estimated population mean = 6.239 ± 0.121 SE based on 1000 samples of size 100\"\n\nse_median &lt;- favstats(~median, data = samp_dist_median)$sd\n# or, se_median &lt;- sd(samp_dist_median$median)\n\n(paste0(\"Estimated population median = \", round(favstats(~median, data = samp_dist_median)$mean,\n    3), \" ± \", round(se_median, 3), \" SE based on \", reps, \" samples of size \",\n    n))\n\n## [1] \"Estimated population median = 6.381 ± 0.134 SE based on 1000 samples of size 100\"\n\n\nAs we might expect for any measure of uncertainty or error, the SE [1] increases with the variability in a sample (i.e., estimates based on high-variability samples should be more uncertain) and [2] decreases with the size of the sample (i.e., estimates based on larger samples should be less uncertain). The SE thus reflects a ratio of variance to sample size.\n\n\n\nQuestions to Explore\n\nHow does changing the size of our samples (n) impact the mean and SE of our estimate of the population parameter?\nHow does changing the number of replicate samples (reps) impact the mean and SE of our estimate of the population parameter?\n\nWe can explore these two questions with the following code… note that we just take one set of 2000 replicates and then resample from that set randomly to explore the impact of different, smaller numbers of replicates:\n\nreps &lt;- 2000\ns &lt;- tibble(n = numeric(), mean = numeric(), .index = numeric())\n\nfor (n in seq(from = 20, to = 200, by = 20)) {\n    samp_dist &lt;- {\n        do(reps) * sample_n(d, size = n, replace = FALSE)\n    } |&gt;\n        group_by(.index) |&gt;\n        dplyr::summarise(mean = mean(averageRating)) |&gt;\n        mutate(n = n)\n    s &lt;- bind_rows(s, samp_dist)\n}\n\noutput &lt;- tibble(n = numeric(), reps = numeric(), samp_dist_mean = numeric(), samp_dist_se = numeric())\n\nfor (reps in c(10, 25, 50, 100, 250, 500, 1000, 2000)) {\n    subsample &lt;- s |&gt;\n        group_by(n) |&gt;\n        sample_n(reps, replace = FALSE) |&gt;\n        dplyr::summarise(samp_dist_mean = mean(mean), samp_dist_se = sd(mean)) |&gt;\n        mutate(reps = reps)\n    output &lt;- bind_rows(output, subsample)\n}\n\n\np1 &lt;- ggplot(data = output, aes(x = n, y = samp_dist_mean)) + geom_line() + facet_grid(~reps) +\n    xlab(\"Sample Size\") + ylab(\"Mean\") + ylim(6, 6.5)\np2 &lt;- ggplot(data = output, aes(x = n, y = samp_dist_se)) + geom_line() + facet_grid(~reps) +\n    xlab(\"Sample Size\") + ylab(\"SE\") + ylim(0, 0.5)\np3 &lt;- ggplot(data = output, aes(x = n)) + geom_line(aes(y = samp_dist_mean + samp_dist_se),\n    color = \"blue\") + geom_line(aes(y = samp_dist_mean)) + geom_line(aes(y = samp_dist_mean -\n    samp_dist_se), color = \"blue\") + facet_grid(~reps) + xlab(\"Sample Size\") + ylab(\"Mean ± SE\") +\n    ylim(5.8, 6.7)\n\n\nlibrary(cowplot)\nplot_grid(p1, p2, p3, nrow = 3)\n\n\n\n\n\n\n\ndetach(package:cowplot)\n\nNotice that as the number of replicates increases, our estimates of both the mean and SE become less variable, while as the sample size increases, our SE decreases (and our estimate of the mean thus becomes less uncertain), regardless of number of replicates.\n\nCalculating SEs from the Population Variance\nNote that above, we estimated the SE by taking the standard deviation of a sampling distribution, where we derived that distribution by taking multiple samples from a perfectly known population. However, if we know the actual population variance (\\(\\sigma^2\\)) or population standard deviation (\\(\\sigma\\)), which we do in this case, we can actually calculate the expected SE for samples of a given size directly, without basing that on an empirical sampling distribution that we have derived from repeated sampling. This is because, mathematically, the variance of a fully realized sampling distribution (i.e., of taking all possible samples of size \\(n\\) from a population) is equal to the variance of the population divided by the sample size. The square root of the variance of the sampling distribution is the standard deviation, i.e., the standard error.\n\\[SE = \\sqrt{\\frac{\\sigma^2}{n}} = \\frac{\\sigma}{\\sqrt{n}}\\]\nwhere \\(\\sigma^2\\) is the population variance (and \\(\\sigma\\) is thus the population standard deviation) and \\(n\\) is the sample size.\nWe can compare the SEs for different sample sizes estimated above from our empirical sampling distributions with the SEs calculated directly from the population variance using the code below.\n\n# select estimated SEs for different sample sizes with 1000 reps from our\n# output tibble\nsampling_output &lt;- filter(output, reps == 1000) |&gt;\n    select(n, samp_dist_se)\n# create a tibble of SEs based on the known population variance\npop_output &lt;- tibble(n = numeric(), pop_se = numeric())\nfor (n in seq(from = 10, to = 200, by = 10)) {\n    pop_se &lt;- sqrt(pop_var(d$averageRating)/n)\n    pop_output &lt;- bind_rows(pop_output, c(n = n, pop_se = pop_se))\n}\n(compare &lt;- inner_join(sampling_output, pop_output, by = \"n\"))\n\n## # A tibble: 10 × 3\n##        n samp_dist_se pop_se\n##    &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n##  1    20       0.257  0.259 \n##  2    40       0.183  0.183 \n##  3    60       0.150  0.150 \n##  4    80       0.129  0.130 \n##  5   100       0.114  0.116 \n##  6   120       0.102  0.106 \n##  7   140       0.0981 0.0981\n##  8   160       0.0871 0.0917\n##  9   180       0.0859 0.0865\n## 10   200       0.0775 0.0820\n\n# the sample_dist_se and pop_se columns should be very close in value!\n\n\n\nEstimating SEs from a Single Sample\nOf course, in practice we often do not know the true population variance or standard deviation, nor do we have the opportunity to generate a sampling distribution empirically and then use this to estimate the SE. Instead, we collect typically just collect a single sample from a population about which we know very little.\nIn these cases, we use some statistic about our single sample as a point estimate for the parameter value in our population (e.g., \\(\\bar{x}\\) for \\(\\mu\\)), and we use the variance or standard deviation and size of that single sample to estimate the SE around that point estimate for our statistic of interest: i.e., square root of (sample variance / sample size) or sample standard deviation / (square root of sample size)\n\\[SE = \\sqrt{\\frac{s^2}{n}} = \\frac{s}{\\sqrt{n}}\\]\nwhere \\(s^2\\) is the sample variance and \\(s\\) is the sample standard deviation.\n\n\n\nCHALLENGE\n\nWrite your own function to calculate the standard error of the mean for a vector of values representing a single sample of observations from a population. You can use either your own function for the sample variance that you created above or the built-in var() function. There are, of course, several ways you could do this. Then, use your new function with summarize() and sample_n() to extract a single sample of size “n=100” from d and calculate an estimate of the population SE for that sample size.\n\n\n\nShow Code\nn &lt;- 100\nset.seed(100)\nmy_se1 &lt;- function(x) {\n    sqrt(sample_var(x)/length(x))\n}\nmy_se2 &lt;- function(x) {\n    sqrt(var(x)/length(x))\n}\nmy_se3 &lt;- function(x) {\n    sd(x)/sqrt(length(x))\n}\n\nnew_sample &lt;- sample_n(d, size = n)\nsummarize(new_sample, se1 = my_se1(averageRating), se2 = my_se2(averageRating), se3 = my_se3(averageRating))\n\n\nShow Output\n## # A tibble: 1 × 3\n##     se1   se2   se3\n##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 0.118 0.118 0.118\n\n\n\nThe package {sciplot} includes the function, se(), for calculating standard errors (as do others).\n\nlibrary(sciplot)\nsummarize(new_sample, se4 = sciplot::se(averageRating))\n\n## # A tibble: 1 × 1\n##     se4\n##   &lt;dbl&gt;\n## 1 0.118\n\ndetach(package:sciplot)",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Descriptive Statisics and Sampling</span>"
    ]
  },
  {
    "objectID": "12-module.html#concept-review",
    "href": "12-module.html#concept-review",
    "title": "12  Descriptive Statisics and Sampling",
    "section": "Concept Review",
    "text": "Concept Review\n\n\\(\\mu, \\bar{x}\\) = population mean, sample mean\n\n\\(\\bar{x}\\) is an estimator for \\(\\mu\\)\n\n\\(\\sigma^2, s^2\\) = population variance, sample variance\n\\(\\sigma, s\\) = population standard deviation, sample standard deviation\n\n\\(s\\) is an estimator for \\(\\sigma\\)\n\nSample variance and standard deviation: var(), sd()\n\nThese are measures of the variation/spread in a sample\n\nSampling distributions and standard errors\n\nMean of the sampling distribution of \\(\\bar{x}\\) is an estimate of \\(\\mu\\)\nStandard deviation of the sampling distribution of \\(\\bar{x}\\) = \\(\\frac{\\sigma}{\\sqrt{n}}\\) = standard error\n\nThis is a measure of the variation/spread in a sample statistic\n\nStandard errors: sciplot::se()\n\nUsing the sample_n() function from {dplyr} and do() * &lt;function&gt; construction from {mosaic} to generate sampling distributions",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Descriptive Statisics and Sampling</span>"
    ]
  },
  {
    "objectID": "13-module.html",
    "href": "13-module.html",
    "title": "13  Probability and Distributions",
    "section": "",
    "text": "13.1 Objectives",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Probability and Distributions</span>"
    ]
  },
  {
    "objectID": "13-module.html#objectives",
    "href": "13-module.html#objectives",
    "title": "13  Probability and Distributions",
    "section": "",
    "text": "The objective of this module is to gently begin our discussion of statistical inference and statistical modeling. Doing so requires that we first cover some basics of probability and statistical distributions.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Probability and Distributions</span>"
    ]
  },
  {
    "objectID": "13-module.html#preliminaries",
    "href": "13-module.html#preliminaries",
    "title": "13  Probability and Distributions",
    "section": "13.2 Preliminaries",
    "text": "13.2 Preliminaries\n\nInstall and load this package in R: {manipulate}\nLoad {tidyverse}, {mosaic}, and {cowplot}",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Probability and Distributions</span>"
    ]
  },
  {
    "objectID": "13-module.html#probability",
    "href": "13-module.html#probability",
    "title": "13  Probability and Distributions",
    "section": "13.3 Probability",
    "text": "13.3 Probability\nThe term probability is applied to population-level variables to describe the magnitude of chance associated with particular observations or events. Probabilities summarize the relative frequencies of different possible outcomes and are properties of distributions of variables. Every variable has a distribution that can be described empirically and visualized, as we have done in some of our previous modules. And, sometimes, these empirical distributions are nicely approximated by particular theoretical distributions with well-known mathematical properties, a fact which forms the basis for traditional frequentist (or classical) statistical inference.\nProbabilities for events or collections of events necessarily vary between zero and one. Outcomes or combinations of outcomes that are impossible have \\(Pr = 0\\), those that are certain have \\(Pr = 1\\).\n\nEXAMPLE: If we roll a (fair, unbiased) die, there are 6 possible outcomes, and each has a probability of occurring of 1 in 6. We can estimate these probabilities using data on the outcome of lots of observations of independent die rolls. This is referred to as a frequentist or classical way of thinking about the probability of these different outcomes… the relative frequency with which a particular event occurs over numerous identical, independent, objective trials.\n\nWe will use the {manipulate} package and the sample() function to explore the effects of sample size on estimates of the probability of different outcomes of the a process of rolling a (fair, unbiased) die. The {manipulate} package allows us to create an interactive plot that lets us dynamically change something about the values being plotted. We will set up a simulation where the probability of each possible outcome of the process of rolling a die one time (“1”, “2”,…, “6”) is 1 in 6, but our estimate of the probability of each possible outcome will change with sample size. In the code below, we use the powerful sample() function, which takes several arguments - a set of elements to sample from (x=), the number of elements to draw (size=), and whether or not to draw with replacement (replace=). After typing in and running the code below, play with the slider to change the number of die rolls being simulated.\n\noutcomes &lt;- c(1, 2, 3, 4, 5, 6)\nmanipulate(histogram(sample(x = outcomes, size = n, replace = TRUE), breaks = c(0.5,\n    1.5, 2.5, 3.5, 4.5, 5.5, 6.5), type = \"density\", main = paste(\"Histogram of Outcomes of \",\n    n, \" Die Rolls\", sep = \"\"), xlab = \"Roll\", ylab = \"Probability\"), n = slider(0,\n    10000, initial = 100, step = 100))\n\n\nCHALLENGE\nWrite your own function, roll(), to simulate rolling a die where you pass the number of rolls as an argument (nrolls=) with a default value of 1. Then, use your function to simulate rolling two dice a total 1000 times and take the sum of the rolls. Plot a histogram of those results. What happens if you roll each die 100 times? 10,000 times?\n\n\nShow Code\nroll &lt;- function(nrolls = 1) {\n    sample(1:6, nrolls, replace = TRUE)\n}  # function with default of 1 roll\nnrolls &lt;- 1000\ntwo_dice &lt;- roll(nrolls) + roll(nrolls)\nhistogram(two_dice, breaks = c(1.5:12.5), type = \"density\", main = \"Rolling Two Dice\",\n    xlab = \"Sum of Rolls\", ylab = \"Probability\")",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Probability and Distributions</span>"
    ]
  },
  {
    "objectID": "13-module.html#rules-of-probability",
    "href": "13-module.html#rules-of-probability",
    "title": "13  Probability and Distributions",
    "section": "13.4 Rules of Probability",
    "text": "13.4 Rules of Probability\nThe following are a set of standard rule of probability that are worth reviewing:\n\n\\(Pr (+)\\) = Probability that something occurs = 1\n\\(Pr (\\emptyset)\\) = Probability that nothing occurs = 0\n\\(Pr (A)\\) = Probability that a particular event, \\(A\\), occurs\n\n\\[0 \\leq Pr (A) \\leq 1\\]\n\n\\(Pr (A \\cup B)\\) = Probability that a particular event \\(A\\) or a particular event \\(B\\) occurs = UNION\n\n\\[Pr (A \\cup B) = Pr (A) + Pr (B) - Pr (A \\cap B)\\]\n\nIf events \\(A\\) and \\(B\\) are mutually exclusive, then this simplifies to… \\[Pr (A) + Pr (B)\\]\n\n\n\\(Pr (A \\cap B)\\) = Probability that both \\(A\\) and \\(B\\) occur simultaneously = INTERSECTION\n\n\\[Pr (A \\cap B) = Pr (A \\vert B) \\times Pr (B) = Pr (B \\vert A) \\times Pr (A)\\]\n\nThe pipe operator ( \\(\\vert\\) ) can be read as “given” and indicates conditional probability (see below)\nIf \\(Pr (A \\cap B) = 0\\), then we say the events are mutually exclusive (e.g., you cannot have a die roll be 1 and 2)\nIf the two events are independent (i.e., if the probability of one does not depend on the probability of the other), then \\(Pr (A \\cap B)\\) simplifies to… \\[Pr (A) \\times Pr (B)\\]\n\n\nProbability of the COMPLEMENT of \\(A\\) (i.e., not \\(A\\)) = \\(Pr (Ā) = 1 - Pr (A)\\)\nCONDITIONAL PROBABILITY is the probability of an event occuring after taking into account the occurrence of another event, i.e., one event is conditioned on the occurrence of a different event. For example, the probability of a die coming up as a “1” given that we know the die came up as an odd number (“1”, “3”, or “5”) is a conditional probability.\n\n\\[Pr (A \\vert B) = Pr (A \\cap B) \\div Pr (B)\\]\n\nIf event \\(A\\) and event \\(B\\) are independent, then \\[Pr (A \\vert B) = [Pr (A) \\times Pr (B) ] \\div Pr (B) = Pr (A)\\]\nIf event \\(A\\) and \\(B\\) are not independent, then \\[Pr (A \\vert B) ≠ Pr (A)\\]\n\n\nCHALLENGE\nYou have a deck of 52 cards… Ace to 10 plus 3 face cards in each suit. You draw a card at random.\n\nWhat is the probability that you draw a face card?\n\n\n\\(Pr (face\\ card)\\)\n\n12 of 52 cards = 0.2307692\n\n\n\nWhat is the probability that you draw a King?\n\n\n\\(Pr(King)\\)\n\n3 of 52 cards = 0.05769231\n\n\n\nWhat is the probability that you draw a spade?\n\n\n\\(Pr(spade)\\)\n\n13 of 52 cards = 0.25\n\n\n\nWhat is the probability that you draw a spade given that you draw a face card? (CONDITIONAL, INDEPENDENT EVENTS)\n\n\nIntuitively…\n\n3 of 12 cards = 0.25\n\nFormally…\n\\(Pr(spade \\vert face\\ card) = Pr(spade) \\times Pr(face\\ card) \\div Pr(face\\ card)\\)\n\n(13 of 52) \\(\\times\\) (12 of 52) \\(\\div\\) (12 of 52) = 0.25\n\n\n\nWhat is the probability that you draw a King given that you draw a face card? (CONDITIONAL, NOT INDEPENDENT EVENTS)\n\n\nIntuitively…\n\n4 of 12 cards = 0.3333333\n\nFormally…\n\\(Pr(King \\vert face\\ card) = Pr(King \\cap face\\ card) \\div Pr(face\\ card) =\\)\n\\(Pr(face\\ card \\vert King) \\times Pr(King) \\div Pr(face\\ card)\\)\n\n1 \\(\\times\\) (4 of 52) \\(\\div\\) (12 of 52) = 0.3333333\n\n\n\nWhat is the probability that you draw a card that is both from a red suit (hearts or diamonds) and a face card? (INTERSECTION, INDEPENDENT EVENTS)\n\n\nIntuitively…\n\n6 of 52 cards = 0.1153846\n\nFormally…\n\\(Pr (red \\cap face\\ card) = Pr (red \\vert face\\ card) \\times Pr (face\\ card) =\\)\n\\([Pr(red) \\times Pr(face\\ card)] \\div Pr(face\\ card) \\times Pr(face\\ card)\\)\nwhere…\n\\(Pr (red)\\)\n\n26 of 52 cards = 0.5\n\n\\(Pr (face\\ card)\\)\n\n12 of 52 cards = 0.2307692\n\nso…\n\n[(26 of 52) \\(\\times\\) (12 of 52)] \\(\\div\\) (12 of 52) \\(\\times\\) (12 of 52) = 0.1153846\n\n\n\nWhat is the probability that you draw a card that is either a club or not a face card? (UNION, NOT INDEPENDENT EVENTS)\n\n\nIntuitively…\n\n(13 club cards \\(+\\) 40 not face cards \\(-\\) 10 club cards that are not face cards) of 52 cards = 43 of 52 cards = 0.8269231\n\nFormally…\n\\(Pr (club \\cup not\\ a\\ face\\ card) =\\)\n\\(Pr (club) + Pr (not\\ a\\ face\\ card) - Pr (club \\cap not\\ a\\ face\\ card) =\\)\n\\(Pr (club) + Pr (not\\ a\\ face\\ card) - Pr (club \\vert not\\ a\\ face\\ card) \\times Pr (not\\ a\\ face\\ card)\\)\nwhere…\n\\(Pr (club)\\)\n\n13 of 52 cards = 0.25\n\n\\(Pr (not\\ a\\ face\\ card)\\)\n\n40 of 52 cards = 0.7692308\n\n\\(Pr (club \\vert not\\ a\\ face\\ card)\\)\n\n10 of 40 = 0.25\n\nso…\n\n(13 of 52) \\(+\\) (40 of 52) \\(-\\) [(10 of 40) \\(\\times\\) (40 of 52)] = 0.8269231",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Probability and Distributions</span>"
    ]
  },
  {
    "objectID": "13-module.html#random-variables",
    "href": "13-module.html#random-variables",
    "title": "13  Probability and Distributions",
    "section": "13.5 Random Variables",
    "text": "13.5 Random Variables\nA random variable is a variable whose outcomes are assumed to arise by chance or according to some random or stochastic process. The chances of observing a specific outcome, or an outcome value within a specific interval, has associated with it a probability.\nRandom variables come in two varieties:\n\nDiscrete Random Variables are random variables that can assume only a countable number of discrete possibilities (e.g., counts of outcomes in a particular category, e.g., rolls of a die). We can assign a probability to each possible outcome.\nContinuous Random Variables are random variables that can assume any real number value within a given range (e.g., measurements of body weight, number of offspring, linear dimensions, etc.). We cannot assign a specific probability to each possible outcome value as the set of possible outcomes is (theoretically) infinite, but we can assign probabilites to intervals of outcome values.\n\nWith these basics in mind, we can define a few more terms:\nA probability function is a mathematical function that describes the chance associated with a random variable either having particular outcomes (for discrete variables) or falling within a given range of outcome values (for continuous variables). Familiar statistical distributions (e.g., the normal distribution, the beta distribution, the Poisson distribution) are all examples of probability functions.\nWe can distinguish two types of probability functions, associated with these different kinds of random variables.\n\nProbability Mass Functions\nProbability Mass Functions (PMFs) are associated with discrete random variables. These functions describe the probability that a random variable takes a particular discrete value.\nTo be a valid PMF, a function \\(f(x)\\) must satisfy the following conditions:\n\nThere are \\(k\\) distinct outcomes \\(x_1, x_2, ... ,x_k\\)\n\\(0 \\leq Pr (X=x_i) \\leq 1\\) for all \\(x_i\\)\n\\(\\sum\\limits_{i=1}^{k} Pr (X=x_i) = 1\\)\n\n\nEXAMPLE: Flipping a Fair Coin\n\noutcomes &lt;- c(\"heads\", \"tails\")\nprob &lt;- c(1/2, 1/2)\nbarplot(prob, ylim = c(0, 0.6), names.arg = outcomes, space = 0.1, xlab = \"outcome\",\n    ylab = \"Pr(X = outcome)\", main = \"Probability Mass Function\")\n\n\n\n\n\n\n\ncumprob &lt;- cumsum(prob)\ncumoutcomes &lt;- c(\"heads\", \"heads + tails\")\nbarplot(cumprob, names.arg = cumoutcomes, space = 0.1, xlab = \"outcome\", ylab = \"Cumulative Pr(X)\",\n    main = \"Cumulative Probability\")\n\n\n\n\n\n\n\n\n\n\nEXAMPLE: Rolling a Fair Die\n\noutcomes &lt;- c(1, 2, 3, 4, 5, 6)\nprob &lt;- c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6)\nbarplot(prob, ylim = c(0, 0.5), names.arg = outcomes, space = 0.1, xlab = \"outcome\",\n    ylab = \"Pr(X = outcome)\", main = \"Probability Mass Function\")\n\n\n\n\n\n\n\ncumprob &lt;- cumsum(prob)\ncumoutcomes &lt;- c(\"1\", \"1 to 2\", \"1 to 3\", \"1 to 4\", \"1 to 5\", \"1 to 6\")\nbarplot(cumprob, names.arg = cumoutcomes, space = 0.1, xlab = \"outcome\", ylab = \"Cumulative Pr(X)\",\n    main = \"Cumulative Probability\")\n\n\n\n\n\n\n\n\n\n\n\nProbability Density Functions\nProbability Density Functions (PDFs) are associated with continuous random variables. These functions describe the probability that a random variable falls within a given range of outcome values. The probability associated with that range equals the area under the density function for that range.\nTo be a valid PDF, a function \\(f(x)\\) must satisfy the following:\n\n\\(f(x)\\geq 0\\) for all \\(-\\infty \\leq x \\leq +\\infty\\). That is, the function \\(f(x)\\) is non-negative everywhere.\n\\(\\int_\\limits{-\\infty}^{+\\infty} f(x) dx = 1\\). That is, the total area under the function \\(f(x)\\) = 1\n\n\nExample: Exploring the Beta Distribution\nThe Beta Distribution refers to a family of continuous probability distributions defined over the interval [0, 1] and parameterized by two positive shape coefficients, denoted by \\(\\alpha\\) and \\(\\beta\\), that appear as exponents of the random variable \\(x\\) and control the shape of the distribution. The beta distribution function is…\n\\[f(x) = x^{\\alpha-1}(1-x)^{\\beta-1}\\]\n\nNOTE: There is nothing special about the Beta Distribution for this example, we are just using it to show how probabilities are equivalent to areas under a function.\n\nWe can explore the beta distribution using the {manipulate} package by entering the code below. With the domain of \\(x\\) restricted to [0, 1], and with \\(\\alpha\\) initially set to 2 and \\(\\beta\\) initially set to 1, we see the PDF for the beta distribution is triangular. Try playing with different values for \\(\\alpha\\) and \\(\\beta\\). The dbeta() function provides the value of the function at the specified values of x.\n\nmanipulate(ggplot(data = data.frame(x = c(0, 1)), aes(x)) + stat_function(fun = dbeta,\n    args = list(shape1 = alpha, shape2 = beta), n = 1000) + xlab(\"x\") + ylab(\"f(x)\") +\n    labs(title = \"Exploring the Beta Distribution\", subtitle = paste0(\"Cumulative Probability = \",\n        round(pbeta(x, alpha, beta), 2))) + stat_function(fun = dbeta, xlim = c(0,\n    x), args = list(shape1 = alpha, shape2 = beta), n = 1000, geom = \"area\"), alpha = slider(0,\n    10, initial = 2, step = 0.1), beta = slider(0, 10, initial = 1, step = 0.1),\n    x = slider(0, 1, initial = 0, step = 0.01))\n\nIs this a PDF? Why or why not? Yes… it satisfies both criteria for a PDF.\n\n\\(f(x) \\geq 0\\) for all \\(-\\infty \\leq x \\leq +\\infty\\)\nThe total area under \\(f(x)\\) = 1\n\nWe can show this interactively by playing with the slider for \\(x\\) and looking at the shaded area, which represents the cumulative probability integrated across \\(f(x)\\) from \\(-\\infty\\) to \\(x\\).\n\n\n\nCumulative Distribution Functions\nThe cumulative distribution function, or CDF, of a random variable is defined as the probability of observing a random variable \\(X\\) taking the value of \\(x\\) or less, i.e., \\(F(x) = Pr (X \\leq x)\\).\nThis definition actually applies regardless of whether \\(X\\) is discrete or continuous. Note here we are using the notation \\(F(x)\\) for the cumulative distribution function rather than \\(f(x)\\), which we use for the probability density or mass function. For a continuous variable, the PDF is simply the first derivative of the CDF, i.e., \\(f(x) = dF(x)\\).\nThe built in R “probability” function for the Beta Distribution, pbeta(), gives us this cumulative probability directly, if we specify the values of \\(\\alpha\\) and \\(\\beta\\). E.g., for \\(\\alpha\\) = 2 and \\(\\beta\\) = 1…\n\npbeta(0.75, 2, 1)  # cumulative probability for x ≤ 0.75\n\n## [1] 0.5625\n\npbeta(0.5, 2, 1)  # cumulative probability for x ≤ 0.50\n\n## [1] 0.25\n\n\nIn general, we find the cumulative probability for a continuous random variable by calculating the area under the probability density function of interest from \\(-\\infty\\) to \\(x\\). This is what is is being returned from pbeta().\n\nThe other related R functions for the Beta Distribution, i.e., rbeta(), dbeta(), and qbeta(), are also useful. rbeta() draws random observations from a specfied beta distribution. dbeta() gives the point estimate of the beta density function at the value of the argument \\(x\\), and qbeta() is essentially the converse of pbeta(), i.e., it tells you the value of \\(x\\) that is associated with a particular cumulative probability, or quantile, of the cumulative distribution function.\n\n\ndbeta(0.75, 2, 1)\n\n## [1] 1.5\n\nqbeta(0.5625, 2, 1)\n\n## [1] 0.75\n\n\nWe can also define the survival function for a random variable \\(X\\) as:\n\\[S(x) = Pr (X \\gt x) = 1 - Pr (X \\leq x) = 1 - F(x)\\]\nPMFs and PDFs for many other standard or well studied distributions have comparable r, d, p, and q functions to those for the Beta Distribution.\nTL/DR: R has four built-in functions that can be applied to a variety of standard and well understood statistical distributions:\n\nr - the random generation function, which draws a random variable from the given distribution.\nd - the density function, PMF or PDF, which describes the distribution of values for the function across the range of \\(x\\) values: \\(f(x)\\).\np - the cumulative distribution function, CDF, which gives the cumulative probability for all values from \\(-\\infty\\) to \\(x\\) for a given distribution: \\(F(x)\\) = Pr \\((X\\) \\(\\leq\\) \\(x)\\).\nq - the quantile function, which is the converse of p: the value of \\(x\\) at which the CDF has the value \\(q\\), i.e., \\(F(x_q) = q\\).\n\nNote the relationship between the p and q functions:\n\npbeta(0.7, 2, 1)  # yields 0.49 - x values ≤ 0.7 comprise 49% of the CDF\n\n## [1] 0.49\n\nqbeta(0.49, 2, 1)  # yields 0.7 - 49% of the CDF falls in the range x ≤ 0.7\n\n## [1] 0.7\n\n\n\n\nExpected Mean and Variance\nThe mean value (or expectation) of a discrete random variable with a given probability mass function is equivalent to a “long-term average”, i.e., what you would expect the average value of the variable to be if you sampled from the PMF many, many times. This expectation can be expressed generally as follows:\n\\[\\mu_X = Expectation\\ for\\ X = \\sum x_i \\times Pr (X=x_i)\\ for\\ all\\ x\\ from\\ x_i\\ to\\ x_k\\]\nLikewise, the expected variance of a discrete random variable, \\(X\\), across a large sample from the PMF is:\n\\[\\sigma_X^2 = Variance\\ of\\ X = \\sum (x_i - \\mu_X)^2 \\times Pr (X=x_i)\\ for\\ all\\ x\\ from\\ x_i\\ to\\ x_k\\]\nApplying these formulae to die rolls, we could calculate the expectation for \\(X\\) for a large set of die rolls as follows…\n\\[(1 \\times 1/6) + (2 \\times 1/6)\\  +\\ ...\\ +\\ (6 \\times 1/6) = 3.5\\]\n\nm &lt;- sum(seq(1:6) * 1/6)\nm\n\n## [1] 3.5\n\n\nAnd the expected variance would be…\n\\[[(1 - 3.5)^2 \\times (1/6)]\\ +\\  [(2 - 3.5)^2 \\times (1/6)]\\ +\\ ...\\ +\\ [(6 - 3.5)^2 \\times (1/6)] = 2.916667\\]\n\nvar &lt;- sum((seq(1:6) - mean(seq(1:6)))^2 * (1/6))\nvar\n\n## [1] 2.916667\n\n\nLikewise, we can calculate the expectation and variance for a continuous random variable, \\(X\\), with a given probability density function generally as follows:\n\\[\\mu_X = Expectation\\ for\\ X = \\int\\limits_{-\\infty}^{+\\infty} x f(x) dx\\]\n\\[\\sigma_X^2 = Variance\\ of\\ X = \\int\\limits_{-\\infty}^{+\\infty} (x - \\mu_X)^2 f(x) dx\\]\n\nNOTE: To demonstrate these numerically would require a bit of calculus, i.e., integration, which we will not go through here.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Probability and Distributions</span>"
    ]
  },
  {
    "objectID": "13-module.html#useful-probability-distributions",
    "href": "13-module.html#useful-probability-distributions",
    "title": "13  Probability and Distributions",
    "section": "13.6 Useful Probability Distributions",
    "text": "13.6 Useful Probability Distributions\n\nProbability Mass Functions\n\nThe Bernoulli Distribution\nThe Bernoulli Distribution is the probability distribution of a binary random variable, i.e., a variable that has only two possible outcomes, such as success or failure, heads or tails, true or false. If \\(p\\) is the probability of one outcome, then \\(1-p\\) has to be the probabilty of the alternative. For flipping a fair coin, for example, \\(p\\) = 0.5 and \\(1-p\\) also = 0.5.\nFor the Bernoulli Distribution, the probability mass function is:\n\\[f(x) = p^x(1-p)^{1-x}\\]\nwhere x = {0 or 1}\nFor this distribution, the expectation and variance across a large set of trials would be: \\(\\mu_X = p\\) and \\(\\sigma_X^2 = p(1-p)\\)\n\n\n\nCHALLENGE\nUsing the Bernoulli distribution, calculate the expectation for drawing a spade from a deck of cards? What is the variance in this expectation across a large number of draws?\n\\[Pr (spade) = (13/52)^1 \\times (39/52)^0 = 0.25\\]\n\\[Var (spade) = (13/52) \\times (1-13/52) = (0.25) \\times (0.75) = 0.1875\\]\nIn code, we can simulate this…\n\nreps &lt;- 1e+05\ncard &lt;- sample(1:52, reps, replace = TRUE)\nspade &lt;- ifelse(card &lt;= 13, TRUE, FALSE)\n(exp_spade &lt;- sum(spade)/reps)\n\n## [1] 0.25201\n\n(exp_var_spade &lt;- var(spade))\n\n## [1] 0.1885028\n\n\n\nThe Binomial Distribution\nThe Bernoulli distribution is a special case of the Binomial Distribution. The binomial distribution is typically used to model the probability of a number of “successes”, k, out of a set of “trials”, n, i.e., for counts of a particular outcome.\nAgain, the probability of success on each trial = \\(p\\) and the probability of not success = \\(1-p\\).\nFor the Binomial Distribution, the probability mass function is:\n\\[f(x)=\\binom{n}{k} p^k (1-p)^{n-k}\\]\nwhere \\(x\\) = {0, 1, 2, … , n} and where\n\\[\\binom{n}{k}=\\frac{n!}{k!(n-k)!}\\]\nThis is read as “\\(n\\) choose \\(k\\)”, i.e., the probability of \\(k\\) successes out of \\(n\\) trials. This is also called the “binomial coefficient”.\nFor this distribution, the expectation (i.e., expected number of successes out of \\(k\\) trials) and the expected variance are as follows:\n\\[\\mu_X = np\\] and\n\\[\\sigma_X^2 = np(1-p)\\]\nRecall, \\(\\mu_X\\) = expected number of successes in \\(n\\) trials. Where \\(n\\) = 1, the binomial distribution simplifies to the Bernoulli distribution.\n\n\n\nCHALLENGE\n\nWhat is the chance of getting a “1” on each of six consecutive rolls of a die? Recall that rolling a “1” = a success, while rolling something other than a “1” is not a success.\nWhat about of getting exactly three “1”s (i.e., 3 successes).\nWhat is the expected number of “1”s to occur in six consecutive rolls (i.e., what is the expected number of successes) and the variance around that expectation?\n\n\nn &lt;- 6  # number of trials\nk &lt;- 6  # exact number of successes\np &lt;- 1/6\n(all_ones &lt;- (factorial(n)/(factorial(k) * factorial(n - k))) * (p^k) * (1 - p)^(n -\n    k))\n\n## [1] 2.143347e-05\n\nk &lt;- 3  # exact number of successes\n(three_ones &lt;- (factorial(n)/(factorial(k) * factorial(n - k))) * (p^k) * (1 - p)^(n -\n    k))\n\n## [1] 0.05358368\n\n# expected number of successes\n(expected_ones &lt;- n * p)\n\n## [1] 1\n\n(expected_variance &lt;- n * p * (1 - p))\n\n## [1] 0.8333333\n\n\nAs for other distributions, R has a built in d (density) function, the dbinom() function, that you can use to solve for the probability of a given outcome directly, i.e., Pr \\((X = x)\\).\n\ndbinom(x = k, size = n, prob = p)\n\n## [1] 0.05358368\n\n\nWe can also use the built in function pbinom() to return the value of the cumulative distribution function for the binomial distribution, i.e., the probability of observing up to and including a given number of successes in \\(n\\) trials.\nSo, for example, the chances of observing exactly 0, 1, 2, 3, … 6 rolls of “1” on 6 rolls of a die are…\n\nprobset &lt;- dbinom(x = 0:6, size = 6, prob = 1/6)\n# x is number of successes, size is number of trials\nbarplot(probset, names.arg = 0:6, space = 0, xlab = \"outcome = # of 'ones' seen in 6 rolls\",\n    ylab = \"Pr(X = outcome)\", main = \"Probability Mass Function\")\n\n\n\n\n\n\n\ncumprob = cumsum(probset)\nbarplot(cumprob, names.arg = 0:6, space = 0.1, xlab = \"outcome\", ylab = \"Cumulative Pr(X)\",\n    main = \"Cumulative Probability\")\n\n\n\n\n\n\n\nsum(probset)  # equals 1, as it should\n\n## [1] 1\n\n\nThe chance of observing exactly 3 rolls of “1” is…\n\ndbinom(x = 3, size = 6, prob = 1/6)\n\n## [1] 0.05358368\n\n\nAnd the chance of observing up to and including 3 rolls of “1” in 6 rolls is…\n\npbinom(q = 3, size = 6, prob = 1/6)\n\n## [1] 0.991298\n\n# note the name of the argument is `q=` not `x=`\n\n… which can also be calculated by summing the relevant individual outcome probabilities…\n\n# this sums the probabilities of 0, 1, 2, and 3 successes\nsum(dbinom(x = 0:3, size = 6, prob = 1/6))\n\n## [1] 0.991298\n\n\nThe probability of observing more than 3 rolls of “1” is given as…\n\n1 - pbinom(q = 3, size = 6, prob = 1/6)\n\n## [1] 0.008701989\n\n\nor, alternatively…\n\npbinom(q = 3, size = 6, prob = 1/6, lower.tail = FALSE)\n\n## [1] 0.008701989\n\n\nThe probability of observing three or more rolls of “1” is…\n\n1 - pbinom(q = 2, size = 6, prob = 1/6)\n\n## [1] 0.06228567\n\n# note here that the `q=` argument is '2'\n\nor, alternatively…\n\npbinom(q = 2, size = 6, prob = 1/6, lower.tail = FALSE)\n\n## [1] 0.06228567\n\n\nAbove, we were using the theoretical binomial distribution to answer our questions about expectations… Let’s do this same process by simulation!\n\n# simulate num_rolls rolls reps times\nreps &lt;- 1e+06\nnum_rolls &lt;- 6\n# set up vectors to hold simulation results\nall_ones &lt;- vector(mode = \"logical\", length = reps)\nthree_ones &lt;- vector(mode = \"logical\", length = reps)\ncount_ones &lt;- vector(mode = \"logical\", length = reps)\nfor (i in 1:reps) {\n    rolls &lt;- sample(c(1, 2, 3, 4, 5, 6), size = num_rolls, replace = TRUE)\n    all_ones[[i]] &lt;- all(rolls == 1)\n    if (sum(rolls == 1) == 3) {\n        three_ones[[i]] &lt;- TRUE\n    } else {\n        three_ones[[i]] &lt;- FALSE\n    }\n    count_ones[[i]] &lt;- sum(rolls == 1)\n}\n# all 'ones' in 6 rolls\nsum(all_ones)/reps\n\n## [1] 2.7e-05\n\n# three 'ones' in 6 rolls\nsum(three_ones)/reps\n\n## [1] 0.053532\n\n# expected # 'ones' in 6 rolls\nmean(count_ones)\n\n## [1] 0.999991\n\n# variance in expected # 'ones' across rolls\nsum((count_ones - mean(count_ones))^2)/(length(count_ones))\n\n## [1] 0.833331\n\n\n\nThe Poisson Distribution\nThe Poisson Distribution is often used to model open ended counts of independently occuring events, for example the number of cars that pass a traffic intersection over a given interval of time or the number of times a monkey scratches itself during a given observation interval. The probability mass function for the Poisson distribution is described by a single parameter, \\(\\lambda\\), where \\(\\lambda\\) can be interpreted as the mean number of occurrences of the event in the given interval.\nThe probability mass function for the Poisson Distribution is:\n\\[f(x)=\\frac{\\lambda^x\\exp(-\\lambda)}{x!}\\]\nwhere \\(x\\) = {0, 1, 2, …}\nFor this distribution, \\(\\mu_X = \\lambda\\) and \\(\\sigma_X^2 = \\lambda\\)\nNote that for the Poisson Distribution the mean and the variance are the same!\nLet’s use R and the {moasic} package to look at the probability mass functions for different values of \\(\\lambda\\):\n\nl &lt;- 3.5\np1 &lt;- plotDist(\"pois\", lambda = l, main = paste0(\"Poisson Distribution\\nwith lambda=\",\n    l), xlab = \"x\", ylab = \"Pr(X=x)\")\nl &lt;- 10\np2 &lt;- plotDist(\"pois\", lambda = l, main = paste0(\"Poisson Distribution\\nwith lambda=\",\n    l), xlab = \"x\", ylab = \"Pr(X=x)\")\nl &lt;- 20\np3 &lt;- plotDist(\"pois\", lambda = l, main = paste0(\"Poisson Distribution\\nwith lambda=\",\n    l), xlab = \"x\", ylab = \"Pr(X=x)\")\nplot_grid(p1, p2, p3, nrow = 1)\n\n\n\n\n\n\n\n\nAs we did for other distributions, we can also use the built in p function for the Poisson distribution, ppois(), to return the value of the cumulative distribution function, i.e., the probability of observing up to and including a specific number of events in the given interval.\n\nl &lt;- 3.5\np1 &lt;- plotDist(\"pois\", lambda = l, kind = \"cdf\", main = paste0(\"Cumulative Probability\\nwith lambda=\",\n    l), xlab = \"x\", ylab = \"Pr(X≤x)\", type = \"l\")\nl &lt;- 10\np2 &lt;- plotDist(\"pois\", lambda = l, kind = \"cdf\", main = paste0(\"Cumulative Probability\\nwith lambda=\",\n    l), xlab = \"x\", ylab = \"Pr(X≤x)\", type = \"l\")\nl &lt;- 20\np3 &lt;- plotDist(\"pois\", lambda = l, kind = \"cdf\", main = paste0(\"Cumulative Probability\\nwith lambda=\",\n    l), xlab = \"x\", ylab = \"Pr(X≤x)\", type = \"l\")\nplot_grid(p1, p2, p3, nrow = 1)\n\n\n\n\n\n\n\n\n\n\n\nProbability Density Functions\n\nThe Uniform Distribution\nThe Uniform Distribution is the simplest probability density function describing a continuous random variable. The probability is uniform and does not fluctuate across the range of \\(x\\) values in a given interval.\nThe probability density function for the Uniform Distribution is:\n\\[f(x)=\\frac{1}{b-a}\\]\nwhere \\(a \\leq x \\leq b\\) and the function is 0 for \\(x &lt; a\\) and \\(x &gt; b\\)\nWhat would you predict the expectation (mean) should be for a uniform distribution? Not surprisingly, for this distribution:\n\\[\\mu_x = \\frac{a+b}{2}\\]\nand\n\\[\\sigma_x^2 = \\frac{(b-a)^2}{12}\\]\nLet’s plot a uniform distribution across a given range, from \\(a\\) = 4 to \\(b\\) = 8…\n\na &lt;- 4\nb &lt;- 8\nx &lt;- seq(from = a - 1, to = b + 1, by = 0.01)\nfx &lt;- dunif(x, min = a, max = b)  # dunif() evaluates the density at each x\nplot(x, fx, ylim = c(0, max(fx) + 0.1), type = \"l\", xlab = \"x\", ylab = \"f(x)\", main = \"Probability Density Function\")\n\n\n\n\n\n\n\n\nNote that for the uniform distribution, the cumulative density function increases linearly over the given interval.\n\n# punif() is the cumulative probability density up to a given x\nplot(x, punif(q = x, min = a, max = b), ylim = c(0, 1.1), type = \"l\", xlab = \"x\",\n    ylab = \"Pr(X ≤ x)\", main = \"Cumulative Probability\")\n\n\n\n\n\n\n\n\n\n\n\nCHALLENGE\nSimulate a sample of 10,000 random numbers from a uniform distribution in the interval between \\(a\\) = 6 and \\(b\\) = 8. Calculate the mean and variance of this simulated sample and compare it to the expectation for these parameters.\n\n\nShow Code\na &lt;- 6\nb &lt;- 8\nnums &lt;- runif(n = 10000, min = a, max = b)\n(m &lt;- mean(nums))\n\n\nShow Output\n## [1] 7.002742\n\n\n\nShow Code\n(v &lt;- var(nums))\n\n\nShow Output\n## [1] 0.3346235\n\n\n\nShow Code\n(expected_mean &lt;- (a + b)/2)\n\n\nShow Output\n## [1] 7\n\n\n\nShow Code\n(expected_variance &lt;- ((b - a)^2)/12)\n\n\nShow Output\n## [1] 0.3333333\n\n\n\n\nThe Normal Distribution\nThe Normal or Gaussian Distribution is perhaps the most familiar and most commonly applied probability density functions for modeling continuous random variables. Why is the normal distribution so important? Well, many traits are normally distributed, and the additive combination of many random factors is also commonly normally distributed. Even more importantly, as we will see in Module 14, the sampling distribution for many summary statistics (e.g., sample means) tends to be normally distributed when sample size is sufficiently large, and this fact is central to statistical inference.\nFor the normal distribution, two parameters, \\(\\mu\\) and \\(\\sigma\\), are used to describe the shape of the distribution.\nThe code below allows us to visualize and play around with a normal distribution. First, try maniupulating \\(\\mu\\) and \\(\\sigma\\).\n\nmanipulate(ggplot(data = data.frame(x = c(mu - 6 * sigma, mu + 6 * sigma)), aes(x)) +\n    stat_function(fun = dnorm, args = list(mean = mu, sd = sigma), n = 1000) + xlab(\"x\") +\n    ylab(\"f(x)\") + labs(title = \"Exploring the Normal Distribution\", subtitle = paste0(\"Probability ± \",\n    n_sigma, \" SD around Mean = \", round(pnorm(mu + n_sigma * sigma, mu, sigma) -\n        pnorm(mu - n_sigma * sigma, mu, sigma), 4))) + stat_function(fun = dnorm,\n    xlim = c(mu - n_sigma * sigma, mu + n_sigma * sigma), args = list(mean = mu,\n        sd = sigma), n = 1000, geom = \"area\", fill = \"red\", alpha = 0.5, color = \"red\"),\n    mu = slider(-100, 100, initial = 0, step = 10), sigma = slider(0, 30, initial = 5,\n        step = 1), n_sigma = slider(0, 4, initial = 0, step = 0.25))\n\nThe function, dnorm() gives the value of the normal probabilty density function at a given value of \\(x\\). \\(x\\) can range from -\\(\\infty\\) to +\\(\\infty\\). [Recall, it does not really make sense to talk about the “probability” associated with a given specific value of a continuous variable as this is a density not a mass function… but we can talk about the probability of \\(x\\) falling within a given interval.]\nThe pnorm() function, as with the p function for other distributions, returns the cumulative probability of observing a value less than or equal to a given value of \\(x\\), i.e., Pr \\((X\\) \\(\\leq\\) \\(x)\\). We can use the pnorm() function to calculate the probability of an observation drawn from the population falling within a particular, arbitrary interval.\nFor example, for a normally distributed population variable with \\(\\mu\\) = 6 and \\(\\sigma\\) = 2, the probability of a random observation falling between 7 and 8 is…\n\np &lt;- pnorm(8, mean = 6, sd = 2) - pnorm(7, mean = 6, sd = 2)\np\n\n## [1] 0.1498823\n\n\nIn the code above, this is what we are doing when we play interactively with \\(n\\_sigma\\)… we are using the difference between two pnorm() calculations to determine the probability of an observation falling within \\(n\\_sigma\\) standard deviations of the mean of a particular normal distribution.\n\nNOTE: Regardless of the specific values of \\(\\mu\\) and \\(\\sigma\\), about 95% of the normal distribution falls within 2 standard deviations of the mean, and about 68% of the distribution falls within 1 standard deviation. Check this out by playing with the \\(n\\_sigma\\) slider.\n\n\n\n\n\n\n\n\n\n\nAnother one of the family of normal distribution functions in R - the qnorm() or quantile function - will tell us the value of \\(x\\) below which a given proportion of the cumulative probability function falls. For example, if we wanted to see the value of \\(x\\) below which 2.5% of a normal distribution with mean of 5 and standard deviation of 6 falls, we could run the following:\n\nqnorm(0.025, mean = 5, sd = 3, lower.tail = TRUE)\n\n## [1] -0.879892\n\n# lower.tail = TRUE by default...if we used lower.tail = FALSE this would give\n# us the value of x for the upper.tail of the distribution\n\n\n\n\nCHALLENGE\n\nCreate a vector, v, containing n random numbers selected from a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). Use 1000 for n, 3.5 for \\(\\mu\\), and 4 for \\(\\sigma\\).\n\n\nHINT: Such a function exists! rnorm(). We also call set.seed() before rnorm() so that each time this function is run, it returns the same sequence of random numbers until the seed is reset.\n\n\nCalculate the mean, variance, and standard deviation for your sample of random numbers.\nPlot a histogram of your random numbers.\n\n\n\nShow Code\nn &lt;- 1000\nmu &lt;- 3.5\nsigma &lt;- 4\nset.seed(1)\nv &lt;- rnorm(n, mu, sigma)\nmean(v)\n\n\nShow Output\n## [1] 3.453407\n\n\n\nShow Code\nvar(v)\n\n\nShow Output\n## [1] 17.13681\n\n\n\nShow Code\nsd(v)\n\n\nShow Output\n## [1] 4.139663\n\n\n\nShow Code\nhistogram(v, main = paste0(\"Random Draws from a Normal Distribution\\nwith Mean = \",\n    mu, \" and SD = \", sigma), type = \"density\", center = mu)\n\n\n\n\n\n\n\n\n\nWe can easily plot the density and cumulative probability functions for this normal distribution using the plotDist() function from the {mosaic} package:\n\npdf &lt;- plotDist(\"norm\", mean = mu, sd = sigma, xlab = \"X\", ylab = \"Density\")\ncdf &lt;- plotDist(\"norm\", mean = mu, sd = sigma, kind = \"cdf\", xlab = \"X\", ylab = \"Cumulative Probability\")\nplot_grid(pdf, cdf)\n\n\n\n\n\n\n\n\n\nQ-Q Plots\nA quantile-quantile or Q-Q plot can be used to look at whether a set of data seem to follow a normal distribution. A Q–Q plot is a general graphical method for comparing two probability distributions. To examine a set of data for normality graphically, you plot the quantiles for your actual data (as the y values) versus theoretical quantiles (as the x values) pulled from a normal distribution. If the two distributions being compared are similar, the points in the plot should lie approximately on the line y = x.\nIf we do this for the random variables we just generated, this should be apparent since we have simulated our initial vector of data from a normal distribution.\nTo quickly do a Q-Q plot, call the two R functions qqnorm() and qqline() using the vector of data you want to examine as an argument.\n\nqqnorm(v, main = \"QQ Plot - Random Normal Variable\")\nqqline(v, col = \"gray\")\n\n\n\n\n\n\n\n\nThis is the same as doing the following:\n\nStep 1: Generate a sequence of probability points in the interval from 0 to 1 equivalent in length to vector v:\n\n\np &lt;- ppoints(length(v))\n# the `ppoints()` function generates evenly distributed points between 0 and 1\nhead(p)\n\n## [1] 0.0005 0.0015 0.0025 0.0035 0.0045 0.0055\n\ntail(p)\n\n## [1] 0.9945 0.9955 0.9965 0.9975 0.9985 0.9995\n\n\n\nStep 2: Calculate the theoretical quantiles for this set of probabilities based on the distribution you want to compare to (in this case, the normal distribution):\n\n\ntheoretical_q &lt;- qnorm(p)\n# generates quantiles for each p based on a normal distribution (in this case,\n# the standard normal)\n\n\nStep 3: Calculate the quantiles for your set of observed data for the same number of points:\n\n\nobserved_q &lt;- quantile(v, ppoints(v))  # finds quantiles in our actual data\n\n\nStep 4: Plot these quantiles against one another:\n\n\nplot(theoretical_q, observed_q, main = \"QQ Plot - Random Normal Variable\", xlab = \"Theoretical Quantiles\",\n    ylab = \"Sample Quantiles\")\n\n\n\n\n\n\n\n\n\n\n\nCHALLENGE\nWhat happens if you simulate fewer observations in your vectors? Try with n = 100.\n\n\nShow Code\nn &lt;- 100\nmu &lt;- 3.5\nsigma &lt;- 4\nv &lt;- rnorm(n, mu, sigma)\nqqnorm(v, main = \"QQ Plot - Random Normal Variable\")\nqqline(v, col = \"gray\")\n\n\n\n\n\n\n\n\n\nShow Code\n# with fewer simulated points, line isn't quite as good a fit\n\n\nWhat happens if you simulate observations from a different distribution?\n\n\nShow Code\nn &lt;- 1000\nv &lt;- rbeta(n, shape1 = 1.3, shape2 = 2)\nqqnorm(v, main = \"QQ Plot - Random Beta Variable\")\nqqline(v, col = \"gray\")\n\n\n\n\n\n\n\n\n\nShow Code\n# with a different distribution, Q-Q plot isn't linear!\n\n\n\nThe “Standard Normal” Distribution\nAny normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) can be converted into what is called the standard normal distribution, where the mean is 0 and the standard deviation is 1. This is done by subtracting the mean from all observations and dividing these differences by the standard deviation. The resultant values are referred to a Z scores, and they reflect the number of standard deviations an observation is from the mean.\n\nmu &lt;- 5\nsigma &lt;- 8\n# simulate from a normal distribution with mean 5 and SD 8\nx &lt;- rnorm(10000, mean = mu, sd = sigma)\nx_plot &lt;- histogram(x, center = mu, main = paste0(\"Mean = \", round(mean(x), 3), \"\\nSD = \",\n    round(sd(x), 3)))\n# standardize the scores\nz &lt;- (x - mean(x))/sd(x)\nz_plot &lt;- histogram(z, center = 0, main = paste0(\"Mean = \", round(mean(z), 3), \"\\nSD = \",\n    round(sd(z), 3)))\nplot_grid(x_plot, z_plot)",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Probability and Distributions</span>"
    ]
  },
  {
    "objectID": "13-module.html#sampling-distributions-redux",
    "href": "13-module.html#sampling-distributions-redux",
    "title": "13  Probability and Distributions",
    "section": "13.7 Sampling Distributions Redux",
    "text": "13.7 Sampling Distributions Redux\nIt is important to recognize that, above, we were dealing with probability distributions of discrete and continuous random variables as they relate to populations. But, as we have talked about before, we almost never measure entire populations… instead, we measure samples from populations and we characterize our samples using various statistics. The theoretical probability distributions described above (and others) are models for how we connect observed sample data to populations, taking into account various assumptions, and this is what allows us to do many types of inferential statistics. The most fundamental assumption is that the observations we make are independent from one another and are identically distributed, an assumption often abbreviated as iid. Obvious cases of violation of this assumption are rife in the scientific literature, and we should always be cautious about making this assumption!\nHowever, the important thing for us to know is that we can get unbiased estimates of population level parameters based on sample statistics.\n\n\nCHALLENGE\nLet’s imagine a population of 1 million zombies whose age at zombification is characterized by a normal distribution with a mean of 25 years and a standard deviation of 5 years.\nCreate a variable x that describes this population and calculate the mean and standard deviation of the age at zombification.\n\n\nShow Code\nset.seed(1)\nx &lt;- rnorm(1e+06, 25, 5)\nhistogram(x, type = \"density\")\n\n\n\n\n\n\n\n\n\nShow Code\n(mu &lt;- mean(x))  # population mean\n\n\nShow Output\n## [1] 25.00023\n\n\n\nShow Code\n(sigma &lt;- sqrt(sum((x - mean(x))^2)/length(x)))  # population SD\n\n\nShow Output\n## [1] 5.000924\n\n\n\n\nNOTE: We do not use the sd() function as this would divide by length(x)-1. Check that out using sd(x)\n\n\nsd(x)\n\n## [1] 5.000926\n\n\nNow, suppose we now sample from the zombie population by trapping sets of zombies and determining the mean age at zombification in each set. We sample without replacement from the original population for each set…\n\n\nCHALLENGE\nCreate a loop to sample the zombie population 5000 times with samples of size 10 and store these results in a list, s.\n\n\nShow Code\nk &lt;- 5000  # number of samples\nn &lt;- 10  # size of each sample\ns &lt;- list()  # create a dummy variable to hold each sample\n# dummy variable needs to be a list, not a vector, since each element will hold\n# a vector of numbers\nfor (i in 1:k) {\n    s[[i]] &lt;- sample(x, size = n, replace = FALSE)\n}\nhead(s, 1)  # an example of one sample of 10 ages\n\n\nShow Output\n## [[1]]\n##  [1] 32.48970 25.31725 25.52090 33.68150 22.90267 14.67928 26.26720 23.67120\n##  [9] 17.89460 31.55967\n\n\n\nFor each of these samples, we can then calculate a mean age, which is a statistic describing each sample. That statistic itself is a random variable with a mean and distribution! As we saw in Module 12, this is known as a sampling distribution.\n\n\nCHALLENGE\nCreate a loop to calculate the mean of each of your 5000 samples and store the result in a vector, m.\n\n\nShow Code\nm &lt;- vector(length = k)  # create a dummy variable to hold the mean of each sample\n# here the dummy variable can be a vector and we can preallocate its length\nfor (i in 1:k) {\n    m[[i]] &lt;- mean(s[[i]])\n}\nhead(m)\n\n\nShow Output\n## [1] 25.39840 22.48537 26.99638 26.14543 24.84191 25.72639\n\n\n\nSo, how does the sampling distribution of mean ages compare to the population distribution of ages? The mean of the two is pretty close to the same! The sampling distribution mean, which is an average of the set of sample averages, is an unbiased estimator for the population mean.\n\nmean(m)  # this is the mean of our set of sample means\n\n## [1] 25.0161\n\n# i.e., the mean of the sampling distribution is almost equal to...\nmu  # the true population mean\n\n## [1] 25.00023\n\n\nAgain, what we have just calculated is the mean of the sampling distribution, which is simply the average of the means of each sample we have made. This value should be really close to the population mean, and the sampling distribution - the distribution of sample means - should be about normally distributed around the true population mean.\n\n(p &lt;- ggplot(data = as.data.frame(m), aes(x = m)) + geom_histogram(binwidth = function(x) (max(m) -\n    min(m))/20, alpha = 0.75) + labs(title = paste0(\"Sampling Distribution\"), subtitle = paste0(\"Means of \",\n    k, \" samples of size \", n)) + xlab(\"Sample Mean\") + ylab(\"Frequency\") + geom_vline(xintercept = mu,\n    color = \"blue\", linetype = 2, size = 1) + annotate(geom = \"text\", x = mu, y = k *\n    0.06, label = \"True Population Mean\\n\", color = \"blue\", angle = 90, size = 6) +\n    geom_vline(xintercept = mean(m), color = \"red\", linetype = 2, size = 1) + annotate(geom = \"text\",\n    x = mean(m) + 1, y = k * 0.06, label = \"Mean of Sample Means\\n\", color = \"red\",\n    angle = 90, size = 6))\n\n## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n## ℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nNow, let’s take a closer look at our samples of zombies that we extracted from the population of zombies…\n\n\nCHALLENGE\nCalculate a vector of standard error estimates based on the 5000 samples of zombies you have drawn from your population and then plot a histogram of the distribution of these standard errors. That is, for each sample, estimate the standard error of the sample (\\(\\frac{s}{\\sqrt{n}}\\)). How does the mean of this set of standard errors compare to the standard error estimated from the population variance? How does it compare to the standard error calculated from the standard deviation of the sampling distribution?\n\n\nShow Code\nsample_sd &lt;- vector(length = k)  # create a dummy variable to hold the SD of each sample\nfor (i in 1:k) {\n    sample_sd[[i]] &lt;- sd(s[[i]])  # a vector of SDs for each sample\n}\nsample_se &lt;- sample_sd/sqrt(n)  # a vector of SEs estimated from each sample\npop_se &lt;- sigma/(sqrt(n))  # a single value estimated from the population SD\nsampling_dist_se &lt;- sd(m)  # a single value calculated from our sampling distibution SD\n\n(p &lt;- ggplot(data = as.data.frame(sample_se), aes(x = sample_se)) + geom_histogram(binwidth = function(x) (max(sample_se) -\n    min(sample_se))/20, alpha = 0.75) + labs(title = paste0(\"Distribution of SEs estimated from\\n\",\n    k, \" samples of size \", n)) + xlab(\"Estimated SE\") + ylab(\"Frequency\") + geom_vline(xintercept = mean(sample_se),\n    color = \"red\", linetype = 2, size = 1) + annotate(geom = \"text\", x = mean(sample_se),\n    y = k * 0.075, label = \"Mean Estimated SE\\n\", color = \"red\", angle = 90, size = 4) +\n    geom_vline(xintercept = pop_se, color = \"blue\", linetype = 2, size = 1) + annotate(geom = \"text\",\n    x = pop_se, y = k * 0.075, label = \"\\n\\nSE calculated from known population\\nstandard deviation\",\n    color = \"blue\", angle = 90, size = 4) + geom_vline(xintercept = sampling_dist_se,\n    color = \"green\", linetype = 2, size = 1) + annotate(geom = \"text\", x = sampling_dist_se,\n    y = k * 0.075, label = \"\\nSE estimated as SD of sampling distribution\", color = \"green\",\n    angle = 90, size = 4))\n\n\n\n\n\n\n\n\n\nShow Code\nmean(sample_se)  # which is almost equal to...\n\n\nShow Output\n## [1] 1.532473\n\n\n\nShow Code\npop_se  # which is almost equal to...\n\n\nShow Output\n## [1] 1.581431\n\n\n\nShow Code\nsampling_dist_se\n\n\nShow Output\n## [1] 1.604008\n\n\n\nThus, the standard error calculated from an individual sample can be used as an estimator for the standard deviation of the sampling distribution. This is useful, since it means that, if our sample is large enough, we do not have to repeatedly sample from the population to get an estimate of the sampling distribution - we can instead estimate it directly using our data!\nNote that as our sample size increases, the standard error calculated from the population variance should decrease, as should the standard deviation in the sampling distribution, i.e., in estimates of the population mean drawn from successive samples. This should be apparent intuitively… as each sample drawn from a population gets larger, the estimate of the mean value of those samples should vary less and less and thus have lower “error”.\nDespite their similarities, the standard error of the mean estimated for a given sample and the standard deviation of that sample tell us different things. The standard error is an estimate of how far a given sample mean is likely to be from the population mean - it is a measure of uncertainty. The standard deviation of a sample is a measure of the degree to which individual values within a sample differ from the mean for that sample.\n\n# | include: false\ndetach(package:cowplot)\ndetach(package:manipulate)\ndetach(package:mosaic)\ndetach(package:tidyverse)",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Probability and Distributions</span>"
    ]
  },
  {
    "objectID": "13-module.html#concept-review",
    "href": "13-module.html#concept-review",
    "title": "13  Probability and Distributions",
    "section": "Concept Review",
    "text": "Concept Review\n\nUseful R functions for distributions: r (random value generation), d (point density), p (cumulative probability distribution), q (quantile)\nUseful distributions:\n\nPMFs for discrete random variables - Bernoulli, Binomial, Poisson\nPDFs for continuous random variables - Beta, Uniform, Normal and Standard Normal\n\nStandard error of the mean for a sample of size \\(n\\):\n\n(population standard deviation, or \\(\\sigma\\)) \\(\\div\\) \\(\\sqrt{n}\\)\n≈ standard deviation of the sampling distribution for a large set of samples of size \\(n\\)\n≈ (standard deviation of a single sample of size \\(n\\), or \\(s\\)) \\(\\div\\) \\(\\sqrt{n}\\)",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Probability and Distributions</span>"
    ]
  },
  {
    "objectID": "14-module.html",
    "href": "14-module.html",
    "title": "14  Confidence Intervals",
    "section": "",
    "text": "14.1 Objectives",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "14-module.html#objectives",
    "href": "14-module.html#objectives",
    "title": "14  Confidence Intervals",
    "section": "",
    "text": "The objective of this module is to provide an introduction to statistical inference and hypothesis testing by introducing the idea of confidence intervals around statistics.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "14-module.html#preliminaries",
    "href": "14-module.html#preliminaries",
    "title": "14  Confidence Intervals",
    "section": "14.2 Preliminaries",
    "text": "14.2 Preliminaries\n\nInstall and load this package in R: {boot}\nLoad {tidyverse}, {manipulate}, and {mosaic}",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "14-module.html#confidence-intervals",
    "href": "14-module.html#confidence-intervals",
    "title": "14  Confidence Intervals",
    "section": "14.3 Confidence Intervals",
    "text": "14.3 Confidence Intervals\nThe standard error for a statistic that we calculate given its sampling distribution or that we estimate based on a particular sample can be used to derive another measure of uncertainty in the statistic value - the confidence interval, or CI. The CI is another way of describing a statistic’s sampling distribution, and it plays a central role in basic inferential statistics.\nConceptually, the confidence interval is an interval around our estimate of mean of the sampling distribution for a particular statistic (typically a mean), and it gives us a range of values into which subsequent estimates of a statistic would be expected to fall some critical proportion of the time, if the sampling exercise were to be repeated. We can talk thus about different confidence intervals (e.g., 50%, 95%, 99%), where, intuitively, higher confidence is associated with a wider interval. The “95% CI” around a statistic, for example, describes the range of values into which a new estimate of the statistic, derived from a subsequent sample, would be expected to fall 95% of the time. The “99% CI” around the initial statistic would be slightly wider, while the “50% CI” would be narrower.\n\nCalculating Theoretical CIs\nTheoretical confidence intervals typically represent the range of values corresponding to the central X% of a given sampling distribution with a mean of \\(m\\) and a standard deviation of \\(se\\). We can calculate theoretical CIs in several ways.\nThe typical way to calculate a confidence interval is as the value of the statistic being considered \\(±\\) some critical value \\(\\times\\) the standard error of the statistic, where the critical value is determined with respect to the quantiles bracketing the central proportion of interest for a particular zero-centered, standardized theoretical distribution (e.g., the standard normal or the Student’s \\(t\\)). The lower and upper limits of this CI thus should bracket the central proportion of interest of the sampling distribution of the statistic.\nAlternatively, we can calculate the lower and upper limits directly from the appropriate quantile values of the relevant non-standardized version of the particular theoretical distribution (e.g., the normal, with mean \\(m\\) and standard deviation \\(se\\)).\nFor large sample sizes (\\(n\\) ≥ ~30), the theoretical distribution used to define the critical values and the relevant quantiles is typically the normal distribution, while for smaller sample sizes (\\(n\\) &lt; ~30), a Student’s t distribution is used. This is because, according to the Central Limit Theorem (see below), the sampling distribution for many summary statistics (such as the mean) tends to be normal when sample size is reasonably large.\nAs an example CI calculation, we will generate a vector, v, of 10,000 random numbers selected from a normal distribution with a mean of 25 and standard deviation of 10. We then will estimate the mean, standard deviation, and standard error based on a sample of 40 observations drawn from that vector and then use these to calculate the theoretical 95% CI.\nFirst, we generate our vector of random numbers and calculate the relevant summary statistics…\n\nn &lt;- 10000\nmu &lt;- 25\nsigma &lt;- 10\nset.seed(1)\nv &lt;- rnorm(n, mu, sigma)\ns &lt;- sample(v, size = 40, replace = FALSE)\n(m &lt;- mean(s))\n\n## [1] 24.50788\n\n(se &lt;- sigma/sqrt(length(s)))  # if population standard deviation is known\n\n## [1] 1.581139\n\n(sd &lt;- sd(s))\n\n## [1] 10.35182\n\n(se &lt;- sd(s)/sqrt(length(s)))  # if population standard deviation is unknown\n\n## [1] 1.636766\n\n# alternatively, we could use `sciplot::se(s)`\n(se &lt;- sciplot::se(s))\n\n## [1] 1.636766\n\n\nNow, to calculate the 95% CI around our estimate of the mean, we need to calculate the critical values that delimit the central 95% of the standard normal distribution and then use these to define the upper and lower bounds of the 95% CI around our sample mean. The central 95% of a distribution is that falling between the 2.5% and 97.5% quantiles (i.e., all but 2.5% of the distribution in the lower and upper tails). If we define \\(\\alpha\\) as \\(1-CI/100\\), then what we want to find is the \\(\\alpha/2\\) and \\(1 - (\\alpha/2)\\) quantiles, i.e., the \\(0.025\\) and \\(0.975\\) quantiles. [As we will see below, mathematically, this is equivalent to ± the \\(0.975\\) quantile.]\nWe use the “quantile” function for the standard normal function (qnorm() with mean=0 and sd=1, which are the default argument values) to generate these critical values.\nRecall that when we give the qnorm() function a particular percentile value as an argument, it returns a number, \\(X\\), below which that proportion of the cumulative probability distribution falls, and when we are considering a standard normal distribution, \\(X\\) is in units of standard deviations. Thus, qnorm(0.025, mean=0, sd=1) tells us the number of standard deviations (roughly -1.96) away from a mean (of 0) that corresponds to up to 2.5% of the cumulative probability distribution for a standard normal curve. Similarly, qnorm(0.975, mean=0, sd=1) tells us the number of standard deviations (roughly +1.96) away from the mean up to which 97.5% of the standard normal distribution falls. 95% of the standard normal distribution falls between these two values. Thus, to calculate the CI for our sampling distribution, we can do the following:\n\npercent_ci &lt;- 95\nalpha &lt;- 1 - percent_ci/100  # alpha = 0.05\nlower &lt;- m + qnorm(alpha/2) * se\n# where qnorm(alpha /2) is the 2.5% quantile of the standard normal\n# distribution\nupper &lt;- m + qnorm(1 - alpha/2) * se\n# where qnorm(1 - alpha / 2) is the 97.5% quantile of the standard normal\n# distribution\n(ci &lt;- c(lower, upper))\n\n## [1] 21.29988 27.71589\n\n\nAn easier way to do this is simply as follows:\n\n(ci &lt;- m + c(-1, 1) * qnorm(1 - alpha/2) * se)\n\n## [1] 21.29988 27.71589\n\n\nbecause these ways of calculating the lower CI bound are equivalent:\n\n(lower &lt;- m + 1 * qnorm(alpha/2) * se)\n\n## [1] 21.29988\n\n(lower &lt;- m + -1 * qnorm(1 - alpha/2) * se)\n\n## [1] 21.29988\n\n\nWe could also do this…\n\n# get the value associated with the alpha / 2 lower tail\n(lower &lt;- m + qnorm(alpha/2) * se)\n\n## [1] 21.29988\n\n# get the value associated with the alpha / 2 upper tail\n(upper &lt;- m + qnorm(alpha/2, lower.tail = FALSE) * se)\n\n## [1] 27.71589\n\n\nAnd we can also make our own generic CI() function based on assuming that our the sampling distribution for our estimate of the mean is normal:\n\nCI &lt;- function(x, level = 0.95) {\n    alpha &lt;- 1 - level\n    ci &lt;- mean(x) + c(-1, 1) * qnorm(1 - (alpha/2)) * sqrt(var(x)/length(x))\n    return(ci)\n}\n\nCI(s)\n\n## [1] 21.29988 27.71589\n\n\nFinally, we could also characterize the central CI% of the sampling distribution directly by calling the qnorm() function on a non-standardized normal distribution. This is less common, though, because it is less easily applied to distributions other than the normal family (e.g., to the \\(t\\) distribution).\n\n(lower &lt;- qnorm(alpha/2, m, se))\n\n## [1] 21.29988\n\n(upper &lt;- qnorm(1 - alpha/2, m, se))\n\n## [1] 27.71589\n\n\nThe following code will let us interactively visualize CIs around an estimate of a mean with a normal sampling distribution.\n\nmanipulate(ggplot(data = data.frame(x = c(sampling_dist_mean - 4 * sampling_dist_sd,\n    sampling_dist_mean + 4 * sampling_dist_sd)), aes(x)) + stat_function(fun = dnorm,\n    args = list(mean = sampling_dist_mean, sd = sampling_dist_sd), n = 1000) + xlab(\"Sampling Distribution Mean\") +\n    ylab(\"\") + labs(title = \"Exploring Confidence Intervals\", subtitle = paste0(\"Sampling Distribution SD (= SE): \",\n    sampling_dist_sd, \"\\n\", round(percent_CI, 2), \"% CI: \", round(sampling_dist_mean -\n        qnorm((1 - percent_CI/100)/2) * sampling_dist_sd, 2), \" to \", round(sampling_dist_mean +\n        qnorm((1 - percent_CI/100)/2) * sampling_dist_sd, 2))) + geom_vline(xintercept = sampling_dist_mean -\n    qnorm((1 - percent_CI/100)/2) * sampling_dist_sd, color = \"blue\", linetype = \"dashed\") +\n    geom_vline(xintercept = sampling_dist_mean + qnorm((1 - percent_CI/100)/2) *\n        sampling_dist_sd, color = \"blue\", linetype = \"dashed\") + geom_vline(xintercept = sampling_dist_mean,\n    color = \"black\", linetype = \"solid\") + stat_function(fun = dnorm, xlim = c(sampling_dist_mean -\n    qnorm((1 - percent_CI/100)/2) * sampling_dist_sd, sampling_dist_mean + qnorm((1 -\n    percent_CI/100)/2) * sampling_dist_sd), args = list(mean = sampling_dist_mean,\n    sd = sampling_dist_sd), n = 1000, geom = \"area\", fill = \"red\", alpha = 0.5, color = \"red\"),\n    sampling_dist_mean = slider(-100, 100, initial = 0, step = 10), sampling_dist_sd = slider(0,\n        100, initial = 1, step = 1), percent_CI = slider(0, 99, initial = 95, step = 1))\n\n\n\nInterpretation of CIs\nThere are two ways that CIs are generally interpreted:\n\nBased on the given data (with a particular mean, variance, and sample size), we are \\(X\\)% confident that the true mean of the population lies between the lower and upper bounds.\nThe mean of a repeated sample of the same size drawn from this same underlying distribution is expected to fall into the given interval \\(X\\)% of the time.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "14-module.html#the-central-limit-theorem",
    "href": "14-module.html#the-central-limit-theorem",
    "title": "14  Confidence Intervals",
    "section": "14.4 The Central Limit Theorem",
    "text": "14.4 The Central Limit Theorem\nOur construction of CIs thus far has implicitly taken advantage of one of the most important theorems in statistics, the Central Limit Theorem (CLT). The key importance of the CLT for us is that it states that the sampling distribution of averages (or sums or other summary statistics…) of “independent and identically distributed” (or \\(iid\\)) random variables approaches a normal distribution as the sample size increases. It is this fact that allows us to have a good sense of the mean and distribution of average events in a population even though we only observe one or a small number of samples of those events and even though we do not know what the actual population distribution is! In fact, the CLT says nothing about the probability distribution for events in the original population, and that is exactly where its usefulness lies… that original probability distribution can be normal, skewed, exponential, or even all kinds of odd!\n\nTL/DR: The CLT means can we typically assume normality for the sampling distribution, i.e., for the distribution of the sample mean (or of the sample sum or the sample mode, etc…) no matter what kind of probability distribution characterizes the initial population, as long as our sample size is large enough and our samples are independent and identically distributed. It is thus the CLT that allows us to make inferences about a population based on a sample.\n\nJust to explore this a bit, let’s do some simulations. We are going to take lots of averages of samples from a particular non-normal distribution and then look at the distribution of those averages.\nImagine we have some event that occurs in a population according to some probability mass function like the Poisson where we know \\(\\lambda=14\\). Recall that the expectations of \\(\\mu\\) and \\(\\sigma^2\\) for the Poisson distribution are both equal to \\(\\lambda\\).\nNow let’s imagine taking a bunch of random samples of size 10 from this population. We will take 1000 random samples of this size, calculate the average of each sample, and plot a histogram of those averages… it will be close to normal, and the standard deviation of those averages - i.e., the SD of the sampling distribution - should be roughly equal to the estimated standard error, i.e., the square root of (expected variance / sample size). Recall that for a Poisson distribution that \\(\\lambda\\) is the expected variance, so the estimated standard error is simply the \\(\\sqrt{\\lambda/n}\\)\n\nlambda &lt;- 14\nn &lt;- 10\n(se &lt;- sqrt(lambda/n))\n\n## [1] 1.183216\n\n# the estimated SE, here, based on an estimate of the population variance if\n# available\npar(mfrow = c(1, 3))\nhist(rpois(10000, lambda = lambda), probability = TRUE, xlab = \"X\", main = \"Original Distribution\")\nx &lt;- vector(length = 1000)  # dummy variable to hold sample means\nfor (i in 1:1000) {\n    x[[i]] &lt;- mean(rpois(n = n, lambda = lambda))\n}\nhist(x, breaks = seq(from = min(x), to = max(x), length.out = 20), probability = TRUE,\n    xlab = \"Mean x\", main = \"Sampling Distribution\")\n(se &lt;- sd(x))\n\n## [1] 1.160762\n\n# the estimated SE, here based on the standard deviation of the actual sampling\n# distribution\nqqnorm(x)\nqqline(x)\n\n\n\n\n\n\n\n\nLet’s now do the same for samples of size 1000. We see that the mean of the sampling distribution stays essentially the same, the distribution is still normal, but the standard deviation - the spread - of the sampling distribution is lower.\n\nlambda &lt;- 14\nn &lt;- 1000\n(se &lt;- sqrt(lambda/n))\n\n## [1] 0.1183216\n\n# the estimated SE, here, based on an estimate of the population variance if\n# available\npar(mfrow = c(1, 3))\nhist(rpois(10000, lambda = lambda), probability = TRUE, xlab = \"x\", main = \"Original Distribution\")\nx &lt;- vector(length = 1000)\nfor (i in 1:1000) {\n    x[[i]] &lt;- mean(rpois(n = n, lambda = lambda))\n}\nhist(x, breaks = seq(from = min(x), to = max(x), length.out = 20), probability = TRUE,\n    xlab = \"Mean X\", main = \"Sampling Distribution\")\n(se &lt;- sd(x))\n\n## [1] 0.1179025\n\n# the estimated SE, here based on the standard deviation of the actual sampling\n# distribution\nqqnorm(x)\nqqline(x)\n\n\n\n\n\n\n\n\nNote that the sampling distribution gets narrower as the sample size increases.\nWe can convert the sampling distribution to a standard normal by subtracting off the mean of the distribution and dividing by the standard deviation and then plotting a histogram of those values along with a normal curve.\n\npar(mfrow = c(1, 2))\nhist(x, breaks = seq(from = floor(min(x)), to = ceiling(max(x)), length.out = 20),\n    probability = TRUE)\nz &lt;- (x - mean(x))/sd(x)  # converts the vector of means to z scores\nhist(z, breaks = seq(from = min(z), to = max(z), length.out = 20), probability = TRUE,\n    ylim = c(0, 0.5))  # plots the histogram of z scores\ncurve(dnorm(x, 0, 1), -4, 4, ylim = c(0, 0.5), xlab = \"Z\", ylab = \"Density\", add = TRUE)  # plots a standard normal curve\n\n\n\n\n\n\n\n\nPretty normal looking, right?\nHere’s an example of the CLT in action using the sum() of our random variables instead of the mean() as the statistic of interest …\n\nlambda &lt;- 14\nn &lt;- 1000\npar(mfrow = c(1, 3))\nhist(rpois(10000, lambda = lambda), probability = TRUE, xlab = \"X\", main = \"Original Distribution\")\nx &lt;- vector(length = 1000)\nfor (i in 1:1000) {\n    x[[i]] &lt;- sum(rpois(n = n, lambda = lambda))\n}\nhist(x, breaks = seq(from = min(x), to = max(x), length.out = 20), probability = TRUE,\n    xlab = \"Sum X\", main = \"Sampling Distribution\")\n# convert the sampling distribution to standard normal\nz &lt;- (x - mean(x))/sd(x)\nhist(z, breaks = seq(from = -4, to = 4, length.out = 20), probability = TRUE)\n# plots a standard normal curve\ncurve(dnorm(x, 0, 1), -4, 4, ylim = c(0, 0.5), xlab = \"Z\", ylab = \"Density\", add = TRUE)\n\n\n\n\n\n\n\n\nAgain, pretty normal looking!\nTrying this for an intial beta rather than Poisson distribution…\n\nalpha &lt;- 1.1\nbeta &lt;- 2.9\nn &lt;- 1000\npar(mfrow = c(1, 3))\nhist(rbeta(10000, shape1 = alpha, shape2 = beta), probability = TRUE, xlab = \"X\",\n    main = \"Original Distribution\")\nx &lt;- vector(length = 1000)\nfor (i in 1:1000) {\n    x[[i]] &lt;- mean(rbeta(n = n, shape1 = alpha, shape2 = beta))\n}\nhist(x, breaks = seq(min(x), max(x), length.out = 20), probability = TRUE, xlab = \"Mean X\",\n    main = \"Sampling Distribution\")\n# convert the sampling distribution to standard normal\nz &lt;- (x - mean(x))/sd(x)\nhist(z, breaks = seq(from = -4, to = 4, length.out = 20), probability = TRUE)\n# plots a standard normal curve\ncurve(dnorm(x, 0, 1), -4, 4, ylim = c(0, 0.5), xlab = \"Z\", ylab = \"Density\", add = TRUE)\n\n\n\n\n\n\n\n\nOnce again, the sampling distribution is normal, even though the original distribution is far from it!\nTake Home Points:\n[1] The CLT states that, regardless of the underlying probability distribution of a population of independent, identically distributed (\\(iid\\)) random variables, the distribution of a statistic (e.g., means, or sums or standard deviations, etc…) for samples drawn from that underlying distribution:\n\nwill be approximately normal,\nwill be centered at the population mean for the statistic, and - will have a standard deviation roughly equal to the standard error of the mean for the statistic.\n\nAdditionally, it suggests that variables that are expected to be the sum of multiple independent processes (e.g., measurement errors) will also have distributions that are nearly normal.\n[2] Calculating a statistic based on this distribution (e.g., the mean) and adding/subtracting the relevant standard quantile \\(\\times\\) the standard error yields a confidence interval for the relevant statistic, which gets wider as the coverage increases and gets narrower with less variability or larger sample sizes.\n[3] As sample size increases, the standard error of the statistic decreases and the sampling distribution becomes more and more normal (i.e., has less skew and kurtosis, which are higher order moments of the distribution).\nFor a nice interactive simulation demonstrating the Central Limit Theorem, check out this cool website.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "14-module.html#cis-for-sample-proportions",
    "href": "14-module.html#cis-for-sample-proportions",
    "title": "14  Confidence Intervals",
    "section": "14.5 CIs for Sample Proportions",
    "text": "14.5 CIs for Sample Proportions\nSo far, we’ve talked about CIs for sample means of a continuous random variable, but what about for other statistics, e.g., sample proportions for discrete binary variables. For example, if you have a sample of n trials where you record the success or failure of a binary event, you can obtain an estimate of the proportion of successes, \\(x/n\\). If you perform another n trials, the new estimate will vary from sample to sample in the same way that averages of a continuous random variable (e.g., zombie age) will vary.\nTaking a similar approach as we did above for normally distributed continuous random variables, we can generate confidence intervals for the proportion of successes across trials, i.e., for a discrete binary variable, by taking the expectation and adding/subtracting a critical value \\(\\times\\) a standard error.\nRecall from our discussion of discrete random binary variables that the expectation for proportion of successes, which we will denote as \\(\\pi\\) (where \\(\\pi\\), for “proportion”, is analogous to \\(\\mu\\), for “mean”) is simply the average number of successes across multiple trials, and the expectation for the variance in this proportion is \\(\\pi \\times (1-\\pi)\\).\nAs for the mean of a continuous random variable, the expected sampling distribution for the proportion of successes across many trials of sample size \\(n\\) is approximately normal and centered at \\(\\pi\\), and its standard deviation is estimated by \\(\\sqrt{\\pi(1-\\pi)/n}\\), which is, essentially, the standard error of the mean: it is the square root of (the expected variance / sample size). As above for \\(\\mu\\), if we do not already have a population estimate for \\(\\pi\\), we can estimate this from a sample as \\(\\hat{p}=x/n\\)\n\nNOTE: The approximation of normality for the sampling distribution of proportion of successes holds true only if both \\(n\\times\\pi\\) and \\(n\\times(1-\\pi)\\) are greater than roughly 5, so we should always check this when working with proportion data!\n\n\nCHALLENGE\nSuppose a polling group in the United States is interested in the proportion of voting-age citizens in their state that already know they will vote for Bernie Sanders in the upcoming presidential election. The group obtains a “yes”” or “no” answer from 1000 randomly selected individuals. Of these individuals, 856 say they know they’ll vote for Sanders How would we characterize the mean and variability associated with this proportion?\n\nn &lt;- 1000\nx &lt;- 856\np_hat &lt;- x/n  # our estimate of pi\np_hat\n\n## [1] 0.856\n\n\nAre \\(n\\times\\pi\\) and \\(n\\times(1-\\pi)\\) both &gt; 5? Yes!\n\nn * p_hat\n\n## [1] 856\n\nn * (1 - p_hat)\n\n## [1] 144\n\nse &lt;- sqrt((p_hat) * (1 - p_hat)/n)  # estimated SE for proportion data\nse\n\n## [1] 0.01110243\n\n\nSo, what is the 95% CI around our estimate of the proportion of people who already know how they will vote?\n\npar(mfrow = c(1, 1))\ncurve(dnorm(x, mean = p_hat, sd = se), p_hat - 4 * se, p_hat + 4 * se, xlab = \"\",\n    ylab = \"Density\", main = \"95% Confidence Interval around\\nExpected Proportion of Decided Voters \")\nupper &lt;- p_hat + qnorm(0.975) * se\nlower &lt;- p_hat - qnorm(0.975) * se\n(ci &lt;- c(lower, upper))\n\n## [1] 0.8342396 0.8777604\n\n# or\n(ci &lt;- p_hat + c(-1, 1) * qnorm(0.975) * se)\n\n## [1] 0.8342396 0.8777604\n\npolygon(cbind(c(ci[1], seq(from = ci[1], to = ci[2], length.out = 1000), ci[2]),\n    c(0, dnorm(seq(from = ci[1], to = ci[2], length.out = 1000), mean = p_hat, sd = se),\n        0)), border = \"black\", col = \"gray\")\nabline(v = ci)\nabline(v = p_hat)\nabline(h = 0)",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "14-module.html#small-sample-cis",
    "href": "14-module.html#small-sample-cis",
    "title": "14  Confidence Intervals",
    "section": "14.6 Small Sample CIs",
    "text": "14.6 Small Sample CIs\nThus far, we have discussed creating confidence intervals for sample statistics based on the CLT and the normal distribution, and our intervals took the form:\nmean \\(±\\) \\(Z\\) (the critical value, based on a quantile from the standard normal curve) \\(\\times\\) standard error\nBut, when the size of our sample is small (\\(n\\) &lt; 30), instead of using the normal distribution to calculate our CIs, statisticians typically use a different distribution to generate the relevant quantiles to multiply the standard error by… the t distribution (a.k.a., Gosset’s \\(t\\) or Student’s \\(t\\) distribution).\n\nNOTE: This is a typical case that we will encounter, as we often do not have information about the population that a sample is drawn from!\n\nThe \\(t\\) distribution is a continuous probability distribution very similar in shape to the normal, and is generally used when dealing with statistics (such as means and standard deviations) that are estimated from a sample rather than being known parameters about a population. Any particular \\(t\\) distribution looks a lot like a normal distribution in that it is bell-shaped, symmetric, unimodal, and (if standardized) zero-centered.\nThe choice of the appropriate \\(t\\) distribution to use in any particular statistical test is based on the number of degrees of freedom (df), i.e., the number of individual components in the calculation of a given statistic (such as the mean or standard deviation) that are “free to change”.\nWe can thus think of the \\(t\\) distribution as representing a family of curves that, as the number of degrees of freedom increases, approaches the normal curve. At low numbers of degrees of freedom, the tails of the distribution get fatter and the peak of the distribution gets lower.\nConfidence intervals based on the \\(t\\) distribution are of the form:\nmean \\(±\\) \\(T\\) (the critical value, based on a quantile from the \\(t\\) distribution) \\(\\times\\) standard error\nThe only change from CIs based on the normal distribution is that we have replaced the \\(Z\\) quantile of the standard normal with a \\(T\\) quantile.\nLet’s explore this a bit…\nRecall that a standard normal distribution is generated by normalizing a set of data (subtracting the mean from each observation and then dividing all of these differences by the standard deviation of the distribution)…\n\\[(\\bar{x}-\\mu)/\\sigma\\]\nFor a sample, if we subtract the population mean from each observation and then divide each difference, instead, by the standard error of the mean, i.e., \\((\\bar{x}-\\mu)/SE\\), the result is not a normal distribution, but rather a \\(t\\) distribution! We are taking into account sample size by dividing by the standard error of the mean rather than by the population standard deviation.\nThe code below plots a standard normal distribution in red and then superimposes several \\(t\\) distributions with varying degrees of freedom, specified using the df= argument.\n\nmu &lt;- 0\nsigma &lt;- 1\ncurve(dnorm(x, mu, 1), mu - 4 * sigma, mu + 4 * sigma, main = \"Normal Distribution (red) and\\nStudent's t Distributions (blue)\",\n    xlab = \"x\", ylab = \"f(x)\", col = \"red\", lwd = 3)\nfor (i in c(1, 2, 3, 4, 5, 10, 20, 100)) {\n    curve(dt(x, df = i), mu - 4 * sigma, mu + 4 * sigma, main = \"T Curve\", xlab = \"x\",\n        ylab = \"f(x)\", add = TRUE, col = \"blue\", lty = 5)\n}\n\n\n\n\n\n\n\n\nAs for other distributions, R implements d (density), p (cumulative probability), q (quantile), and r (random draw) functions for the t distribution.\nThe fatter tails of the \\(t\\) distibution at small sample sizes lead naturally to more extreme quantile values given a specific probability than we would see for the normal distribution. If we define a CI based on quantiles of the \\(t\\) distribution, then, they will be correspondingly slightly wider than those based on the normal distribution for small values of \\(df\\).\nWe can see this in an example, where we draw samples from a population of values defined by a normal distribution with a particular mean and standard deviation. Recall that above we estimated the 95% CI for such a sample drawn from a normal distribution as follows:\n\nn &lt;- 1e+05\nmu &lt;- 3.5\nsigma &lt;- 4\nalpha &lt;- 0.05\nx &lt;- rnorm(n, mu, sigma)  # x is our large population\nhist(x)\n\n\n\n\n\n\n\nsample_size &lt;- 30\ns &lt;- sample(x, size = sample_size, replace = FALSE)  # s is a sample from that population\n(m &lt;- mean(s))\n\n## [1] 2.921573\n\n(sd &lt;- sd(s))\n\n## [1] 4.198638\n\n(se &lt;- sd(s)/sqrt(length(s)))\n\n## [1] 0.7665629\n\nci_normal &lt;- m + c(-1, 1) * qnorm(1 - 0.05/2) * se\n# (1-alpha)/2 each in the upper and lower tails of the distribution\nci_normal\n\n## [1] 1.419138 4.424009\n\n\nNow, let’s look at the CIs calculated based using the \\(t\\) distribution for the same sample size. For sample size 30, the difference in the CIs is negligible.\n\nci_t &lt;- m + c(-1, 1) * qt(1 - 0.05/2, df = sample_size - 1) * se\n# (1-alpha)/2 each in the upper and lower tails of the distribution\nci_t\n\n## [1] 1.353776 4.489371\n\n\nHowever, if we use a sample size of 5, the equivalent CI based on the \\(t\\) distribution is much wider.\n\nsample_size &lt;- 5\nsmall_s &lt;- sample(x, size = sample_size, replace = FALSE)\n(m &lt;- mean(small_s))\n\n## [1] 5.307108\n\n(sd &lt;- sd(small_s))\n\n## [1] 2.599329\n\n(se &lt;- sd(small_s)/sqrt(length(small_s)))\n\n## [1] 1.162455\n\nci_normal &lt;- m + c(-1, 1) * qnorm(1 - 0.05/2) * se\n# (1-alpha)/2 each in the upper and lower tails of the distribution\nci_normal\n\n## [1] 3.028738 7.585479\n\nci_t &lt;- m + c(-1, 1) * qt(1 - 0.05/2, df = sample_size - 1) * se\n# (1-alpha)/2 each in the upper and lower tails of the distribution\nci_t\n\n## [1] 2.079615 8.534602",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "14-module.html#calculating-cis-by-bootstrapping",
    "href": "14-module.html#calculating-cis-by-bootstrapping",
    "title": "14  Confidence Intervals",
    "section": "14.7 Calculating CIs by Bootstrapping",
    "text": "14.7 Calculating CIs by Bootstrapping\nAn alternative (and arguably better) way to calculate a confidence interval for a given statistic is by “bootstrapping” from the data in a single sample using a Monte Carlo simulation process. Bootstrapping allows us to approximate a sampling distribution even without access to the population from which samples are drawn and without making any assumptions about the theoretical shape of the sampling distribution. The distribution we generate is sometimes referred to as a bootstrap distribution.\n\nWriting our Own Bootstrap\nBelow, we use the sample() function to sample, with replacement, bootstrap replicates of equivalent sample size from our original vector of sample data, s, a total of 10000 times. This is just one approach to how we might perform bootstrapping, using {base} R functions.\n\nNOTE: If we wanted to sample rows from a data frame instead of items from a vector, we could use the {dplyr} functions sample_n() or slice_sample().\n\n\nn_boot &lt;- 10000\nboot &lt;- vector(length = n_boot)  # set up a dummy variable to hold our simulations\nn &lt;- length(s)\n# the size of each bootstrap sample should equivalent to the size our original\n# sample\nfor (i in 1:n_boot) {\n    boot[[i]] &lt;- mean(sample(s, n, replace = TRUE))\n}\n\nThe following code visualizes the bootstrap sampling distribution we have just generated and also compares the various methods we have explored for calculating CIs.\n\nhist(boot, breaks = 25, ylim = c(0, 1600), xlab = \"Mean\", main = \"Bootstrap Sampling Distribution\")\nabline(v = mean(boot), col = \"blue\", lwd = 3)  # mean of our simulated samples\ntext(x = mean(boot) + 0.2, y = 700, \"mean of bootstrap distribution \", col = \"blue\",\n    srt = 90, pos = 3)\nabline(v = mean(s), col = \"black\", lwd = 3, lty = 2)\n# mean of our original vector of samples, s\ntext(x = mean(s) - 0.125, y = 700, \"mean of original sample\", col = \"black\", srt = 90,\n    pos = 3)\n\nabline(v = quantile(boot, 0.025), col = \"blue\")\n# lower ci bound inferred by bootstrapping\nabline(v = quantile(boot, 0.975), col = \"blue\")\n# upper ci bound inferred by bootstrapping\ntext(x = quantile(boot, 0.025) + 0.25, y = 700, \"lower CI - bootstrap\", col = \"blue\",\n    srt = 90, pos = 3)\ntext(x = quantile(boot, 0.975) - 0.125, y = 700, \"upper CI - bootstrap\", col = \"blue\",\n    srt = 90, pos = 3)\n\nci_normal &lt;- mean(s) + c(-1, 1) * qnorm(1 - 0.05/2) * sciplot::se(s)\nci_t &lt;- mean(s) + c(-1, 1) * qt(1 - 0.05/2, df = length(s) - 1) * sciplot::se(s)\n\nabline(v = ci_normal[1], col = \"red\")\n# calculated lower ci bound based on the se of our vector and assuming that our\n# observations are drawn from a normal distribution\nabline(v = ci_normal[2], col = \"red\")\n# calculated upper ci bound based on the se of our vector and assuming that our\n# observations are drawn from a normal distribution\ntext(x = ci_normal[1] - 0.12, y = 700, \"lower CI - normal\", col = \"red\", srt = 90,\n    pos = 3)\ntext(x = ci_normal[2] + 0.245, y = 700, \"upper CI - normal\", col = \"red\", srt = 90,\n    pos = 3)\n\nabline(v = ci_t[1], col = \"green\")\n# calculated lower ci bound based on the se of our vector and assuming that our\n# observations are drawn from a t distribution\nabline(v = ci_t[2], col = \"green\")\n# calculated upper ci bound based on the se of our vector and assuming that our\n# observations are drawn from a t distribution\ntext(x = ci_t[1] - 0.15, y = 700, \"lower CI - t\", col = \"green\", srt = 90, pos = 3)\ntext(x = ci_t[2] + 0.275, y = 700, \"upper CI - t\", col = \"green\", srt = 90, pos = 3)\n\n\n\n\n\n\n\n\nIn this example, we used the quantile() function to return the observations satisfying the \\(n^{th}\\) quantile of our particular distribution of bootstrapped sample means. We use this function to define the lower and upper bounds for the bootstrap confidence interval around the mean of our variable, x.\nNote that this compares very favorably to the CIs estimated by assuming that the sampling distribution follows either a normal or \\(t\\) distribution\n\n\nOther Bootstrapping Options\nAs is typically the case with R, we have many choices for perform we might perform boostrapping! All of the options below are alternatives to the process we used above and generate similar bootstrap sampling distributions and corresponding CIs.\n\nUsing the {mosaic} Package\nWe are already familiar with the do(reps) * formulation from the {mosaic} package…\n\nboot &lt;- do(n_boot) * mean(~sample(s, length(s), replace = TRUE))\nhist(boot$mean, breaks = 25, ylim = c(0, 1600), xlab = \"Mean\", main = \"Bootstrap Sampling Distribution\\nfrom do(reps) *\")\n\n\n\n\n\n\n\n\n\n\nUsing the {infer} Package\nAlternatively, can use the rep_sample_n() function from the {infer} package to perform boostrapping. This function takes a data frame as an argument and randomly samples from that data frame a specified number of times (defined by the “size=” argument) to produce a large data object consisting of samples for each replicate and a column data that indicates which replicate each sample datum comes from. To use rep_sample_n(), we need to first convert our original sample vector s into a data frame. We can then use group_by() and summarize() from {dplyr} to generate a bootstrap sampling distribution, as we did above.\n\nlibrary(infer)\nboot &lt;- as.data.frame(s) |&gt;\n    rep_sample_n(size = length(s), replace = TRUE, reps = n_boot) |&gt;\n    group_by(replicate) |&gt;\n    summarize(mean = mean(s))\nhist(boot$mean, breaks = 25, ylim = c(0, 1500), xlab = \"Mean\", main = \"Bootstrap Sampling Distribution\\nfrom rep_sample_n()\")\n\n\n\n\n\n\n\n\n\n\nUsing the {boot} Package\nWe can also use the fast and versatile {boot} package to do bootstrapping for any statistic.\nFor this approach, we define a custom function that we will use to generate the statistic we are interested in that we will then calculate for each bootstrapped sample:\n\nlibrary(boot)\n# here, the `stat()` function we define calculates the mean value of a\n# bootstrap sample taken with replacement from a vector of interest passed into\n# `data` the function we write for `statistic=` has two arguments, a data set\n# and a set of indices, generated at random by the `boot()` function, to sample\n# from the dataset this corresponds to the default argument `stype='i'` in the\n# `boot()` call\nstat &lt;- function(data, indices) {\n    return(mean(data[indices]))\n}\n\nThen, we run the boot() function passing in the data to be resampled from (“data=”), the statistic we want to calculate (“statistic=”), and the number of bootstrap replicates (or “resamples”) we want using the argument “R=”.\n\nboot &lt;- boot(data = s, statistic = stat, R = n_boot)  # stype='i' by default\n# the object returned includes a table, `t`, of `stat`s results from each\n# bootstrap\n\nAgain, it is interesting to plot the bootstrap sampling distibution and compare methods for calculating CIs.\n\n# Visualizing the results and comparing methods for calculating CIs\nhist(boot$t, breaks = 25, ylim = c(0, 1500), xlab = \"Mean\", main = \"Bootstrap Sampling Distribution\\nfrom boot()\")\n\nabline(v = mean(boot$t), col = \"blue\", lwd = 3)  # mean of our simulated samples\ntext(x = mean(boot$t) + 0.2, y = 700, \"mean of bootstrap distribution \", col = \"blue\",\n    srt = 90, pos = 3)\nabline(v = mean(s), col = \"black\", lwd = 3, lty = 2)\n# mean of our original vector of samples, s\ntext(x = mean(s) - 0.125, y = 700, \"mean of original sample\", col = \"black\", srt = 90,\n    pos = 3)\n\n# calculate and plot bootstrap CIs\nlower_boot &lt;- quantile(boot$t, 0.025)\nabline(v = lower_boot, col = \"blue\")\nupper_boot &lt;- quantile(boot$t, 0.975)\nabline(v = upper_boot, col = \"blue\")\ntext(x = lower_boot + 0.25, y = 700, \"lower CI - bootstrap\", col = \"blue\", srt = 90,\n    pos = 3)\ntext(x = upper_boot - 0.125, y = 700, \"upper CI - bootstrap\", col = \"blue\", srt = 90,\n    pos = 3)\n\n\n\n\n\n\n\n\nWe can also use the boot.ci() function from the {boot} package to calculate CIs for using various methods, although the available methods do not include exactly the one we most want, i.e., the quantile method. Instead, all the options for boot.ci() generate equi-tailed CIs based on presumed theoretical shapes for the sampling distribution. The “basic” and “percent” intervals are the closest to the empirical quantile method.\n\nhist(boot$t, breaks = 25, ylim = c(0, 1500), xlab = \"Mean\", main = \"Bootstrap Sampling Distribution\\nfrom boot()\")\nci &lt;- boot.ci(boot)\nabline(v = ci$basic[4], col = \"green\", lwd = 2)\nabline(v = ci$basic[5], col = \"green\", lwd = 2)\nabline(v = ci$percent[4], col = \"red\", lwd = 2)\nabline(v = ci$percent[5], col = \"red\", lwd = 2)\nabline(v = ci$bca[4], col = \"purple\", lwd = 2)\nabline(v = ci$bca[5], col = \"purple\", lwd = 2)\nabline(v = ci$normal[2], col = \"black\", lwd = 2)\nabline(v = ci$normal[3], col = \"black\", lwd = 2)\n\n\n\n\n\n\n\n\nThese alternative CIs are all very close to one another and are close to what we calculated using the quantile() approach!\n\n\n\nCHALLENGE\n\nHow does the CI calculated by bootstrap simulation compare to that calculated based on assuming a normal or \\(t\\) distribution?\nHow does the width of the CI change with decreasing or increasing \\(n\\) (the number of observations drawn from your sample with replacement)? For example, if we set \\(n\\) at 5? At 50? At 500?",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "14-module.html#concept-review",
    "href": "14-module.html#concept-review",
    "title": "14  Confidence Intervals",
    "section": "Concept Review",
    "text": "Concept Review\n\nWe can generate confidence intervals around a statistic (e.g., a mean) estimated from a sample as:\n\nmean \\(±\\) critical value (i.e., the relevant quantile for the standardized version of a particular theoretical sampling distribution such as the normal or Student’s \\(t\\)) \\(\\times\\) standard error in the statistic, which is typically estimated as the standard deviation of a sample divided by the square root of the sample size, or \\(\\frac{s}{\\sqrt{n}}\\)\n\nBootstrapping allows us to estimate confidence intervals by simulation from a sample, without assuming a particular shape for the sampling distribution",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "15-module.html",
    "href": "15-module.html",
    "title": "15  Classical Hypothesis Testing",
    "section": "",
    "text": "15.1 Objectives",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Classical Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "15-module.html#objectives",
    "href": "15-module.html#objectives",
    "title": "15  Classical Hypothesis Testing",
    "section": "",
    "text": "The objective of this module is to introduce basic hypothesis testing from a classical or “frequentist” statistics approach.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Classical Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "15-module.html#preliminaries",
    "href": "15-module.html#preliminaries",
    "title": "15  Classical Hypothesis Testing",
    "section": "15.2 Preliminaries",
    "text": "15.2 Preliminaries\n\nLoad {tidyverse} and {mosaic}",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Classical Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "15-module.html#null-and-alternative-hypotheses",
    "href": "15-module.html#null-and-alternative-hypotheses",
    "title": "15  Classical Hypothesis Testing",
    "section": "15.3 Null and Alternative Hypotheses",
    "text": "15.3 Null and Alternative Hypotheses\n\n\n\n\n\n\n\n\n\n\n\nClassical hypothesis testing typically involves formally stating a claim - the null hypothesis - which is then followed up by statistical evaluation of the null versus an alternative hypothesis. The null hypothesis is interpreted as a baseline hypothesis and is the claim that is presumed to be true. That claim is typically that a particular value of a population parameter estimated by a sample statistic we have calculated is consistent with a particular null expectation. The alternative hypothesis is the conjecture that we are testing, usually that the sample statistic is inconsistent with a null expectation.\nTypically, our null and alternative hypotheses are expressed something like as follows:\n\n\\(H_0\\) = null hypothesis = a sample statistic shows no deviation from what is expected or neutral based on the parameter space of possible outcomes under the presumed random sampling process.\n\n\nNOTE: This parameter space is defined both by how we sample and how we intend to sample, including our stopping rules (e.g., number of observations, amount of time we plan to sample, and so on).\n\n\n\\(H_A\\) = alternative hypothesis = a sample statistic deviates more than expected by chance from what is expected or neutral.\n\nWe can test several different comparisons between \\(H_0\\) and \\(H_A\\).\n\n\\(H_A &gt; H_0\\), which constitutes an “upper one-tailed test” (i.e., our sample statistic is greater than that expected under the null)\n\\(H_A &lt; H_0\\), which constitutes a “lower one-tailed test” (i.e., our sample statistic is less than that expected under the null)\n\\(H_A ≠ H_0\\), which constitutes a “two-tailed test” (i.e., our sample statistic is different, maybe greater maybe less, than that expected under the null)\n\nTo formally do any statistical test under such a “null hypothesis significance testing” (or NHST) framework, we need some kind of statistical evidence - typically based on probabilities and confidence intervals - to reject the null hypothesis in favor of an alternative hypothesis. This evidence involves considering some measure of how unexpected it would be for a sample we have collected to have been drawn, by chance, from a particular null distribution.\nTo effect a hypothesis test, we then need to…\n\nCalculate a test statistic based on our data.\nCalculate the p value associated with that test statistic, which is the probability of obtaining, by chance, a test statistic that is as high or higher than our calculated one, assuming the null hypothesis is true. Classically, this is done by comparing the value to some appropriate standardized sampling distribution with well-known mathematical properties (e.g., normal or \\(t\\)) to yield the p value.\nEvaluate whether the p value is less than or greater than the significance level, or \\(\\alpha\\), that we set for our test. That is, \\(\\alpha\\) can be thought of as the cutoff level for p values below which we feel comfortable rejecting a null hypothesis (i.e., we reject \\(H_0\\) when \\(p &lt; \\alpha\\)).\n\nUnder the null hypothesis significance testing framework, there are then four possible outcomes to our statistical decision, each with an associated probability:\n\n\n\n\n\n\n\n\nWhat is True\nWhat We Decide\nResult\n\n\n\n\n\\(H_0\\)\n\\(H_0\\)\nCorrectly “accept” the null (\\(1 - \\alpha\\))\n\n\n\\(H_0\\)\n\\(H_A\\)\nFalsely reject the null (Type I error) \\(\\alpha\\)\n\n\n\\(H_A\\)\n\\(H_0\\)\nFalsely “accept” the null (Type II error) \\(\\beta\\)\n\n\n\\(H_A\\)\n\\(H_A\\)\nCorrectly reject the null (\\(1 - \\beta\\)), or “power”\n\n\n\nWe typically approach hypothesis testing by trying to minimize our probability of committing a Type I error (\\(\\alpha\\))… i.e., we aim for having a high bar for falsely rejecting the null hypothesis (e.g., for incorrectly finding an innocent person guilty) and thus set \\(\\alpha\\) very low (0.05 or 0.01) to reflect what we decide is an acceptable level of Type I error.\nWhen we set a high bar for falsely rejecting the null, we necessarily lower the bar for incorrectly accepting (i.e., for failing to reject) the null (e.g., for concluding that a guilty person is innocent). The probability of incorrectly “accepting” the null is referred to as Type II error (\\(\\beta\\)). The “power” of a statistical test is (\\(1 - \\beta\\)), the probability with which we correctly reject the null.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Classical Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "15-module.html#parametric-hypothesis-testing",
    "href": "15-module.html#parametric-hypothesis-testing",
    "title": "15  Classical Hypothesis Testing",
    "section": "15.4 Parametric Hypothesis Testing",
    "text": "15.4 Parametric Hypothesis Testing\nIn traditional parametric statistics, we make the assumption that the sampling distribution of our statistic of interest (e.g., a mean) takes the form of a particular well-understood mathematical distribution (e.g., the normal), and we calculate a test statistic that basically summarizes the “location” of a summary statistic about our data relative to that implied, theoretical sampling distribution. The particular value of our test statistic is determined by both the difference between the original sample statistic and the expected null value (e.g., the difference between the mean of our sample and the expected population mean) and the standard error of the sample statistic. The value of our test statistic (i.e., the “distance” of that test statistic from what is expected) and the shape of the presumed sampling distribution for that statistic are the sole drivers of the smallness of the p value. The p value effectively represents the area under the sampling distribution associated with test statistic values as or more extreme than the one we observed.\nHow do we calculate the p value for a parametric test?\n\nSpecify the sample statistic we want to evaluate (e.g., the mean).\nSpecify the test statistic of interest and the form of the sampling distribution for that statistic (e.g., \\(Z\\) or \\(T\\) and a standard normal or T distribution).\nCalculate the tail probability, i.e., the probability of obtaining a statistic (e.g., a mean) as or more extreme than was observed assuming that null distribution.\n\n\nWorking with Means\n\nOne Sample \\(Z\\) and \\(T\\) Test\nLet’s do an example where we try to evaluate whether the mean of a single set of observations is significantly different than that expected under a null hypothesis… i.e., this is a ONE-SAMPLE test.\nSuppose we have a vector describing the adult weights of vervet monkeys trapped in South Africa during the 2015 trapping season. We have the sense they are heavier than vervets we trapped in previous years, which averaged 5.0 kilograms. We calculate the mean of our sample of trapped vervets and then need to decide whether that mean is significantly greater than our expectation based on prior information.\n\n\n\n\n\n\n\n\n\nFirst, we read in our data:\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/vervet-weights.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\nhead(d)\n\n## # A tibble: 6 × 2\n##      id weight\n##   &lt;dbl&gt;  &lt;dbl&gt;\n## 1     1   5.17\n## 2     2   7.13\n## 3     3   4.7 \n## 4     4   6.1 \n## 5     5   6.36\n## 6     6   4.93\n\nmean(d$weight)\n\n## [1] 5.323922\n\n\n\nWhat is our \\(H_0\\)?\nWhat is our \\(H_A\\)?\nWhat is the hypothesis we want to test? Is it two-tailed? Upper-tailed? Lower-tailed?\nCalculate the mean, standard deviation, and SE of the sample\n\n\n\nShow Code\nmu &lt;- 5\nx &lt;- d$weight  # current weights\nn &lt;- length(x)\n(m &lt;- mean(x))\n\n\n## [1] 5.323922\n\n\nShow Code\n(s &lt;- sd(x))\n\n\n## [1] 0.9754016\n\n\nShow Code\n(se &lt;- s/sqrt(n))\n\n\n## [1] 0.1365835\n\n\n\nPlot a histogram of the sample\n\n\n\nShow Code\nhistogram(x, breaks = seq(from = m - 4 * s, to = m + 4 * s, length.out = 20), main = \"Histogram of Vervet\\nWeights\",\n    xlab = \"X\", ylab = \"Proportion of Total\", type = \"density\", ylim = c(0, 3), col = rgb(0,\n        0, 1, 0.5))\nladd(panel.abline(v = mu, col = \"black\", lty = 1, lwd = 2))  # expected mean\nladd(panel.abline(v = m, col = \"black\", lty = 3, lwd = 2))  # observed mean\n\n\n\n\n\n\n\n\n\n\nUsing the plotDist() function from {mosaic}, plot the location of your sample mean, the sampling distribution suggested by your sample’s standard error, and the location of the expected mean from the 2015 data.\n\n\n\nShow Code\nplotDist(\"norm\", mean = m, sd = se, xlim = c(m - 4 * se, m + 4 * se), add = TRUE,\n    lwd = 1, col = \"black\", lty = 1)\n\n\n\n\n\n\n\n\n\nThe following code demonstrates that more than 95% of the sampling distribution suggested by your sample is above the expected mean of 5.0…\n\nz &lt;- qnorm(0.05)  # define lower bound of upper 95% of distribution\nladd(panel.polygon(cbind(c(m + z * se, seq(from = m + z * se, to = max(x), length.out = 1000),\n    max(x)), c(0, dnorm(seq(from = m + z * se, to = max(x), length.out = 1000), m,\n    se), 0)), border = \"black\", col = rgb(1, 0, 1, 0.5)))\n\n\n\n\n\n\n\n\nFor a formal statistical test of whether our sample mean is greater than the expected mean, we first calculate the test statistic. It takes a familiar form… it is effectively the position of our sample mean relative to the expected population mean and the expected population standard deviation (which we have estimated from our sample).\n\\[Z = \\frac{\\bar{x}-\\mu}{s/\\sqrt{n}}\\]\nwhere:\n\n\\(\\bar{x}\\) = mean of sample observations\n\\(\\mu\\) = expected mean\n\\(s\\) = sample standard deviation (as an estimate of the population SD)\n\\(n\\) = number of sample observations\n\nOr, to use our variables from above…\n\nz &lt;- (m - mu)/se\nz\n\n## [1] 2.3716\n\n\nIn this case, our test statistic, \\(Z\\), is a quantile… the estimated number of standard errors away from the expected population mean that our sample mean falls based on the implied sampling distribution. If our sample mean is greater than expected, then the test statistic is positive; if our sample mean is less than expected, then the test statistic is negative.\nTo evaluate whether \\(Z\\) is “significant” under a NHST framework, we need to calculate the total probability of our seeing a deviation from the expected mean as great or greater than this by chance. For a two-tailed test, this deviation can be in either the positive or negative direction, while for a one-tailed test, we are only interested in the probability associated with our specific alternative hypothesis (i.e., \\(\\mu_{H_A} \\gt \\mu_{H_0}\\) or \\(\\mu_{H_A} \\lt \\mu_{H_0}\\)).\nTo calculate these probabilities, we can use the pnorm() function. Because in calculating a \\(Z\\) score we have converted our sample mean to the standard normal scale, the mean= and sd= arguments of pnorm() are the defaults of 0 and 1, respectively.\nFor this specific example, where we want to test the idea that our sample mean is greater than expected, we want the probability of seeing a \\(Z\\) as large or larger by chance. This corresponds to the area under the normal curve to the right of \\(Z\\) (the “upper.tail”), which we can calculate as:\n\np &lt;- 1 - pnorm(z)\np\n\n## [1] 0.008855621\n\n# or\np &lt;- pnorm(z, lower.tail = FALSE)\np\n\n## [1] 0.008855621\n\n\nTo visualize this on a standard normal curve…\n\nplotDist(\"norm\", main = paste0(\"Standard Normal Distribution\\nblue area = \", round(p,\n    4) * 100, \"%\"), ylab = \"\", xlab = \"SD\")\nladd(panel.abline(v = z, col = \"blue\", lty = 1, lwd = 2))\nladd(panel.polygon(cbind(c(z, seq(from = z, to = 4, length.out = 100), 4), c(0, dnorm(seq(from = z,\n    to = 4, length.out = 100), 0, 1), 0)), border = \"black\", col = rgb(0, 0, 1, 0.5)))\n\n\n\n\n\n\n\n\nWe can also consider the “significance” in a different way, by asking does our value for \\(Z\\) fall in the upper 5% of the normal distribution for \\(Z\\) scores. For this, we can use qnorm() to return the critical value demarking the boundary of the upper 5% of the distribution and see of \\(Z\\) falls to the right of that, which indeed it does.\n\nplotDist(\"norm\", main = paste0(\"Standard Normal Distribution\\nred area = upper 5%\"),\n    ylab = \"\", xlab = \"SD\")\nladd(panel.abline(v = z, col = \"blue\", lty = 1, lwd = 2))\ncritical_val &lt;- qnorm(0.95)\nladd(panel.abline(v = critical_val, col = \"red\", lty = 2, lwd = 2))\nladd(panel.polygon(cbind(c(critical_val, seq(from = critical_val, to = 4, length.out = 100),\n    4), c(0, dnorm(seq(from = critical_val, to = 4, length.out = 100), 0, 1), 0)),\n    border = \"black\", col = rgb(1, 0, 0, 0.5)))\nladd(panel.polygon(cbind(c(z, seq(from = z, to = 4, length.out = 100), 4), c(0, dnorm(seq(from = z,\n    to = 4, length.out = 100), 0, 1), 0)), border = \"black\", col = rgb(0, 0, 1, 0.5)))\n\n\n\n\n\n\n\n\nAnother way to think about whether there is a “significant” difference between our observed and expected mean weight is to look at the 95% CI around our estimate of mean weight. We can calculate this by hand for the one-tailed test…\n\nalpha &lt;- 0.05\nci &lt;- m - qnorm(1 - alpha, mean = 0, sd = 1) * se  # by hand... lower limit for 95% of sampling distribution\nhistogram(x, breaks = seq(from = m - 4 * s, to = m + 4 * s, length.out = 20), main = \"Histogram of Vervet\\nWeights\",\n    xlab = \"X\", ylab = \"Proportion of Total\", type = \"density\", ylim = c(0, 3), col = rgb(0,\n        0, 1, 0.5))\nladd(panel.abline(v = mu, col = \"black\", lty = 1, lwd = 2))  # expected mean\nladd(panel.abline(v = m, col = \"black\", lty = 3, lwd = 2))  # observed mean\nladd(panel.abline(v = ci, col = \"red\", lty = 3, lwd = 2))  # lower bound for 95% CI\nz &lt;- qnorm(0.05)  # define lower bound of upper 95% of distribution\nladd(panel.polygon(cbind(c(m + z * se, seq(from = m + z * se, to = max(x), length.out = 1000),\n    max(x)), c(0, dnorm(seq(from = m + z * se, to = max(x), length.out = 1000), m,\n    se), 0)), border = \"black\", col = rgb(1, 0, 1, 0.5)))\n\n\n\n\n\n\n\n\n… and for a two-tailed CI…\n\nci &lt;- m + c(-1, 1) * qnorm(1 - alpha/2, mean = 0, sd = 1) * se  # by hand...\nladd(panel.abline(v = ci, col = \"red\", lty = 3, lwd = 2))  # upper and lower bounds for 95% CI\n\n\n\n\n\n\n\n\nAs noted above, our sample size from a population is typically limited. So, instead of using the normal distribution (as we did here) to determine the p value of our statistic, we actually should use the \\(t\\) distribution, which, as we have seen, has slightly fatter tails. The statistic and process is exactly the same, though, as for the normal distribution.\n\\[T = \\frac{\\bar{x}-\\mu}{s/\\sqrt{n}}\\]\nor, equivalently…\n\\[T = \\frac{\\bar{x}-\\mu}{\\sqrt{s^2/n}}\\]\nwith\n\\[df = n - 1\\]\n\nz &lt;- (m - mu)/se\n(p &lt;- 1 - pt(z, df = n - 1))\n\n## [1] 0.01080157\n\n(p &lt;- pt(z, df = n - 1, lower.tail = FALSE))\n\n## [1] 0.01080157\n\n(critical_val &lt;- qt(0.95, df = n - 1))\n\n## [1] 1.675905\n\n\n\nplotDist(\"t\", df = n - 1, main = paste0(\"t Distribution with DF = \", n - 1, \"\\nblue area = \",\n    round(p, 4) * 100, \"%\"), ylab = \"\", xlab = \"SD\")\nladd(panel.abline(v = z, col = \"blue\", lty = 1, lwd = 2))\nladd(panel.polygon(cbind(c(z, seq(from = z, to = 4, length.out = 100), 4), c(0, dt(seq(from = z,\n    to = 4, length.out = 100), df = n - 1), 0)), border = \"black\", col = rgb(0, 0,\n    1, 0.5)))\n\n\n\n\n\n\n\n\nR has built into it a single function, t.test(), that lets us do all this in one line. We give it our data and the expected population mean, \\(\\mu\\), along with the kind of test we want to do.\n\nt_stat &lt;- t.test(x = x, mu = mu, alternative = \"greater\")\nt_stat  # the value of the t statistic is a Z score\n\n## \n##  One Sample t-test\n## \n## data:  x\n## t = 2.3716, df = 50, p-value = 0.0108\n## alternative hypothesis: true mean is greater than 5\n## 95 percent confidence interval:\n##  5.095021      Inf\n## sample estimates:\n## mean of x \n##  5.323922\n\n\nNote that the t.test() function to also calculates \\(t\\)- distribution based CIs for us easily. For the above test, the 95% CI is open-ended on the upper end because we are doing a one-tailed test. Doing the same thing by hand…\n\nalpha &lt;- 0.05\nci &lt;- c(m - qt(1 - alpha, df = n - 1) * se, Inf)  # lower bound calculated by hand... note we do not divide alpha by 2\nci\n\n## [1] 5.095021      Inf\n\n\nFor an equivalent two-tailed test with the same data…\n\nt_stat &lt;- t.test(x = x, mu = mu, alternative = \"two.sided\")\nci &lt;- t_stat$conf.int  # using t test\nci\n\n## [1] 5.049585 5.598258\n## attr(,\"conf.level\")\n## [1] 0.95\n\nci &lt;- m + c(-1, 1) * qt(1 - alpha/2, df = n - 1) * se  # upper and lower bounds calculated by hand\nci\n\n## [1] 5.049585 5.598258\n\n\n\n\n\nCHALLENGE\nAdult lowland woolly monkeys are reported to have an average body weight of 7.2 kilograms. You are working with an isolated population of woolly monkeys from the Colombian Andes that you think may be a different species from lowland form, and you collect a sample of 15 weights from adult individuals at that site. From your sample, you calculate a mean of 6.43 kilograms and a standard deviation of 0.89 kilograms. Perform a hypothesis test to evaluate whether body weights in your population are different from the reported average for lowland woolly monkeys by setting up a “two-tailed” hypothesis, carrying out the analysis, and interpreting the p value (assume the significance level is \\(\\alpha\\) = 0.05).\n\n\n\n\n\n\n\n\n\n\nHINT: Your sample size is &lt; 30, so you should use the \\(t\\) distribution and do a t test. Do your calculations both by hand and using the t.test() function and confirm that they match.\n\nFirst, read in the data:\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/woolly-weights.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\nhead(d)\n\n## # A tibble: 6 × 2\n##      id weight\n##   &lt;dbl&gt;  &lt;dbl&gt;\n## 1     1   6.14\n## 2     2   6.19\n## 3     3   7.08\n## 4     4   5.67\n## 5     5   4.83\n## 6     6   6.83\n\n\nThen, calculate the mean, standard deviation, and SE of your sample:\n\n\nShow Code\nx &lt;- d$weight\nn &lt;- length(x)\n(m &lt;- mean(x))\n\n\nShow Output\n## [1] 6.427333\n\n\n\nShow Code\n(s &lt;- sd(x))\n\n\nShow Output\n## [1] 0.8968235\n\n\n\nShow Code\n(se &lt;- s/sqrt(n))\n\n\nShow Output\n## [1] 0.2315588\n\n\n\nFinally, calculate the \\(T\\) statistic and then determine the p value associated with that statistic.\n\nNOTE: Because this is a two-tailed test, we need to calculate the probability of seeing a \\(T\\) statistic as far from the mean in either the positive or negative direction. This is why we sum the probabilities associated with the upper and lower tails…\n\n\n\nShow Code\nmu &lt;- 7.2\nt_stat &lt;- (m - mu)/se\nt_stat\n\n\nShow Output\n## [1] -3.336805\n\n\n\nShow Code\np_upper &lt;- 1 - pt(abs(t_stat), df = n - 1)\n# or 1 - pt(t_stat, df=n-1, lower.tail = FALSE)\np_lower &lt;- pt(-1 * abs(t_stat), df = n - 1)\n# or pt(t_stat, df=n-1, lower.tail = TRUE)\np &lt;- p_upper + p_lower\np\n\n\nShow Output\n## [1] 0.004890693\n\n\n\nTo visualize this…\n\nplotDist(\"t\", df = n - 1, main = paste0(\"t Distribution with DF = \", n - 1, \"\\nred area = 2.5% in each tail\",\n    \"\\nblue = \", round(p, 4) * 100, \"%\"), ylab = \"\", xlab = \"SD\", xlim = c(-4, 4))\nladd(panel.abline(v = abs(t_stat), col = \"blue\", lty = 1, lwd = 2))\nladd(panel.abline(v = -1 * abs(t_stat), col = \"blue\", lty = 1, lwd = 2))\n\n# plot upper tail\nladd(panel.polygon(cbind(c(abs(t_stat), seq(from = abs(t_stat), to = 4, length.out = 100),\n    4), c(0, dt(seq(from = abs(t_stat), to = 4, length.out = 100), df = n - 1), 0)),\n    border = \"black\", col = rgb(0, 0, 1, 0.5)))\n\n# plot lower tail\nladd(panel.polygon(cbind(c(-4, seq(from = -4, to = -1 * abs(t_stat), length.out = 100),\n    -1 * abs(t_stat)), c(0, dt(seq(from = -4, to = -1 * abs(t_stat), length.out = 100),\n    df = n - 1), 0)), border = \"black\", col = rgb(0, 0, 1, 0.5)))\n\nalpha &lt;- 0.05\ncritical_val &lt;- qt(1 - alpha/2, df = n - 1)  # identify critical values - boundaries for 95% of the t distribution\nladd(panel.abline(v = abs(critical_val), col = \"red\", lty = 2, lwd = 2))\nladd(panel.abline(v = -1 * abs(critical_val), col = \"red\", lty = 2, lwd = 2))\n\nladd(panel.polygon(cbind(c(critical_val, seq(from = critical_val, to = 4, length.out = 100),\n    4), c(0, dt(seq(from = critical_val, to = 4, length.out = 100), df = n - 1),\n    0)), border = \"black\", col = rgb(1, 0, 0, 0.5)))\n\nladd(panel.polygon(cbind(c(-4, seq(from = -4, to = -1 * abs(critical_val), length.out = 100),\n    -1 * abs(critical_val)), c(0, dt(seq(from = -4, to = -1 * abs(critical_val),\n    length.out = 100), df = n - 1), 0)), border = \"black\", col = rgb(1, 0, 0, 0.5)))\n\n\n\n\n\n\n\n\nWe can implement a simple test to see if the value of our \\(T\\) statistic is farther away from zero than the critical value…\n\ntest &lt;- (abs(t_stat) &gt; critical_val)\n# boolean test as to whether t is larger than the critical value at either tail\ntest\n\n## [1] TRUE\n\n\nAnother way to think about whether there is a significant difference between our observed and expected mean weight is to look at the 95% CI around our estimate of mean weight. We can calculate this by hand or use the output of the t test…\n\n(ci &lt;- m + c(-1, 1) * critical_val * se)  # by hand\n\n## [1] 5.930689 6.923978\n\nt.test(x = x, mu = mu, alternative = \"two.sided\")  # `t.test()` function\n\n## \n##  One Sample t-test\n## \n## data:  x\n## t = -3.3368, df = 14, p-value = 0.004891\n## alternative hypothesis: true mean is not equal to 7.2\n## 95 percent confidence interval:\n##  5.930689 6.923978\n## sample estimates:\n## mean of x \n##  6.427333\n\n\nThe 95% CI does not include the expected mean weight.\n\nTwo Sample \\(Z\\) and \\(T\\) Test\nSometimes we want to compare two groups of measurements to one another, which boils down to a hypothesis test for the difference between two means, \\(\\mu1\\) and \\(\\mu2\\). The null hypothesis is that the difference between these means is zero.\nBefore getting to the appropriate test, there are a couple of things that we need to consider:\n[1] How, if at all, are the two samples related to one another? Sometimes we may have PAIRED samples (e.g., the same individuals before and after some treatment) and sometimes the samples are UNPAIRED or INDEPENDENT (e.g., weights for different samples of black-and-white colobus monkeys collected in the rainy versus dry seasons).\n[2] Are the variances in the two samples roughly equal or not? E.g., if we are comparing male and female heights, are the variances comparable?\n\nSamples with Unequal Variances\nFor the most generic case, where the two samples are independent and we cannot assume the variances of the two samples are equal, we can do what is called Welch’s t test where our test statistic is:\n\\[T = \\frac{\\bar{x_2} - \\bar{x_1} - \\mu}{\\sqrt{s_1^2/n_1 + s_2^2/n_2}}\\]\nwhere:\n\n\\(\\bar{x_1}\\) and \\(\\bar{x_2}\\) = means of observations in each sample group\n\\(\\mu\\) = expected difference in means between sample groups under the null hypothesis, which is usually zero\n\\(s_1\\) and \\(s_2\\) = standard deviations of each sample group\n\\(n_1\\) and \\(n_2\\) = numbers of observations in each sample group\n\n\n\n\n\nCHALLENGE\nLet’s do an example. Load in a file of black-and-white colobus weights and examine the str() of the file.\n\n\n\n\n\n\n\n\n\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/colobus-weights.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\nhead(d)\n\n## # A tibble: 6 × 3\n##      id weight sex  \n##   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;\n## 1     1   7.24 male \n## 2     2   6.09 male \n## 3     3   6.97 male \n## 4     4   6.98 male \n## 5     5   6.08 male \n## 6     6   6.22 male\n\n\nThen, create two vectors, x and y, for male and female weights. Plot these in boxplots side by side and then calculate the mean, sd, and sample size for both males and females. Do the variances look similar for the two sexes?\n\n\nShow Code\nx &lt;- d$weight[d$sex == \"male\"]\ny &lt;- d$weight[d$sex == \"female\"]\npar(mfrow = c(1, 2))\nminval &lt;- min(c(x, y)) - 0.1\nmaxval &lt;- max(c(x, y)) + 0.1\nboxplot(x, ylim = c(minval, maxval), main = \"Weight (kg)\", xlab = \"Males\")\n# `ylim=` argument uses x and y ranges to set range for y axis\nboxplot(y, ylim = c(minval, maxval), main = \"Weight (kg)\", xlab = \"Females\")\n\n\n\n\n\n\n\n\n\nShow Code\nm1 &lt;- mean(x)\nm2 &lt;- mean(y)\nmu &lt;- 0  # you could leave this out... the default argument value is 0\ns1 &lt;- sd(x)\ns2 &lt;- sd(y)\nn1 &lt;- length(x)\nn2 &lt;- length(y)\n\n\nNow calculate the \\(T\\) statistic and test the two-tailed hypothesis that the sample means differ. Note that for the Welch’s t test, the number of degrees of freedom is calculated as:\n\\[df = \\frac{(s_1^2/n_1 + s_2^2/n_2)^2}{(s_1^2/n_1)^2/(n_1-1)+(s_2^2/n_2)^2/(n_2-1)}\\]\n\ndf &lt;- (s2^2/n2 + s1^2/n1)^2/((s2^2/n2)^2/(n2 - 1) + (s1^2/n1)^2/(n1 - 1))\ndf\n\n## [1] 31.21733\n\n\n\n\nShow Code\nalpha &lt;- 0.05\nt_stat &lt;- (m2 - m1 - mu)/sqrt(s2^2/n2 + s1^2/n1)\nt_stat\n\n\nShow Output\n## [1] -11.45952\n\n\n\nShow Code\n# note that because our hypothesis is 2-tailed, it does not matter which group\n# (males or females) is m1 and which is m2, so we take the absolute values of t\n# below when testing whether it is greater than the critical value\ncritical_val &lt;- qt(1 - alpha/2, df = df)\n# identify the critical value, i.e., how far apart the means of the two samples\n# need to be to be more extreme than expected by chance at the given alpha\n# level\ncritical_val\n\n\nShow Output\n## [1] 2.038938\n\n\n\nShow Code\ntest &lt;- abs(t_stat) &gt; critical_val  # boolean test\ntest  # if true, the two means are significantly different\n\n\nShow Output\n## [1] TRUE\n\n\n\nWe can visualize this as below by plotting histograms of body weights for males and females and adding the sampling distribution curves and 95% CIs around the estimates of the means for the two sexes atop of the histogram.\n\npar(mfrow = c(1, 1))\nhist(x, breaks = seq(from = minval, to = maxval, length.out = 15), xlim = c(minval,\n    maxval), ylim = c(0, 9), main = \"Histogram and Sampling Distributions\\nfor Body Weights\\n(red = females, blue = males)\",\n    col = rgb(0, 0, 1, 0.5))\n# `xlim=` argument uses x and y ranges to set range for y axis\nhist(y, breaks = seq(from = minval, to = maxval, length.out = 15), xlim = c(minval,\n    maxval), col = rgb(1, 0, 0, 0.5), add = TRUE)\ncurve(dnorm(x, m1, s1/sqrt(n1)), n = 1000, add = TRUE)\ncurve(dnorm(x, m2, s2/sqrt(n2)), n = 1000, add = TRUE)\n\nabline(v = m1 - qt(1 - alpha/2, df = df) * s1/sqrt(n1))\nabline(v = m1 + qt(1 - alpha/2, df = df) * s1/sqrt(n1))\nabline(v = m2 - qt(1 - alpha/2, df = df) * s2/sqrt(n2))\nabline(v = m2 + qt(1 - alpha/2, df = df) * s2/sqrt(n2))\n\n\n\n\n\n\n\n\n\nNOTE: The 95% CIs around each sample mean do not overlap, so the means are significantly different.\n\nWe can do the same thing using the t.test() function.\n\n(abs(m1 - m2))  # difference between means\n\n## [1] 1.449\n\nt_stat &lt;- t.test(x = x, y = y, mu = 0, alternative = \"two.sided\", var.equal = FALSE)\n# var.equal = FALSE is the DEFAULT for `t.test()`\nt_stat\n\n## \n##  Welch Two Sample t-test\n## \n## data:  x and y\n## t = 11.46, df = 31.217, p-value = 1.023e-12\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  1.191186 1.706814\n## sample estimates:\n## mean of x mean of y \n##     6.689     5.240\n\n\n\nNOTE: The 95% CI returned by the t test is the interval around the DIFFERENCE between the means.\n\n\nSamples with Equal Variances\nThere’s a simpler \\(T\\) statistic we can use if the variances of the two samples are more or less equal.\n\\[T=\\frac{\\bar{x_2}-\\bar{x_1}-\\mu}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\]\nwhere:\n\n\\(s_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2}\\)\n\\(\\bar{x_1}\\) and \\(\\bar{x_2}\\) = means of observations in each sample group\n\\(\\mu\\) = expected difference in means between sample groups (usually set to zero)\n\\(s_p\\) = pooled sample standard deviation\n\\(n_1\\) and \\(n_2\\) = numbers of observations in each sample group\n\nThe degrees of freedom for this test is calculated as follows:\n\\[df=n_1+n_2-2\\]\nCalculating the \\(t\\) statistic and degrees of freedom by hand…\n\ns &lt;- sqrt((((n1 - 1) * s1^2) + ((n2 - 1) * s2^2))/(n1 + n2 - 2))\nt_stat &lt;- (m2 - m1 - mu)/(sqrt(s^2 * (1/n1 + 1/n2)))\nt_stat\n\n## [1] -11.45952\n\ndf &lt;- n1 + n2 - 2\ndf\n\n## [1] 38\n\n\n… matches what we see when we use t.test().\n\nt_stat &lt;- t.test(x = x, y = y, mu = 0, var.equal = TRUE, alternative = \"two.sided\")\nt_stat\n\n## \n##  Two Sample t-test\n## \n## data:  x and y\n## t = 11.46, df = 38, p-value = 6.787e-14\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  1.193025 1.704975\n## sample estimates:\n## mean of x mean of y \n##     6.689     5.240\n\n\n\nNOTE: A crude test for equality of variances is to divide the larger by the smaller and if the result is &lt; 2, you can go ahead and used the pooled variance version of the test (which has many fewer degrees of freedom).\n\nIn our case, we should not really use the equal variances version of the t test, since the ratio of variances exceeds 2…\n\nvar(x)/var(y)\n\n## [1] 2.746196\n\n\nWe can use the var.test() function to conduct an actual statistical test on the ratio of variances, which compares the ratio test statistic we just calculated to an \\(F\\) distribution. The \\(F\\) distribution is often used to model ratios of random variables and thus is useful in regression applications and, as here, for testing whether variances from two samples are different. It is dependent upon the specification of a pair of degrees of freedom values supplied as the arguments df1= and df2= (or inferred from the number of observations in each sample).\nBelow, the results of var.test() are saved to a variable. Calling the variable provides a brief descriptive summary.\n\nvt &lt;- var.test(x, y)\nvt\n\n## \n##  F test to compare two variances\n## \n## data:  x and y\n## F = 2.7462, num df = 19, denom df = 19, p-value = 0.03319\n## alternative hypothesis: true ratio of variances is not equal to 1\n## 95 percent confidence interval:\n##  1.086978 6.938128\n## sample estimates:\n## ratio of variances \n##           2.746196\n\n\n\n\nPaired Samples\nFor a paired samples test, the null hypothesis is that the mean of individual paired differences between the two samples (e.g., before and after) is zero.\nOur test statistic is:\n\\[T = \\frac{d-\\mu}{\\sqrt{s_d^2/n}}\\]\nwhere:\n\n\\(\\bar{d}\\) = mean of difference between paired samples\n\\(\\mu\\) = expected mean difference between paired samples (usually set to zero)\n\\(s_d\\) = standard deviation in the set of differences between paired samples\n\\(n\\) = number of sample pairs\n\nAgain, note that \\(\\mu\\) here is the expected difference between the means under the null hypothesis, which is zero, and we are dividing by the standard error of the mean for the set of differences between pairs.\n\n\n\nCHALLENGE\nLet’s play with a sample… test scores of individuals taking a certain statistics course pre and post a lecture on null hypothesis significance testing. Load in the test_scores.csv data file, look at it, plot a barchart of values before and after and construct a paired t test to evaluate the means before and after.\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/test_scores.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\nhead(d)\n\n## # A tibble: 6 × 3\n##      id `Score before` `Score after`\n##   &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;\n## 1     1           89.0          95.4\n## 2     2           90.0          94.2\n## 3     3           90.7          92.2\n## 4     4           82.1          92.4\n## 5     5           87.4          94.8\n## 6     6           88.3          90.6\n\nx &lt;- d$`Score after` - d$`Score before`\nm &lt;- mean(x)\nmu &lt;- 0  # can leave this out\ns &lt;- sd(x)\nn &lt;- length(x)\nse &lt;- s/sqrt(n)\npar(mfrow = c(1, 2))\nboxplot(d$`Score before`, ylim = c(80, 100), main = \"Score\", xlab = \"Before\")\nboxplot(d$`Score after`, ylim = c(80, 100), main = \"Score\", xlab = \"After\")\n\n\n\n\n\n\n\nt_stat &lt;- (m - mu)/se\nt_stat\n\n## [1] 1.789636\n\nalpha &lt;- 0.05\ncritical_val &lt;- qt(1 - alpha/2, df = n - 1)  # identify critical values\ncritical_val\n\n## [1] 2.093024\n\ntest &lt;- abs(t_stat) &gt; critical_val  # boolean test\ntest\n\n## [1] FALSE\n\nt.test(d$`Score before`, d$`Score after`, df = n - 1, alternative = \"two.sided\",\n    paired = TRUE)\n\n## \n##  Paired t-test\n## \n## data:  d$`Score before` and d$`Score after`\n## t = -1.7896, df = 19, p-value = 0.08946\n## alternative hypothesis: true mean difference is not equal to 0\n## 95 percent confidence interval:\n##  -4.830292  0.377435\n## sample estimates:\n## mean difference \n##       -2.226429\n\n\n\n\nWorking with Proportions\n\nOne Sample \\(Z\\) Test\nAs we have seen, the theoretical sampling distribution of sample means for independent and identically distributed random continuous variables is roughly normal (and, as shown by the CLT, this distribution increasingly approaches normal as sample size increases). Similarly, the sampling distribution for another kind of sample statistic, the number of “successes” \\(x\\) out of a series of \\(k\\) trials is also roughly normally distributed. If the true population proportion of “successes” is \\(\\pi\\), then the sampling distribution for the proportion of successes in a sample of size \\(n\\) is expected to be roughly normally distributed with mean = \\(\\pi\\) and standard error = \\(\\sqrt{\\pi(1-\\pi)/n}\\).\nLet’s set up a simulation to show this…\nFirst we create a population of 500 “1”s and 500 “0”s, i.e., where \\(\\pi\\) = 0.5…\n\npop &lt;- c(rep(0, 500), rep(1, 500))\n\nNow, we will take 1000 random samples of size \\(n\\)=10 from that population and calculate the proportion of “1”s in each sample…\n\npi &lt;- 0.5\nx &lt;- NULL\nn &lt;- 10\nfor (i in 1:1000) {\n    x[i] &lt;- mean(sample(pop, size = n, replace = FALSE))\n    # taking the mean of a bunch of 0s and 1s yields the proportion of 1s!\n}\nm &lt;- mean(x)\nm\n\n## [1] 0.502\n\ns &lt;- sd(x)\ns\n\n## [1] 0.1590638\n\nse &lt;- sqrt(pi * (1 - pi)/n)\nse  # the SE is an estimate of the SD of the sampling distribution\n\n## [1] 0.1581139\n\n\nThe same is true if we create a population of 800 “1”s and 200 “0”s, i.e., where \\(\\pi\\) = 0.8…\n\npop &lt;- c(rep(0, 800), rep(1, 200))\npi &lt;- 0.8\nx &lt;- NULL\nn &lt;- 10\nfor (i in 1:1000) {\n    x[i] &lt;- mean(sample(pop, size = n, replace = FALSE))\n    # taking the mean of a bunch of 0s and 1s yields the proportion of 1s!\n}\nm &lt;- mean(x)\nm\n\n## [1] 0.1984\n\ns &lt;- sd(x)\ns\n\n## [1] 0.1285845\n\nse &lt;- sqrt(pi * (1 - pi)/n)\nse  # the SE is an estimate of the SD of the sampling distribution\n\n## [1] 0.1264911\n\n\nThis normal approximation is true as long as \\(n\\) is fairly large and \\(\\pi\\) is not close to 0 or 1. One rule of thumb is to check that both \\(n\\times\\pi\\) and \\(n\\times(1-\\pi)\\) are greater than 5.\nWith all this in mind, we can construct \\(Z\\) statistics for proportions just like we constructed \\(Z\\) and \\(T\\) statistics for means and test those proportions for differences from an expected value or for differences between two sample proportions. The \\(Z\\) statistic for proportions takes the same general form as that for means…\n\\(Z =\\) (observed statistic - expected statistic) / expected standard error\nor,\n\\[Z = \\frac{\\hat{p}-\\pi}{\\sqrt{\\pi(1-\\pi)/n}}\\]\nwhere:\n\n\\(\\hat{p}\\) = proportion in sample\n\\(\\pi\\) = expected proportion\n\\(n\\) = number of observations in sample\n\n\n\n\nCHALLENGE\nA neotropical ornithological working in the western Amazon deploys 30 mist nets in a 100 ha grid.\n\n\n\n\n\n\n\n\n\nShe monitors the nets on one morning and records whether or not she captures any birds in the net (i.e., a “success” or “failure” for every net during a netting session). The following vector summarizes her netting results:\n\nv &lt;- c(0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n    0, 1, 0, 1, 1)\n\nHer netting success over the previous three seasons suggests that she should catch birds in 80% of her nets. This season, she feels, her success rate is lower than in previous years (so… this implies that we want to do a lower one-tailed test). Does her trapping data support this hypothesis?\n\nWhat is \\(H_0\\)?\nWhat is \\(H_A\\)?\nAre both \\(n\\times\\pi\\) and \\(n\\times(1-\\pi)\\) &gt; 5?\nCalculate \\(Z\\) statistic and the p value associated with \\(Z\\). We use the lower.tail=TRUE argument to pnorm() because we’re testing a lower-tailed one-tailed hypothesis.\n\n\n\nShow Code\nphat &lt;- mean(v)  # the mean of 0,1 variables yields the proportion!\nphat\n\n\nShow Output\n## [1] 0.6\n\n\n\nShow Code\npi &lt;- 0.8\nn &lt;- 30\nz &lt;- (phat - pi)/sqrt(pi * (1 - pi)/30)\n# we use the population expected proportion in the denominator\nz\n\n\nShow Output\n## [1] -2.738613\n\n\n\nShow Code\np &lt;- pnorm(z, lower.tail = TRUE)\np\n\n\nShow Output\n## [1] 0.00308495\n\n\n\nGraphically, our \\(Z\\) value (in blue) is farther from the center of the standard normal curve than the critical value marking the lower 5% of the total distribution (in red).\n\npar(mfrow = c(1, 1))\ncurve(dnorm(x, 0, 1), xlim = c(-4, 4), ylab = \"\", yaxt = \"n\", xlab = \"SD\", main = \"Sampling Distribution\")\n\nabline(v = z, col = \"blue\", lwd = 2)\nabline(v = qnorm(0.05), col = \"red\")\n\n\n\n\n\n\n\n\nThe 95% confidence interval around the sample proportion can be estimated, based on the normal distribution, as follows:\n\nlower &lt;- phat - qnorm(0.975) * sqrt(phat * (1 - phat)/30)\nupper &lt;- phat + qnorm(0.975) * sqrt(phat * (1 - phat)/30)\nci &lt;- c(lower, upper)\nci\n\n## [1] 0.4246955 0.7753045\n\n\nThis approach using quantiles of the standard normal distribution is but one method of calculating CIs for proportion data, and it generates a CI referred to as a Wald confidence interval. Note that this CI does not include the value of \\(\\pi\\)… rather, \\(\\pi\\) is greater than the upper bound of the CI, suggesting that the observed success rate is indeed lower than in previous years.\nWe can do the same test with a one-liner in R…\n\npt &lt;- prop.test(x = sum(v), n = length(v), p = 0.8, conf.level = 0.95, alternative = \"less\",\n    correct = FALSE)  # use correct=FALSE if we satisfy that n*pi and n*(1-pi) are both &gt;5\npt\n\n## \n##  1-sample proportions test without continuity correction\n## \n## data:  sum out of lengthv out of v\n## X-squared = 7.5, df = 1, p-value = 0.003085\n## alternative hypothesis: true p is less than 0.8\n## 95 percent confidence interval:\n##  0.0000000 0.7328738\n## sample estimates:\n##   p \n## 0.6\n\n\n\nNOTE: The CI returned here is different than we calculated based on the normal distribution, although the p value is the same… prop.test() implements a slightly different procedure for estimating the CI rather than basing this on the normal distribution and the CLT.\n\n\nTwo Sample \\(Z\\) Test\nThe \\(Z\\) statistic for the two-sample test comparing proportions is also very similar to that for comparing means.\n\\[Z=\\frac{\\hat{p_2}-\\hat{p_1}-\\pi}{\\sqrt{p^*(1-p^*)(1/n_1+1/n_2)}}\\]\nwhere:\n\n\\(p^* = \\frac{x_1 + x_2}{n_1+n_2}\\) = pooled proportion\n\\(\\hat{p_1}\\) and \\(\\hat{p_2}\\) = proportions of “successes” in each sample group\n\\(\\pi\\) = expected difference in proportions between sample groups (usually set to zero)\n\\(n_1\\) and \\(n_2\\) = numbers of observations in each sample group\n\n\n\n\nCHALLENGE\nA biologist studying two species of tropical bats captures females of both species in a mist net over the course of a week of nightly netting. For each species, the researcher records whether females is lactating or not.\n\n\n\n\n\n\n\n\n\nThe two vectors below summarize the data for each species.\n\nspecies1 &lt;- c(1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,\n    1, 0)\nspecies2 &lt;- c(1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,\n    0, 1, 1, 0, 1, 1, 1)\n\nBased on your mist netting data, do the species differ significantly in the proportion of lactating females? What are \\(H_0\\) and \\(H_A\\)?\n\npstar &lt;- (sum(species1) + sum(species2))/(length(species1) + length(species2))\npstar\n\n## [1] 0.6363636\n\nphat1 &lt;- mean(species1)\nphat1\n\n## [1] 0.56\n\nphat2 &lt;- mean(species2)\nphat2\n\n## [1] 0.7\n\npi &lt;- 0\nz &lt;- (phat2 - phat1 - pi)/sqrt((pstar * (1 - pstar)) * (1/length(species1) + 1/length(species2)))\nz\n\n## [1] 1.074709\n\np_upper &lt;- 1 - pnorm(z, lower.tail = TRUE)\np_lower &lt;- pnorm(z, lower.tail = FALSE)\n# two-tailed probability, so we add the upper and lower tails\np &lt;- p_upper + p_lower\np\n\n## [1] 0.2825049\n\ncritical_val &lt;- qnorm(1 - alpha/2)  # identify critical values\ncritical_val\n\n## [1] 1.959964\n\ntest &lt;- abs(z) &gt; critical_val  # boolean test\ntest\n\n## [1] FALSE\n\n\nWe can use the prop.test() function to do this in one line.\n\nprop.test(x = c(sum(species2), sum(species1)), n = c(length(species2), length(species1)),\n    alternative = \"two.sided\", correct = FALSE)\n\n## \n##  2-sample test for equality of proportions without continuity correction\n## \n## data:  c out of csum(species2) out of length(species2)sum(species1) out of length(species1)\n## X-squared = 1.155, df = 1, p-value = 0.2825\n## alternative hypothesis: two.sided\n## 95 percent confidence interval:\n##  -0.1144634  0.3944634\n## sample estimates:\n## prop 1 prop 2 \n##   0.70   0.56\n\n# use correct=FALSE if we satisfy that n*pi and n*(1-pi) are both &gt;5",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Classical Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "15-module.html#concept-review",
    "href": "15-module.html#concept-review",
    "title": "15  Classical Hypothesis Testing",
    "section": "Concept Review",
    "text": "Concept Review\n\n\\(Z\\) and \\(T\\) tests are used to evaluate whether a given sample statistic (e.g., a mean or proportion) deviates significantly from what is expected under a null model or whether two samples statistics deviate significantly from one another\nThey are used for dealing with normally distributed, continuous variables or those that can be approximated closely by the normal distribution (e.g., proportion data)\n\nWe REJECT a \\(H_0\\) if the p value obtained for a given \\(Z\\) or \\(T\\) test statistic is &lt; \\(\\alpha\\)\nCIs for our sample statistic are calculated as \\(mean ± [T_{(1-\\alpha/2)}\\) or \\(Z_{(1-\\alpha/2)}] \\times SE\\), and we can REJECT an \\(H_0\\) if the \\((1-\\alpha)\\) CI around does not include the expected value of the statistic\nWhen we are dealing with data for a population or for sample sizes &gt; 30, or when we are dealing with proportions, we use \\(Z\\) distribution quantiles for calculating CIs and p values, but for sample sizes &lt; 30, we use \\(t\\) distribution quantiles\n\n\n\nFormula Summary\n\nThe \\(Z\\) or \\(T\\) Statistic for Testing a Single Mean\n\\[Z\\ {\\rm or}\\ T = \\frac{\\bar{x}-\\mu}{s/\\sqrt{n}}\\]\n\n\nThe \\(T\\) Statistic for Comparing Means\n\nUnequal Variance\n\\[Z\\ {\\rm or}\\ T = \\frac{\\bar{x_2} - \\bar{x_1} - \\mu}{\\sqrt{s_1^2/n_1 + s_2^2/n_2}}\\]\n\n\nEqual Variance\n\\[Z\\ {\\rm or}\\ T =\\frac{\\bar{x_2}-\\bar{x_1}-\\mu}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\]\n\n\nPaired Samples\n\\[Z\\ {\\rm or}\\ T = \\frac{d-\\mu}{\\sqrt{s_d^2/n}}\\]\n\n\n\nThe \\(Z\\) Statistic for Testing a Single Proportion\n\\[Z = \\frac{\\hat{p}-\\pi}{\\sqrt{\\pi(1-\\pi)/n}}\\]\n\n\nThe \\(Z\\) Statistic for Comparing Proportions\n\\[Z = \\frac{\\hat{p_2}-\\hat{p_1}-\\pi}{\\sqrt{p^*(1-p^*)(1/n_1+1/n_2)}}\\]",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Classical Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "16-module.html",
    "href": "16-module.html",
    "title": "16  Using Permutation Tests",
    "section": "",
    "text": "16.1 Objectives",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Using Permutation Tests</span>"
    ]
  },
  {
    "objectID": "16-module.html#objectives",
    "href": "16-module.html#objectives",
    "title": "16  Using Permutation Tests",
    "section": "",
    "text": "The objective of this module is to extend our hypothesis testing framework to include permutation/randomization tests.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Using Permutation Tests</span>"
    ]
  },
  {
    "objectID": "16-module.html#preliminaries",
    "href": "16-module.html#preliminaries",
    "title": "16  Using Permutation Tests",
    "section": "16.2 Preliminaries",
    "text": "16.2 Preliminaries\n\nInstall the following packages in R: {coin}, {jmuOutlier}, and {infer}\nLoad {tidyverse} and {mosaic}",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Using Permutation Tests</span>"
    ]
  },
  {
    "objectID": "16-module.html#permutation-methods",
    "href": "16-module.html#permutation-methods",
    "title": "16  Using Permutation Tests",
    "section": "16.3 Permutation Methods",
    "text": "16.3 Permutation Methods\nThe inferential framework described in Module 15 is grounded in the idea that the sampling distributions for summary statistics of interest are appropriately modeled using well-known theoretical distributions… but that may not always be the case. An alternative approach is for us to build up (rather than assume) sampling distributions for statistics of interest using so-called “permutation” or “randomization” methods, which are a type of simulation. These approaches are conceptually related to the bootstrapping that we employed in Module 14 as an alternative method of generating sampling distributions and CIs.\nLike bootstrapping, permutation/randomization tests generate a kind of sampling distribution (called a “permutation distribution” or “null distribution) for the null hypothesis to which we compare a test statistic calculated from our observations. We generate the permutation or null distribution by resampling from a set of actually observed outcomes while permuting (”shuffling”) some attribute of our actual observations in accordance with what we would expect if our null hypothesis were true.\nThus, whereas a “bootstrap sampling distribution” for generating CIs is created via simulation by repeatedly resampling observations from a single sample with replacement, a “permutation distribution” or “null distribution” is generated by resampling many times from that sample without replacement while shuffling attributes of each observation. Depending on what type of test we are doing (e.g., comparing a mean or proportion from a single sample to a hypothesized expectation, comparing means or proportions between two samples, evaluating whether regression coefficients are significant, etc.), exactly what we are shuffling in each resampling simulation may vary. In general permutation/randomization tests are extremely flexible and can be used for virtually any kind of test we may imagine.\nFor a true “permutation test”, we would sample all possible permutations of our original data to construct the permutation distribution, which can be computationally prohibitive if the sample size is even moderately large. However, we can conduct “approximate permutation tests” by simply using a large number of resamples to approximate the permutation distribution. These are generally referred to as randomization tests.\nLet’s use the same data we used in Module 15 and conduct the same kinds of statistical tests we did previously, but this time we will use randomization to generate approximate permutation distributions for test statistics under the null hypothesis.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Using Permutation Tests</span>"
    ]
  },
  {
    "objectID": "16-module.html#working-with-means",
    "href": "16-module.html#working-with-means",
    "title": "16  Using Permutation Tests",
    "section": "16.4 Working with Means",
    "text": "16.4 Working with Means\n\nOne Sample Permutation Test\nRecall that in one of the tests we conducted in Module 15 was to evaluate whether vervet monkeys trapped during the 2015 trapping season in South Africa differed in weight from the expectation based on previous years (i.e., \\(H_0: \\mu_0 = 5.0\\ kg\\)). Here, we start by generating a test statistic that is the actual difference between our observed mean and the expected mean rather than an \\(Z\\) or \\(T\\) score estimating how many SEs our observed mean is away from the expected population mean.\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/vervet-weights.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\nx &lt;- d$weight  # current weights\nn &lt;- length(x)\nm &lt;- mean(x)  # average weight of our sample\nmu &lt;- 5  # expected weight (null hypothesis)\n(actual_diff &lt;- m - mu)  # difference between our sample and expected\n\n## [1] 0.3239216\n\n# we use this as our test statistic!\n\nRecall that permutation/randomization tests work by resampling and shuffling attributes of the observed data many times in order to come up with a distribution of test statistics that we might plausibly get under a particular null hypothesis. They do not rely on assuming that the data come from a particular theoretical distribution.\nWhat is it that we would need to permute about our vervet body size data for it to be consistent with the null hypothesis of no difference between our sample mean of 2015 weights and the expectation based on weights from previous seasons, i.e., that \\(mean\\ observed - expected\\ weights\\) (\\(\\mu_A - \\mu_0\\)) = ZERO? Well, we could simply randomize the sign of the value of \\(individual\\ observed - expected\\ weights\\) in each simulation and then take the mean to generate a permutation distribution of \\(mean\\ observed - expected\\ weights\\) under the null hypothesis.\nThe p value calculated for a permutation/randomization test is the probability of getting a test statistic, by chance, as or more extreme than our observed one. We can calculate this empirically based on the permutation distribution, simply by counting the number of simulated test statistics that exceed our observed test statistic and then dividing by the number of permutations.\n\nNOTE: Sometimes, we add 1 to both the numerator and denominator for this calculation to ensure that we never have a p value of exactly 0.\n\nBelow, we do 10,000 permutations where we randomly shuffle the sign of each individual observation’s difference from the expected mean under the null hypothesis. We then compare the observed difference between the mean of our sample and the expected mean to this distribution and calculate the p value associated with getting a difference as or more extreme than we observed. Lastly, we plot the approximate permutation distribution we generated and superimpose our observed difference as the test statistic.\n\nnperm &lt;- 10000  # number of permutation simulations\npermuted_diff &lt;- vector(length = n)  # set up a dummy vector to hold results for each permutation\nfor (i in 1:nperm) {\n    # now we scramble the sign of individual observed - expected weights, and\n    # then take mean\n    permuted_diff[[i]] &lt;- mean(sample(c(-1, 1), length(x), replace = TRUE) * abs(x -\n        mu))\n}\n\n# calculate the two.sided p value\n(p &lt;- (sum(permuted_diff &gt;= abs(actual_diff)) + sum(permuted_diff &lt;= -abs(actual_diff)))/nperm)\n\n## [1] 0.0194\n\n# or\n(p &lt;- sum(abs(permuted_diff) &gt;= abs(actual_diff))/nperm)\n\n## [1] 0.0194\n\n# or add 1 to both numerator and denominator to avoid p = 0\n\nhistogram(permuted_diff, type = \"count\", xlab = \"\", ylab = \"# Permutations\", main = \"Histogram of Permutation Distribution\")\nladd(panel.abline(v = actual_diff, lty = 3, lwd = 2))\nladd(panel.text(x = actual_diff, y = nperm * 0.08, \"Test Statistic\", srt = 90, pos = 4,\n    offset = 1))\n\n\n\n\n\n\n\n\nThe function perm.test from the package {jmuOutlier} conducts the same kind of test. By default, the function runs 10,000 simulations, and the test statistic being calculated is the mean, but other functions are possible.\n\nlibrary(jmuOutlier)\n# first show plot\nperm.test(x, alternative = \"two.sided\", mu = mu, plot = TRUE, num.sim = nperm)\nabline(v = actual_diff, lty = 3, lwd = 2)\ntext(x = actual_diff, y = nperm * 0.065, \"Test Statistic\", srt = 90, pos = 4, offset = 1)\n\n\n\n\n\n\n\n# then 2 tailed p value\nperm.test(x, alternative = \"two.sided\", mu = mu, plot = FALSE, num.sim = nperm)\n\n## [[1]]\n## [1] \"One-sample permutation test was performed.\"\n## \n## [[2]]\n## [1] \"p-value was estimated based on 10000 simulations.\"\n## \n## $alternative\n## [1] \"two.sided\"\n## \n## $mu\n## [1] 5\n## \n## $p.value\n## [1] 0.0216\n\ndetach(package:jmuOutlier)\n\nWe can also write our own custom function to perform the same kind of 1-sample permutation test!\n\nperm1samp &lt;- function(x, stat = \"mean\", mu = 0, nperm = 10000, alternative = c(\"two.sided\",\n    \"less\", \"greater\")) {\n    # can modify function by adding alternative `stat=`\n    test_data &lt;- x - mu\n    n &lt;- length(test_data)\n    if (stat == \"mean\") {\n        myfun &lt;- function(x) {\n            mean(x)\n        }\n    }\n    test_stat &lt;- myfun(test_data)\n    perm_stat &lt;- vector(length = nperm)\n    for (i in 1:nperm) {\n        perm_stat[[i]] &lt;- myfun(sample(c(-1, 1), n, replace = TRUE) * abs(test_data))\n    }\n    # or, as a 1-liner that does not require a loop perm_stat &lt;-\n    # replicate(nperm, myfun(sample(c(-1,1), n, replace=TRUE) *\n    # abs(test_data)))\n\n    if (alternative[1] == \"less\") {\n        p_perm &lt;- sum(perm_stat &lt;= test_stat)/nperm\n    } else if (alternative[1] == \"greater\") {\n        p_perm &lt;- sum(perm_stat &gt;= test_stat)/nperm\n    } else {\n        p_perm &lt;- sum(abs(perm_stat) &gt;= abs(test_stat))/nperm\n    }\n    list(test_stat = test_stat, perm_stat = perm_stat, p_perm = p_perm, type = alternative[1])\n}\n\nnperm &lt;- 10000\noutput &lt;- perm1samp(x, stat = \"mean\", mu = mu, nperm = nperm)\n\nhistogram(output$perm_stat, type = \"count\", ylab = \"# Permutations\", main = \"Histogram of Permutation Distribution\",\n    xlab = ifelse(output$type == \"less\" | output$type == \"greater\", paste0(\"One-Sided Test\\nProportion of distribution \",\n        output$type, \" than test stat = \", output$p_perm), paste0(\"Two-Sided Test\\nProportion of distribution more extreme than test stat = \",\n        output$p_perm)))\nladd(panel.abline(v = output$test_stat, lty = 3, lwd = 2))\nif (output$type == \"two.sided\") {\n    ladd(panel.abline(v = -output$test_stat, lty = 3, lwd = 2, col = \"red\"))\n}\nladd(panel.text(x = output$test_stat, y = nperm * 0.08, \"Test Statistic\", srt = 90,\n    pos = 4, offset = 1))\n\n\n\n\n\n\n\n\n\n\nTwo Sample Permutation Test\nAnother of the tests we conducted in Module 15 was to evaluate whether male and female black-and-white colobus differ in body weight. Let’s load in that data again and then calculate, as a test statistic, the difference in average weights for females versus males.\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/colobus-weights.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\nhead(d)\n\n## # A tibble: 6 × 3\n##      id weight sex  \n##   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;\n## 1     1   7.24 male \n## 2     2   6.09 male \n## 3     3   6.97 male \n## 4     4   6.98 male \n## 5     5   6.08 male \n## 6     6   6.22 male\n\nx &lt;- d[d$sex == \"female\", ]$weight\ny &lt;- d[d$sex == \"male\", ]$weight\n(actual_diff &lt;- mean(x) - mean(y))\n\n## [1] -1.449\n\n\nTo test whether this difference is significant by permutation, we would generate a permutation distribution consistent with a null hypothesis of no difference in weight between the sexes by simulation, where, for each simulation, we permute the sex assigned to each observation and then calculate the difference (female - male) between the sexes in each permuted set. We then calculate the p value associated with getting a difference between male and female mean weights as or more extreme than the one we observed. [We again calculate this as a “two-sided” probability, i.e., we are testing the \\(H_A\\) that…\n\\(mean(female) - mean(male) ≠ 0\\)\nrather than…\n\\(mean(female) - mean(male) &gt; 0\\)\nor…\n\\(mean(female) - mean(male) &lt; 0\\)\nboth of which would be “one-sided” tests.]\nLastly, we visualize the approximate permutation distribution and superimpose our observed difference in weights between males and females.\n\nnperm &lt;- 10000  # number of permutation simulations\n# create a dummy vector to hold results for each permutation\npermuted_diff &lt;- vector(length = n)\ntest_data &lt;- d\nfor (i in 1:nperm) {\n    # scramble the sex vector `sample()` with a vector as an argument yields a\n    # random permutation of the vector\n    test_data$sex &lt;- sample(test_data$sex)\n    x &lt;- test_data[test_data$sex == \"female\", ]$weight\n    y &lt;- test_data[test_data$sex == \"male\", ]$weight\n    permuted_diff[[i]] &lt;- mean(x) - mean(y)\n}\n\n(p &lt;- (sum(permuted_diff &gt;= abs(actual_diff)) + sum(permuted_diff &lt;= -abs(actual_diff)))/nperm)\n\n## [1] 0\n\nhistogram(permuted_diff, type = \"count\", xlab = \"\", ylab = \"# Permutations\", main = \"Histogram of Permutation Distribution\",\n    xlim = c(-1.75, 1.75))\n\n\n\n\n\n\n\nladd(panel.abline(v = actual_diff, lty = 3, lwd = 2))\n\n\n\n\n\n\n\nladd(panel.text(x = actual_diff, y = nperm * 0.08, \"Test Statistic\", srt = 90, pos = 4,\n    offset = 1))\n\n\n\n\n\n\n\n\nThe same perm.test() function from {jmuOutlier} that we used above for a 1-sample permutation test also allows for 2-sample tests.\n\n# library(jmuOutlier) x &lt;- d[d$sex=='female',]$weight y &lt;-\n# d[d$sex=='male',]$weight mu &lt;- 0 # expected difference between means under\n# null perm.test(x, y, alternative = 'two.sided', mu = mu, plot = TRUE, num.sim\n# = nperm) abline(v=actual_diff) perm.test(x, y, alternative = 'two.sided', mu\n# = mu, plot = FALSE, num.sim = nperm) detach(package:jmuOutlier)\n\n\nNOTE: In this example, the vertical line showing our actual difference in means is off-scale on the left-hand side of the plot!\n\nThe function independence_test() from the {coin} package conducts the same kind of 2-sample test with a simpler construction that does not require us specifying two vectors to compare…\n\nlibrary(coin)\n\n## Loading required package: survival\n\nindependence_test(weight ~ as.factor(sex), alternative = \"two.sided\", distribution = \"approximate\",\n    data = d)\n\n## \n##  Approximative General Independence Test\n## \n## data:  weight by as.factor(sex) (female, male)\n## Z = -5.4998, p-value &lt; 1e-04\n## alternative hypothesis: two.sided\n\ndetach(package:coin)\n\nFinally, we can also write our own generic 2-sample permutation function that allows us to compare custom test statistics. Here, we first define a function for our test statistic, mean_diff(), i.e., a difference of means, and then define a function for the test.\n\nperm2samp &lt;- function(x, y, stat = \"mean_diff\", nperm = 10000, alternative = c(\"two.sided\",\n    \"less\", \"greater\")) {\n    # can modify function by adding alternative `stat=`\n    if (stat == \"mean_diff\") {\n        myfun &lt;- function(x, y) {\n            mean(x) - mean(y)\n        }\n    }\n    test_stat &lt;- myfun(x, y)\n    test_data &lt;- c(x, y)  # puts two vectors of samples together\n    perm_stat &lt;- vector(length = nperm)\n    for (i in 1:nperm) {\n        indexes &lt;- sample(length(test_data), length(x), replace = FALSE)\n        # the row above generates a randomized vector of indices that is the\n        # length of the x sample... this is equivalent to permuting which group\n        # the x sample belongs to\n        s1 &lt;- test_data[indexes]\n        s2 &lt;- test_data[-indexes]\n        perm_stat[[i]] &lt;- myfun(s1, s2)\n    }\n    if (alternative[1] == \"less\") {\n        p_perm &lt;- sum(perm_stat &lt;= test_stat)/nperm\n    } else if (alternative[1] == \"greater\") {\n        p_perm &lt;- sum(perm_stat &gt;= test_stat)/nperm\n    } else {\n        p_perm &lt;- sum(abs(perm_stat) &gt;= abs(test_stat))/nperm\n    }\n    list(test_stat = test_stat, perm_stat = perm_stat, p_perm = p_perm, type = alternative[1])\n}\n\nx &lt;- d[d$sex == \"female\", ]$weight\ny &lt;- d[d$sex == \"male\", ]$weight\n\noutput &lt;- perm2samp(x, y, stat = \"mean_diff\", nperm = 10000)\n\nhistogram(output$perm_stat, type = \"count\", ylab = \"# Permutations\", main = \"Histogram of Permutation Distribution\",\n    xlab = ifelse(output$type == \"less\" | output$type == \"greater\", paste0(\"One-Sided Test\\nProportion of distribution \",\n        output$type, \" than test stat = \", output$p_perm), paste0(\"Two-Sided Test\\nProportion of distribution more extreme than test stat = \",\n        output$p_perm)), xlim = c(-1.75, 1.75))\nladd(panel.abline(v = output$test_stat, lty = 3, lwd = 2))\nif (output$type == \"two.sided\") {\n    ladd(panel.abline(v = -output$test_stat, lty = 3, lwd = 2, col = \"red\"))\n}\nladd(panel.text(x = output$test_stat, y = nperm * 0.08, \"Test Statistic\", srt = 90,\n    pos = 4, offset = 1))\n\n\n\n\n\n\n\n\nHere’s a graphical version of what we are doing…",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Using Permutation Tests</span>"
    ]
  },
  {
    "objectID": "16-module.html#working-with-proportions",
    "href": "16-module.html#working-with-proportions",
    "title": "16  Using Permutation Tests",
    "section": "16.5 Working with Proportions",
    "text": "16.5 Working with Proportions\nWe can take the same permutation/randomization approach and use the same 1- and 2-sample tests with proportion data.\nIn Module 15, we looked at a set of data on captures of birds in 30 nets on a morning of mist netting (i.e., a “success” or “failure” for every net during a netting session) and tested whether the proportion of “successes” was lower than the expectation of 80% was lower than in previous years. The code below lets us do this using a 1-sample randomization test.\n\nv &lt;- c(0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n    0, 1, 0, 1, 1)\nnperm &lt;- 10000\n\n# using {jmuOutlier} library(jmuOutlier) perm.test(v, alternative='less',\n# mu=0.8, plot=FALSE, num.sim = nperm) detach(package:jmuOutlier)\n\n# using our custom function\noutput &lt;- perm1samp(v, stat = \"mean\", mu = 0.8, alternative = \"less\", nperm = nperm)\n\nhistogram(output$perm_stat, type = \"count\", ylab = \"# Permutations\", main = \"Histogram of Permutation Distribution\",\n    xlab = ifelse(output$type == \"less\" | output$type == \"greater\", paste0(\"One-Sided Test\\nProportion of distribution \",\n        output$type, \" than test stat = \", output$p_perm), paste0(\"Two-Sided Test\\nProportion of distribution more extreme than test stat = \",\n        output$p_perm)))\nladd(panel.abline(v = output$test_stat, lty = 3, lwd = 2))\nif (output$type == \"two.sided\") {\n    ladd(panel.abline(v = -output$test_stat, lty = 3, lwd = 2, col = \"red\"))\n}\nladd(panel.text(x = output$test_stat, y = nperm * 0.08, \"Test Statistic\", srt = 90,\n    pos = 4, offset = 1))\n\n\n\n\n\n\n\n\nIn Module 15, we also compared data on captures of two species of bats over the course of a week of nightly mist-netting. For each species, the record indicates whether a captured female was lactating (1) or not (0), and we wanted to test whether this proportion differed between species. We can do this with a 2-sample randomization test.\n\nspecies1 &lt;- c(1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,\n    1, 0)\nspecies2 &lt;- c(1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,\n    0, 1, 1, 0, 1, 1, 1)\n(actual_diff &lt;- mean(species1) - mean(species2))\n\n## [1] -0.14\n\n# this calculates a difference in proportions\nnperm &lt;- 10000\n\n# using {jmuOutlier} library(jmuOutlier) perm.test(species1, species2,\n# alternative = 'two.sided', plot=FALSE, num.sim = nperm)\n# detach(package:jmuOutlier)\n\n# using our custom function\noutput &lt;- perm2samp(species1, species2, stat = \"mean_diff\", nperm = nperm)\n\nhistogram(output$perm_stat, type = \"count\", ylab = \"# Permutations\", main = \"Histogram of Permutation Distribution\",\n    xlab = ifelse(output$type == \"less\" | output$type == \"greater\", paste0(\"One-Sided Test\\nProportion of distribution \",\n        output$type, \" than test stat = \", output$p_perm), paste0(\"Two-Sided Test\\nProportion of distribution more extreme than test stat = \",\n        output$p_perm)))\nladd(panel.abline(v = output$test_stat, lty = 3, lwd = 2))\nif (output$type == \"two.sided\") {\n    ladd(panel.abline(v = -output$test_stat, lty = 3, lwd = 2, col = \"red\"))\n}\nladd(panel.text(x = output$test_stat, y = nperm * 0.08, \"Test Statistic\", srt = 90,\n    pos = 4, offset = 1))\n\n\n\n\n\n\n\n\n\nNOTE: The p values associated with these permutation/randomization based tests on proportions are not so close to those using the prop.test() function for the same data that we ran in Module 15. This is because prop.test() is calculating the p value associated with our test statistic presuming that the distribution of sampling proportions is roughly normal, whereas this test is calculating p values by counting the proportion of simulations!",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Using Permutation Tests</span>"
    ]
  },
  {
    "objectID": "16-module.html#using-the-infer-package",
    "href": "16-module.html#using-the-infer-package",
    "title": "16  Using Permutation Tests",
    "section": "16.6 Using the {infer} Package",
    "text": "16.6 Using the {infer} Package\nThe {infer} package offers a convenient set of functions and a standard workflow for using permutation methods for hypothesis testing, whether we are dealing with means, differences between means, proportions, or differences in proportions.\n\n\n\n\n\n\n\n\n\nThe {infer} package includes a set of functions that allow us to easily generate shuffled or permuted datasets and then conduct hypothesis testing with these datasets just as we have done above. We will explore how we do this with the {infer} package using the black-and-white colobus weight data we used above.\nWe first read in our data and load the {infer} package:\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/colobus-weights.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\nlibrary(infer)\n\nThen, we use the function specify() to indicate the variables we are interested in. The specify() function takes an argument of formula= response~explanatory. If we are working with proportion data, we also need to provide an argument for what value of the explanatory variable counts as a success, using success=, but that is not the case for our colobus weight data, where our response variable is numeric, not binary.\n\nd &lt;- d |&gt;\n    specify(formula = weight ~ sex)\n# this does not change the data frame but adds some meta data\nhead(d)\n\n## Response: weight (numeric)\n## Explanatory: sex (factor)\n## # A tibble: 6 × 2\n##   weight sex  \n##    &lt;dbl&gt; &lt;fct&gt;\n## 1   7.24 male \n## 2   6.09 male \n## 3   6.97 male \n## 4   6.98 male \n## 5   6.08 male \n## 6   6.22 male\n\n\nThe function hypothesize() is then used to declare the null hypothesis we wish to test. We use the argument null=\"point\" if we are doing a test of a single mean (mu=) or proportion (p=) or null=\"independence\" if we are doing a test of the independence of two variables (i.e., a two-sample test).\n\nd &lt;- d |&gt;\n    hypothesize(null = \"independence\")\n# again, this does not change the data frame but adds some meta data\nhead(d)\n\n## Response: weight (numeric)\n## Explanatory: sex (factor)\n## Null Hypothesis: independence\n## # A tibble: 6 × 2\n##   weight sex  \n##    &lt;dbl&gt; &lt;fct&gt;\n## 1   7.24 male \n## 2   6.09 male \n## 3   6.97 male \n## 4   6.98 male \n## 5   6.08 male \n## 6   6.22 male\n\n\nWe then use the generate() function to generate replicates of “shuffled” or “permuted” data under the assumption that the null hypothesis is true. That is, we generating datasets from which we then derived a null distribution for our statistic of interest.\n\nd_permutations &lt;- d |&gt;\n    generate(reps = 1000, type = \"permute\")\nnrow(d)\n\n## [1] 40\n\nnrow(d_permutations)\n\n## [1] 40000\n\n\nNext, we use calculate() to calculate summary statistics of interest for each replicate. We can calculate a variety of different statistics, including the mean (“mean”), proportion (“prop”), difference in means (“diff in means”), difference in proportions (“diff in props”), and others. This step generates a null distribution for our summary statistic. For differences in means or proportions, we need to also include an argument specifying the order of the levels of the explanatory variable for subtraction.\n\nnull_distribution &lt;- d_permutations |&gt;\n    calculate(stat = \"diff in means\", order = c(\"male\", \"female\"))\n# to subtract female weight from male weight\n\nWe can then use the function visualize() to examine the null distribution (alternatively, we can use histogram(), in which case we then need to specify the variable name=\"stat\")\n\nvisualize(null_distribution, bins = 20)\n\n\n\n\n\n\n\nhistogram(null_distribution$stat, nint = 20)\n\n\n\n\n\n\n\n\nSo, now we have a null or permutation distribution for the test statistic… then, we can also use {infer} package functions to easily calculate hte observed summary statistic that we want to compare to this null distribution by simply leaving out the hypothesize() and generate() steps.\n\nobserved_stat &lt;- read_csv(f, col_names = TRUE) |&gt;\n    specify(weight ~ sex) |&gt;\n    calculate(stat = \"diff in means\", order = c(\"male\", \"female\"))\n\n## Rows: 40 Columns: 3\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (1): sex\n## dbl (2): id, weight\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe can plot this observed test statistic atop the null distribution using the shade_p_value() function, which has two arguments: a value for obs_stat= and a direction= (“left,”right”, or “both”) corresponding to two different one-tailed or a two-tailed alternative hypothesis.\n\nvisualize(null_distribution, bins = 20) + shade_p_value(obs_stat = observed_stat,\n    direction = \"both\")\n\n\n\n\n\n\n\n\n\nNOTE: This is not very interesting because the observed statistic is way outside of the null distribution!\n\nThe function get_p_value() with arguments of the null distribution, the observed statistic, and the direction will give us the p value generated by permutation.\n\nget_p_value(null_distribution, observed_stat, direction = \"both\")\n\n## Warning: Please be cautious in reporting a p-value of 0. This result is an approximation\n## based on the number of `reps` chosen in the `generate()` step.\n## ℹ See `get_p_value()` (`?infer::get_p_value()`) for more information.\n\n\n## # A tibble: 1 × 1\n##   p_value\n##     &lt;dbl&gt;\n## 1       0\n\n\nAs an additional example of this approach, let’s consider the vervet trapping data from above once again. Recall that we want to evaluate whether vervet monkeys trapped during the 2015 trapping season differed in weight from the expectation based on previous years, which was 5.0 kg. How do we do this using the {infer} workflow?\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/vervet-weights.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\nnull_distribution &lt;- d |&gt;\n    specify(response = weight) |&gt;\n    # here we specify just a response, not a formula\nhypothesize(null = \"point\", mu = 5) |&gt;\n    # here we are comparing a sample to an expectation\ngenerate(reps = 1000, type = \"bootstrap\") |&gt;\n    calculate(stat = \"mean\")\n# Note that here type='bootstrap'! This is actually doing something a little\n# different than we did above... it is sampling *with replacement* from our set\n# of weights rather than randomly permuting the sign of the difference of each\n# value from the mean! (Here, the number of observations sampled is equal to\n# the original number of observations)\n\nobserved_stat &lt;- d |&gt;\n    specify(response = weight) |&gt;\n    calculate(stat = \"mean\")\n\nvisualize(null_distribution, bins = 20) + shade_p_value(obs_stat = observed_stat,\n    direction = \"both\")\n\n\n\n\n\n\n\nget_p_value(null_distribution, observed_stat, direction = \"both\")\n\n## # A tibble: 1 × 1\n##   p_value\n##     &lt;dbl&gt;\n## 1   0.014\n\n\nNote that this p value is pretty similar to that returned by jmuOutlier::perm.test() and by our custom perm1samp() function, though as noted above, those use slightly different permutation processes.\n\nCHALLENGE\nAs a final example of this approach, we will again examine data on captures of two species of bats over the course of a week of nightly mist-netting. For each species, remember that the record indicates whether a captured female was lactating (1) or not (0).\n\nspecies1 &lt;- c(1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,\n    1, 0)\nspecies2 &lt;- c(1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,\n    0, 1, 1, 0, 1, 1, 1)\n\nFirst, we need to massage this data into the correct shape:\n\ns1 &lt;- tibble(species = \"species1\", lactating = species1)\ns2 &lt;- tibble(species = \"species2\", lactating = species2)\nd &lt;- bind_rows(s1, s2)\nd$lactating &lt;- factor(d$lactating)\n\nNow, use the {infer} workflow to test whether the proportion of lactating females among the captured bats differs between species.\n\n\nShow Code\nnull_distribution &lt;- d |&gt;\n    specify(lactating ~ species, success = \"1\") |&gt;\n    hypothesize(null = \"independence\") |&gt;\n    generate(reps = 1000, type = \"permute\") |&gt;\n    calculate(stat = \"diff in props\", order = c(\"species1\", \"species2\"))\n\n# NOTE: This runs a LOT slower than our custom permutation function for the\n# same number of reps!\nvisualize(null_distribution, bins = 20)\n\n\n\n\n\n\n\n\n\nShow Code\nobserved_stat &lt;- d |&gt;\n    specify(lactating ~ species, success = \"1\") |&gt;\n    calculate(stat = \"diff in props\", order = c(\"species1\", \"species2\"))\n\nvisualize(null_distribution, bins = 10) + shade_p_value(observed_stat, direction = \"both\")\n\n\n\n\n\n\n\n\n\nShow Code\nget_p_value(null_distribution, observed_stat, direction = \"both\")\n\n\nShow Output\n## # A tibble: 1 × 1\n##   p_value\n##     &lt;dbl&gt;\n## 1    0.48\n\n\n\nShow Code\ndetach(package:infer)\n\n\nThis p value is similar to that returned by our custom permutation test!",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Using Permutation Tests</span>"
    ]
  },
  {
    "objectID": "16-module.html#advantages-of-permutation-tests",
    "href": "16-module.html#advantages-of-permutation-tests",
    "title": "16  Using Permutation Tests",
    "section": "16.7 Advantages of Permutation Tests",
    "text": "16.7 Advantages of Permutation Tests\nSome of the key advantages of permutation tests include the following:\n\nThey are “distribution-free” - i.e., they do not make any assumptions about the distribution of the underlying data\nThere is no presumption of random sampling from some imaginary hypothetical population\nUnlike some other nonparametric tests, they do not depend on large sample sizes for their validity\nMany common nonparametric tests are permutation tests, just carried out on RANKS\nThey can be used with many kinds of data (nominal, ordinal, interval/ratio)\nThey are relatively straightforward to conduct and interpret",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Using Permutation Tests</span>"
    ]
  },
  {
    "objectID": "16-module.html#concept-review",
    "href": "16-module.html#concept-review",
    "title": "16  Using Permutation Tests",
    "section": "Concept Review",
    "text": "Concept Review\n\nPermutation/randomization tests involve these steps:\n\nCompute a sample statistic of your choice (mean, median, proportion, etc.) using the set of original observations\nRearrange attributes of the original observations in all or a very large number of random possible permutations, computing the test statistic each time, to yield a permutation or null distribution for the test statistic under the null hypothesis\nLook at the value of the sample statistic relative to the permutation distribution\nBased on the permutation distribution, calculate the probability of seeing a sample statistic as high or higher than that observed, i.e., the portion of the permutation distribution that equals or exceed the value of the sample statistic… this is the permutation test p value\n\njmuOutlier::perm.test(), coin::indepedence_test(), or write your own!\n{infer} workflow\n\n\nNOTE: This permutation-based approach to statistical inference is consistent with the philosophy promoted by statistician Allen Downey that, “There is really only test”, and that at all statistical tests are based on the same framework. Note that this is still a form of null hypothesis significance testing, or NHST!",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Using Permutation Tests</span>"
    ]
  },
  {
    "objectID": "17-module.html",
    "href": "17-module.html",
    "title": "17  Error, Power, and Effect Size",
    "section": "",
    "text": "17.1 Objectives",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Error, Power, and Effect Size</span>"
    ]
  },
  {
    "objectID": "17-module.html#objectives",
    "href": "17-module.html#objectives",
    "title": "17  Error, Power, and Effect Size",
    "section": "",
    "text": "The objective of this module is to discuss the concepts of Type I and Type II error, the multiple testing problem, statistical power, and effect size and outline how we can use R to investigate these via simulation and built-in functions.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Error, Power, and Effect Size</span>"
    ]
  },
  {
    "objectID": "17-module.html#preliminaries",
    "href": "17-module.html#preliminaries",
    "title": "17  Error, Power, and Effect Size",
    "section": "17.2 Preliminaries",
    "text": "17.2 Preliminaries\n\nLoad {tidyverse} and {manipulate}",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Error, Power, and Effect Size</span>"
    ]
  },
  {
    "objectID": "17-module.html#overview",
    "href": "17-module.html#overview",
    "title": "17  Error, Power, and Effect Size",
    "section": "17.3 Overview",
    "text": "17.3 Overview\nLet’s return to the concepts of error and power. Recall that Type I error occurs when you incorrectly reject a true \\(H_0\\). In any given hypothesis test, the probability of a Type I error is equivalent to the significance level, \\(\\alpha\\), and it is this type of error we are often trying to minimize when we are doing classical statistical inference. Type II error occurs when you incorrectly fail to reject a false \\(H_0\\) (in other words, fail to find evidence in support of a true \\(H_A\\)). Since we do not know what the true \\(H_A\\) actually is, the probability of committing such an error, labeled \\(\\beta\\), is not usually known in practice.\n\n\n\n\n\n\n\n\nWhat is True\nWhat We Decide\nResult\n\n\n\n\n\\(H_0\\)\n\\(H_0\\)\nCorrectly ‘accept’ the null\n\n\n\\(H_0\\)\n\\(H_A\\)\nFalsely reject the null (Type I error)\n\n\n\\(H_A\\)\n\\(H_0\\)\nFalsely ‘accept’ the null (Type II error)\n\n\n\\(H_A\\)\n\\(H_A\\)\nCorrectly reject the null",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Error, Power, and Effect Size</span>"
    ]
  },
  {
    "objectID": "17-module.html#type-i-error-and-multiple-testing",
    "href": "17-module.html#type-i-error-and-multiple-testing",
    "title": "17  Error, Power, and Effect Size",
    "section": "17.4 Type I Error and Multiple Testing",
    "text": "17.4 Type I Error and Multiple Testing\nBecause of how we define \\(\\alpha\\), the chance probability of falsely rejecting \\(H_0\\) when \\(H_0\\) is actually true, we would expect to find some “significant” results if we run enough independent hypothesis tests. For example, if we set \\(\\alpha\\) at 0.05, we would expect to find one “significant” result in roughly every 20 tests we run, just by chance. The relation of \\(\\alpha\\) to the distribution of a variable under a null hypothesis (\\(\\mu\\) = \\(\\mu_0\\)) versus an alternative hypothesis (e.g., \\(\\mu\\) &gt; \\(\\mu_0\\)) is shown in the figure below (this example is for an upper one-tailed test). It should be clear that we can reduce the chance of Type I error by decreasing \\(\\alpha\\) (shifting the critical value to the right in the \\(H_0\\) distribution). Type I error will also be reduced as the means get further apart or as the standard deviation of the distributions shrinks.\n\n\n\n\n\n\n\n\n\nLet’s explore this via simulation.\nWe will write some code to simulate a bunch of random datasets from a normal distribution where we set the expected population mean (\\(\\mu_0\\)) and standard deviation (\\(\\sigma\\)) and then calculate a \\(Z\\) (or \\(T\\)) statistic and p value for each one. We will then look at the “Type I” error rate… the proportion of times that, based on our sample, we would conclude that it was not drawn from the distribution we know to be true.\nFirst, let’s set up a skeleton function we will call typeI() to evaluate the Type I error rate. It should take, as arguments, the parameters of the normal distribution for the null hypothesis we want to simulate from (\\(\\mu_0\\) and \\(\\sigma\\)), our sample size, our \\(\\alpha\\) level, what “alternative” type of \\(Z\\) (or \\(T\\)) test we want to do (“greater”, “less”, or “two.tailed”), and the number of simulated datasets we want to generate. Copy and paste in the code below (and note that we set default values for \\(\\alpha\\) and the number of simulations). Note that we can use the “t” family of functions instead of the “norm” family.\n\n# function skeleton\ntypeI &lt;- function(mu0, sigma, n, alternative = \"two.tailed\", alpha = 0.05, k = 10000) {\n}\n\nNow, we will add the body of the function.\n\ntypeI &lt;- function(mu0, sigma, n, alternative = \"two.tailed\", alpha = 0.05, k = 10000) {\n    p &lt;- rep(NA, k)\n    # set a vector of k empty p values, one for each simulation\n    for (i in 1:k) {\n        # sets up a loop to run k simulations\n        x &lt;- rnorm(n = n, mean = mu0, sd = sigma)\n        # draws a sample of size n from our distribution\n        m &lt;- mean(x)  # calculates the mean\n        s &lt;- sd(x)  # calculates the standard deviation\n        z &lt;- (m - mu0)/(s/sqrt(n))\n        # calculates the Z statistic for the sample drawn from the null\n        # distribution relative to the null distribution alternatively use t &lt;-\n        # (m-mu0)/(s/sqrt(n))\n        if (alternative == \"less\") {\n            p[[i]] &lt;- pnorm(z, lower.tail = TRUE)\n            # calculates the associated p value alternatively, use p[[i]] &lt;-\n            # pt(t, df = n-1, lower.tail = TRUE)\n        }\n        if (alternative == \"greater\") {\n            p[[i]] &lt;- pnorm(z, lower.tail = FALSE)\n            # calculates the associated p value alternatively, use p[[i]] &lt;-\n            # pt(t, df = n-1, lower.tail=FALSE)\n        }\n        if (alternative == \"two.tailed\") {\n            if (z &gt; 0) {\n                p[[i]] &lt;- 2 * pnorm(z, lower.tail = FALSE)\n            }\n            # alternatively, use if (t &gt; 0) {p[[i]] &lt;- pt(t, df = n-1,\n            # lower.tail = FALSE)}\n            if (z &lt; 0) {\n                p[[i]] &lt;- 2 * pnorm(z, lower.tail = TRUE)\n            }\n            # alternatively, use if (t &lt; 0) {p[[i]] &lt;- pt(t, df = n-1,\n            # lower.tail = TRUE)}\n        }\n    }\n\n    curve(dnorm(x, mu0, sigma/sqrt(n)), mu0 - 4 * sigma/sqrt(n), mu0 + 4 * sigma/sqrt(n),\n        main = paste(\"Sampling Distribution Under Null Hypothesis\\n\n      Type I error rate from simulation = \",\n            length(p[p &lt; alpha])/k, sep = \"\"), xlab = \"x\", ylab = \"Pr(x)\", col = \"red\",\n        xlim = c(mu0 - 4 * sigma/sqrt(n), mu0 + 4 * sigma/sqrt(n)), ylim = c(0, dnorm(mu0,\n            mu0, sigma/sqrt(n))))\n    abline(h = 0)\n\n    if (alternative == \"less\") {\n        polygon(cbind(c(mu0 - 4 * sigma/sqrt(n), seq(from = mu0 - 4 * sigma/sqrt(n),\n            to = mu0 - qnorm(1 - alpha) * sigma/sqrt(n), length.out = 100), mu0 -\n            qnorm(1 - alpha) * sigma/sqrt(n))), c(0, dnorm(seq(from = mu0 - 4 * sigma/sqrt(n),\n            to = mu0 - qnorm(1 - alpha) * sigma/sqrt(n), length.out = 100), mean = mu0,\n            sd = sigma/sqrt(n)), 0), border = \"black\", col = \"grey\")\n        q &lt;- pnorm(mu0 - qnorm(1 - alpha) * sigma/sqrt(n), mean = mu0, sd = sigma/sqrt(n)) -\n            pnorm(mu0 - 4 * sigma/sqrt(n), mean = mu0, sd = sigma/sqrt(n))\n    }\n\n    if (alternative == \"greater\") {\n        polygon(cbind(c(mu0 + qnorm(1 - alpha) * sigma/sqrt(n), seq(from = mu0 +\n            qnorm(1 - alpha) * sigma/sqrt(n), to = mu0 + 4 * sigma/sqrt(n), length.out = 100),\n            mu0 + 4 * sigma/sqrt(n))), c(0, dnorm(seq(from = mu0 + qnorm(1 - alpha) *\n            sigma/sqrt(n), to = mu0 + 4 * sigma/sqrt(n), length.out = 100), mean = mu0,\n            sd = sigma/sqrt(n)), 0), border = \"black\", col = \"grey\")\n        q &lt;- pnorm(mu0 + 4 * sigma/sqrt(n), mean = mu0, sd = sigma/sqrt(n)) - pnorm(mu0 +\n            qnorm(1 - alpha) * sigma/sqrt(n), mean = mu0, sd = sigma/sqrt(n))\n    }\n    if (alternative == \"two.tailed\") {\n        polygon(cbind(c(mu0 - 4 * sigma/sqrt(n), seq(from = mu0 - 4 * sigma/sqrt(n),\n            to = mu0 - qnorm(1 - alpha/2) * sigma/sqrt(n), length.out = 100), mu0 -\n            qnorm(1 - alpha/2) * sigma/sqrt(n))), c(0, dnorm(seq(from = mu0 - 4 *\n            sigma/sqrt(n), to = mu0 - qnorm(1 - alpha/2) * sigma/sqrt(n), length.out = 100),\n            mean = mu0, sd = sigma/sqrt(n)), 0), border = \"black\", col = \"grey\")\n        polygon(cbind(c(mu0 + qnorm(1 - alpha/2) * sigma/sqrt(n), seq(from = mu0 +\n            qnorm(1 - alpha/2) * sigma/sqrt(n), to = mu0 + 4 * sigma/sqrt(n), length.out = 100),\n            mu0 + 4 * sigma/sqrt(n))), c(0, dnorm(seq(from = mu0 + qnorm(1 - alpha/2) *\n            sigma/sqrt(n), to = mu0 + 4 * sigma/sqrt(n), length.out = 100), mean = mu0,\n            sd = sigma/sqrt(n)), 0), border = \"black\", col = \"grey\")\n        q &lt;- pnorm(mu0 - qnorm(1 - alpha/2) * sigma/sqrt(n), mean = mu0, sd = sigma/sqrt(n)) -\n            pnorm(mu0 - 4 * sigma/sqrt(n), mean = mu0, sd = sigma/sqrt(n)) + pnorm(mu0 +\n            4 * sigma/sqrt(n), mean = mu0, sd = sigma/sqrt(n)) - pnorm(mu0 + qnorm(1 -\n            alpha/2) * sigma/sqrt(n), mean = mu0, sd = sigma/sqrt(n))\n    }\n    # print(round(q,digits=3)) this prints area in the shaded portion(s) of the\n    # curve\n    return(length(p[p &lt; alpha])/k)\n    # returns the proportion of simulations where p &lt; alpha or\n    # `return(mean(p&lt;alpha))`\n}\n\nHere, \\(\\mu_0\\) is our sample mean and \\(\\sigma\\) is our sample standard deviation.\nCan you explain what each step of this code is doing?\nNow, run our Type I error test function with a couple of different values of \\(\\mu_0\\), \\(\\sigma\\), and \\(\\alpha\\). What error rates are returned? They should be always be close to \\(\\alpha\\)!\n\neI &lt;- typeI(mu0 = -3, sigma = 2, n = 1000, alternative = \"greater\", alpha = 0.05)\n\n\n\n\n\n\n\neI &lt;- typeI(mu0 = 5, sigma = 2, n = 1000, alternative = \"less\", alpha = 0.01)\n\n\n\n\n\n\n\neI &lt;- typeI(mu0 = 10, sigma = 4, n = 1000, alternative = \"two.tailed\", alpha = 0.05)\n\n\n\n\n\n\n\n\n\nCHALLENGE\nHow does the Type I error rate change with \\(n\\)? With \\(\\sigma\\)? With \\(\\alpha\\)?\n\nHINT: It shouldn’t change much… the Type I error rate is defined by \\(\\alpha\\)!",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Error, Power, and Effect Size</span>"
    ]
  },
  {
    "objectID": "17-module.html#multiple-comparison-corrections",
    "href": "17-module.html#multiple-comparison-corrections",
    "title": "17  Error, Power, and Effect Size",
    "section": "17.5 Multiple Comparison Corrections",
    "text": "17.5 Multiple Comparison Corrections\nOne way we can address the multiple testing problem mentioned above is by using what is called the Bonferroni correction, which suggests that when doing a total of \\(k\\) independent hypothesis tests, each with a significance level of \\(\\alpha\\), we should adjust the \\(\\alpha\\) level we use to interpret statistical significance as follow: \\(\\alpha_B = \\alpha/k\\). For example, if we run 10 independent hypothesis tests, then we should set our adjusted \\(\\alpha\\) level for each test as 0.05/10 = 0.005.\nWith the Bonferroni correction, we are essentially saying that we want to control the rate at which we have even one incorrect rejection of \\(H_0\\) given the entire family of tests we do. This is also referred to as limiting the “family-wise error rate” to level \\(\\alpha\\).\n\n# example Bonferroni correction\nalpha &lt;- 0.05\npvals &lt;- c(1e-04, 0.003, 0.005, 0.01, 0.02, 0.04, 0.045, 0.11, 0.18, 0.23)\n# vector of p values associated with a set of tests\nsig &lt;- pvals &lt;= alpha/length(pvals)\n# returns a boolean vector of pvals less than or equal to the adjusted alpha\nsig  # first 3 values are less than the adjusted alpha\n\n##  [1]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nMany statisticians consider the Bonferroni correction to be an overly conservative one, and there are other corrections we might use to account for multiple testing.\nOne common alternative is the Benjamini & Hochberg correction, which is less conservative. It attempts to control for the “false discovery rate”, which is different than the “family-wise error rate”. Here, we aim to limit the number of false “discoveries” (i.e., incorrect rejections of the null hypothesis) out of a set of discoveries (i.e., out of the set of results where we would reject the null hypothesis) to \\(\\alpha\\).\n\nCalculate \\(p\\) values for all tests\nOrder \\(p\\) values from smallest to largest (from \\(p_1\\) to \\(p_m\\))\nCall any \\(p\\) value where \\(p_i ≤ \\alpha \\times i/m\\) significant\n\n\nalpha &lt;- 0.05\npsig &lt;- NULL\npvals &lt;- c(1e-04, 0.003, 0.005, 0.01, 0.02, 0.04, 0.045, 0.11, 0.18, 0.27)\nfor (i in 1:length(pvals)) {\n    psig[i] &lt;- alpha * i/length(pvals)\n}\nd &lt;- tibble(rank = c(1:10), pvals = pvals, psig = psig)\np &lt;- ggplot(data = d, aes(x = rank, y = pvals)) + geom_point() + geom_line(aes(x = rank,\n    y = psig))\np\n\n\n\n\n\n\n\nsig &lt;- pvals &lt;= psig  # vector of significant pvalues\nsig  # first 5 values are less than the adjusted alpha, the remaining are not\n\n##  [1]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n\n\nAn alternative way of thinking about this is to adjust p values themselves rather than the \\(\\alpha\\) levels. We can do this with a built-in R function, p.adjust(), which makes the calculation easy. Using this function, we can specify the kind of correction method we want to apply.\n\nsig &lt;- p.adjust(pvals, method = \"bonferroni\") &lt;= 0.05\nsig  # first 3 adjusted p values are less alpha\n\n##  [1]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\nsig &lt;- p.adjust(pvals, method = \"BH\") &lt;= 0.05\nsig  # first 5 adjusted p values are less alpha\n\n##  [1]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Error, Power, and Effect Size</span>"
    ]
  },
  {
    "objectID": "17-module.html#type-ii-error",
    "href": "17-module.html#type-ii-error",
    "title": "17  Error, Power, and Effect Size",
    "section": "17.6 Type II Error",
    "text": "17.6 Type II Error\nBy reducing the \\(\\alpha\\) level we use as our criterion for statistical significance, we can reduce the chance of committing a Type I error (i.e., of incorrectly rejecting a true null hypothesis)… but doing so directly increases our chance of committing a Type II error (i.e., of incorrectly failing to reject a false null). The shaded area in the figure below, \\(\\beta\\), is the probability of incorrectly failing to reject the null…\n\n\n\n\n\n\n\n\n\nIt should be clear from this figure that if the critical value (which, again, is defined by \\(\\alpha\\)) is shifted to the right (i.e., if we reduce \\(\\alpha\\)), or if \\(\\mu\\) under the alternative hypothesis shifts left, then \\(\\beta\\), the area under the alternative hypothesis distribution curve to the left of the critical value, increases! Intuitively, this makes sense: the lower the difference between the true \\(\\mu_A\\) value and \\(\\mu_0\\) and/or the higher the \\(\\alpha\\) level, the harder it will be to reject the null hypothesis that \\(\\mu\\) = \\(\\mu_0\\).\nIn practice, we cannot usually calculate \\(\\beta\\) because of the need to know where the true distribution is really centered (i.e., we need to know the value of \\(\\mu_A\\), which is often unknown). However, we can explore via simulation what \\(\\beta\\) is expected to look like under different alternative hypotheses (e.g., under different \\(\\mu_A\\)) and under different sample sizes and \\(\\alpha\\) levels.\nLet’s do this using the simulation approach we developed above. Again, we will write some code to simulate a bunch of random datasets, this time drawn from a normal distribution associated with a particular alternative hypothesis, \\(H_A\\), that we define… i.e., where we specify \\(\\mu_A\\) and \\(\\sigma\\), i.e., a mean and a standard deviation. We then calculate a \\(Z\\) (or \\(T\\)) statistic based on each sample dataset relative to \\(\\mu_0\\), the expected mean under \\(H_0\\), and determine the associated p value for each one. Based on this, we can calculate the Type II error rate… the proportion of times that, based on our sample, we would conclude that it was drawn from the \\(H_0\\) distribution rather than the \\(H_A\\) distribution that we set to be true. Note that, as above, we can use the “t” family of functions in lieu of the “norm” family.\n\ntypeII &lt;- function(mu0, muA, sigma, n, alternative = \"two.tailed\", alpha = 0.05,\n    k = 10000) {\n    p &lt;- rep(NA, k)  # sets up a vector of empty p values\n    for (i in 1:k) {\n        x &lt;- rnorm(n = n, mean = muA, sd = sigma)\n        # draw a sample of size n from Ha\n        m &lt;- mean(x)\n        s &lt;- sd(x)\n        z &lt;- (m - mu0)/(s/sqrt(n))\n        # calculates the Z statistic for the sample drawn from Ha relative to\n        # the null distribution or... t &lt;- (m-mu0)/(s/sqrt(n)) # calculates the\n        # t statistic\n        if (alternative == \"greater\") {\n            p[[i]] &lt;- pnorm(z, lower.tail = FALSE)\n            # calculates the associated p value for the Z statistic or...\n            # p[[i]] &lt;- pt(t,lower.tail=FALSE,df=n-1) calculates the associated\n            # p value for the t statistic\n            hyp &lt;- \"muA &gt; mu0\"\n        }\n        if (alternative == \"less\") {\n            p[[i]] &lt;- pnorm(z, lower.tail = TRUE)\n            # or... p[[i]] &lt;- pt(t, lower.tail = TRUE, df = n - 1)\n            hyp &lt;- \"muA &lt; mu0\"\n        }\n        if (alternative == \"two.tailed\") {\n            if (z &gt; 0) {\n                p[[i]] &lt;- 2 * pnorm(z, lower.tail = FALSE)\n            }\n            if (z &lt; 0) {\n                p[[i]] &lt;- 2 * pnorm(z, lower.tail = TRUE)\n            }\n            # if (t &gt; 0) {p[[i]] &lt;- 2 * pt(t, lower.tail = FALSE, df = n-1)} if\n            # (t &lt; 0) {p[[i]] &lt;- 2 * pt(t, lower.tail = TRUE, df = n-1)}\n            hyp &lt;- \"muA ≠ mu0\"\n        }\n    }\n\n    curve(dnorm(x, mu0, sigma/sqrt(n)), mu0 - 4 * sigma/sqrt(n), mu0 + 4 * sigma/sqrt(n),\n        main = paste(\"Sampling Distributions Under Null (red) and Alternative (blue) Hypotheses\\n\n      Type II error rate from simulation = \",\n            length(p[p &gt;= alpha])/k, sep = \"\"), xlab = \"x\", ylab = \"Pr(x)\", col = \"red\",\n        xlim = c(min(c(mu0 - 4 * sigma/sqrt(n), muA - 4 * sigma/sqrt(n))), max(c(mu0 +\n            4 * sigma/sqrt(n), muA + 4 * sigma/sqrt(n)))), ylim = c(0, max(c(dnorm(mu0,\n            mu0, sigma/sqrt(n))), dnorm(muA, muA, sigma/sqrt(n)))))\n\n    curve(dnorm(x, muA, sigma/sqrt(n)), muA - 4 * sigma/sqrt(n), muA + 4 * sigma/sqrt(n),\n        col = \"blue\", add = TRUE)\n\n    abline(h = 0)\n\n    if (alternative == \"less\") {\n        polygon(cbind(c(mu0 - qnorm(1 - alpha) * sigma/sqrt(n), seq(from = mu0 -\n            qnorm(1 - alpha) * sigma/sqrt(n), to = muA + 4 * sigma/sqrt(n), length.out = 100),\n            muA + 4 * sigma/sqrt(n))), c(0, dnorm(seq(from = mu0 - qnorm(1 - alpha) *\n            sigma/sqrt(n), to = muA + 4 * sigma/sqrt(n), length.out = 100), mean = muA,\n            sd = sigma/sqrt(n)), 0), border = \"black\", col = \"grey\")\n        abline(v = mu0 - qnorm(1 - alpha) * sigma/sqrt(n), col = \"black\", lty = 3,\n            lwd = 2)\n    }\n\n    if (alternative == \"greater\") {\n        polygon(cbind(c(muA - 4 * sigma/sqrt(n), seq(from = muA - 4 * sigma/sqrt(n),\n            to = mu0 + qnorm(1 - alpha) * sigma/sqrt(n), length.out = 100), mu0 +\n            qnorm(1 - alpha) * sigma/sqrt(n))), c(0, dnorm(seq(from = muA - 4 * sigma/sqrt(n),\n            to = mu0 + qnorm(1 - alpha) * sigma/sqrt(n), length.out = 100), mean = muA,\n            sd = sigma/sqrt(n)), 0), border = \"black\", col = \"grey\")\n        abline(v = mu0 + qnorm(1 - alpha) * sigma/sqrt(n), col = \"black\", lty = 3,\n            lwd = 2)\n    }\n\n    if (alternative == \"two.tailed\") {\n        abline(v = mu0 - qnorm(1 - alpha/2) * sigma/sqrt(n), col = \"black\", lty = 3,\n            lwd = 2)\n        abline(v = mu0 + qnorm(1 - alpha/2) * sigma/sqrt(n), col = \"black\", lty = 3,\n            lwd = 2)\n        if (z &gt; 0) {\n            # greater\n            polygon(cbind(c(muA - 4 * sigma/sqrt(n), seq(from = muA - 4 * sigma/sqrt(n),\n                to = mu0 + qnorm(1 - alpha/2) * sigma/sqrt(n), length.out = 100),\n                mu0 + qnorm(1 - alpha/2) * sigma/sqrt(n))), c(0, dnorm(seq(from = muA -\n                4 * sigma/sqrt(n), to = mu0 + qnorm(1 - alpha/2) * sigma/sqrt(n),\n                length.out = 100), mean = muA, sd = sigma/sqrt(n)), 0), border = \"black\",\n                col = \"grey\")\n        }\n\n        if (z &lt; 0) {\n            # less\n            polygon(cbind(c(mu0 - qnorm(1 - alpha/2) * sigma/sqrt(n), seq(from = mu0 -\n                qnorm(1 - alpha/2) * sigma/sqrt(n), to = muA + 4 * sigma/sqrt(n),\n                length.out = 100), muA + 4 * sigma/sqrt(n))), c(0, dnorm(seq(from = mu0 -\n                qnorm(1 - alpha/2) * sigma/sqrt(n), to = muA + 4 * sigma/sqrt(n),\n                length.out = 100), mean = muA, sd = sigma/sqrt(n)), 0), border = \"black\",\n                col = \"grey\")\n        }\n    }\n\n    return(length(p[p &gt;= alpha])/k)\n}\n\n\nCHALLENGE\nExplore this function using different values of \\(\\mu_0\\), \\(\\sigma\\), \\(n\\), and different types of one- and two-tailed tests.\n\n# Ha &lt; H0\neII &lt;- typeII(mu0 = 5, muA = 2, sigma = 4, n = 18, alternative = \"less\")\n\n\n\n\n\n\n\n# Ha ≠ H0\neII &lt;- typeII(mu0 = 5, muA = 7, sigma = 2, n = 15, alternative = \"two.tailed\")\n\n\n\n\n\n\n\n# Ha &gt; H0\neII &lt;- typeII(mu0 = 0, muA = 0.5, sigma = 1, n = 30, alternative = \"greater\", alpha = 0.01)\n\n\n\n\n\n\n\n\n\nWhat happens if you increase \\(\\sigma\\) keeping other values constant?\nWhat happens if you reduce \\(n\\) or increase \\(n\\)?\nWhat happens if you increase the difference between \\(\\mu_A\\) and \\(\\mu_0\\)?",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Error, Power, and Effect Size</span>"
    ]
  },
  {
    "objectID": "17-module.html#power-and-effect-size",
    "href": "17-module.html#power-and-effect-size",
    "title": "17  Error, Power, and Effect Size",
    "section": "17.7 Power and Effect Size",
    "text": "17.7 Power and Effect Size\nPower is the probability of correctly rejecting a null hypothesis that is untrue. For a test that has a Type II error rate of \\(\\beta\\), the statistical power is defined, simply, as \\(1-\\beta\\). Power values of 0.8 or greater are conventionally considered to be “high”. Power for any given test depends on the difference in means between groups or treatments, \\(\\alpha\\), \\(n\\), and \\(\\sigma\\).\nGenerally speaking, effect size is a quantitative measure of the strength of a phenomenon. Here, we are interested in comparing two sample means, and the most common way to describe the effect size is as a standardized difference between the means of the groups being compared. In this case, we divide the absolute values of the difference between the means by the standard deviation: \\(\\frac{\\vert(\\mu_0 - \\mu_A)\\vert}{\\sigma}\\). This results in a scaleless measure. Conventionally, effect sizes of 0.2 or less are considered to be low and of 0.8 or greater are considered to be high.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Error, Power, and Effect Size</span>"
    ]
  },
  {
    "objectID": "17-module.html#visualizing-power-and-effect-size",
    "href": "17-module.html#visualizing-power-and-effect-size",
    "title": "17  Error, Power, and Effect Size",
    "section": "17.8 Visualizing Power and Effect Size",
    "text": "17.8 Visualizing Power and Effect Size\nThe code below lets you explore power and effect size interactively. You can use the sliders to set \\(\\mu_0\\) and \\(\\mu_A\\) for a one-sample test (or, alternatively, think about these as \\(\\mu_1\\) and \\(\\mu_2\\) for a two-sample test), \\(\\sigma\\), \\(\\alpha\\), and \\(n\\), and you can choose whether you are testing a one-sided hypothesis of \\(\\mu_A\\) being “greater” or “less” than \\(\\mu_0\\) or are testing the two-sided hypothesis that \\(\\mu_A\\) ≠ \\(\\mu_0\\) (“two.tailed”). The figure will output power and effect size. Note that \\(n\\) is the number of observations (or differences) being considered and \\(\\sigma\\) is the standard deviation in the observation (or difference values).\nFirst, we define a function that plots the relationship between these factors and power:\n\npower.plot &lt;- function(mu0, muA, n, sigma, alpha, alternative = \"two.tailed\") {\n    pow &lt;- 0\n    z &lt;- (muA - mu0)/(sigma/sqrt(n))\n    g &lt;- ggplot(data.frame(mu = c(min(mu0 - 4 * sigma/sqrt(n), muA - 4 * sigma/sqrt(n)),\n        max(mu0 + 4 * sigma/sqrt(n), muA + 4 * sigma/sqrt(n)))), aes(x = mu)) + ggtitle(paste0(\"Explore Power and Effect Size for Z Test\n      for a sample of size \",\n        n))\n\n    g &lt;- g + ylim(c(0, max(dnorm(mu0, mu0, sigma/sqrt(n)) + 0.1, dnorm(muA, muA,\n        sigma/sqrt(n)) + 0.1)))\n\n    g &lt;- g + stat_function(fun = dnorm, geom = \"line\", args = list(mean = mu0, sd = sigma/sqrt(n)),\n        size = 1, col = \"red\", show.legend = TRUE)\n    g &lt;- g + stat_function(fun = dnorm, geom = \"line\", args = list(mean = muA, sd = sigma/sqrt(n)),\n        size = 1, col = \"blue\", show.legend = TRUE)\n\n    if (alternative == \"greater\") {\n        if (z &gt; 0) {\n            xcrit = mu0 + qnorm(1 - alpha) * sigma/sqrt(n)\n            g &lt;- g + geom_segment(x = xcrit, y = 0, xend = xcrit, yend = max(dnorm(mu0,\n                mu0, sigma/sqrt(n)) + 0.025, dnorm(muA, muA, sigma/sqrt(n)) + 0.025),\n                size = 0.5, linetype = 3)\n            g &lt;- g + geom_polygon(data = data.frame(cbind(x = c(xcrit, seq(from = xcrit,\n                to = muA + 4 * sigma/sqrt(n), length.out = 1000), muA + 4 * sigma/sqrt(n)),\n                y = c(0, dnorm(seq(from = xcrit, to = muA + 4 * sigma/sqrt(n), length.out = 1000),\n                  mean = muA, sd = sigma/sqrt(n)), 0))), aes(x = x, y = y), fill = \"blue\",\n                alpha = 0.5)\n            pow &lt;- pnorm(muA + 4 * sigma/sqrt(n), muA, sigma/sqrt(n)) - pnorm(xcrit,\n                muA, sigma/sqrt(n))\n        }\n    }\n\n    if (alternative == \"less\") {\n        if (z &lt; 0) {\n            xcrit = mu0 - qnorm(1 - alpha) * sigma/sqrt(n)\n            g &lt;- g + geom_segment(x = xcrit, y = 0, xend = xcrit, yend = max(dnorm(mu0,\n                mu0, sigma/sqrt(n)) + 0.025, dnorm(muA, muA, sigma/sqrt(n)) + 0.025),\n                size = 0.5, linetype = 3)\n            g &lt;- g + geom_polygon(data = data.frame(cbind(x = c(muA - 4 * sigma/sqrt(n),\n                seq(from = muA - 4 * sigma/sqrt(n), to = xcrit, length.out = 1000),\n                xcrit), y = c(0, dnorm(seq(from = muA - 4 * sigma/sqrt(n), to = xcrit,\n                length.out = 1000), mean = muA, sd = sigma/sqrt(n)), 0))), aes(x = x,\n                y = y), fill = \"blue\", alpha = 0.5)\n            pow &lt;- pnorm(xcrit, muA, sigma/sqrt(n)) - pnorm(muA - 4 * sigma/sqrt(n),\n                muA, sigma/sqrt(n))\n        }\n    }\n\n    if (alternative == \"two.tailed\") {\n        if (z &gt; 0) {\n            xcrit = mu0 + qnorm(1 - alpha/2) * sigma/sqrt(n)\n            g &lt;- g + geom_segment(x = xcrit, y = 0, xend = xcrit, yend = max(dnorm(mu0,\n                mu0, sigma/sqrt(n)) + 0.025, dnorm(muA, muA, sigma/sqrt(n)) + 0.025),\n                size = 0.5, linetype = 3)\n            g &lt;- g + geom_polygon(data = data.frame(cbind(x = c(xcrit, seq(from = xcrit,\n                to = muA + 4 * sigma/sqrt(n), length.out = 1000), muA + 4 * sigma/sqrt(n)),\n                y = c(0, dnorm(seq(from = xcrit, to = muA + 4 * sigma/sqrt(n), length.out = 1000),\n                  mean = muA, sd = sigma/sqrt(n)), 0))), aes(x = x, y = y), fill = \"blue\",\n                alpha = 0.5)\n            pow &lt;- pnorm(muA + 4 * sigma/sqrt(n), muA, sigma/sqrt(n)) - pnorm(xcrit,\n                muA, sigma/sqrt(n))\n        }\n\n        if (z &lt; 0) {\n            xcrit = mu0 - qnorm(1 - alpha/2) * sigma/sqrt(n)\n            g &lt;- g + geom_segment(x = xcrit, y = 0, xend = xcrit, yend = max(dnorm(mu0,\n                mu0, sigma/sqrt(n)) + 0.025, dnorm(muA, muA, sigma/sqrt(n)) + 0.025),\n                size = 0.5, linetype = 3)\n            g &lt;- g + geom_polygon(data = data.frame(cbind(x = c(muA - 4 * sigma/sqrt(n),\n                seq(from = muA - 4 * sigma/sqrt(n), to = xcrit, length.out = 1000),\n                xcrit), y = c(0, dnorm(seq(from = muA - 4 * sigma/sqrt(n), to = xcrit,\n                length.out = 1000), mean = muA, sd = sigma/sqrt(n)), 0))), aes(x = x,\n                y = y), fill = \"blue\", alpha = 0.5)\n            pow &lt;- pnorm(xcrit, muA, sigma/sqrt(n)) - pnorm(muA - 4 * sigma/sqrt(n),\n                muA, sigma/sqrt(n))\n        }\n    }\n\n    g &lt;- g + annotate(\"text\", x = max(mu0, muA) + 2 * sigma/sqrt(n), y = max(dnorm(mu0,\n        mu0, sigma/sqrt(n)) + 0.075, dnorm(muA, muA, sigma/sqrt(n)) + 0.075), label = paste(\"Effect Size = \",\n        round(abs((muA - mu0))/sigma, digits = 3), \"\\nPower = \", round(pow, digits = 3),\n        sep = \"\"))\n    g &lt;- g + annotate(\"text\", x = min(mu0, muA) - 2 * sigma/sqrt(n), y = max(dnorm(mu0,\n        mu0, sigma/sqrt(n)) + 0.075, dnorm(muA, muA, sigma/sqrt(n)) + 0.075), label = \"Red = mu0\\nBlue = muA\")\n    g\n}\n\n\nmanipulate(power.plot(mu0, muA, n, sigma, alpha, alternative), mu0 = slider(-10,\n    10, step = 0.1, initial = 0), muA = slider(-10, 10, step = 0.1, initial = 2),\n    n = slider(1, 50, step = 1, initial = 15), sigma = slider(1, 4, step = 0.1, initial = 2),\n    alpha = slider(0.01, 0.1, step = 0.01, initial = 0.05), alternative = picker(\"two.tailed\",\n        \"greater\", \"less\"))\n\nIn most cases, since we are dealing with limited samples from a population, we will want to use the t rather than the normal distribution as the basis for making our power evaluations. The power.t.test() function lets us easily implement power calculations based on the t distribution, and the results of using it should be very similar to those we found above by simulation. The power.t.test() function takes as possible arguments the sample size, n= (\\(n\\)), the difference, delta= (\\(\\delta\\)) between group means, the standard deviation of the differences between means (sd=, \\(\\sigma\\)), the significance level (sig.level=, \\(\\alpha\\)), the test type= (“two.sample”, “one.sample”, or “paired”), the alternative= test to run (“two.sided”, “one.sided”), and the desired_power= (\\(1-\\beta\\)). Power, \\(n\\), or the difference between means is left as null and the other arguments are specified. The function then calculates the missing argument.\n\nCHALLENGE\nUsing the code below, which graphs the Type II error rate (\\(\\beta\\)) and power (\\(1-\\beta\\)) for T tests (using the power.t.test() function), explore the effects of changing \\(\\alpha\\), within sample variability (\\(\\sigma\\)), and the difference between sample means (i.e., \\(\\mu_0\\) and \\(\\mu_A\\) for a one sample test (or, equivalently, \\(\\mu_1\\) and \\(\\mu_2\\) for a two sample test). The plot shows the effect size, given the difference between the means, (i.e., \\(\\vert(\\mu_0 - \\mu_A)\\vert/\\sigma\\)) and marks the sample size (\\(n\\)) needed to achieve a power of 0.8.\n\npower.test &lt;- function(mu0, muA, sigma, alpha = 0.05, desired_power = 0.8, type,\n    alternative) {\n\n    p &lt;- 0\n    for (i in 2:200) {\n        x &lt;- power.t.test(n = i, delta = abs(muA - mu0), sd = sigma, sig.level = alpha,\n            power = NULL, type = type, alternative = alternative)\n        p &lt;- c(p, x$power)\n    }\n    d &lt;- data.frame(cbind(1:200, p, 1 - p))\n    critn &lt;- 0\n    for (i in 1:199) {\n        if (p[i] &lt; desired_power && p[i + 1] &gt;= desired_power) {\n            critn &lt;- i + 1\n        } else {\n            critn &lt;- critn\n        }\n    }\n    names(d) &lt;- c(\"n\", \"power\", \"beta\")\n    g &lt;- ggplot(data = d) + xlab(\"sample size n\") + ylab(\"Type II Error Rate, Beta  (Red)\\nand\\nPower, 1-Beta (Blue)\") +\n        ggtitle(\"Power Explorer for T Tests\\n\n      (assuming equal n and variance across the two groups)\") +\n        ylim(0, 1) + geom_point(aes(x = n, y = power), color = \"blue\", alpha = 0.5,\n        size = 0.5) + geom_line(aes(x = n, y = power), colour = \"blue\", alpha = 0.5) +\n        geom_line(aes(x = n, y = desired_power), color = \"black\", lty = 3) + geom_point(aes(x = n,\n        y = beta), color = \"red\", alpha = 0.5, size = 0.5) + geom_line(aes(x = n,\n        y = beta), color = \"red\", alpha = 0.5) + geom_linerange(aes(x = critn, ymin = 0,\n        ymax = desired_power), color = \"black\", alpha = 0.25, lwd = 0.1) + annotate(\"text\",\n        x = 150, y = 0.5, label = paste(\"Effect Size = \", round(abs(mu0 - muA)/sigma,\n            digits = 3), \"\\nCritical n = \", critn, sep = \"\"))\n    print(g)\n}\n\n\nmanipulate(power.test(mu0, muA, sigma, alpha, desired_power, type, alternative),\n    mu0 = slider(-10, 10, initial = 3, step = 1), muA = slider(-10, 10, initial = 0,\n        step = 1), sigma = slider(1, 10, initial = 3, step = 1), alpha = slider(0.01,\n        0.1, initial = 0.05, step = 0.01), desired_power = slider(0, 1, initial = 0.8,\n        step = 0.05), alternative = picker(\"two.sided\", \"one.sided\"), type = picker(\"two.sample\",\n        \"one.sample\", \"paired\"))",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Error, Power, and Effect Size</span>"
    ]
  },
  {
    "objectID": "17-module.html#concept-review",
    "href": "17-module.html#concept-review",
    "title": "17  Error, Power, and Effect Size",
    "section": "Concept Review",
    "text": "Concept Review\n\nClassical null hypothesis significance testing tries to minimize the rate of type I error (\\(\\alpha\\), i.e., the probability of incorrectly rejecting a null hypothesis when it’s correct)\nWhen we try to reduce the rate of type I error (e.g., by decreasing \\(\\alpha\\)), all else being equal, we increase the rate of type II error (\\(\\beta\\), i.e., the probability of incorrectly failing to reject the null hypothesis when it’s wrong)\nThe power of a statistical test, \\(1-\\beta\\), is our ability to correctly reject the null hypothesis when it is false\nPower depends on the difference between sample means, the standard deviation in the population the sample is drawn from, and the sample size",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Error, Power, and Effect Size</span>"
    ]
  },
  {
    "objectID": "18-module.html",
    "href": "18-module.html",
    "title": "18  Introduction to Linear Modeling",
    "section": "",
    "text": "18.1 Objectives",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Introduction to Linear Modeling</span>"
    ]
  },
  {
    "objectID": "18-module.html#objectives",
    "href": "18-module.html#objectives",
    "title": "18  Introduction to Linear Modeling",
    "section": "",
    "text": "The objective of this module is to discuss the use of simple linear regression to explore the relationship among two continuous variables: a single predictor variable and a single response variable.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Introduction to Linear Modeling</span>"
    ]
  },
  {
    "objectID": "18-module.html#preliminaries",
    "href": "18-module.html#preliminaries",
    "title": "18  Introduction to Linear Modeling",
    "section": "18.2 Preliminaries",
    "text": "18.2 Preliminaries\n\nInstall the following package in R: {lmodel2}\nInstall and load the following package in R: {broom}\nLoad {tidyverse}, {manipulate}, {patchwork}, and {infer}",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Introduction to Linear Modeling</span>"
    ]
  },
  {
    "objectID": "18-module.html#covariance-and-correlation",
    "href": "18-module.html#covariance-and-correlation",
    "title": "18  Introduction to Linear Modeling",
    "section": "18.3 Covariance and Correlation",
    "text": "18.3 Covariance and Correlation\nSo far, we have looked principally at single variables, but one of the main things we are often interested in is the relationships among two or more variables. Regression modeling is one of the most powerful and important set of tools for looking at relationships among more than one variable. With our zombie apocalypse survivors dataset, we started to do this using simple bivariate scatterplots… let’s look at those data again and do a simple bivariate plot of height by weight.\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/zombies.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\nhead(d)\n\n## # A tibble: 6 × 10\n##      id first_name last_name gender height weight zombies_killed\n##   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;          &lt;dbl&gt;\n## 1     1 Sarah      Little    Female   62.9   132.              2\n## 2     2 Mark       Duncan    Male     67.8   146.              5\n## 3     3 Brandon    Perez     Male     72.1   153.              1\n## 4     4 Roger      Coleman   Male     66.8   130.              5\n## 5     5 Tammy      Powell    Female   64.7   132.              4\n## 6     6 Anthony    Green     Male     71.2   153.              1\n## # ℹ 3 more variables: years_of_education &lt;dbl&gt;, major &lt;chr&gt;, age &lt;dbl&gt;\n\nplot(data = d, height ~ weight)\n\n\n\n\n\n\n\n\nThese variables clearly seem to be related to one another, in that as weight increases, height increases. There are a couple of different ways we can quantify the relationship between these variables. One is by calculating the covariance, which expresses how much two numeric variables “change together” and whether that change is positive or negative.\nRecall that the variance in a variable is simply the sum of the squared deviatiations of each observation from the mean divided by sample size (n for population variance or n-1 for sample variance). Thus, sample variance is:\n\\[var(x)=\\sum\\frac{(x-\\bar{x})^2}{(n-1)}\\]\nSimilarly, the covariance is simply the product of the deviations of each of two variables from their respective means divided by sample size. Thus, for two vectors, \\(x\\) and \\(y\\), each of length \\(n\\), representing two variables describing a sample…\n\\[cov(x,y) = \\sum\\frac{(x-\\bar{x})(y-\\bar{y})}{(n-1)}\\]\n\nCHALLENGE\nWhat is the covariance between zombie apocalypse survivor weight and zombie apocalypse survivor height? What does it mean if the covariance is positive versus negative? Does it matter if you switch the order of the two variables?\n\n\nShow Code\nw &lt;- d$weight\nh &lt;- d$height\nn &lt;- length(w)  # or length(h)\ncov_wh &lt;- sum((w - mean(w)) * (h - mean(h)))/(n - 1)\ncov_wh\n\n\nShow Output\n## [1] 66.03314\n\n\n\nThe built-in R function cov() yields the same.\n\ncov(w, h)\n\n## [1] 66.03314\n\n\nWe often describe the relationship between two variables using the correlation coefficient, which is a standardized form of the covariance that summarizes, on a scale from -1 to +1, both the strength and direction of a relationship. The correlation is simply the covariance divided by the product of the standard deviations of the two variables.\n\\[cor(x,y)=\\frac{cov(x,y)}{sd(x)sd(y)}\\]\n\n\nCHALLENGE\nCalculate the correlation between zombie apocalypse survivor weight and zombie apocalypse survivor height.\n\n\nShow Code\nsd_w &lt;- sd(w)\nsd_h &lt;- sd(h)\ncor_wh &lt;- cov_wh/(sd_w * sd_h)\ncor_wh\n\n\nShow Output\n## [1] 0.8325862\n\n\n\nAgain, there is a built-in R function cor() which yields the same.\n\ncor(w, h)\n\n## [1] 0.8325862\n\ncor(w, h, method = \"pearson\")\n\n## [1] 0.8325862\n\n\nThis formulation of the correlation coefficient is referred to as Pearson’s product-moment correlation coefficient and is often abbreviated as \\(\\rho\\).\nThere are other, nonparametric forms of the correlation coefficient we might also calculate, which are based on the relationship among rank scores for the two variables:\n\ncor(w, h, method = \"kendall\")\n\n## [1] 0.6331932\n\ncor(w, h, method = \"spearman\")\n\n## [1] 0.82668",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Introduction to Linear Modeling</span>"
    ]
  },
  {
    "objectID": "18-module.html#regression",
    "href": "18-module.html#regression",
    "title": "18  Introduction to Linear Modeling",
    "section": "18.4 Regression",
    "text": "18.4 Regression\nRegression refers to a set of tools that lets us explore the relationships between variables further. In regression analysis, we are typically identifying and exploring linear models, or functions, that describe the relationship between variables. There are a couple of main purposes for undertaking regression analyses:\n\nTo use one or more variables to predict the value of another\nTo develop and choose among different models of the relationship between variables\nTo do analyses of covariation among sets of variables to identify/explore their relative explanatory power\n\nThe general purpose of linear regression is to come up with a model or function that estimates the expected value of one variable (a mean, a proportion, etc.), i.e., the response or outcome variable, given the particular value(s) of another variable (or set of variables), i.e., the predictor variable(s).\nWe are going to start off with simple bivariate regression, where we have a single predictor and a single response variable. In our case, we may be interested in coming up with a model that estimates the expected value for zombie apocalypse survivor height (as the response variable) given zombie apocalypse survivor weight (as the predictor variable). That is, we want to explore possible functions that link these two variables and choose the best one.\nIn general, the model for linear regression represents a dependent (or response) variable, \\(Y\\) as a linear function of an independent (or predictor) variable, \\(X\\).\n\\[Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\]\nThe function has two coefficients. The first, \\(\\beta_0\\) is the intercept, i.e., the value of \\(Y\\) when \\(X\\) = 0. The second \\(\\beta_1\\) is the slope of the line describing the relationship between the predictor and response. The error term, \\(\\epsilon_i\\), is a normal random variable, \\(\\epsilon_i \\sim N(0,\\sigma^2)\\), with the standard deviation assumed to be constant across all values of \\(X\\). A regression analysis calls for estimating the values of all three parameters (\\(\\beta_0\\), \\(\\beta_1\\), and the residual or error term). How this is accomplished will depend on what assumptions are employed in the analysis.\nLooking at our scatterplot above, it seems pretty clear that there is indeed some linear relationship among these variables, and so a reasonable function to connect height to weight should simply be some kind of line of best fit. Recall that the general formula for a line is:\n\\[\\hat{y} = slope \\times x + intercept\\]\nwhere \\(\\hat{y}\\) = our predicted (or expected, or mean) value for y given a particular value of x.\nIn regression parlance…\n\\[\\hat{y_i} = \\beta_1x_i + \\beta_0\\]\nHere, \\(\\beta_1\\) and \\(\\beta_0\\) are referred to as the regression coefficients, and it is those coefficients that our regression analysis is trying to estimate, while minimizing, according to some criterion, the error term. This process of estimation is called “fitting the model.”\n\n18.4.1 Ordinary Least Squares (OLS)\nA typical linear regression analysis further assumes that \\(X\\), our “independent” or predictor variable, is controlled and thus measured with much greater precision than \\(Y\\), our “dependent” or response variable. Thus the error, \\(\\epsilon_i\\) is assumed to be restricted to the \\(Y\\) dimension, with little or no error in measuring \\(X\\), and we employ “ordinary least squares” as our criterion for best fit.\nWhat does this mean? Well, we can imagine a family of lines with different slope (\\(\\beta_1\\)) and intercept (\\(\\beta_0\\)) going through any cloud of points, and one “best fit” criterion we could use is to find the line whose coefficients (\\(\\beta_1\\) and \\(\\beta_0\\), or slope and intercept) minimize the sum of the squared deviations of each observation in the \\(Y\\) direction from that predicted by the line. This is the basis of ordinary least squares or OLS regression. We want to wind up with an equation that tells us how \\(Y\\) varies in response to changes in \\(X\\).\nIn other words, we want to find \\(\\beta_1\\) and \\(\\beta_0\\) that minimizes…\n\\[\\sum(y_i-\\hat{y})^2\\]\nor, equivalently,\n\\[\\sum(y_i-(\\beta_1 x_i + \\beta_0))^2\\]\nIn terms of our variables, this is…\n\\[\\sum(height - (\\beta_1 weight + \\beta_0))^2\\]\nLet’s first fit such a model by hand… The first thing to do is estimate the slope, which we can do if we first “center” each of our variables by subtracting the mean from each value (essentially, this shifts the distribution to eliminate the intercept term).\n\nd &lt;- mutate(d, centered_height = height - mean(height))\nd &lt;- mutate(d, centered_weight = weight - mean(weight))\n\np1 &lt;- ggplot(data = d, aes(x = weight, y = height)) + geom_point()\np2 &lt;- ggplot(data = d, aes(x = centered_weight, y = centered_height)) + geom_point()\n\np1 + p2\n\n\n\n\n\n\n\n\nOnce we do this, we just need to minimize…\n\\[\\sum(y_{centered} - (\\beta_1 x_{centered}))^2\\]\nWe can explore finding the best slope (\\(\\beta_1\\)) for this line using an interactive approach.\nFirst we define a custom function…\n\nslope.test &lt;- function(beta1, data) {\n    g &lt;- ggplot(data = data, aes(x = centered_weight, y = centered_height))\n    g &lt;- g + geom_point()\n    g &lt;- g + geom_abline(intercept = 0, slope = beta1, size = 1, colour = \"blue\",\n        alpha = 1/2)\n    ols &lt;- sum((data$centered_height - beta1 * data$centered_weight)^2)\n    g &lt;- g + ggtitle(paste(\"Slope = \", beta1, \"\\nSum of Squared Deviations = \", round(ols,\n        3)))\n    g\n}\n\n… that we can then play with interactively!\n\nNOTE: The following code is not run… to use it, copy and paste it into RStudio and then run it.\n\n\nmanipulate(slope.test(beta1, data = d), beta1 = slider(-1, 1, initial = 0, step = 0.005))\n\nSimilarly, we can calculate \\(\\beta_1\\) analytically as follows …\n\\[\\beta_1 = cor(x,y)\\frac{sd(y)}{sd(x)}=\\frac{cov(x,y)}{var(x)}=\\frac{SS_{XY}}{SS_X}\\]\nWe can also calculate estimates for the standard errors in our regression coefficients analytically as follows:\n\\[SE_{\\beta_1} = \\sqrt\\frac{\\sum\\epsilon_i^2}{(n-2)\\sum(x_i -\\bar{x})^2} = \\sqrt\\frac{\\sum(\\hat{y_i}-\\bar{y})^2}{(n-2)\\sum(x_i -\\bar{x})^2}\\] \\[SE_{\\beta_0} = SE_{\\beta_1}\\sqrt\\frac{\\sum{x_i}^2}{n}\\] As for any standard errors, these are estimates of the standard deviation in the sampling distribution of the two regression coefficients.\n\n\nCHALLENGE\nSolve for \\(\\beta_1\\) by hand…\n\n\nShow Code\n(beta1 &lt;- cor(w, h) * (sd(h)/sd(w)))\n\n\nShow Output\n## [1] 0.1950187\n\n\n\nShow Code\n(beta1 &lt;- cov(w, h)/var(w))\n\n\nShow Output\n## [1] 0.1950187\n\n\n\nShow Code\n(beta1 &lt;- sum((h - mean(h)) * (w - mean(w)))/sum((w - mean(w))^2))\n\n\nShow Output\n## [1] 0.1950187\n\n\n\nThen, to find \\(\\beta_0\\), we can simply plug back into our original regression model. The line of best fit has to run through the centroid of our data points, which is the point determined by the mean of the \\(X\\) values and the mean of the \\(Y\\) values, so we can use the following:\n\\[\\bar{y}=\\beta_1\\bar{x}+\\beta_0\\]\nwhich, rearranged to solve for \\(\\beta_0\\) gives…\n\\[\\beta_0=\\bar{y}-\\beta_1\\bar{x}\\]\n\n\nCHALLENGE\nSolve for \\(\\beta_0\\) by hand…\n\n(beta0 &lt;- mean(h) - beta1 * mean(w))\n\n## [1] 39.56545\n\n\nNote that in the example above, we have taken our least squares criterion to mean minimizing the deviation of each of our \\(Y\\) variables from a line of best fit in a dimension perpendicular to the \\(Y\\) axis. In general, this kind of regression - where deviation is measured perpendicular to one of the axes - is known as Model I regression, and is used when the levels of the predictor variable are either measured without error (or, practically speaking, are measured with much less uncertainty than those of the response variable) or are set by the researcher (e.g., for defined treatment variables in an ecological experiment).\n\n\n18.4.2 The lm() Function\nThe function lm() (“linear model”) in R makes all of the calculations we did above for Model I regression very easy! Below, we pass the zombies dataframe (d) and variables directly to lm() and assign the result to an R object called m. We can then look at the various elements that R calculates about this model.\n\nm &lt;- lm(height ~ weight, data = d)\nm\n\n## \n## Call:\n## lm(formula = height ~ weight, data = d)\n## \n## Coefficients:\n## (Intercept)       weight  \n##      39.565        0.195\n\nnames(m)  # components of the object, m\n\n##  [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n##  [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n##  [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"\n\nm$coefficients  # regression coefficients\n\n## (Intercept)      weight \n##  39.5654460   0.1950187\n\nhead(m$model)  # x values and fitted y values\n\n##     height   weight\n## 1 62.88951 132.0872\n## 2 67.80277 146.3753\n## 3 72.12908 152.9370\n## 4 66.78484 129.7418\n## 5 64.71832 132.4265\n## 6 71.24326 152.5246\n\n\nApplying the tidy() function from the {broom} package to our model makes it easy to extract certain output of interest, such as beta coefficients.\n\ntidy(m)\n\n## # A tibble: 2 × 5\n##   term        estimate std.error statistic   p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)   39.6     0.596        66.4 0        \n## 2 weight         0.195   0.00411      47.5 2.65e-258\n\n\nThe glance() function also returns other information of interest about the model.\n\nglance(m)\n\n## # A tibble: 1 × 12\n##   r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n##       &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1     0.693         0.693  2.39     2255. 2.65e-258     1 -2289. 4583. 4598.\n## # ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nIn {ggplot}, we can easily create a plot that adds the linear model along with confidence intervals around the estimated value of y, or \\(\\hat{y}\\) at each x by calling geom_smooth() with the argument method= specified as “lm”. Those intervals are important for when we move on to talking about inference in the regression context.\n\ng &lt;- ggplot(data = d, aes(x = weight, y = height))\ng &lt;- g + geom_point()\ng &lt;- g + geom_smooth(method = \"lm\", formula = y ~ x)\ng\n\n\n\n\n\n\n\n\n\n\n18.4.3 Alternatives to OLS\nThe assumption of greater uncertainty in our response variable than in our predictor variable may be reasonable in controlled experiments, but for natural observations, measurement of the \\(X\\) variable also typically involves some error and, in fact, in many cases we may not be concered about PREDICTING \\(Y\\) from \\(X\\) but rather want to treat both \\(X\\) and \\(Y\\) as independent variables and explore the relationship between them or consider that both are dependent on some additional parameter, which may be unknown. That is, both are measured rather than “controlled” and both include uncertainty. We thus are not seeking an equation of how \\(Y\\) varies with changes in \\(X\\), but rather we are look for how they both co-vary in response to some other variable or process. Under these conditions Model II regression analysis may be more appropriate. In Model II approaches, a line of best fit is chosen that minimizes in some way the direct distance of each point to the best fit line. There are several different types of Model II regression, and which to use depends upon the specifics of the case. Common approaches are know as major axis, ranged major axis, and reduced major axis (a.k.a. standard major axis) regression.\nThe {lmodel2} package allows us to do Model II regression easily (as well as Model I). In this package, the signficance of the regression coefficients, which we discuss below, is determined based on permutation.\n\nlibrary(lmodel2)  # load the lmodel2 package\n# Run the regression\nmII &lt;- lmodel2(height ~ weight, data = d, range.y = \"relative\", range.x = \"relative\",\n    nperm = 1000)\nmII\n\n## \n## Model II regression\n## \n## Call: lmodel2(formula = height ~ weight, data = d, range.y =\n## \"relative\", range.x = \"relative\", nperm = 1000)\n## \n## n = 1000   r = 0.8325862   r-square = 0.6931998 \n## Parametric P-values:   2-tailed = 2.646279e-258    1-tailed = 1.32314e-258 \n## Angle between the two OLS regression lines = 4.677707 degrees\n## \n## Permutation tests of OLS, MA, RMA slopes: 1-tailed, tail corresponding to sign\n## A permutation test of r is equivalent to a permutation test of the OLS slope\n## P-perm for SMA = NA because the SMA slope cannot be tested\n## \n## Regression results\n##   Method Intercept     Slope Angle (degrees) P-perm (1-tailed)\n## 1    OLS  39.56545 0.1950187        11.03524       0.000999001\n## 2     MA  39.10314 0.1982313        11.21246       0.000999001\n## 3    SMA  33.92229 0.2342325        13.18287                NA\n## 4    RMA  36.80125 0.2142269        12.09153       0.000999001\n## \n## Confidence intervals\n##   Method 2.5%-Intercept 97.5%-Intercept 2.5%-Slope 97.5%-Slope\n## 1    OLS       38.39625        40.73464  0.1869597   0.2030778\n## 2     MA       37.92239        40.28020  0.1900520   0.2064362\n## 3    SMA       32.74259        35.06211  0.2263120   0.2424301\n## 4    RMA       35.51434        38.06296  0.2054593   0.2231695\n## \n## Eigenvalues: 351.6888 5.48735 \n## \n## H statistic used for computing C.I. of MA: 6.212738e-05\n\npar(mfrow = c(2, 2))\nplot(mII, \"OLS\")\nplot(mII, \"MA\")\nplot(mII, \"SMA\")\nplot(mII, \"RMA\")\n\n\n\n\n\n\n\ndetach(package:lmodel2)\n\nNote that, here, running lmodel2() and using OLS to detemine the best coefficients yields equivalent results to our Model I regression done above using lm().\n\nmI &lt;- lm(height ~ weight, data = d)\nsummary(mI)  # show lm() results\n\n## \n## Call:\n## lm(formula = height ~ weight, data = d)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -7.1519 -1.5206 -0.0535  1.5167  9.4439 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 39.565446   0.595815   66.41   &lt;2e-16 ***\n## weight       0.195019   0.004107   47.49   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.389 on 998 degrees of freedom\n## Multiple R-squared:  0.6932, Adjusted R-squared:  0.6929 \n## F-statistic:  2255 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n# show lmodel2() OLS results\nfilter(mII$regression.results, Method == \"OLS\")\n\n##   Method Intercept     Slope Angle (degrees) P-perm (1-tailed)\n## 1    OLS  39.56545 0.1950187        11.03524       0.000999001\n\npar(mfrow = c(1, 2))\nplot(mII, main = \"lmodel2() OLS\", xlab = \"weight\", ylab = \"height\")\nplot(data = d, height ~ weight, main = \"lm()\")\nabline(mI, col = \"red\")\n\n\n\n\n\n\n\n\n\n\nCHALLENGE\nUsing the zombie apocalypse survivors dataset, try the following…\n\nPlot survivor height as a function of age\nDerive by hand the ordinary least squares regression coefficients \\(\\beta_1\\) and \\(\\beta_0\\) for these data\nConfirm that you get the same results using the lm() function\nRepeat the analysis above for males and females separately\nDo your regression coefficients differ by sex? How might you determine this?\n\n\npar(mfrow = c(1, 1))\nplot(data = d, height ~ age)\n\n\n\n\n\n\n\nhead(d)\n\n## # A tibble: 6 × 12\n##      id first_name last_name gender height weight zombies_killed\n##   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;          &lt;dbl&gt;\n## 1     1 Sarah      Little    Female   62.9   132.              2\n## 2     2 Mark       Duncan    Male     67.8   146.              5\n## 3     3 Brandon    Perez     Male     72.1   153.              1\n## 4     4 Roger      Coleman   Male     66.8   130.              5\n## 5     5 Tammy      Powell    Female   64.7   132.              4\n## 6     6 Anthony    Green     Male     71.2   153.              1\n## # ℹ 5 more variables: years_of_education &lt;dbl&gt;, major &lt;chr&gt;, age &lt;dbl&gt;,\n## #   centered_height &lt;dbl&gt;, centered_weight &lt;dbl&gt;\n\nbeta1 &lt;- cor(d$height, d$age) * sd(d$height)/sd(d$age)\nbeta1\n\n## [1] 0.9425086\n\nbeta0 &lt;- mean(d$height) - beta1 * mean(d$age)\nbeta0\n\n## [1] 48.73566\n\n(m &lt;- lm(height ~ age, data = d))\n\n## \n## Call:\n## lm(formula = height ~ age, data = d)\n## \n## Coefficients:\n## (Intercept)          age  \n##     48.7357       0.9425\n\nmales &lt;- filter(d, gender == \"Male\")\n\n(m &lt;- lm(height ~ age, data = males))\n\n## \n## Call:\n## lm(formula = height ~ age, data = males)\n## \n## Coefficients:\n## (Intercept)          age  \n##      49.341        1.011\n\nfemales &lt;- filter(d, gender == \"Female\")\n(m &lt;- lm(height ~ age, data = females))\n\n## \n## Call:\n## lm(formula = height ~ age, data = females)\n## \n## Coefficients:\n## (Intercept)          age  \n##     48.1811       0.8691",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Introduction to Linear Modeling</span>"
    ]
  },
  {
    "objectID": "18-module.html#inference-in-regression",
    "href": "18-module.html#inference-in-regression",
    "title": "18  Introduction to Linear Modeling",
    "section": "18.5 Inference in Regression",
    "text": "18.5 Inference in Regression\n\nClassical Theory-Based Inference\nOnce we have our linear model and associated regression coefficients, we want to know a bit more about the model. First, we want to be able to evaluate whether there is statistical evidence that there is indeed a relationship between these variables. If so, then our regression coefficients can indeed allow us to estimate or predict the value of one variable given another. Additionally, we also would like to be able to extend our estimates from our sample out to the population they are drawn from. These next steps involve the process of statistical inference.\nThe output of the lm() function provides a lot of information useful for inference. Run the command summary() on the output of lm(data = d, height ~ weight)\n\nm &lt;- lm(data = d, height ~ weight)\nsummary(m)\n\n## \n## Call:\n## lm(formula = height ~ weight, data = d)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -7.1519 -1.5206 -0.0535  1.5167  9.4439 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 39.565446   0.595815   66.41   &lt;2e-16 ***\n## weight       0.195019   0.004107   47.49   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.389 on 998 degrees of freedom\n## Multiple R-squared:  0.6932, Adjusted R-squared:  0.6929 \n## F-statistic:  2255 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\nOne of the outputs for the model, seen in the 2nd to last line in the output above, is the “R-squared” value, or the coefficient of determination, which is a summary of the total amount of variation in the y variable that is explained by the x variable. In our regression, ~69% of the variation in zombie height is explained by zombie weight.\nAnother output is the standard error of the estimate of each regression coefficient, along with a corresponding t value and p value. Recall that t statistics are calculated as the difference between an observed and expected value divided by a standard error. The p value comes from evaluating the magnitude of the t statistic against a t distribution with n-2 degrees of freedom. That is, we can calculate p values mathematically for our statistics (the regression slope and intercept) if the sampling distribution of these statistics conform to a t distribution.\nAs we saw above, running the tidy() function from the {broom} package pulls out a clean table of the relevant information.\n\nm.summary &lt;- tidy(m)\n\nWe can confirm the t statistic and p value output by lm() by hand by calculating t and p based on the regression coefficient estimates and the standard errors of those estimates. The t statistic is simply the value of the estimated regression coefficient divided by the estimate of the standard error for that coefficient (i.e., \\(\\beta/SE_\\beta\\))\n\nm.summary$calc.statistic &lt;- (m.summary$estimate - 0)/m.summary$std.error\nm.summary$calc.p.value &lt;- 2 * pt(m.summary$calc.statistic, df = nrow(d) - 2, lower.tail = FALSE)\n# we use 2 * pt to get the 2-tailed p value alternatively, we could do...\n# m.summary$calc.p.value &lt;- pt(-1*abs(m.summary$calc.statistic), df = nrow(d) -\n# 2, lower.tail = TRUE) + pt(abs(m.summary$calc.statistic), df = nrow(d) - 2,\n# lower.tail = FALSE) or m.summary$calc.p.value &lt;-\n# pt(-1*abs(m.summary$calc.statistic), df = nrow(d) - 2, lower.tail = TRUE) +\n# (1-pt(abs(m.summary$calc.statistic), df = nrow(d) - 2, lower.tail = TRUE))\nm.summary\n\n## # A tibble: 2 × 7\n##   term        estimate std.error statistic   p.value calc.statistic calc.p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;\n## 1 (Intercept)   39.6     0.596        66.4 0                   66.4    0        \n## 2 weight         0.195   0.00411      47.5 2.65e-258           47.5    2.65e-258\n\n\nNote that the glance() function also returns other information of interest about a regression model.\n\nglance(m)\n\n## # A tibble: 1 × 12\n##   r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n##       &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1     0.693         0.693  2.39     2255. 2.65e-258     1 -2289. 4583. 4598.\n## # ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nWe can get t distribution-based confidence intervals for our estimates easily, too, using either the by-hand approach we’ve used before or by using a built-in function.\n\nalpha &lt;- 0.05\n# extract CIs from the model with using the results of lm()\n(CI &lt;- confint(m, level = 1 - alpha))\n\n##                  2.5 %     97.5 %\n## (Intercept) 38.3962527 40.7346393\n## weight       0.1869597  0.2030778\n\n# using tidy()\n(CI &lt;- tidy(m, conf.int = TRUE, conf.level = 1 - alpha))\n\n## # A tibble: 2 × 7\n##   term        estimate std.error statistic   p.value conf.low conf.high\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)   39.6     0.596        66.4 0           38.4      40.7  \n## 2 weight         0.195   0.00411      47.5 2.65e-258    0.187     0.203\n\n# by hand\nlower &lt;- m.summary$estimate - qt(1 - alpha/2, df = nrow(d) - 2) * m.summary$std.error\nupper &lt;- m.summary$estimate + qt(1 - alpha/2, df = nrow(d) - 2) * m.summary$std.error\nCI &lt;- cbind(lower, upper)\nrownames(CI) &lt;- c(\"(Intercept)\", \"weight\")\ncolnames(CI) &lt;- c(paste0(as.character(alpha/2 * 100), \" %\"), paste0(as.character((1 -\n    alpha/2) * 100), \" %\"))\nCI\n\n##                  2.5 %     97.5 %\n## (Intercept) 38.3962527 40.7346393\n## weight       0.1869597  0.2030778\n\n\n\nNOTE: Remember that these mathematical methods of calculating p values and CIs are only valid if the LINE technical conditions for linear regression are met. These conditions are:\n\nLinearity of the relationship between the explanatory and response variables\nIndependence of observations and their residuals\nNormality of the residual values of the response variable around the regression line\nEquality of the variance in residual values across the range of explanatory variable values\n\n\n\n\nSimulation-Based Inference\n\nSignificance of Coefficients by Permutation\nAs we saw in Module 16, the {infer} package offers a convenient set of functions and a standard workflow for using permutation methods for statistical hypothesis testing, rather than relying on mathematical assumptions. In that module, we considered whether means, differences between means, proportions, or differences in proportions differed from what we would expect under a particular null hypothesis… and we can apply the same workflow to evaluating regression coefficients! The permutation approach lets us relax the normality condition of the LINE assumptions for linear regression. To demonstrate this process, we will run the same model of height ~ weight using the zombie apocalypse survivors dataset that we ran above:\nFirst, we run the model and pull out \\(\\beta_1\\), the regression slope, and its estimated standard error:\n\n# first define alpha, CI boundaries, and critical values\nalpha &lt;- 0.05\nconfidence_level &lt;- 1 - alpha\np_lower &lt;- alpha/2\np_upper &lt;- 1 - (alpha/2)\ndegrees_of_freedom &lt;- nrow(d) - 2\ncritical_value &lt;- qt(p_upper, df = degrees_of_freedom)\n\n# original slope\noriginal.slope &lt;- lm(data = d, height ~ weight) |&gt;\n    # tidy the model and add the CI based on the t distribution\ntidy(conf.int = TRUE, conf.level = confidence_level) |&gt;\n    # or manually calculate the CI based on the t distribution\nmutate(lower = estimate - std.error * critical_value, upper = estimate + std.error *\n    critical_value) |&gt;\n    filter(term == \"weight\")\noriginal.slope  # show model results for slope of weight\n\n## # A tibble: 1 × 9\n##   term   estimate std.error statistic   p.value conf.low conf.high lower upper\n##   &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 weight    0.195   0.00411      47.5 2.65e-258    0.187     0.203 0.187 0.203\n\n\nWe can generate a null distribution for our slope estimates via permutation using {infer} package functions… essentially, each permutation involves randomly shuffling values of the response variable under the null hypothesis such that they are independent of the explanatory variable (and vice versa).\n\npermuted.slope &lt;- d |&gt;\n    # specify model\nspecify(height ~ weight) |&gt;\n    # use a null hypothesis of independence\nhypothesize(null = \"independence\") |&gt;\n    # generate permutation replicates\ngenerate(reps = 1000, type = \"permute\") |&gt;\n    # calculate the slope statistic\ncalculate(stat = \"slope\")\n\nhead(permuted.slope)  # slopes from first few permutation replicates\n\n## Response: height (numeric)\n## Explanatory: weight (numeric)\n## Null Hypothesis: independence\n## # A tibble: 6 × 2\n##   replicate     stat\n##       &lt;int&gt;    &lt;dbl&gt;\n## 1         1 -0.00420\n## 2         2 -0.00500\n## 3         3 -0.00640\n## 4         4 -0.00868\n## 5         5  0.00961\n## 6         6  0.0139\n\n# create confidence intervals\n\npermuted.slope.summary &lt;- permuted.slope |&gt;\n    # summarize the mean (which should be very close to zero), standard error,\n    # CI based on the SE and t distribution, and CI based on the quantile\n    # (percentile) method\nsummarize(estimate = mean(stat), std.error = sd(stat), lower = estimate - std.error *\n    critical_value, upper = estimate + std.error * critical_value, perm.lower = quantile(stat,\n    p_lower), perm.upper = quantile(stat, p_upper))\n\n# show summary of permuted sampling distribution\npermuted.slope.summary\n\n## # A tibble: 1 × 6\n##   estimate std.error   lower  upper perm.lower perm.upper\n##      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n## 1 0.000280   0.00739 -0.0142 0.0148    -0.0144     0.0146\n\n\nThe get_ci() function from {infer} can be used to return these different types of confidence intervals, too… i.e., either based on the standard error of the permutation distribution or actual quantiles from that distribution.\n\nget_ci(permuted.slope, level = 1 - alpha, type = \"percentile\")\n\n## # A tibble: 1 × 2\n##   lower_ci upper_ci\n##      &lt;dbl&gt;    &lt;dbl&gt;\n## 1  -0.0144   0.0146\n\nget_ci(permuted.slope, level = 1 - alpha, type = \"se\", point_estimate = pull(permuted.slope.summary,\n    estimate))\n\n## # A tibble: 1 × 2\n##   lower_ci upper_ci\n##      &lt;dbl&gt;    &lt;dbl&gt;\n## 1  -0.0142   0.0148\n\n\nWe can also visualize the null distribution of slope coefficients based on permutation and superimpose our actual slope coefficient.\n\n# plot the null distribution based on permutation\nhist(permuted.slope$stat, main = \"Histogram of Permuted\\nSlope Values\", xlab = \"Slope Coefficient\")\n\n\n\n\n\n\n\n# or using `visualize()` from {infer}\nvisualize(permuted.slope) + shade_p_value(obs_stat = original.slope$estimate, direction = \"two_sided\")\n\n\n\n\n\n\n\n\nFinally, to determine the two-sided p value associated with our observed slope coefficient, we determine the proportion of permuted simulations that yielded a slope estimate as great or greater than the the one based on our original data:\n\np.value &lt;- permuted.slope |&gt;\n    # add a column of the absolute value of the slope\nmutate(abs_stat = abs(stat)) |&gt;\n    # calculate a summary statistic as the proportion of cases where the\n    # absolute value of the permuted slope is greater than or equal to the\n    # absolute value of the observed slope\nsummarize(estimate = mean(abs_stat &gt;= abs(pull(original.slope, estimate))))\n\np.value\n\n## # A tibble: 1 × 1\n##   estimate\n##      &lt;dbl&gt;\n## 1        0\n\n# the function `get_p_value()` returns this value directly...\n\n(p.value &lt;- permuted.slope |&gt;\n    get_p_value(obs_stat = original.slope$estimate, direction = \"two_sided\"))\n\n## Warning: Please be cautious in reporting a p-value of 0. This result is an approximation\n## based on the number of `reps` chosen in the `generate()` step.\n## ℹ See `get_p_value()` (`?infer::get_p_value()`) for more information.\n\n\n## # A tibble: 1 × 1\n##   p_value\n##     &lt;dbl&gt;\n## 1       0\n\n\n\n\nCIs for Coefficients by Bootstrapping\nWe can also use the {infer} package to generate CIs around our regression coefficient estimates using bootstrapping.\n\nboot.slope &lt;- d |&gt;\n    # specify model\nspecify(height ~ weight) |&gt;\n    # generate bootstrap replicates\ngenerate(reps = 1000, type = \"bootstrap\") |&gt;\n    # calculate the slope statistic\ncalculate(stat = \"slope\")\n\nhead(boot.slope)  # slopes from first few bootstrap replicates\n\n## Response: height (numeric)\n## Explanatory: weight (numeric)\n## # A tibble: 6 × 2\n##   replicate  stat\n##       &lt;int&gt; &lt;dbl&gt;\n## 1         1 0.198\n## 2         2 0.201\n## 3         3 0.194\n## 4         4 0.194\n## 5         5 0.192\n## 6         6 0.194\n\n# create confidence intervals for regression coefficients\n\nboot.slope.summary &lt;- boot.slope |&gt;\n    # summarize the mean, standard error, CI based on the SE and t\n    # distribution, and CI based on the quantile (percentile) method\nsummarize(estimate = mean(stat), std.error = sd(stat), lower = estimate - std.error *\n    critical_value, upper = estimate + std.error * critical_value, boot.lower = quantile(stat,\n    p_lower), boot.upper = quantile(stat, p_upper))\n\n# show summary of bootstrap sampling distribution\nboot.slope.summary\n\n## # A tibble: 1 × 6\n##   estimate std.error lower upper boot.lower boot.upper\n##      &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n## 1    0.195   0.00413 0.187 0.203      0.187      0.203\n\n\nAgain, the get_ci() function from {infer} can be used to return these different types of confidence intervals…\n\nCI.percentile &lt;- get_ci(boot.slope, level = 1 - alpha, type = \"percentile\")\nCI.theory &lt;- get_ci(boot.slope, level = 1 - alpha, type = \"se\", point_estimate = pull(boot.slope.summary,\n    estimate))\n\nWe can also easily visualize the sampling distribution (and CI) based on bootstrapping.\n\n# plot the sampling distribution for based on bootstrapping plus a CI\nhist(boot.slope$stat, main = \"Histogram of Bootstrapped\\nSlope Values\", xlab = \"Slope Coefficient\")\nabline(v = CI.percentile)\n\n\n\n\n\n\n\n# or...\nvisualize(boot.slope) + shade_ci(endpoints = CI.percentile)",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Introduction to Linear Modeling</span>"
    ]
  },
  {
    "objectID": "18-module.html#interpreting-regression-results",
    "href": "18-module.html#interpreting-regression-results",
    "title": "18  Introduction to Linear Modeling",
    "section": "18.6 Interpreting Regression Results",
    "text": "18.6 Interpreting Regression Results\nAs shown above, estimating our regression coefficients is pretty straightforward. But what do they mean? How do we interpret them?\n\nThe intercept, \\(\\beta_0\\), is the PREDICTED value of \\(y\\), our response variable, when the value of \\(x\\), our explanatory variable, is zero.\nThe slope, \\(\\beta_1\\) is EXPECTED CHANGE in units of \\(y\\) for every 1 unit of change in \\(x\\).\nThe overall equation allows us to calculate PREDICTED values of \\(y\\) for new observations of \\(x\\). We can also calculate CONFIDENCE INTERVALS (CIs) around the predicted mean value of \\(y\\) for each value of \\(x\\) (which addresses our uncertainly in the estimate of the mean), and we can get PREDICTION INTERVALS (PIs) around our prediction (which gives the range of actual values of \\(y\\) that we might expect to see at a given value of \\(x\\)). That is, for each value of \\(x\\), we can calculate a CI, which is the the range of values into which \\(\\hat{y}\\) is expected to fall 95% of the time, and a PI, which is the range of y values into which 95% of predicted individual y values are expected to fall.\n\n\nCHALLENGE\n\nIf zombie apocalypse survivor weight is measured in pounds and zombie apocalypse survivor height is measured in inches, what is the expected height of a survivor weighing 150 pounds?\n\n\n\nShow Code\nbeta0 &lt;- m.summary |&gt;\n    filter(term == \"(Intercept)\") |&gt;\n    pull(estimate)\nbeta1 &lt;- m.summary |&gt;\n    filter(term == \"weight\") |&gt;\n    pull(estimate)\n(h.hat &lt;- beta1 * 150 + beta0)\n\n\nShow Output\n## [1] 68.81825\n\n\n\n\nWhat is the predicted difference in height between a survivor weighing 180 and 220 pounds?\n\n\n\nShow Code\n(h.hat.difference &lt;- (beta1 * 220 + beta0) - (beta1 * 180 + beta0))\n\n\nShow Output\n## [1] 7.800749\n\n\n\n\n\nPredicted Values\nThe predict() function allows us to generate predicted (i.e., \\(\\hat{y}\\)) values for a vector of values of x. Note the structure of the 2nd argument in the function, newdata=… it has to be a data frame and include the variable name(s) we are going to use to predict y on the basis of. Here, we pass it a vector of actual x values.\n\nm &lt;- lm(data = d, height ~ weight)\ny.hat &lt;- predict(m, newdata = data.frame(weight = d$weight))\ndf &lt;- data.frame(cbind(d$weight, d$height, y.hat))\nnames(df) &lt;- c(\"x\", \"y\", \"yhat\")\nhead(df)\n\n##          x        y     yhat\n## 1 132.0872 62.88951 65.32492\n## 2 146.3753 67.80277 68.11137\n## 3 152.9370 72.12908 69.39103\n## 4 129.7418 66.78484 64.86753\n## 5 132.4265 64.71832 65.39109\n## 6 152.5246 71.24326 69.31059\n\n\nThe augment() function from {broom} also generates comparable predicted or “fitted” values for a model. With default arguments, it generates .fitted and .resid values for the original data and can also calculate .se.fit values (standard errors around the fitted values) by setting se_fit=TRUE (it is FALSE by default). With the newdata= argument, it generates .fitted values for novel data. As for predict(), the newdata= argument has to be a data frame and include the predictor variable name(s).\n\ndf &lt;- augment(m, se_fit = TRUE)\nhead(df)\n\n## # A tibble: 6 × 9\n##   height weight .fitted .se.fit .resid    .hat .sigma    .cooksd .std.resid\n##    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n## 1   62.9   132.    65.3  0.0898 -2.44  0.00141   2.39 0.000737       -1.02 \n## 2   67.8   146.    68.1  0.0762 -0.309 0.00102   2.39 0.00000851     -0.129\n## 3   72.1   153.    69.4  0.0841  2.74  0.00124   2.39 0.000817        1.15 \n## 4   66.8   130.    64.9  0.0953  1.92  0.00159   2.39 0.000515        0.803\n## 5   64.7   132.    65.4  0.0890 -0.673 0.00139   2.39 0.0000553      -0.282\n## 6   71.2   153.    69.3  0.0834  1.93  0.00122   2.39 0.000400        0.810\n\n\n\ng &lt;- ggplot(data = df, aes(x = weight, y = .fitted))\ng &lt;- g + geom_point(size = 0.25)\ng &lt;- g + geom_point(aes(x = weight, y = height), color = \"red\")\ng &lt;- g + geom_segment(aes(x = weight, y = .fitted, xend = weight, yend = height),\n    alpha = 0.25)\ng\n\n\n\n\n\n\n\n\nEach vertical line in the figure above represents a residual, the difference between the observed and the fitted or predicted value of \\(y\\) (\\(\\hat{y}\\)) at the given \\(x\\) value.\n\n\nConfidence Intervals around Predicted Means\nThe predict() function also allows us to easily generate confidence intervals around our predicted \\(\\hat{y}\\) values.\n\nci &lt;- predict(m, newdata = data.frame(weight = 150), interval = \"confidence\", level = 1 -\n    alpha)  # for a single value\nci\n\n##        fit      lwr     upr\n## 1 68.81825 68.66211 68.9744\n\nci &lt;- predict(m, newdata = data.frame(weight = d$weight), interval = \"confidence\",\n    level = 1 - alpha)  # for a vector of values\nci &lt;- data.frame(ci)\nci &lt;- cbind(df$weight, ci)\nnames(ci) &lt;- c(\"weight\", \"c.fit\", \"c.lwr\", \"c.upr\")\ng &lt;- ggplot(data = df, aes(x = weight, y = height))\ng &lt;- g + geom_point(alpha = 0.5)\ng &lt;- g + geom_line(data = ci, aes(x = weight, y = c.fit), color = \"black\")\ng &lt;- g + geom_line(data = ci, aes(x = weight, y = c.lwr), color = \"blue\")\ng &lt;- g + geom_line(data = ci, aes(x = weight, y = c.upr), color = \"blue\")\ng\n\n\n\n\n\n\n\n\nThis can be also be done by hand using the data from the augment()ed model:\n\ndf &lt;- df |&gt;\n    # calculate a confidence interval for the predicted values\nmutate(c.lwr = .fitted - qt(1 - alpha/2, nrow(df) - 2) * .se.fit, c.upr = .fitted +\n    qt(1 - alpha/2, nrow(df) - 2) * .se.fit)\nhead(df)\n\n## # A tibble: 6 × 11\n##   height weight .fitted .se.fit .resid    .hat .sigma   .cooksd .std.resid c.lwr\n##    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n## 1   62.9   132.    65.3  0.0898 -2.44  0.00141   2.39   7.37e-4     -1.02   65.1\n## 2   67.8   146.    68.1  0.0762 -0.309 0.00102   2.39   8.51e-6     -0.129  68.0\n## 3   72.1   153.    69.4  0.0841  2.74  0.00124   2.39   8.17e-4      1.15   69.2\n## 4   66.8   130.    64.9  0.0953  1.92  0.00159   2.39   5.15e-4      0.803  64.7\n## 5   64.7   132.    65.4  0.0890 -0.673 0.00139   2.39   5.53e-5     -0.282  65.2\n## 6   71.2   153.    69.3  0.0834  1.93  0.00122   2.39   4.00e-4      0.810  69.1\n## # ℹ 1 more variable: c.upr &lt;dbl&gt;\n\ng &lt;- ggplot(data = df, aes(x = weight, y = height))\ng &lt;- g + geom_point(alpha = 0.5)\ng &lt;- g + geom_line(aes(x = weight, y = .fitted), color = \"black\")\ng &lt;- g + geom_line(aes(x = weight, y = c.lwr), color = \"blue\")\ng &lt;- g + geom_line(aes(x = weight, y = c.upr), color = \"blue\")\ng\n\n\n\n\n\n\n\n\n\n\nPrediction Intervals for Individual Responses\nThe predict() function also allows us to easily generate prediction intervals for the range of possible values of individual observations of \\(y\\) (rather than the 95% CI around \\(\\hat{y}\\) at each \\(x\\).\n\npi &lt;- predict(m, newdata = data.frame(weight = 150), interval = \"prediction\", level = 0.95)  # for a single value\npi\n\n##        fit      lwr      upr\n## 1 68.81825 64.12849 73.50802\n\npi &lt;- predict(m, newdata = data.frame(weight = d$weight), interval = \"prediction\",\n    level = 0.95)  # for a vector of values\npi &lt;- data.frame(pi)\npi &lt;- cbind(d$weight, pi)\nnames(pi) &lt;- c(\"weight\", \"p.fit\", \"p.lwr\", \"p.upr\")\ng &lt;- g + geom_line(data = pi, aes(x = weight, y = p.lwr), color = \"red\")\ng &lt;- g + geom_line(data = pi, aes(x = weight, y = p.upr), color = \"red\")\ng\n\n\n\n\n\n\n\n\nAgain, this can be also be done by hand using the data from the augment()ed model:\n\nsd &lt;- glance(m) |&gt;\n    pull(sigma)  # sd deviation of residuals\n\ndf &lt;- df |&gt;\n    # calculate a confidence interval for the predicted values\nmutate(se.prediction = sqrt(sd^2 + .se.fit^2), p.lower = .fitted - qt(1 - alpha/2,\n    nrow(df) - 2) * se.prediction, p.upper = .fitted + qt(1 - alpha/2, nrow(df) -\n    2) * se.prediction)\ng &lt;- ggplot(data = df, aes(x = weight, y = height))\ng &lt;- g + geom_point(alpha = 0.5)\ng &lt;- g + geom_line(aes(x = weight, y = .fitted), color = \"black\")\ng &lt;- g + geom_line(aes(x = weight, y = c.lwr), color = \"blue\")\ng &lt;- g + geom_line(aes(x = weight, y = c.upr), color = \"blue\")\ng &lt;- g + geom_line(aes(x = weight, y = p.lower), color = \"red\")\ng &lt;- g + geom_line(aes(x = weight, y = p.upper), color = \"red\")\ng\n\n\n\n\n\n\n\n\n\n\nCHALLENGE\nConstruct a linear model using lm() for the regression of zombie apocalypse survivor height on age and predict the mean height (\\(\\hat{h}\\)). Then, use the augment() and mutate() or predict() functions to calculate the t distribution-based 95% confidence interval (CI) around the predicted mean height and the 95% prediction interval (PI) for height for a vector of zombie ages, v &lt;- seq(from=10, to=30, by=1). Finally, plot your points, your regression line, and lines for the lower and upper limits of the CI and of the PI.\n\nNOTE: The augment() and predict() functions work similarly, but whereas predict() returns a vector of fitted \\(y\\) values for each \\(x\\) value in newdata, augment() will return a dataframe of \\(x\\)s, fitted \\(y\\)s, and standard errors for the fitted \\(y\\)s. Helpfully, if the newdata argument is left out for augment(), the function will use the original dataset and simply add in the new fitted \\(y\\), standard errors, etc.\n\n\n\nShow Code\nalpha &lt;- 0.05\nv &lt;- seq(from = 10, to = 30, by = 1)\nm &lt;- lm(data = d, height ~ age)\nsd &lt;- glance(m) |&gt;\n    pull(sigma)\ndf &lt;- augment(m, newdata = data.frame(age = v), se_fit = TRUE) |&gt;\n    # add CI\nmutate(c.lower = .fitted - qt(1 - alpha/2, nrow(d) - 2) * .se.fit, c.upper = .fitted +\n    qt(1 - alpha/2, nrow(d) - 2) * .se.fit) |&gt;\n    # add PI\nmutate(se.prediction = sqrt(sd^2 + .se.fit^2), p.lower = .fitted - qt(1 - alpha/2,\n    nrow(d) - 2) * se.prediction, p.upper = .fitted + qt(1 - alpha/2, nrow(d) - 2) *\n    se.prediction)\n\n# alternatively...  ci &lt;- predict(m, newdata = data.frame(age = v), interval =\n# 'confidence', level = 1 - alpha) pi &lt;- predict(m, newdata = data.frame(age =\n# v), interval = 'prediction', 1 - alpha)\nplot(data = d, height ~ age)\nlines(x = df$age, y = df$.fitted, col = \"black\")\nlines(x = df$age, y = df$c.lower, col = \"blue\")\nlines(x = df$age, y = df$c.upper, col = \"blue\")\nlines(x = df$age, y = df$p.lower, col = \"red\")\nlines(x = df$age, y = df$p.upper, col = \"red\")\n\n\n\n\n\n\n\n\n\nShow Code\n# or\ng1 &lt;- ggplot(data = d, aes(x = age, y = height))\ng1 &lt;- g1 + geom_point(alpha = 0.5)\ng1 &lt;- g1 + geom_line(data = df, aes(x = age, y = .fitted), color = \"black\", lwd = 1)\ng1 &lt;- g1 + geom_line(data = df, aes(x = age, y = c.lower), color = \"blue\")\ng1 &lt;- g1 + geom_line(data = df, aes(x = age, y = c.upper), color = \"blue\")\ng1 &lt;- g1 + geom_line(data = df, aes(x = age, y = p.lower), color = \"red\")\ng1 &lt;- g1 + geom_line(data = df, aes(x = age, y = p.upper), color = \"red\")\ng1\n\n\n\n\n\n\n\n\n\nShow Code\n# or\n\ng2 &lt;- ggplot() + geom_point(data = d, aes(x = age, y = height), alpha = 0.5) + geom_line(data = df,\n    aes(x = age, y = .fitted), color = \"black\", lwd = 1) + geom_ribbon(data = df,\n    aes(x = age, ymin = c.lower, ymax = c.upper), alpha = 0.2, fill = \"blue\") + geom_ribbon(data = df,\n    aes(x = age, ymin = p.lower, ymax = p.upper), alpha = 0.2, fill = \"red\")\ng2\n\n\n\n\n\n\n\n\n\nWe can use a one-liner in {ggplot} to plot the CIs, too…\n\ng3 &lt;- ggplot(data = d, aes(x = age, y = height)) + geom_point(alpha = 0.5) + geom_smooth(method = \"lm\",\n    formula = y ~ x, se = TRUE)\ng3\n\n\n\n\n\n\n\n\nAgain, in these plots, the CI band shows where the mean height (\\(\\hat{h}\\)) is expected to fall in 95% of samples, while the PI band shows where the individual points are expected to fall 95% of the time.\n\n\nCHALLENGE\nFor the same regression model in the challenge above, use bootstrapping with 1000 replicates to generate a quantile-based 95% CI around the estimate of the slope of the relationship. Plot the regression lines associated with each of your first 20 replicates atop a bivariate scatterplot of height ~ age.\n\n\nShow Code\nset.seed(1)\nalpha &lt;- 0.05\np_lower &lt;- alpha/2\np_upper &lt;- 1 - (alpha/2)\nboot.slope &lt;- d |&gt;\n    # specify model\nspecify(height ~ age) |&gt;\n    # generate bootstrap replicates\ngenerate(reps = 1000, type = \"bootstrap\") |&gt;\n    # calculate the slope statistic\ncalculate(stat = \"slope\") |&gt;\n    summarize(boot.mean = mean(stat), boot.lower = quantile(stat, p_lower), boot.upper = quantile(stat,\n        p_upper))\n\nset.seed(1)\nboot.slope &lt;- d |&gt;\n    # specify model\nspecify(height ~ age) |&gt;\n    # generate bootstrap replicates\ngenerate(reps = 20, type = \"bootstrap\")\n\ng &lt;- ggplot(data = d, aes(x = age, y = height)) + geom_point() + geom_smooth(data = boot.slope,\n    aes(x = age, y = height, group = replicate), method = \"lm\", se = FALSE, lwd = 0.1)\ng\n\n\n## `geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Introduction to Linear Modeling</span>"
    ]
  },
  {
    "objectID": "18-module.html#residuals",
    "href": "18-module.html#residuals",
    "title": "18  Introduction to Linear Modeling",
    "section": "18.7 Residuals",
    "text": "18.7 Residuals\nFrom our various plots above, it’s clear that our model is not explaining all of the variation we see in our dataset… our y points do not all fall on the \\(\\hat{y}\\) line but rather are distributed around it. The distance of each of these points from the predicted value for \\(y\\) at that value of \\(x\\) is known as the “residual”. We can think about the residuals as “what is left over” after accounting for the predicted relationship between \\(x\\) and \\(y\\). Residuals are often thought of as estimates of the “error” term in a regression model, and most regression analyses assume that residuals are random normal variables with uniform variance across the range of \\(x\\) values (more on this later). In ordinary least squares regression, the line of best fit is defined as that which minimizes the sum of the squared residuals, and the expected value for a residual is 0.\nResiduals are also used to create “covariate adjusted” variables, as they can be thought of as the response variable, \\(y\\), with the linear effect of other predictor variables removed. We’ll return to this idea when we move on to multivariate regression.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Introduction to Linear Modeling</span>"
    ]
  },
  {
    "objectID": "18-module.html#concept-review",
    "href": "18-module.html#concept-review",
    "title": "18  Introduction to Linear Modeling",
    "section": "Concept Review",
    "text": "Concept Review\n\nRegression analysis is used to identify and explore models describing the relationship between variables of interest\nWe can use regression analysis…\n\nto predict the value of a response variable given the value of one or more predictor variables\nto evaluate alternative models of the relationship between a response variable and possible predictor variables\nto explore the relative explanatory power of different predictor variables\n\nLinear regression involves finding the “line of best fit” between the predictor and response variable\nThis means finding the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) that define the line that minizes the sum of the squared deviations of observed and predicted values for the response variable, i.e, that minimizes \\(\\sum(y - \\hat{y})\\) where \\(\\hat{y}= \\beta_0 + \\beta_1 x\\)\nThere are LOTS of types of regression analyses, and we will explore more of them in the modules to come [see Table 8.1 in R in Action]",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Introduction to Linear Modeling</span>"
    ]
  },
  {
    "objectID": "19-module.html",
    "href": "19-module.html",
    "title": "19  Elements of Regression Analysis",
    "section": "",
    "text": "19.1 Objectives",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Elements of Regression Analysis</span>"
    ]
  },
  {
    "objectID": "19-module.html#objectives",
    "href": "19-module.html#objectives",
    "title": "19  Elements of Regression Analysis",
    "section": "",
    "text": "The objective of this module is to continue our discussion of simple linear regression analysis to understand how the goal of regression is to partition variance in the response variable among different sources, i.e., into that explained by the regression model itself (and, we will see later on in our discussion of multivariate regression, among different factors in that model) versus the left-over error or residual variance. We also go through how to calculate the standard errors for our various regression coefficients and for the predicted values of our response variable based on our regression model, which, as we have seen, are returned by the lm() function. Finally, we also briefly discuss ways to transform non-normally distributed data to make them more appropriate for analysis using linear regression.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Elements of Regression Analysis</span>"
    ]
  },
  {
    "objectID": "19-module.html#preliminaries",
    "href": "19-module.html#preliminaries",
    "title": "19  Elements of Regression Analysis",
    "section": "19.2 Preliminaries",
    "text": "19.2 Preliminaries\n\nInstall the following package in R: {ggpubr}\nLoad {tidyverse}, {mosiac}, {car}, {broom}",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Elements of Regression Analysis</span>"
    ]
  },
  {
    "objectID": "19-module.html#analysis-of-variance-tables",
    "href": "19-module.html#analysis-of-variance-tables",
    "title": "19  Elements of Regression Analysis",
    "section": "19.3 Analysis of Variance Tables",
    "text": "19.3 Analysis of Variance Tables\nIn our linear models, we can separate or “partition” the total variation in our y variable (the sum of squares of y, or \\(SSY\\)) into that explained by our model (the regression sum of squares, or \\(SSR\\)) and that which is left over as “error” (the error sum of squares, or \\(SSE\\)):\n\\[SSY = SSR + SSE\\]\nGraphically…\n\n\n\n\n\n\n\n\n\nLet’s make sure we have our zombie apocalypse survivors dataset loaded…\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/zombies.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\n\nNow, we run a straightforward bivariate regression model and, using the raw data (which are duplicated in the $model data structure within our model object), we will calculate the various sums of squares of our variables and identify the numbers of degrees of freedom associated with each source of variation. This allows us to generate the ANOVA table for our model, which is a summary of how variance is partitioned among different sources.\n\nm &lt;- lm(data = d, height ~ weight)\n# height - mean(height)\nSSY &lt;- sum((m$model$height - mean(m$model$height))^2)\nSSY\n\n## [1] 18558.61\n\n# predicted height - mean height\nSSR &lt;- sum((m$fitted.values - mean(m$model$height))^2)\nSSR\n\n## [1] 12864.82\n\n# height - predicted height\nSSE &lt;- sum((m$model$height - m$fitted.values)^2)\nSSE\n\n## [1] 5693.785\n\n# or\nSSE &lt;- sum(m$residuals^2)\nSSE\n\n## [1] 5693.785\n\n\nFrom here, we can calculate the variance in each of these components, typically referred to as the mean square, by dividing each sum of squares by its corresponding degrees of freedom (recall that a variance can be thought of as an average “sum of squares”).\nThe degrees of freedom for the regression sum of squares (\\(SSR\\)) is equal to the number of predictor variables (\\(p\\)), which in this case is one (given our regression equation, we need to know only one piece of information, the value of our predictor variable, in order to calculate the predicted value of the response variable). The number of degrees of freedom for the error sum of squares (\\(SSE\\)) is equal to \\(n-2\\) (or \\(n - p - 1\\)). This is because we need to estimate two parameters (\\(\\beta_0\\) and \\(\\beta_1\\)) from our data before we can calculate the error sum of squares. Finally, the number of degrees of freedom for the total sum of squares (\\(SSY\\)) is \\(n-1\\)… we need to estimate one parameter from our data (the mean value of y) before we can calculate \\(SSY\\).\n\n(df_regression &lt;- 1)  # p = 1\n\n## [1] 1\n\n(df_error &lt;- nrow(d) - df_regression - 1)  # n - p - 1\n\n## [1] 998\n\n(df_y &lt;- nrow(d) - df_regression)  # n - p\n\n## [1] 999\n\nMSR &lt;- SSR/df_regression  # mean variance explained by the regression equation\nMSE &lt;- SSE/df_error  # mean remaining variance\nMSY &lt;- SSY/df_y  # mean overall variance\n\nThe last item we need to calculate is the F ratio, the ratio of the variance explained by the regression model to the remaining, unexplained variance: \\(MSR/MSE\\).\n\nfratio &lt;- MSR/MSE\nfratio\n\n## [1] 2254.931\n\n\nTogether, the values we have calculated above form the main entries in the ANOVA Table for our regression.\n\n\n\n\n\n\n\n\n\n\nSource\nSum of Squares\nDegrees of Freedom\nMean Square (SS/df)\nF ratio\n\n\n\n\nRegression\n\\(SSR\\) = 12864.82\n1\n\\(MSR\\) = 12864.82\n2254.139\n\n\nError\n\\(SSE\\) = 5693.79\n\\(n-2\\) = 998\n\\(MSE\\) = 5.7072\n\n\n\nTotal\n\\(SSY\\) = 18558.61\n\\(n-1\\) = 999\n\\(MSY\\) = 18.57719\n\n\n\n\nWe can then test the overall significance of our regression model by evaluating our F ratio test statistic against an \\(F\\) distribution, taking into account the number of degrees of freedom in each. The \\(F\\) distribution is a continuous probability distribution, defined for \\(x≥0\\) and governed by two parameters, \\(df1\\) and \\(df2\\). The critical value for the \\(F\\) statistic above which we would reject the idea that the variance in our two sources is comparable is given by qf(p, df1, df2), where p is 1-\\(\\alpha\\) and df1 and df2 are the degrees of freedom in the sources being compared (regression versus error).\n\nplotDist(\"f\", df1 = 1, df2 = 10, col = \"green\", lty = 3, lwd = 2, main = \"Some Example F Distributions\",\n    sub = \"red vertical line shows critical value\\n\n            for df1=1, df2=998\",\n    ylab = \"f(x)\", xlab = \"x\", xlim = c(0, 5), ylim = c(0, 1.5), key = list(space = \"right\",\n        text = list(c(\"df1=1, df2=1\", \"df1=2, df2=2\", \"df1=4, df2=4\", \"df1=8, df2=100\",\n            \"df1=1, df2=998\")), lines = list(col = c(\"green\", \"blue\", \"red\", \"purple\",\n            \"black\"), lty = c(3, 3, 3, 3, 1), lwd = 2, bty = \"n\", cex = 0.75)))\nplotDist(\"f\", df1 = 2, df2 = 2, col = \"blue\", lty = 3, lwd = 2, add = TRUE)\nplotDist(\"f\", df1 = 4, df2 = 4, col = \"red\", lty = 3, lwd = 2, add = TRUE)\nplotDist(\"f\", df1 = 8, df2 = 100, col = \"purple\", lty = 3, lwd = 2, add = TRUE)\nplotDist(\"f\", df1 = 1, df2 = 998, col = \"black\", lty = 1, lwd = 2, add = TRUE)\ncrit &lt;- qf(p = 0.95, df1 = 1, df2 = 998)\ncrit\n\n## [1] 3.850793\n\nladd(panel.abline(v = crit, col = \"red\", lty = 1, lwd = 1))\n\n\n\n\n\n\n\n\nThe qf() function allows us to calculate critical values under an \\(F\\) distribution given the number of degrees of freedom for the regression and error.\n\nplotDist(\"f\", df1 = 1, df2 = 998, main = \"df1 = 1, df2 = 998\", xlab = \"x\", ylab = \"f(x)\",\n    xlim = c(0, 5), ylim = c(0, 1.5))\ncrit &lt;- qf(p = 0.95, df1 = 1, df2 = 998)\ncrit\n\n## [1] 3.850793\n\nladd(panel.abline(v = crit, col = \"red\", lty = 1, lwd = 1))\nladd(panel.polygon(cbind(c(crit, seq(from = crit, to = 12, length.out = 1000), 12),\n    c(0, df(seq(from = crit, to = 12, length.out = 1000), df1 = 1, df2 = 998), 0)),\n    border = \"black\", col = rgb(0, 0, 1, 0.5)))\n\n\n\n\n\n\n\n\nFor our data, the value for the F ratio statistic well exceeds this critical value!\nAlternatively, we can use the following formulation to directly estimate a p value associated with our value for the F statistic. The p value is simply the area under the \\(F\\) distribution curve to the right of the F statistic value (or 1 minus the cumulative probability up to that point):\n\npf(q = fratio, df1 = 1, df2 = 998, lower.tail = FALSE)\n\n## [1] 2.646279e-258\n\n# or\n1 - pf(q = fratio, df1 = 1, df2 = 998)\n\n## [1] 0\n\n\n… and we see that the p value associated with this high of an F statistic is infintessimally small.\nAs usual, R can handle all of the calculations above for easily. The aov() function, like the lm() function, returns a model object that we can use summary() on to look at the results we want. Alternatively, we can run the function summary.aov() using the model object resulting from lm() as an argument. In either case, the results returned are the same as we calculated by hand above.\n\na &lt;- aov(data = d, height ~ weight)\nsummary(a)\n\n##              Df Sum Sq Mean Sq F value Pr(&gt;F)    \n## weight        1  12865   12865    2255 &lt;2e-16 ***\n## Residuals   998   5694       6                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary.aov(m)\n\n##              Df Sum Sq Mean Sq F value Pr(&gt;F)    \n## weight        1  12865   12865    2255 &lt;2e-16 ***\n## Residuals   998   5694       6                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nRecall that the results returned by summary() of our regression model also shows the coefficient of determination, or the “R-squared value”, which we defined above as the fraction of the total variation explained by the model. We can calculate this value directly from our ANOVA table as simply \\(SSR/SSY\\). The correlation coefficient, \\(\\rho\\), between our response and predictor variable is simply the square root of this value.\n\nrsq &lt;- SSR/SSY\nrsq\n\n## [1] 0.6931998\n\nrho &lt;- sqrt(rsq)\nrho\n\n## [1] 0.8325862",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Elements of Regression Analysis</span>"
    ]
  },
  {
    "objectID": "19-module.html#standard-errors-of-coefficients",
    "href": "19-module.html#standard-errors-of-coefficients",
    "title": "19  Elements of Regression Analysis",
    "section": "19.4 Standard Errors of Coefficients",
    "text": "19.4 Standard Errors of Coefficients\nRecall that lm() returned the standard errors associated with each of the various components of our regression model, i.e., the slope and intercept and each predicted value of y. We can calculate standard errors directly to show how R is deriving them.\nThe formula for the standard error of the regression slope, \\(\\beta_1\\), is calculated as:\n\\[SE_{\\beta_1} = \\sqrt{\\frac{MSE}{SSX}}\\]\nUsing our data…\n\nSSX &lt;- sum((m$model$weight - mean(m$model$weight))^2)  # how much x variation there is\nSEbeta1 &lt;- sqrt(MSE/SSX)\nSEbeta1\n\n## [1] 0.004106858\n\n\nThe standard error of the intercept, \\(\\beta_0\\), is calculated as:\n\\[SE_{\\beta_0} = \\sqrt{\\frac{MSE \\times \\sum{x^2}}{n \\times SSX}} = SE_{\\beta_1} \\times \\sqrt{\\frac{\\sum{x^2}}{n}} \\]\n\nSEbeta0 &lt;- sqrt((MSE * sum(m$model$weight^2))/(1000 * SSX))\nSEbeta0\n\n## [1] 0.5958147\n\n\nFinally, the standard error of each predicted value of y is calculated as:\n\\[SE_{\\hat{y}} = \\sqrt{MSE \\times {\\biggl[\\frac{1}{n} + \\frac{(x-\\hat{x})^2}{SSX}}{\\biggr]}}\\]\n\nSEyhat &lt;- sqrt(MSE * (1/1000 + (m$model$weight - mean(m$model$weight))^2/SSX))\nhead(SEyhat)  # just the first 6 rows\n\n## [1] 0.08978724 0.07620966 0.08414480 0.09533986 0.08904151 0.08341218\n\n\nThese same standard errors for \\(\\beta_0\\) and \\(\\beta_1\\) are exactly what are returned by the lm() function.\n\nsummary(m)\n\n## \n## Call:\n## lm(formula = height ~ weight, data = d)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -7.1519 -1.5206 -0.0535  1.5167  9.4439 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 39.565446   0.595815   66.41   &lt;2e-16 ***\n## weight       0.195019   0.004107   47.49   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.389 on 998 degrees of freedom\n## Multiple R-squared:  0.6932, Adjusted R-squared:  0.6929 \n## F-statistic:  2255 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\nNote that the t statistic is calculated as simply the estimate value divided by the corresponding SE value, and the p value just comes from comparing that statistic to a \\(t\\) distribution with the appropriate degrees of freedom (i.e., number of observations - 2). We can run another model with higher p values to see this. Here, too, we use the convenient tidy() function from the {broom} package to pull out a table of results from our linear model very easily.\n\nm &lt;- lm(zombies_killed ~ age, data = d)\ncoefficients &lt;- tidy(m)\ncoefficients &lt;- coefficients |&gt;\n    mutate(t.calc = estimate/std.error)\ncoefficients &lt;- coefficients |&gt;\n    mutate(p.calc = 2 * (1 - pt(abs(t.calc), df = nrow(m$model) - 2)))\n# the p value is 2 times the tail probability implied by the t statistic\ncoefficients\n\n## # A tibble: 2 × 7\n##   term        estimate std.error statistic  p.value  t.calc   p.calc\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n## 1 (Intercept)  3.01       0.378     7.97   4.35e-15  7.97   4.44e-15\n## 2 age         -0.00111    0.0187   -0.0596 9.53e- 1 -0.0596 9.53e- 1",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Elements of Regression Analysis</span>"
    ]
  },
  {
    "objectID": "19-module.html#model-checking",
    "href": "19-module.html#model-checking",
    "title": "19  Elements of Regression Analysis",
    "section": "19.5 Model Checking",
    "text": "19.5 Model Checking\nSo far, we’ve derived a bunch of summary statistics describing our model and we have looked at parametric ways of testing whether those summary statistics are significantly different from zero. That is…\n\nWe have seen whether our overall regression model explains a significant portion of the variance in y by means of the F ratio test.\nWe have calculated standard errors for our \\(\\beta_1\\) and \\(\\beta_0\\) estimates and seen whether they are significantly different from zero by means of t tests.\nWe have calculated standard errors for our prediction of y (i.e., \\(\\hat{y}\\)) at each value of x.\nWe have estimated the proportion of the total variance in y explained by our model (i.e., \\(R-squared\\)).\n\nWhat we have not done yet, however, is checked our model fit critically in other ways… particularly, we have not checked whether two key assumptions of linear modeling are met: that our residuals (or errors) are normally distributed and that there is constancy of variance in our y values across the range of xs (or, “homoscedasticity”).\nWe can investigate our residuals as one way of assessing model fit.\n\nCHALLENGE\nCalculate the residuals from the regression of zombie apocalypse survivor height on weight and plot these in relation to weight (as the x variable). There are lots of ways to do this quickly.\n\n\nShow Code\nm &lt;- lm(data = d, height ~ weight)\ne &lt;- m$residuals\nplot(x = d$weight, y = e)\n\n\n\n\n\n\n\n\n\nShow Code\n# or we could use the function `resid()`\ne &lt;- resid(m)\nplot(x = d$weight, y = e)\n\n\nNow, plot a histogram of your residuals… ideally they are normally distributed!\n\n\nShow Code\nhistogram(e, xlim = c(-4 * sd(e), 4 * sd(e)), breaks = 20, main = \"Histogram of Residuals\")\n\n\n\n\n\n\n\n\n\nAn additional way to quickly examine your residuals is to use the plot() function with your model as an argument. This prints out four plots that each tell you something.\n\npar(mfrow = c(2, 2))\nplot(m)\n\n\n\n\n\n\n\n\nThe first (top left) plot of “Residuals vs Fitted” values of y should, like the plot residuals versus x, not show any structure. We hope to see equally spread residuals around a horizontal line without distinct patterns.\nThe second (top right) plot is “Normal Q-Q” plot of theoretical quantiles versus standardized quantiles for the residual values. These should fall on roughly a straight line if the residuals are normally distributed.\nThe third (bottom left) plot (“Scale-Location”) is similar to the first, but graphs the square root of the standardized residuals versus fitted values of y and shows whether or the magnitude of residuals differs across the fitted values of y. Again, it is good if you see a horizontal line with equally spread points rather than a decrease or increase in spread across the range of fitted ys, which would indicate that the error variance increases or decreases across the relationship between y and x in your model.\nThe fourth (bottom right) plot (“Residuals vs. Leverage”) highlights whether there are any particular observations with disproportionate influence on the model. In particular, we look to see if there are cases that fall in the upper or lower right portion of the plot.\nWe can also do a QQ plot of our residuals…\n\nqqnorm(m$residuals)\nqqline(m$residuals)\n\n\n\n\n\n\n\n\nPerhaps more helpful QQ plots can be found in the {car} and {ggpubr} package. The functions qqPlot() and ggqqplot() provides a trend line and confidence intervals that allow us to see exactly which points make the sample fall outside of normality (if any). Let’s take a look:\n\nqqPlot(m, distribution = \"norm\", id = FALSE)  # qqPlot from {car}\n\n\n\n\n\n\n\n# `id=FALSE` means that outlier observations will not be labelled\nlibrary(ggpubr)\nggqqplot(m$residuals)\n\n\n\n\n\n\n\ndetach(package:ggpubr)\n\nFinally, there are a number of tests for normality that we can run within the R framework and using other packages. A Shapiro-Wilk Normality Test is perhaps the most widely used, where a low p value would indicate deviation from normality (technically, a measure of how far the trend line of the residuals deviates from the Q-Q plot line).\n\n(s &lt;- shapiro.test(m$residuals))\n\n## \n##  Shapiro-Wilk normality test\n## \n## data:  m$residuals\n## W = 0.99713, p-value = 0.07041\n\n\nAs you can see, although there are some points at the higher quantiles that suggest non-normality, the Shapiro-Wilk test suggests that it is not quite non-normal, so our use of parametric statistics should be okay.\nSome other popular tests for normality, and the cases in which they are best used, are listed below:\n\nAnderson-Darling test from the {nortest} package\n\nVery popular, not quite as powerful as Shapiro-Wilk\nBest used when \\(n\\) ≥ 8\nnortest:: ad.test()\n\nMartinez-Iglewicz test from the {PoweR} package\n\nTests for dispersion from the median\nVery powerful for heavy-tailed distributions\nBest with small sample sizes (\\(n\\) ≥ 3)\nPoweR::stat0032.MartinezIglewicz()\n\nKolmogorov-Smirnov test (with Lilliefors adjustment) from the {nortest} package\n\nNot as good as Anderson-Darling, but historically popular\nRequires that \\(n\\) ≥ 4.\nnortest::lillie.test()\n\nD-Agostino Omnibus test (based on assessment of skew and kurtosis) from the {fBasics} package\n\nRobust against identical values in distribution\nSkewness test requires \\(n\\) ≥ 8\nKurtosis test requires \\(n\\) ≥ 20\nfBasics::dagoTest()\n\n\nFor a good discussion/demonstration of the relative power of each of these tests (meaning the probability that the test will correctly reject the null hypothesis) at different sample sizes, check out this link or this pdf, especially the tables on pages 670-8 and 670-9 and the plots on 670-10. This can help you better understand which test is best for a given sample size, and how much faith to put in these tests given your sample!\n\n\nCHALLENGE\nLoad in the “KamilarAndCooper.csv” dataset and develop a linear model to look at the relationship between “weaning age” and “female body mass”. You will probably need to look at the data and variable names again to find the appropriate variables to examine.\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/KamilarAndCooperData.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\nnames(d)\n\n##  [1] \"Scientific_Name\"         \"Family\"                 \n##  [3] \"Genus\"                   \"Species\"                \n##  [5] \"Brain_Size_Species_Mean\" \"Brain_Size_Female_Mean\" \n##  [7] \"Brain_size_Ref\"          \"Body_mass_male_mean\"    \n##  [9] \"Body_mass_female_mean\"   \"Mass_Dimorphism\"        \n## [11] \"Mass_Ref\"                \"MeanGroupSize\"          \n## [13] \"AdultMales\"              \"AdultFemale\"            \n## [15] \"AdultSexRatio\"           \"Social_Organization_Ref\"\n## [17] \"InterbirthInterval_d\"    \"Gestation\"              \n## [19] \"WeaningAge_d\"            \"MaxLongevity_m\"         \n## [21] \"LitterSz\"                \"Life_History_Ref\"       \n## [23] \"GR_MidRangeLat_dd\"       \"Precip_Mean_mm\"         \n## [25] \"Temp_Mean_degC\"          \"AET_Mean_mm\"            \n## [27] \"PET_Mean_mm\"             \"Climate_Ref\"            \n## [29] \"HomeRange_km2\"           \"HomeRangeRef\"           \n## [31] \"DayLength_km\"            \"DayLengthRef\"           \n## [33] \"Territoriality\"          \"Fruit\"                  \n## [35] \"Leaves\"                  \"Fauna\"                  \n## [37] \"DietRef1\"                \"Canine_Dimorphism\"      \n## [39] \"Canine_Dimorphism_Ref\"   \"Feed\"                   \n## [41] \"Move\"                    \"Rest\"                   \n## [43] \"Social\"                  \"Activity_Budget_Ref\"\n\nd &lt;- select(d, WeaningAge = \"WeaningAge_d\", FemaleBodyMass = \"Body_mass_female_mean\")\n# keep select columns\nd &lt;- na.omit(d)  # get rid of NAs\nggplot(data = d, aes(x = FemaleBodyMass, y = WeaningAge)) + geom_point() + geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\nUsing the procedures outlined above and in Module 18, calculate estimates of \\(\\beta_0\\) and \\(\\beta_1\\) by hand and by using the lm() function. Are the regression coefficients estimated under a simple linear model statistically significantly different from zero?\n\n\n\nShow Code\n# by hand\nbeta1 &lt;- cor(d$FemaleBodyMass, d$WeaningAge) * (sd(d$WeaningAge)/sd(d$FemaleBodyMass))\nbeta1\n\n\n## [1] 0.02012737\n\n\nShow Code\nbeta0 &lt;- mean(d$WeaningAge) - beta1 * mean(d$FemaleBodyMass)\nbeta0\n\n\n## [1] 201.634\n\n\nShow Code\n# using lm()\nm &lt;- lm(data = d, WeaningAge ~ FemaleBodyMass)\n\n\n\nConstruct an ANOVA table by hand and compare your values to the results of running lm() and then looking at summary.aov(lm()).\n\n\n# by hand\nSSY &lt;- sum((m$model$WeaningAge - mean(m$model$WeaningAge))^2)\nSSR &lt;- sum((m$fitted.values - mean(m$model$WeaningAge))^2)\nSSE &lt;- sum((m$model$WeaningAge - m$fitted.values)^2)\nDFR &lt;- 1\nDFE &lt;- nrow(d) - DFR - 1\nDFY &lt;- nrow(d) - DFR\nMSR &lt;- SSR/DFR\nMSE &lt;- SSE/DFE\nMSY &lt;- SSY/DFY\nfratio &lt;- MSR/MSE\np &lt;- 1 - pf(q = fratio, df1 = DFR, df2 = DFE)\n(aov_table &lt;- tibble(Source = c(\"Regression\", \"Error\", \"Total\"), df = c(DFR, DFE,\n    DFY), `Sum Sq` = c(SSR, SSE, SSY), `Mean Sq` = c(MSR, MSE, MSY), `F value` = c(fratio,\n    NA, NA), p = c(p, NA, NA)))\n\n## # A tibble: 3 × 6\n##   Source        df `Sum Sq` `Mean Sq` `F value`     p\n##   &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n## 1 Regression     1 3668116.  3668116.      109.     0\n## 2 Error        114 3833822.    33630.       NA     NA\n## 3 Total        115 7501938.    65234.       NA     NA\n\n# using summary.aov()\nsummary.aov(m)\n\n##                 Df  Sum Sq Mean Sq F value Pr(&gt;F)    \n## FemaleBodyMass   1 3668116 3668116   109.1 &lt;2e-16 ***\n## Residuals      114 3833822   33630                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nGenerate the residuals for your linear model, plot them in relation to female body mass, and make a histogram of the residuals. Do they appear to be normally distributed?\n\n\n\nShow Code\nresiduals &lt;- m$residuals\nplot(residuals ~ d$FemaleBodyMass, xlab = \"Female Body Mass\", ylab = \"Residuals\")\n\n\n\n\n\n\n\n\n\nShow Code\nhist(residuals, breaks = 20, main = \"Histogram of Residuals\")\n\n\n\n\n\n\n\n\n\n\nRun the plot() command on the result of lm() and examine the 4 plots produced.\n\n\n\nShow Code\npar(mfrow = c(2, 2))\nplot(m)\n\n\n\n\n\n\n\n\n\nAgain, based on examination of the residuals and the results of Shapiro-Wilks test, does it look like your model has good fit?\n\n\nShow Code\npar(mfrow = c(1, 1))\nqqnorm(m$residuals)\nqqline(m$residuals)\n\n\n\n\n\n\n\n\n\nShow Code\n(s &lt;- shapiro.test(m$residuals))\n\n\n## \n##  Shapiro-Wilk normality test\n## \n## data:  m$residuals\n## W = 0.86291, p-value = 5.825e-09",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Elements of Regression Analysis</span>"
    ]
  },
  {
    "objectID": "19-module.html#data-transformations",
    "href": "19-module.html#data-transformations",
    "title": "19  Elements of Regression Analysis",
    "section": "19.6 Data Transformations",
    "text": "19.6 Data Transformations\nRecall, again, that for linear regression modeling to be appropriate, two important conditions need to be met: [1] our variables (and the error variance in our variables) should be approximately normally distributed and [2] there should be homogeneity of variance (“homoscedasticity”) in our response variable around the range of our predictor variable.\nIn many cases, these conditions may not be met… for example, the continuous metric data we have may not, in fact, be normally distributed. Nonetheless, we can sometimes apply some kind of mathematical transformation to our data to change their distribution to more closely approximate the normal.\nThe logarithmic or “log” transformation (where we take the log value of each data point) is often applied to positive numeric variables with heavy skew to dramatically reduce the overall range of the data and bring extreme observations closer to a measure of centrality. The logarithm for a number is the power to which you must raise a base value (e.g., \\(e\\), the natural log) in order to obtain that number. This is an example of a “power transformation”, other examples of which include the square root transformation and the reciprocal (or multiplicative inverse) transformation.\n\nCHALLENGE\nReturn to the original “KamilarAndCooper.csv” dataset you were looking at above, log() transform both of your variables, and then run a simple, bivariate linear model.\nDo you notice a difference between these results and those obtained using untransformed variables?\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/KamilarAndCooperData.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\nnames(d)\n\n##  [1] \"Scientific_Name\"         \"Family\"                 \n##  [3] \"Genus\"                   \"Species\"                \n##  [5] \"Brain_Size_Species_Mean\" \"Brain_Size_Female_Mean\" \n##  [7] \"Brain_size_Ref\"          \"Body_mass_male_mean\"    \n##  [9] \"Body_mass_female_mean\"   \"Mass_Dimorphism\"        \n## [11] \"Mass_Ref\"                \"MeanGroupSize\"          \n## [13] \"AdultMales\"              \"AdultFemale\"            \n## [15] \"AdultSexRatio\"           \"Social_Organization_Ref\"\n## [17] \"InterbirthInterval_d\"    \"Gestation\"              \n## [19] \"WeaningAge_d\"            \"MaxLongevity_m\"         \n## [21] \"LitterSz\"                \"Life_History_Ref\"       \n## [23] \"GR_MidRangeLat_dd\"       \"Precip_Mean_mm\"         \n## [25] \"Temp_Mean_degC\"          \"AET_Mean_mm\"            \n## [27] \"PET_Mean_mm\"             \"Climate_Ref\"            \n## [29] \"HomeRange_km2\"           \"HomeRangeRef\"           \n## [31] \"DayLength_km\"            \"DayLengthRef\"           \n## [33] \"Territoriality\"          \"Fruit\"                  \n## [35] \"Leaves\"                  \"Fauna\"                  \n## [37] \"DietRef1\"                \"Canine_Dimorphism\"      \n## [39] \"Canine_Dimorphism_Ref\"   \"Feed\"                   \n## [41] \"Move\"                    \"Rest\"                   \n## [43] \"Social\"                  \"Activity_Budget_Ref\"\n\n# keep select columns\nd &lt;- select(d, WeaningAge = \"WeaningAge_d\", FemaleBodyMass = \"Body_mass_female_mean\")\nd &lt;- na.omit(d)  # get rid of NAs\nd$logWeaningAge &lt;- log(d$WeaningAge)\nd$logFemaleBodyMass &lt;- log(d$FemaleBodyMass)\nggplot(data = d, aes(x = logFemaleBodyMass, y = logWeaningAge)) + geom_point() +\n    geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n# or plot(data=d, logWeaningAge ~ logFemaleBodyMass)\nm &lt;- lm(data = d, logWeaningAge ~ logFemaleBodyMass)\nsummary(m)\n\n## \n## Call:\n## lm(formula = logWeaningAge ~ logFemaleBodyMass, data = d)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.10639 -0.32736  0.00848  0.32214  1.11010 \n## \n## Coefficients:\n##                   Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)         1.7590     0.2196   8.011 1.08e-12 ***\n## logFemaleBodyMass   0.4721     0.0278  16.983  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.4532 on 114 degrees of freedom\n## Multiple R-squared:  0.7167, Adjusted R-squared:  0.7142 \n## F-statistic: 288.4 on 1 and 114 DF,  p-value: &lt; 2.2e-16\n\npar(mfrow = c(2, 2))\nplot(m)\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))\nqqPlot(m$residuals)  ## qqPlot from {car}\n\n\n\n\n\n\n\n\n## [1]  24 116\n\n(s &lt;- shapiro.test(m$residuals))\n\n## \n##  Shapiro-Wilk normality test\n## \n## data:  m$residuals\n## W = 0.99367, p-value = 0.8793\n\n\nThe following chart and graphs shows some other common numerical transformations that are often useful for changing a variable’s distribution to more closely approximate the normal.\n\n\n\n\n\n\n\n\n\n\npar(mfrow = c(1, 2))\na &lt;- 2\nb &lt;- 2\n\n# LOG X\nx &lt;- seq(from = 0, to = 100, length.out = 1000)\ny &lt;- a + b * log(x)\nplot(x, y, type = \"l\", main = \"untransformed\")\nplot(log(x), y, type = \"l\", main = \"log(x)\")\n\n\n\n\n\n\n\n# LOG Y - a log transformation for the response variable is useful when the\n# variance in the explanatory variable is not consistent across the range of\n# explanatory variable values\nx &lt;- seq(from = 0, to = 10, length.out = 1000)\ny &lt;- exp(a + b * x)\nplot(x, y, type = \"l\", main = \"untransformed\")\nplot(x, log(y), type = \"l\", main = \"log(y)\")\n\n\n\n\n\n\n\n# ASYMPTOTIC\nx &lt;- seq(from = 1, to = 100, length.out = 100)\ny &lt;- (a * x)/(1 + b * x)\nplot(x, y, type = \"l\", main = \"untransformed\")\nplot(1/x, y, type = \"l\", main = \"1/x\")\n\n\n\n\n\n\n\n# RECIPROCAL\nx &lt;- seq(from = 1, to = 100, length.out = 100)\ny &lt;- a + b/x\nplot(x, y, type = \"l\", main = \"untransformed\")\nplot(1/x, y, type = \"l\", main = \"1/x\")\n\n\n\n\n\n\n\n# POWER - a power transformation for the explanatory variable is useful # if\n# there is a curvilinear relationship between the explanatory variable and the\n# response variable\nx &lt;- seq(from = 1, to = 100, length.out = 100)\ny &lt;- a * x^b\nplot(x, y, type = \"l\", main = \"untransformed\")\nplot(x^b, y, type = \"l\", main = \"x^b\")\n\n\n\n\n\n\n\n# EXPONENTIAL\nx &lt;- seq(from = 1, to = 10, length.out = 100)\ny &lt;- a * exp(b * x)\nplot(x, y, type = \"l\", main = \"untransformed\")\nplot(x, log(y), type = \"l\", main = \"log(y)\")\n\n\n\n\n\n\n\n\nApplying a function to transform a complete dataset is the most common way of attempting to squeeze your data into matching a normal distribution so that you can use parametric statistics. This is a very common practice, and much has been written on this topic, including its drawbacks.\nFor a good discussion on common transformations, see R in Action Chapter 8.5 (“Corrective Measures”), and please do note the section titled A caution concerning transformations. When trying out transformations, keep your normalization tests handy so that you can retest the transformed data and see if the transformation achieved its intended purpose.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Elements of Regression Analysis</span>"
    ]
  },
  {
    "objectID": "19-module.html#concept-review",
    "href": "19-module.html#concept-review",
    "title": "19  Elements of Regression Analysis",
    "section": "Concept Review",
    "text": "Concept Review\n\nRegression analysis partitions the variance in a response variable into components associated with the various predictor variables plus an “error” component, or residual\nThe significance of the overall regression model, and of individual predictors, is evaluated by comparing F ratios (the ratio of the variance explained by the predictor to the unexplained variance) against an F distribution, taking into account the number of degrees of freedom in each\nAfter running a model, we need to confirm that key assumptions for linear regression are met, including that residuals are normally distributed and observations show homogeneity of variance across the range of predictor values\nIf these assumptions are not met, data transformation of the raw variables may sometimes be helpful",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Elements of Regression Analysis</span>"
    ]
  },
  {
    "objectID": "20-module.html",
    "href": "20-module.html",
    "title": "20  Categorical Data Analysis",
    "section": "",
    "text": "20.1 Objectives",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "20-module.html#objectives",
    "href": "20-module.html#objectives",
    "title": "20  Categorical Data Analysis",
    "section": "",
    "text": "In this module, we examine how simple linear regression can be applied to datasets where our predictor variable is discrete or categorical rather than continuous. Indeed, we will see that One-Way (and Two-Way) Analysis of Variance (ANOVA) is a specific application of simple (and multiple) linear regression. We also look at other methods for basic statistical analysis of categorical data.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "20-module.html#preliminaries",
    "href": "20-module.html#preliminaries",
    "title": "20  Categorical Data Analysis",
    "section": "20.2 Preliminaries",
    "text": "20.2 Preliminaries\n\nInstall these packages in R: {permuco}, {dunn.test}, {conover.test}\nInstall and load this package in R: {effectsize}\nLoad {tidyverse}, {broom}, and {infer}",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "20-module.html#categorical-predictors",
    "href": "20-module.html#categorical-predictors",
    "title": "20  Categorical Data Analysis",
    "section": "20.3 Categorical Predictors",
    "text": "20.3 Categorical Predictors\nThus far we have used simple linear regression models involving continuous explanatory variables, but we can also use a discrete or categorical explanatory variable, made up of 2 or more groups that are coded as “factors” (i.e., we use integer values from 1 to \\(k\\) discrete groups as dummy values for our categorical variables). Let’s load in our zombie apocalpyse survivor data again, but this time after doing so, we will convert gender and major to factors using the function as.factor(). [We could also do this by reading the data in using read.csv(), which has as a default argument stringsAsFactors=TRUE.] Then look at the class() and summary() of the variable gender.\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/zombies.csv\"\nz &lt;- read_csv(f, col_names = TRUE)\n\n## Rows: 1000 Columns: 10\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (4): first_name, last_name, gender, major\n## dbl (6): id, height, weight, zombies_killed, years_of_education, age\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nz$gender &lt;- factor(z$gender)  # can also use `as.factor()`\nz$major &lt;- factor(z$major)  # can also use `as.factor()`\n\nclass(z$gender)\n\n## [1] \"factor\"\n\nsummary(z$gender)\n\n## Female   Male \n##    494    506\n\n\nAs with our prior simple regression analysis, we want to evaluate the effect of a predictor variable on a response variable (e.g., height), but this time we want our predictor to be a discrete, or categorical, variable (e.g., gender) rather than a continuous one. We can start off by plotting height by gender using the same formula notation we have been using.\n\nplot(z$height ~ z$gender)\n\n\n\n\n\n\n\n# or, with {ggplot}\nggplot(data = z, aes(x = gender, y = height)) + geom_boxplot()\n\n\n\n\n\n\n\n\nThis immediately gives us a nice boxplot. Note that if we’d try to use this command after loading in gender as character rather than factor data, R would have thrown an error.\nBased on our plot, there indeed seems to be a difference in height between males and females. We can test this directly using linear regression (and, recall, we already know another way to test this, using \\(Z\\) or t tests to compare means).\n\nm &lt;- lm(data = z, height ~ gender)\n\nIf we take a look at the summary() of our model, m, we see the same kind of table of results we have seen before, but because the predictor variable in this case is a factor vector instead of a numeric vector, the coefficients are reported and interpreted a bit differently.\n\nsummary(m)\n\n## \n## Call:\n## lm(formula = height ~ gender, data = z)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -15.4642  -2.4861   0.0876   2.5425  11.0065 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  65.5983     0.1717  382.13   &lt;2e-16 ***\n## genderMale    4.0154     0.2413   16.64   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.815 on 998 degrees of freedom\n## Multiple R-squared:  0.2172, Adjusted R-squared:  0.2164 \n## F-statistic: 276.9 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\nThe coefficient for the intercept, i.e., \\(\\beta_0\\), reflects the estimate of the mean height for the first of our level variables.\n\nlevels(z$gender)\n\n## [1] \"Female\" \"Male\"\n\n\nThe estimate for \\(\\beta_1\\) is reported as “genderMale” and the value for that coefficient, 4.0154, is the estimated difference in mean height associated with being a male. The regression equation is basically:\n\\[height = 65.5983 + 4.0154 \\times gender\\]\nwith males assigned a gender value of 1 and females of 0.\nIn this case, the p value associated with the t statistic for \\(\\beta_1\\) is extremely low, so we conclude that gender has a significant effect on height.\nWe can easily relevel() what is the baseline group. The result is very similar, but the sign of \\(\\beta_1\\) is changed.\n\nz$gender &lt;- relevel(z$gender, ref = \"Male\")\nm &lt;- lm(data = z, height ~ gender)\nsummary(m)\n\n## \n## Call:\n## lm(formula = height ~ gender, data = z)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -15.4642  -2.4861   0.0876   2.5425  11.0065 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   69.6137     0.1696  410.42   &lt;2e-16 ***\n## genderFemale  -4.0154     0.2413  -16.64   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.815 on 998 degrees of freedom\n## Multiple R-squared:  0.2172, Adjusted R-squared:  0.2164 \n## F-statistic: 276.9 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\nThe last line of the summary() output shows the results of the global test of significance of the regression model based on an F statistic compared to an F distribution with, in this case, 1 and 998 degrees of freedom.\n\np &lt;- 1 - pf(q = 276.9, df1 = 1, df2 = 998)\np\n\n## [1] 0\n\n\nWe can extend this approach to the case where we have more than two categories for a variable… in this case we need to dummy code our factor variable into multiple binary variables. R takes care of this for us automatically, but it is good to recognize the procedure.\nLet’s explore this by re-coding the variable major into four levels. We can first use the unique() or levels() function to list all of the different majors in our dataset. The latter does this alphabetically.\n\nunique(z$major)\n\n##  [1] medicine/nursing                      criminal justice administration      \n##  [3] education                             energy studies                       \n##  [5] logistics                             psychology                           \n##  [7] botany                                communication                        \n##  [9] military strategy                     economics                            \n## [11] mechanical engineering                physical education                   \n## [13] philosophy                            biology                              \n## [15] applied sciences                      animal husbandry                     \n## [17] agricultural sciences                 culinary services                    \n## [19] city planning                         integrated water resources management\n## [21] pharmacology                          environmental science                \n## [23] human services                        epidemiology                         \n## [25] business administration               architecture                         \n## 26 Levels: agricultural sciences animal husbandry ... psychology\n\nlevels(z$major)\n\n##  [1] \"agricultural sciences\"                \n##  [2] \"animal husbandry\"                     \n##  [3] \"applied sciences\"                     \n##  [4] \"architecture\"                         \n##  [5] \"biology\"                              \n##  [6] \"botany\"                               \n##  [7] \"business administration\"              \n##  [8] \"city planning\"                        \n##  [9] \"communication\"                        \n## [10] \"criminal justice administration\"      \n## [11] \"culinary services\"                    \n## [12] \"economics\"                            \n## [13] \"education\"                            \n## [14] \"energy studies\"                       \n## [15] \"environmental science\"                \n## [16] \"epidemiology\"                         \n## [17] \"human services\"                       \n## [18] \"integrated water resources management\"\n## [19] \"logistics\"                            \n## [20] \"mechanical engineering\"               \n## [21] \"medicine/nursing\"                     \n## [22] \"military strategy\"                    \n## [23] \"pharmacology\"                         \n## [24] \"philosophy\"                           \n## [25] \"physical education\"                   \n## [26] \"psychology\"\n\n\nWe can then also do some cool batch recoding using the %in% operator and the mutate() function from {dplyr}…\n\nz &lt;- z |&gt;\n    mutate(occupation = case_when(major %in% c(\"agricultural sciences\", \"animal husbandry\",\n        \"applied sciences\", \"biology\", \"botany\", \"energy studies\", \"environmental science\",\n        \"epidemiology\", \"medicine/nursing\", \"pharmacology\") ~ \"natural science\",\n        major %in% c(\"business administration\", \"city planning\", \"economics\", \"human services\",\n            \"logistics\", \"military strategy\") ~ \"logistics\", major %in% c(\"architecture\",\n            \"integrated water resources management\", \"mechanical engineering\") ~\n            \"engineering\", major %in% c(\"communication\", \"criminal justice administration\",\n            \"culinary services\", \"education\", \"philosophy\", \"physical education\",\n            \"psychology\") ~ \"other\"))\nz$occupation &lt;- factor(z$occupation)  # can also use `as.factor()`\nlevels(z$occupation)\n\n## [1] \"engineering\"     \"logistics\"       \"natural science\" \"other\"\n\nz$occupation &lt;- relevel(z$occupation, ref = \"natural science\")\nlevels(z$occupation)\n\n## [1] \"natural science\" \"engineering\"     \"logistics\"       \"other\"\n\n\nAgain, we can plot our variable by group and run a multilevel linear regression. Each \\(\\beta\\) estimate reflects the difference from the estimated mean for the reference level. The lm() function also returns the results of the global significance test of our model.\n\nplot(data = z, zombies_killed ~ occupation)\n\n\n\n\n\n\n\nm &lt;- lm(data = z, zombies_killed ~ occupation)\nsummary(m)\n\n## \n## Call:\n## lm(formula = zombies_killed ~ occupation, data = z)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.3061 -1.0068 -0.0068  1.0092  8.0974 \n## \n## Coefficients:\n##                       Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)            2.90256    0.08848  32.804   &lt;2e-16 ***\n## occupationengineering  0.40356    0.19745   2.044   0.0412 *  \n## occupationlogistics    0.08826    0.14777   0.597   0.5504    \n## occupationother        0.10424    0.13496   0.772   0.4401    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.747 on 996 degrees of freedom\n## Multiple R-squared:  0.004209,   Adjusted R-squared:  0.00121 \n## F-statistic: 1.403 on 3 and 996 DF,  p-value: 0.2403\n\np &lt;- 1 - pf(q = 0.489, df1 = 3, df2 = 996)  # F test\np\n\n## [1] 0.6899872\n\n\nIn this case, we see no significant effect of occupation (based on a broad categorization of college major) on zombie killing proficiency.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "20-module.html#one-way-anova",
    "href": "20-module.html#one-way-anova",
    "title": "20  Categorical Data Analysis",
    "section": "20.4 One-Way ANOVA",
    "text": "20.4 One-Way ANOVA\nRegression with a single categorical predictor run as we have just done above is exactly equivalent to a “One-Way” or “one-factor” Analysis of Variance, or ANOVA. That is, ANOVA is just one type of special case of least squares regression.\nWe can, of course, run an ANOVA with one line in R. Compare the results presented in the summary() output table (or tidy() output table) from an ANOVA with that from the global test reported in summary() (or tidy()) from lm()\n\nm.aov &lt;- aov(data = z, zombies_killed ~ occupation)\nsummary(m.aov)\n\n##              Df Sum Sq Mean Sq F value Pr(&gt;F)\n## occupation    3   12.9   4.285   1.403   0.24\n## Residuals   996 3041.1   3.053\n\ntidy(m.aov)  # a neater table\n\n## # A tibble: 2 × 6\n##   term          df  sumsq meansq statistic p.value\n##   &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n## 1 occupation     3   12.9   4.28      1.40   0.240\n## 2 Residuals    996 3041.    3.05     NA     NA\n\nm.lm &lt;- lm(data = z, zombies_killed ~ occupation)\nsummary(m.lm)\n\n## \n## Call:\n## lm(formula = zombies_killed ~ occupation, data = z)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.3061 -1.0068 -0.0068  1.0092  8.0974 \n## \n## Coefficients:\n##                       Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)            2.90256    0.08848  32.804   &lt;2e-16 ***\n## occupationengineering  0.40356    0.19745   2.044   0.0412 *  \n## occupationlogistics    0.08826    0.14777   0.597   0.5504    \n## occupationother        0.10424    0.13496   0.772   0.4401    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.747 on 996 degrees of freedom\n## Multiple R-squared:  0.004209,   Adjusted R-squared:  0.00121 \n## F-statistic: 1.403 on 3 and 996 DF,  p-value: 0.2403\n\ntidy(m.lm)  # a neater table\n\n## # A tibble: 4 × 5\n##   term                  estimate std.error statistic   p.value\n##   &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)             2.90      0.0885    32.8   1.27e-160\n## 2 occupationengineering   0.404     0.197      2.04  4.12e-  2\n## 3 occupationlogistics     0.0883    0.148      0.597 5.50e-  1\n## 4 occupationother         0.104     0.135      0.772 4.40e-  1\n\npar(mfrow = c(2, 2))\nplot(m.lm)  # plot(m.aov) is equivalent\n\n\n\n\n\n\n\n\nThese two model summaries produce somewhat different tables for the same analysis. The F statistic and omnibus p value given in the aov() ANOVA table indicates whether there are differences between at least some treatments, but we do not know where those differences occur. The summary from lm() shows the effects of the categorical treatments arranged as sequential contrasts (the default in R). The first row gives the mean of the first level of the first factor specified in the model. The following rows give differences from this first mean for each subsequent factor level. Likewise the standard error in the first row is a standard error of a mean, while the entries in subsequent rows are standard errors of the differences between two means.\nIn general, in ANOVA and simple regression using a single categorical variable, we aim to test the \\(H_0\\) that the means of a variable of interest do not differ among groups, i.e., that \\(\\mu_1 = \\mu_2 = ... = \\mu_k\\) are all equal. This is an extension of our comparison of two means that we did with z and t tests.\nThe basic linear model formulation for ANOVA is:\n\\[Y_{i,j} = \\mu + \\beta_i X_i + \\epsilon_{i,j}\\]\nwhere:\n\n\\(\\mu\\) is the grand (overall) population mean\n\\(\\beta_i\\) is the deviation of the mean of treatment level \\(i\\) from the grand mean\n\\(\\epsilon_{i,j}\\) is error variance of individual points, \\(j\\), within each level, \\(i\\), from the grand mean\n\nThe assumptions of ANOVA, similar to those of simple regression, are:\n\nthat samples are independent and identically distributed (\\(iid\\))\nthat the residuals, \\(\\epsilon_{i,j}\\), are normally distributed\nthat within-group variances are similar across all groups (“homoscedastic”)\n\nAdditionally, the following assumption of standard ANOVA makes the interpretation of results more straightforward, but it is not strictly required…\n\nthat our experiment/observations have a balanced design (i.e., an equal number of cases in all groups)\n\nIf this last assumption is violated, it ceases to be true that the total SS of our dataset = within group SS + between group SS, and then the calculations of our MSE and the F statistic, and our associated p value, would be off.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "20-module.html#simulation-based-inference",
    "href": "20-module.html#simulation-based-inference",
    "title": "20  Categorical Data Analysis",
    "section": "20.5 Simulation-Based Inference",
    "text": "20.5 Simulation-Based Inference\nNote that the ANOVA functions used above (aov(), lm()) are evaluating statistical significance and estimating p values based on mathematical theory, i.e., by comparing F statistic values to a theoretical, parametric \\(F\\) distribution to determine how likely we are to see an F statistic as extreme as our observed one by chance under the null model of no difference between groups. However, as with other tests of statistical inference, we can also use permutation/randomization methods to determine p values for ANOVA.\nOne easy way to do this is with the {infer} package. Using {infer}, we can easily permute, a large number of times, the association between our response variable the categorical explanatory variable and, each time, then calculate the associated F statistic. This generates a permutation-based null distribution of F statistics to which we can compare the F statistic generated from our original data. The permutation-based p value is then the proportion of F statistic values generated by simulation that are equal to or greater than our observed F statistic value.\n\noriginal.F &lt;- aov(data = z, zombies_killed ~ occupation) |&gt;\n    tidy() |&gt;\n    filter(term == \"occupation\")\n# show aov results for F statistic and p value for omnibus F test\noriginal.F\n\n## # A tibble: 1 × 6\n##   term          df sumsq meansq statistic p.value\n##   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n## 1 occupation     3  12.9   4.28      1.40   0.240\n\npermuted.F &lt;- z |&gt;\n    # specify model\nspecify(zombies_killed ~ occupation) |&gt;\n    # null hypothesis of independence\nhypothesize(null = \"independence\") |&gt;\n    # generate permutations\ngenerate(reps = 1000, type = \"permute\") |&gt;\n    # calculate the F statistic for the AOV\ncalculate(stat = \"F\")\n\n# plot our F statistic on the distribution of F statistic values generated by\n# permutation\nvisualize(permuted.F) + shade_p_value(obs_stat = original.F$statistic, direction = \"greater\")\n\n\n\n\n\n\n\n(p.value &lt;- permuted.F |&gt;\n    get_p_value(obs_stat = original.F$statistic, direction = \"greater\"))\n\n## # A tibble: 1 × 1\n##   p_value\n##     &lt;dbl&gt;\n## 1   0.253\n\n\nNote that, for these data, the p value estimated through simulated based inference is very similar to that calculated paramterically based on mathematical theory. This is not always the case!\n\noriginal.F$p.value\n\n## [1] 0.2403441\n\np.value\n\n## # A tibble: 1 × 1\n##   p_value\n##     &lt;dbl&gt;\n## 1   0.253\n\n\nAlternatively, the {permuco} package introduces the aovperm() and lmperm() functions as analogs for aov() and lm() for analysis of variance and simple linear regression, where p values are calculated by permutation.\n\nlibrary(permuco)\nm.aovperm &lt;- aovperm(data = z, zombies_killed ~ occupation)\nsummary(m.aovperm)\n\n## Anova Table\n## Resampling test using freedman_lane to handle nuisance variables and 5000 permutations.\n##                 SS  df     F parametric P(&gt;F) resampled P(&gt;F)\n## occupation   12.85   3 1.403           0.2403          0.2478\n## Residuals  3041.08 996\n\nplot(m.aovperm)\n\n\n\n\n\n\n\nm.lmperm &lt;- lmperm(data = z, zombies_killed ~ occupation)\nsummary(m.lmperm)\n\n## Table of marginal t-test of the betas\n## Resampling test using freedman_lane to handle nuisance variables and 5000 permutations.\n##                       Estimate Std. Error t value parametric Pr(&gt;|t|)\n## (Intercept)            2.90256    0.08848 32.8042          1.269e-160\n## occupationengineering  0.40356    0.19745  2.0439           4.123e-02\n## occupationlogistics    0.08826    0.14777  0.5973           5.504e-01\n## occupationother        0.10424    0.13496  0.7724           4.401e-01\n##                       resampled Pr(&lt;t) resampled Pr(&gt;t) resampled Pr(&gt;|t|)\n## (Intercept)                                                               \n## occupationengineering           0.9780           0.0222             0.0410\n## occupationlogistics             0.7240           0.2762             0.5538\n## occupationother                 0.7898           0.2104             0.4386\n\ndetach(package:permuco)\n\nSimilar to {infer}, the {coin} package also implements permutation-based tests of independence, including One-Way ANOVA. Instead of F statistics, the test statistic implemented in {coin} is a Chi-Square statistic (which is essentially to equivalent to an F statistic, normalized by the residual degrees of freedom), and it is tested against a permutation-based null distribution of Chi-Square values.\n\nlibrary(coin)\nm.aov &lt;- oneway_test(data = z, zombies_killed ~ occupation, distribution = \"approximate\")\n# by default, 10000 replicates are used\nm.aov\n\n## \n##  Approximative K-Sample Fisher-Pitman Permutation Test\n## \n## data:  zombies_killed by\n##   occupation (natural science, engineering, logistics, other)\n## chi-squared = 4.2048, p-value = 0.2392\n\ndetach(package:coin)\n\n\nCHALLENGE\nLoad in the “gibbon-femurs.csv” dataset, which contains the lengths, in centimeters, of the femurs of 400 juvenile, subadult, and adult individuals gibbons. Use both ANOVA and simple linear regression to examine the relationship between age and femur length.\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/gibbon-femurs.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\n\n## Rows: 525 Columns: 4\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (2): age, sex\n## dbl (2): id, femur_length\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nd$age &lt;- factor(d$age, levels = c(\"inf\", \"juv\", \"subadult\", \"adult\"))\n# converts age to a factor and order the age levels so that they are in\n# ascending chronological order several ANOVA functions require that\n# categorical variables are represented as factors rather than strings\nd$sex &lt;- factor(d$sex, levels = c(\"female\", \"male\"))\n# convert sex to a factor\nhead(d)\n\n## # A tibble: 6 × 4\n##      id age   sex    femur_length\n##   &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;         &lt;dbl&gt;\n## 1     1 inf   female         7.28\n## 2     2 inf   male           6.3 \n## 3     3 inf   female         7.44\n## 4     4 inf   male           8.69\n## 5     5 inf   female         9.13\n## 6     6 inf   male           9.28\n\n\nBefore beginning, check for normality of data overall…\n\n\nCode\npar(mfrow = c(1, 2))\nhist(d$femur_length)\nqqnorm(d$femur_length)\n\n\n\n\n\n\n\n\n\nCode\n# not great overall...\n\n\nThen, do a boxplot of the data by each age group and check for normality within each group.\n\n\nShow Code\nplot(data = d, femur_length ~ age)  # boxplot with medians\n\n# calculate average and SD by group\nstats &lt;- d |&gt;\n    group_by(age) |&gt;\n    summarize(`mean(femur_length)` = mean(femur_length), `sd(femur_length)` = sd(femur_length))\n\n# add means to plot\npoints(1:4, stats$`mean(femur_length)`, pch = 4, cex = 1.5)\n\n\n\n\n\n\n\n\n\nShow Code\n# subtract relevant group mean from each data point\nmeans.centered &lt;- d$femur_length - stats[as.numeric(d$age), 2]\n\n# graphical test for normality of group-centered means\nqqnorm(means.centered$`mean(femur_length)`)  # looks good!\n\n\n\n\n\n\n\n\n\nShow Code\n# now do graphical test within each group\npar(mfrow = c(1, 2))\nhist(d$femur_length[d$age == \"inf\"], main = \"Infant\", xlab = \"Femur Length (cm)\")\nqqnorm(d$femur_length[d$age == \"inf\"])\n\n\n\n\n\n\n\n\n\nShow Code\nhist(d$femur_length[d$age == \"juv\"], main = \"Juvenile\", xlab = \"Femur Length (cm)\")\nqqnorm(d$femur_length[d$age == \"juv\"])\n\n\n\n\n\n\n\n\n\nShow Code\nhist(d$femur_length[d$age == \"subadult\"], main = \"Subadult\", xlab = \"Femur Length (cm)\")\nqqnorm(d$femur_length[d$age == \"subadult\"])\n\n\n\n\n\n\n\n\n\nShow Code\nhist(d$femur_length[d$age == \"adult\"], main = \"Adult\", xlab = \"Femur Length (cm)\")\nqqnorm(d$femur_length[d$age == \"adult\"])\n\n\n\n\n\n\n\n\n\nAlso, check whether the different groups have roughly equal variances. As a general rule of thumb, we can compare the largest and smallest within-group standard deviations; if the ratio of max-to-min is less, than 2 then the assumption of “equal” variances is usually reasonable.\n\n\nShow Code\n# check that variances are roughly equal (ratio of max/min is &lt;2)\nmax(stats$`sd(femur_length)`)/min(stats$`sd(femur_length)`)\n\n\nShow Output\n## [1] 1.767635\n\n\n\nAll this checking done, we can again plot our data by group and then run our linear or ANOVA model…\n\npar(mfrow = c(1, 1))\nplot(data = d, femur_length ~ age, xlab = \"Age\", ylab = \"Femur Length (cm)\")\n\n\n\n\n\n\n\nm &lt;- lm(data = d, femur_length ~ age)\nsummary(m)\n\n## \n## Call:\n## lm(formula = femur_length ~ age, data = d)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.2052 -1.1104  0.1089  1.0396  5.6748 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   7.7104     0.1180   65.32   &lt;2e-16 ***\n## agejuv        2.3406     0.1803   12.98   &lt;2e-16 ***\n## agesubadult   6.0460     0.2045   29.57   &lt;2e-16 ***\n## ageadult      9.7147     0.2260   42.98   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.669 on 521 degrees of freedom\n## Multiple R-squared:  0.809,  Adjusted R-squared:  0.8079 \n## F-statistic: 735.8 on 3 and 521 DF,  p-value: &lt; 2.2e-16\n\nm.aov &lt;- aov(data = d, femur_length ~ age)  # femur length related to age\nsummary(m.aov)\n\n##              Df Sum Sq Mean Sq F value Pr(&gt;F)    \n## age           3   6152  2050.6   735.8 &lt;2e-16 ***\n## Residuals   521   1452     2.8                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIs the omnibus test of the relationship between age category and femur length significant? YES! Are femur lengths significantly different for juveniles versus subadults? Subadults versus adults? Juveniles versus adults?\n\nHINT: To test some of these additional bivariate options, you will need to relevel() your factors for simple linear regression. Currently, we are just testing each group relative to the first level of the factor age.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "20-module.html#post-hoc-tests-in-anova",
    "href": "20-module.html#post-hoc-tests-in-anova",
    "title": "20  Categorical Data Analysis",
    "section": "20.6 Post-Hoc Tests in ANOVA",
    "text": "20.6 Post-Hoc Tests in ANOVA\nAfter finding a significant omnibus F statistic in an ANOVA, we can test, post-hoc, what group means are different from one another using pairwise t tests with an appropriate p value correction for multiple tests.\n\npairwise.t.test(d$femur_length, d$age, p.adj = \"bonferroni\")\n\n## \n##  Pairwise comparisons using t tests with pooled SD \n## \n## data:  d$femur_length and d$age \n## \n##          inf    juv    subadult\n## juv      &lt;2e-16 -      -       \n## subadult &lt;2e-16 &lt;2e-16 -       \n## adult    &lt;2e-16 &lt;2e-16 &lt;2e-16  \n## \n## P value adjustment method: bonferroni\n\n\nAfter an ANOVA, we can also use a Tukey Honest Significant Differences test with the model as the argument to evaluate this.\n\nNOTE: The TukeyHSD() function is run on the output of the ANOVA (aov()) function.\n\n\n# renaming age categories so we get nice axis labels on plot...\nd &lt;- d |&gt;\n    mutate(age = case_when(age == \"adult\" ~ \"A\", age == \"subadult\" ~ \"S\", age ==\n        \"juv\" ~ \"J\", age == \"inf\" ~ \"I\"))\nm &lt;- aov(femur_length ~ age, data = d)\nposthoc &lt;- TukeyHSD(m, which = \"age\", ordered = TRUE, conf.level = 0.95)\nposthoc  # all age-sex classes differ\n\n##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n##     factor levels have been ordered\n## \n## Fit: aov(formula = femur_length ~ age, data = d)\n## \n## $age\n##         diff      lwr       upr p adj\n## J-I 2.340617 1.875875  2.805359     0\n## S-I 6.045950 5.518982  6.572918     0\n## A-I 9.714750 9.132165 10.297335     0\n## S-J 3.705333 3.149860  4.260806     0\n## A-J 7.374133 6.765643  7.982623     0\n## A-S 3.668800 3.011556  4.326044     0\n\n\nWe can get a visual summary of difference in means between each group and their confidence intervals by passing the Tukey test output to a plot() function. Confidence intervals that cross the vertical line indicate pairs of groups where the difference in mean is not significant, according to the threshold we set.\n\nplot(posthoc, xlim = c(-2, 12))  # xlim set to show zero",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "20-module.html#a-non-parametric-alternative",
    "href": "20-module.html#a-non-parametric-alternative",
    "title": "20  Categorical Data Analysis",
    "section": "20.7 A Non-Parametric Alternative",
    "text": "20.7 A Non-Parametric Alternative\nThe Kruskal-Wallis test is a nonparametric alternative to One-Way ANOVA that relaxes the need for normality in the distribution of data in each group (the different groups should still have roughly equal variances). Essentially, rather than testing the null hypothesis that the means for each group do not differ we are instead testing the null hypothesis that the medians do not differ. The test converts the continuous response variable to a set of RANKS (i.e., it does a uniform transformation) and then works with those ranks. The p value associated with the Kruskall-Wallis test statistic is calculated mathematically relative to a Chi-Square distribution.\n\n(m.kruskal &lt;- kruskal.test(data = d, femur_length ~ age))\n\n## \n##  Kruskal-Wallis rank sum test\n## \n## data:  femur_length by age\n## Kruskal-Wallis chi-squared = 407.13, df = 3, p-value &lt; 2.2e-16\n\n# to show that this is the same as the test using ranks...  use {dplyr} to sort\n# by femur.length...\nd &lt;- arrange(d, femur_length)\n# then use {dplyr} to add new variable of rank femur_length\nd &lt;- mutate(d, femur_rank = row(data.frame(d$femur_length)))\n(m.kruskal &lt;- kruskal.test(data = d, femur_rank ~ age))\n\n## \n##  Kruskal-Wallis rank sum test\n## \n## data:  femur_rank by age\n## Kruskal-Wallis chi-squared = 407.43, df = 3, p-value &lt; 2.2e-16\n\n\nFor a post-hoc test of which groups are different following a Kruskall-Wallis test, we can use either a pairwise Mann-Whitney U test, a Dunn test, or a Conover-Iman test. The latter two tests are implemented using dunn.test() from the {dunn.test} package or conover.test() from the {conover.test} package, which both allow us to show the calculated Krusal-Wallis test statistic as well.\n\n# pairwise Mann-Whitney U test the arguments are a vector of data, a vector\n# with the grouping factor, and the multiple test correction method...\npairwise.wilcox.test(d$femur_length, d$age, p.adjust.method = \"bonferroni\")\n\n## \n##  Pairwise comparisons using Wilcoxon rank sum test with continuity correction \n## \n## data:  d$femur_length and d$age \n## \n##   A      I      J     \n## I &lt;2e-16 -      -     \n## J &lt;2e-16 &lt;2e-16 -     \n## S &lt;2e-16 &lt;2e-16 &lt;2e-16\n## \n## P value adjustment method: bonferroni\n\n# `dunn.test()` and `conover.test()` include another argument, `kw=` which is\n# whether or not to output KW results\nlibrary(dunn.test)\ndunn.test(d$femur_length, g = d$age, method = \"bonferroni\", kw = TRUE)\n\n##   Kruskal-Wallis rank sum test\n## \n## data: x and group\n## Kruskal-Wallis chi-squared = 407.1264, df = 3, p-value = 0\n## \n## \n##                            Comparison of x by group                            \n##                                  (Bonferroni)                                  \n## Col Mean-|\n## Row Mean |          A          I          J\n## ---------+---------------------------------\n##        I |   17.43010\n##          |    0.0000*\n##          |\n##        J |   10.06368  -8.673333\n##          |    0.0000*    0.0000*\n##          |\n##        S |   3.468151  -14.94416  -6.920645\n##          |    0.0016*    0.0000*    0.0000*\n## \n## alpha = 0.05\n## Reject Ho if p &lt;= alpha/2\n\ndetach(package:dunn.test)\nlibrary(conover.test)\nconover.test(d$femur_length, g = d$age, method = \"bonferroni\", kw = TRUE)\n\n##   Kruskal-Wallis rank sum test\n## \n## data: x and group\n## Kruskal-Wallis chi-squared = 407.1264, df = 3, p-value = 0\n## \n## \n##                            Comparison of x by group                            \n##                                  (Bonferroni)                                  \n## Col Mean-|\n## Row Mean |          A          I          J\n## ---------+---------------------------------\n##        I |   36.80108\n##          |    0.0000*\n##          |\n##        J |   21.24798  -18.31245\n##          |    0.0000*    0.0000*\n##          |\n##        S |   7.322488  -31.55238  -14.61191\n##          |    0.0000*    0.0000*    0.0000*\n## \n## alpha = 0.05\n## Reject Ho if p &lt;= alpha/2\n\ndetach(package:conover.test)\n\n\n20.7.1 Effect Sizes\nWe can apply the effectsize() function from the {effectsize} package to our model objects to better interpret the meaning of our coefficients.\nFor an ANOVA, the effect sizes represent the amount of variance in the response variable explained by each of the model’s terms. For this model, we are asking what percent of the total variance in femur length is explained by the categorical variable of age class. This measure is called \\(\\eta^2\\) (“eta-squared”) and is analogous to r-squared.\n\\[\\eta^2 = \\frac{SS_{predictor}}{SS_{total}} = \\frac{SS_{predictor}}{SS_{predictor} + SS_{residuals}}\\]\n\neffectsize(m.aov)\n\n## For one-way between subjects designs, partial eta squared is equivalent\n##   to eta squared. Returning eta squared.\n\n\n## # Effect Size for ANOVA\n## \n## Parameter | Eta2 |       95% CI\n## -------------------------------\n## age       | 0.81 | [0.79, 1.00]\n## \n## - One-sided CIs: upper bound fixed at [1.00].\n\n# or `eta_squared(m.aov)` we can hand-calculate this same value from the ANOVA\n# table as 6152/(6152 + 1452)\n\nHere, 81% of the variance in femur length is accounted for by variance in age.\nApplied to a linear model object, the effectsize() function refits the model and returns standardized coefficients rather than unstandardized ones. This is equivalent to running the original linear model after standardizing all of the variables so that they are represented on the same scale, which is typically done via a \\(Z\\) transformation (i.e., by subtracting its mean from each variable observation and divide by its standard deviation): \\(z_i = \\frac{x_i - \\mu_x}{SD_x}\\)\n\neffectsize(m)\n\n## For one-way between subjects designs, partial eta squared is equivalent\n##   to eta squared. Returning eta squared.\n\n\n## # Effect Size for ANOVA\n## \n## Parameter | Eta2 |       95% CI\n## -------------------------------\n## age       | 0.81 | [0.79, 1.00]\n## \n## - One-sided CIs: upper bound fixed at [1.00].\n\n\n\nNOTE: The {datawizard} package provides the function standardize(), which will take a model and refit it using standardized parameters and return a refitted model object. The {parameters} package function standardize_parameters() will also return standardized coefficients when passed a model object. These two function are designed to replace the effectsize() function from the {effectsize} package.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "20-module.html#multiple-factor-anova",
    "href": "20-module.html#multiple-factor-anova",
    "title": "20  Categorical Data Analysis",
    "section": "20.8 Multiple Factor ANOVA",
    "text": "20.8 Multiple Factor ANOVA\nSometimes the data we are interested in is characterized by multiple grouping variables (e.g., age and sex). In the case of the gibbon femur length data, we are interested in the main effect of each factor on the variable of interest (e.g., do femur lengths vary by age or sex) while accounting for the effects of the other factor. We may also be interested in any interactive effects among factors. Thus, in multiple factor ANOVA we are interested in testing several null hypotheses simultaneously: [1] that each factor has no effect on the mean of our continuous response variable and [2] that there are no interactive effects of sets of factors on the mean of our continuous response variable.\nModel description and testing for multiple-factor ANOVA is a simple extension of the formula notation which we’ve used for single factors. First, though, let’s quickly check that our groups have similar variance.\n\nstats &lt;- d |&gt;\n    group_by(age, sex) |&gt;\n    summarize(`mean(femur_length)` = mean(femur_length), `sd(femur_length)` = sd(femur_length))\n# first we calculate averages by combination of factors\nmax(stats$`sd(femur_length)`)/min(stats$`sd(femur_length)`)\n\n## [1] 1.882171\n\n# check that variances in each group are roughly equal (ratio of max/min is &lt;2)\np &lt;- ggplot(data = d, aes(y = femur_length, x = sex)) + geom_boxplot() + facet_wrap(~age,\n    ncol = 4) + xlab(\"Sex\") + ylab(\"Femur Length (cm)\")\n# and let's plot what the data look like p &lt;- p + geom_point() # uncommenting\n# this shows all points\np &lt;- p + stat_summary(data = d, aes(y = femur_length, x = sex), fun = base::mean,\n    color = \"darkgreen\", geom = \"point\", shape = 8, size = 6)\n# make sure we use {base} version of mean\np\n\n\n\n\n\n\n\n\nIf we look at each variable separately using ANOVA, we see there is an effect of age but not of sex.\n\nsummary(aov(data = d, femur_length ~ age))\n\n##              Df Sum Sq Mean Sq F value Pr(&gt;F)    \n## age           3   6152  2050.6   735.8 &lt;2e-16 ***\n## Residuals   521   1452     2.8                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(aov(data = d, femur_length ~ sex))\n\n##              Df Sum Sq Mean Sq F value Pr(&gt;F)\n## sex           1     28   28.33   1.956  0.163\n## Residuals   523   7576   14.48\n\n\nHowever, if we do a Two-Way ANOVA and consider the factors together, we see that there is still a main effect of age when taking sex into account and there is a main effect of sex when we take age into account.\n\nm.aov &lt;- summary(aov(data = d, femur_length ~ age + sex))\nm.aov\n\n##              Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## age           3   6152  2050.6   756.4  &lt; 2e-16 ***\n## sex           1     42    42.3    15.6 8.89e-05 ***\n## Residuals   520   1410     2.7                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTo examine whether there is an interaction effect, we would modify our model formula a bit using the colon operator (:) to specify a particular interaction term…\n\nm.aov &lt;- aov(data = d, femur_length ~ age + sex + age:sex)\n# the colon (:) operator includes specific interaction terms\nsummary(m)\n\n##              Df Sum Sq Mean Sq F value Pr(&gt;F)    \n## age           3   6152  2050.6   735.8 &lt;2e-16 ***\n## Residuals   521   1452     2.8                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe could also use the asterisk (*) operator, which expands to include all interactions among terms, including 3-way, 4-way, etc., interactions.\n\nm.aov &lt;- aov(data = d, femur_length ~ age * sex)\n# asterisk (*) operator includes all interaction terms\nsummary(m.aov)\n\n##              Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## age           3   6152  2050.6  816.35  &lt; 2e-16 ***\n## sex           1     42    42.3   16.84 4.72e-05 ***\n## age:sex       3    111    37.0   14.73 3.21e-09 ***\n## Residuals   517   1299     2.5                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nm &lt;- lm(data = d, femur_length ~ age * sex)\n# or using the lm() function...\nsummary(m)\n\n## \n## Call:\n## lm(formula = femur_length ~ age * sex, data = d)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.8727 -1.0088  0.1336  1.0469  4.6773 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   16.7159     0.2389  69.960  &lt; 2e-16 ***\n## ageI          -8.8909     0.2854 -31.148  &lt; 2e-16 ***\n## ageJ          -6.7395     0.2968 -22.706  &lt; 2e-16 ***\n## ageS          -4.0428     0.3360 -12.031  &lt; 2e-16 ***\n## sexmale        1.7160     0.3716   4.617 4.91e-06 ***\n## ageI:sexmale  -1.9523     0.4341  -4.498 8.48e-06 ***\n## ageJ:sexmale  -1.5538     0.4534  -3.427 0.000658 ***\n## ageS:sexmale   0.2536     0.4895   0.518 0.604641    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.585 on 517 degrees of freedom\n## Multiple R-squared:  0.8292, Adjusted R-squared:  0.8269 \n## F-statistic: 358.6 on 7 and 517 DF,  p-value: &lt; 2.2e-16\n\n\nWe can use the function interaction.plot() to visualize interactions among categorical variables.\n\ninteraction.plot(x.factor = d$age, xlab = \"Age\", trace.factor = d$sex, trace.label = \"Sex\",\n    response = d$femur_length, fun = base::mean, ylab = \"Mean Femur Length\")\n\n\n\n\n\n\n\n\nHere, it looks like there is indeed a significant main effect of each term as well as an interaction between our two categorical variables. We will return to comparing models to one another (e.g., our model with and without interactions) and to post-hoc tests of what group mean differences are significant when we get into model selection in another few lectures.\nWhen we do summary() of the results of the lm() function, we are estimating eight \\(\\beta\\) coefficients (equivalent to the number of groups we have). \\(\\beta_0\\), the intercept, is the mean femur length for the base level (in this case, “adult females”). Then we have coefficients showing how the different factor combination groups would differ from that base level (e.g., adult males have mean femur lengths 1.716 greater than adult females, etc).\nThe {permuco} package also lets us run multiple factor ANOVA using permutation/randomization to estimate p values.\n\nlibrary(permuco)\nm.aovperm &lt;- aovperm(data = d, femur_length ~ age + sex + age:sex)\nsummary(m.aovperm)\n\n## Anova Table\n## Resampling test using freedman_lane to handle nuisance variables and 5000 permutations.\n##               SS  df      F parametric P(&gt;F) resampled P(&gt;F)\n## age       6136.3   3 814.28        0.000e+00           2e-04\n## sex         91.7   1  36.51        2.914e-09           2e-04\n## age:sex    111.0   3  14.73        3.210e-09           2e-04\n## Residuals 1298.7 517\n\ndetach(package:permuco)\n\nWhen we run effectsize() on a model object resulting from a multiple factor ANOVA, the function by default returns partial \\(\\eta^2\\) values, which is the proportion of the variance in the response variable that is accounted by each predictor when controlling for the other predictors.\n\neffectsize(m.aov)\n\n## # Effect Size for ANOVA (Type I)\n## \n## Parameter | Eta2 (partial) |       95% CI\n## -----------------------------------------\n## age       |           0.83 | [0.81, 1.00]\n## sex       |           0.03 | [0.01, 1.00]\n## age:sex   |           0.08 | [0.04, 1.00]\n## \n## - One-sided CIs: upper bound fixed at [1.00].\n\n# or effectsize(m.aov, partial = TRUE)",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "20-module.html#type-i-ii-and-iii-anova",
    "href": "20-module.html#type-i-ii-and-iii-anova",
    "title": "20  Categorical Data Analysis",
    "section": "20.9 Type I, II, and III ANOVA",
    "text": "20.9 Type I, II, and III ANOVA\nIt is important to recognize that the ORDER in which our factors are entered into a linear model using categorical predictors can result in different values for the entries in our ANOVA table, while the estimation of our regression coefficients is identical regardless. Take a look at this example, where the variables age and sex are entered into our ANOVA formula in different orders:\n\nm1 &lt;- aov(data = d, femur_length ~ age + sex)\nsummary(m1)\n\n##              Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## age           3   6152  2050.6   756.4  &lt; 2e-16 ***\n## sex           1     42    42.3    15.6 8.89e-05 ***\n## Residuals   520   1410     2.7                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nm2 &lt;- aov(data = d, femur_length ~ sex + age)\nsummary(m2)\n\n##              Df Sum Sq Mean Sq F value  Pr(&gt;F)    \n## sex           1     28    28.3   10.45 0.00131 ** \n## age           3   6166  2055.3  758.13 &lt; 2e-16 ***\n## Residuals   520   1410     2.7                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nm1 &lt;- lm(data = d, femur_length ~ age + sex)\nsummary(m1)\n\n## \n## Call:\n## lm(formula = femur_length ~ age + sex, data = d)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.2429 -1.1040  0.1011  1.0560  5.3403 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  17.1896     0.1993   86.27  &lt; 2e-16 ***\n## ageI         -9.7556     0.2232  -43.71  &lt; 2e-16 ***\n## ageJ         -7.4007     0.2329  -31.77  &lt; 2e-16 ***\n## ageS         -3.7467     0.2523  -14.85  &lt; 2e-16 ***\n## sexmale       0.5701     0.1443    3.95 8.89e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.647 on 520 degrees of freedom\n## Multiple R-squared:  0.8146, Adjusted R-squared:  0.8132 \n## F-statistic: 571.2 on 4 and 520 DF,  p-value: &lt; 2.2e-16\n\nm2 &lt;- lm(data = d, femur_length ~ sex + age)\nsummary(m2)\n\n## \n## Call:\n## lm(formula = femur_length ~ sex + age, data = d)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.2429 -1.1040  0.1011  1.0560  5.3403 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  17.1896     0.1993   86.27  &lt; 2e-16 ***\n## sexmale       0.5701     0.1443    3.95 8.89e-05 ***\n## ageI         -9.7556     0.2232  -43.71  &lt; 2e-16 ***\n## ageJ         -7.4007     0.2329  -31.77  &lt; 2e-16 ***\n## ageS         -3.7467     0.2523  -14.85  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.647 on 520 degrees of freedom\n## Multiple R-squared:  0.8146, Adjusted R-squared:  0.8132 \n## F-statistic: 571.2 on 4 and 520 DF,  p-value: &lt; 2.2e-16\n\n\nWhy is this? In the first model, we are looking at the variance within each age group that is explained by gender while in the second case we are looking at the variance within each gender that is explained by age… but we have different numbers of observations in our different groups. This is known as an unbalanced design.\nWe can see the unbalanced design by tabulating the cases for each combination of factors.\n\ntable(d$sex, d$age)\n\n##         \n##            A   I   J   S\n##   female  44 103  81  45\n##   male    31  97  69  55\n\n\nRecall that ANOVA is based on splitting up the sums of squares, so that for a model like:\n\\[Y_{i,j,k} = \\mu + a_i + b_j + ab_{i,j} + \\epsilon_{i,j,k}\\]\nThe sums of squares add up such that:\n\\[SS_{total} = SS_a + SS_b + SS_{ab} + SS_{residual}\\]\nBy default, the aov() function uses something called Type I Sums of Squares (also called “sequential sum of squares”), which gives greater emphasis to the first factor in the model, leaving only residual variation to the remaining factors. It should be used when you want to first control for one factor, leaving the others to explain only any remaining differences. In a Type I ANOVA, the sums of squares for the first term are calculated around the grand mean of our observations, but the next terms are calculated as residuals around the average of the grand mean and the first group mean. This sequential procedure means our results depend on which term shows up first. This can have a marked effect on the sums of squares calculated, and hence on p values, when using an unbalanced design.\nBy contrast, what are know as Type II and Type III Sums of Squares both calculate the deviations between the individual observations within each group from the grand mean rather than the group mean.\nType II Sum of Squares compares the main effects of each factor, assuming that the interaction between factors is minimal. Generally it is a much more appropriate test for comparing among different main effects and is more appropriate for us to use when there is an unbalanced design.\nType III Sum of Squares (or “marginal sum of squares”) is most appropriate when there is a significant interaction effect between factors. Since both Type II and Type III ANOVA calculate sums of squares around the grand mean, these are unaffected by different sample sizes across different categorical groups and do not arbitrarily give preference to one effect over another.\nTo summarize:\n\nWhen our data are balanced, our factors are orthogonal, and all three types of sums of squares give the same results.\nIf our data are strongly unbalanced, we should generally be using Type II or Type III Sums of Squares, since we are generally interested in exploring the significance of one factor while controlling for the level of the other factors.\nIn general, if there is no significant interaction effect between factors, then Type II is more powerful than Type III.\nIf an important interaction is present, then Type II analysis is inappropriate, while Type III analysis can still be used, but the results need to be interpreted with caution (in the presence of interactions, individual main effects of different factors difficult to interpret).\n\nSee this post or this post for further treatments of the issue.\nWe can use the Anova() function in the {car} package to run ANOVA with Type II and Type III Sums of Squares. In the examples below, both linear models, where the order of factors are reversed, give the same results for Type II and Type III ANOVA, but not for Type I. The Anova() function takes as its argument a model object, e.g., the result from a call to either lm() or aov() and recalculates the appropriate sums of squares. Note how the effect sizes (partial \\(\\eta^2\\) values) are slightly different depending on whether they are being calculated from Type I or Type II ANOVA objects.\n\nlibrary(car)\n\n## Loading required package: carData\n\n\n## \n## Attaching package: 'car'\n\n\n## The following object is masked from 'package:dplyr':\n## \n##     recode\n\n\n## The following object is masked from 'package:purrr':\n## \n##     some\n\nm1 &lt;- aov(data = d, femur_length ~ age + sex)\neffectsize(m1)\n\n## # Effect Size for ANOVA (Type I)\n## \n## Parameter | Eta2 (partial) |       95% CI\n## -----------------------------------------\n## age       |           0.81 | [0.79, 1.00]\n## sex       |           0.03 | [0.01, 1.00]\n## \n## - One-sided CIs: upper bound fixed at [1.00].\n\nm1 &lt;- Anova(m1, type = \"II\")\nm1\n\n## Anova Table (Type II tests)\n## \n## Response: femur_length\n##           Sum Sq  Df F value    Pr(&gt;F)    \n## age       6165.8   3 758.126 &lt; 2.2e-16 ***\n## sex         42.3   1  15.604 8.888e-05 ***\n## Residuals 1409.7 520                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nm2 &lt;- aov(data = d, femur_length ~ sex + age)\neffectsize(m2)\n\n## # Effect Size for ANOVA (Type I)\n## \n## Parameter | Eta2 (partial) |       95% CI\n## -----------------------------------------\n## sex       |           0.02 | [0.00, 1.00]\n## age       |           0.81 | [0.79, 1.00]\n## \n## - One-sided CIs: upper bound fixed at [1.00].\n\nm2 &lt;- Anova(m2, type = \"II\")\nm2\n\n## Anova Table (Type II tests)\n## \n## Response: femur_length\n##           Sum Sq  Df F value    Pr(&gt;F)    \n## sex         42.3   1  15.604 8.888e-05 ***\n## age       6165.8   3 758.126 &lt; 2.2e-16 ***\n## Residuals 1409.7 520                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\neffectsize(m1)\n\n## # Effect Size for ANOVA (Type II)\n## \n## Parameter | Eta2 (partial) |       95% CI\n## -----------------------------------------\n## age       |           0.81 | [0.79, 1.00]\n## sex       |           0.03 | [0.01, 1.00]\n## \n## - One-sided CIs: upper bound fixed at [1.00].\n\neffectsize(m2)\n\n## # Effect Size for ANOVA (Type II)\n## \n## Parameter | Eta2 (partial) |       95% CI\n## -----------------------------------------\n## sex       |           0.03 | [0.01, 1.00]\n## age       |           0.81 | [0.79, 1.00]\n## \n## - One-sided CIs: upper bound fixed at [1.00].\n\nm1 &lt;- aov(data = d, femur_length ~ age * sex)\nm1 &lt;- Anova(m1, type = \"III\")\nm1\n\n## Anova Table (Type III tests)\n## \n## Response: femur_length\n##              Sum Sq  Df  F value    Pr(&gt;F)    \n## (Intercept) 12294.6   1 4894.442 &lt; 2.2e-16 ***\n## age          2661.3   3  353.158 &lt; 2.2e-16 ***\n## sex            53.6   1   21.320  4.91e-06 ***\n## age:sex       111.0   3   14.735  3.21e-09 ***\n## Residuals    1298.7 517                       \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nm2 &lt;- aov(data = d, femur_length ~ sex * age)\nm2 &lt;- Anova(m2, type = \"III\")\nm2\n\n## Anova Table (Type III tests)\n## \n## Response: femur_length\n##              Sum Sq  Df  F value    Pr(&gt;F)    \n## (Intercept) 12294.6   1 4894.442 &lt; 2.2e-16 ***\n## sex            53.6   1   21.320  4.91e-06 ***\n## age          2661.3   3  353.158 &lt; 2.2e-16 ***\n## sex:age       111.0   3   14.735  3.21e-09 ***\n## Residuals    1298.7 517                       \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ndetach(package:car)",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "20-module.html#chi-square-tests",
    "href": "20-module.html#chi-square-tests",
    "title": "20  Categorical Data Analysis",
    "section": "20.10 Chi-Square Tests",
    "text": "20.10 Chi-Square Tests\nOne additional type of categorical data we will often encounter are counts of observations that fall into two or more categories (when we were dealing with \\(Z\\) tests for proportion data, we were interested in something similar, though with two categories only). We can use Chi-Square tests to evaluate statistically the distribution of observations across levels of one or more categorical variables. To use the Chi-Square test we first derive a Chi-Square statistic (\\(\\chi^2\\)) , which is calculated as…\n\\[\\chi^2 = \\displaystyle\\sum_{i=1}^k \\frac{(O_i - E_i)^2}{E_i}\\]\nwhere:\n\n\\(O_i\\) = number of observations in the \\(i\\)th category\n\\(E_i\\) = number of observations in the \\(i\\)th category\n\nWe then compare the value of the \\(\\chi^2\\) statistic to the Chi-Square distribution with \\(k-1\\) degrees of freedom. The Chi-Square distribution, like the \\(F\\) distribution, is a continuous probability distribution, defined for \\(x≥0\\). It is governed by a single parameter, \\(df\\).\n\npar(mfrow = c(1, 1))\ncurve(dchisq(x, df = 1), col = \"green\", lty = 3, lwd = 2, xlim = c(0, 20), main = \"Some Example Chi-Square Distributions\",\n    sub = \"(vertical line shows critical value for df=3)\", ylab = \"f(x)\", xlab = \"x\")\ncurve(dchisq(x, df = 3), col = \"blue\", lty = 3, lwd = 2, add = TRUE)\ncurve(dchisq(x, df = 5), col = \"red\", lty = 3, lwd = 2, add = TRUE)\ncurve(dchisq(x, df = 10), col = \"purple\", lty = 3, lwd = 2, add = TRUE)\nlegend(\"right\", c(\"df=1\", \"df=3\", \"df=5\", \"df=10\"), lty = c(3, 3, 3, 3), lwd = 2,\n    col = c(\"green\", \"blue\", \"red\", \"purple\", \"black\"), bty = \"n\", cex = 0.75)\n\ncrit &lt;- qchisq(p = 0.95, df = 3)\ncrit\n\n## [1] 7.814728\n\nabline(v = crit)\nabline(h = 0)\npolygon(cbind(c(crit, seq(from = crit, to = 20, length.out = 1000), 8), c(0, dchisq(seq(from = crit,\n    to = 20, length.out = 1000), df = 3), 0)), border = \"black\", col = rgb(0, 0,\n    1, 0.5))\n\n\n\n\n\n\n\n\n\nCHALLENGE\nLet’s return to the zombie apocalypse survivors dataset, where we defined an occupation based on major for survivors of the zombie apocalypse. Use a Chi-Square test to evaluate the hypothesis that survivors of the zombie apocalypse are more likely than expected by chance to be natural science majors. Assume that assume our null hypothesis is that the proportions of different post-apocalypse occupations are equivalent, i.e., that \\(\\pi_{natural science}\\) = \\(\\pi_{engineering}\\)= \\(\\pi_{logistics}\\) = \\(\\pi_{other}\\) = 0.25.\n\n\nShow Code\n(obs.table &lt;- table(z$occupation))\n\n\nShow Output\n## \n## natural science     engineering       logistics           other \n##             390              98             218             294\n\n\n\nShow Code\n# returns the same as summary()\n(exp.table &lt;- rep(0.25 * length(z$occupation), 4))\n\n\nShow Output\n## [1] 250 250 250 250\n\n\n\nShow Code\n# equal expectation for each of 4 categories\noccupation.matrix &lt;- data.frame(cbind(obs.table, exp.table, (obs.table - exp.table)^2/exp.table))\nnames(occupation.matrix) &lt;- c(\"Oi\", \"Ei\", \"(Oi-Ei)^2/Ei\")\noccupation.matrix\n\n\nShow Output\n##                  Oi  Ei (Oi-Ei)^2/Ei\n## natural science 390 250       78.400\n## engineering      98 250       92.416\n## logistics       218 250        4.096\n## other           294 250        7.744\n\n\n\nShow Code\n(X2 &lt;- sum(occupation.matrix[, 3]))\n\n\nShow Output\n## [1] 182.656\n\n\n\nShow Code\n(p &lt;- 1 - pchisq(q = X2, length(obs.table) - 1))\n\n\nShow Output\n## [1] 0\n\n\n\nHere, we reject the null hypothesis that the proportions of different occupations among the survivors of the zombie apocalypse is equivalent.\nWe can do all this with a 1-liner in R, too.\n\nchisq.test(x = obs.table, p = c(0.25, 0.25, 0.25, 0.25))\n\n## \n##  Chi-squared test for given probabilities\n## \n## data:  obs.table\n## X-squared = 182.66, df = 3, p-value &lt; 2.2e-16\n\n# here p is a vector of expected proportions... default is uniform\nchisq.test(x = obs.table)\n\n## \n##  Chi-squared test for given probabilities\n## \n## data:  obs.table\n## X-squared = 182.66, df = 3, p-value &lt; 2.2e-16\n\nchisq.test(x = obs.table, p = c(0.42, 0.07, 0.22, 0.29))\n\n## \n##  Chi-squared test for given probabilities\n## \n## data:  obs.table\n## X-squared = 13.416, df = 3, p-value = 0.003818\n\n# with a different set of expected proportions... fail to reject H0\n\nThe above was a Chi-Square goodness of fit test for one categorical variable… what about if we have two categorical variables and we are curious if there is an association among them? Then we do a Chi-Square test of independence. In this case, our Chi-Square statistic would be the sum of \\(\\frac{(O-E)^2}{E}\\) across all cells in our table, and our degrees of freedom is (number of rows - 1) \\(\\times\\) (number of columns - 1). Let’s suppose we want to see if there is a relationship among zombie apocalypse survivors between gender and occupation.\nFirst, we determine our table of observed proportions:\n\n(obs.table = table(z$gender, z$occupation))\n\n##         \n##          natural science engineering logistics other\n##   Male               198          49       109   150\n##   Female             192          49       109   144\n\n\nWe can view our data graphically using the mosaic plot() function.\n\nmosaicplot(t(obs.table), main = \"Contingency Table\", col = c(\"darkseagreen\", \"gray\"))\n\n\n\n\n\n\n\n# the `t()` function transposes the table\n\nThen, we determine our table of expected proportions:\n\n(r &lt;- rowSums(obs.table))  # row margins\n\n##   Male Female \n##    506    494\n\n(c &lt;- colSums(obs.table))  # column margins\n\n## natural science     engineering       logistics           other \n##             390              98             218             294\n\n(nr &lt;- nrow(obs.table))  # row dimensions\n\n## [1] 2\n\n(nc &lt;- ncol(obs.table))  # column dimensions\n\n## [1] 4\n\n(exp.table &lt;- matrix(rep(c, each = nr) * r/sum(obs.table), nrow = nr, ncol = nc,\n    dimnames = dimnames(obs.table), byrow = FALSE))\n\n##         \n##          natural science engineering logistics   other\n##   Male            197.34      49.588   110.308 148.764\n##   Female          192.66      48.412   107.692 145.236\n\n# calculates the product of c*r and divides by total\n(X2 &lt;- sum((obs.table - exp.table)^2/exp.table))\n\n## [1] 0.07076686\n\n(p &lt;- 1 - pchisq(q = X2, df = (nr - 1) * (nc - 1)))\n\n## [1] 0.9950981\n\n\nAgain, we can do a one-liner for a test of independence…\n\nchisq.test(x = obs.table)\n\n## \n##  Pearson's Chi-squared test\n## \n## data:  obs.table\n## X-squared = 0.070767, df = 3, p-value = 0.9951",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "20-module.html#concept-review",
    "href": "20-module.html#concept-review",
    "title": "20  Categorical Data Analysis",
    "section": "Concept Review",
    "text": "Concept Review\n\nANOVA is conceptually equivalent linear regression, but with one or more categorical predictor variables\nANOVA effectively tests whether the means of different groups are different\nThe Kruskall-Wallis test is a nonparametric alternative to a One-Way ANOVA that relaxes the need for normality in the distribution of data in each group and tests for a difference in the medians of different groups\nANOVAs should have balanced or roughly balanced numbers of observations within each categorical group\nChi-Square (\\(\\chi^2\\)) goodness-of-fit tests are another type of factor-based categorical data analysis, where we compare between observed and expected counts of observations in different grouping categories for either a single variable or two variables",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "21-module.html",
    "href": "21-module.html",
    "title": "21  Multiple Regression and ANCOVA",
    "section": "",
    "text": "21.1 Objectives",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Multiple Regression and ANCOVA</span>"
    ]
  },
  {
    "objectID": "21-module.html#objectives",
    "href": "21-module.html#objectives",
    "title": "21  Multiple Regression and ANCOVA",
    "section": "",
    "text": "In this module, we extend our simple linear regression and ANOVA models to cases where we have more than one predictor variable. The same approach can be used with combinations of continuous and categorical variables. If all of our predictors variables are continuous, we typically describe this as “multiple linear regression”. If our predictors are combinations of continuous and categorical variables, we typically describe this as “analysis of covariance”, or ANCOVA.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Multiple Regression and ANCOVA</span>"
    ]
  },
  {
    "objectID": "21-module.html#preliminaries",
    "href": "21-module.html#preliminaries",
    "title": "21  Multiple Regression and ANCOVA",
    "section": "21.2 Preliminaries",
    "text": "21.2 Preliminaries\n\nInstall these packages in R: {jtools} {ggeffects}\nInstall and load this package in R: {effects}\nLoad {tidyverse}, {broom}, {car}, and {gridExtra}",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Multiple Regression and ANCOVA</span>"
    ]
  },
  {
    "objectID": "21-module.html#overview",
    "href": "21-module.html#overview",
    "title": "21  Multiple Regression and ANCOVA",
    "section": "21.3 Overview",
    "text": "21.3 Overview\nMultiple linear regression and ANCOVA are pretty straightforward generalizations of the simple linear regression and ANOVA approach that we have considered previously (i.e., Model I regressions, with our parameters estimated using the criterion of OLS). In multiple linear regression and ANCOVA, we are looking to model a response variable in terms of more than one predictor variable so that we can evaluate the effects of several different explanatory variables. When we do multiple linear regression, we are, essentially, looking at the relationship between each of two or more continuous predictor variables and a continuous response variable while holding the effect of all other predictor variables constant. When we do ANCOVA, we are effectively looking at the relationship between one or more continuous predictor variables and a continuous response variable within each of one or more categorical groups.\nTo characterize this using formulas analogous to those we have used before…\nSimple Bivariate Linear Model:\n\\[Y = \\beta_0 + \\beta_1x + \\epsilon\\]\nMultivariate Linear Model:\n\\[Y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_kx_k + \\epsilon\\]\nWe thus now need to estimate several \\(\\beta\\) coefficients for our regression model - one for the intercept plus one for each predictor variable in our model, and, instead of a “line”” of best fit, we are determining a multidimensional “surface” of best fit. The criteria we typically use for estimating best fit is analogous to that we’ve used before, i.e., ordinary least squares, where we want to minimize the multidimensional squared deviation between our observed and predicted values:\n\\[\\sum\\limits_{i=1}^{k}[y_i - (\\beta_0 + \\beta_1x_{1_i} + \\beta_2x_{2_i} + ... + \\beta_kx_{k_i})]^2\\]\nEstimating our set of coefficients for multiple regression is actually done using matrix algebra, which we do not need to explore thoroughly (but see the “digression” below). The lm() function will implement this process for us directly.\n\nNOTE: Besides the least squares approach to parameter estimation, we might also use other approaches, including maximum likelihood and Bayesian approaches. These are mostly beyond the scope of what we will discuss in this class, but the objective is the same… to use a particular criterion to estimate parameter values for describing the relationship between our predictor and response variables, as well as for estimating the amount of uncertainty in those parameter values. It is worth noting that for many common population parameters (such as the mean), as well as for regression slopes and/or factor effects in certain linear models, OLS and ML estimators turn out to be exactly the same when the assumptions behind OLS are met (i.e., normally distributed variables and normally distributed error terms). When response variables or residuals are not normally distributed (e.g., where we have binary or categorical response variables), we use ML or Bayesian approaches, rather than OLS, for parameter estimation.\n\nLet’s work some examples!",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Multiple Regression and ANCOVA</span>"
    ]
  },
  {
    "objectID": "21-module.html#multiple-regression",
    "href": "21-module.html#multiple-regression",
    "title": "21  Multiple Regression and ANCOVA",
    "section": "21.4 Multiple Regression",
    "text": "21.4 Multiple Regression\n\nContinous Response Variable with More than One Continuous Predictor\nWe will start by taking a digression to construct a dataset ourselves of some correlated random normal continuous variables. The following bit of code will let us do that. See also this post for more information on generating toy data with a particular correlation structure. First, we define a matrix of correlations, R, among our variables (you can play with the values in this matrix, but it must be symmetric):\n\nR = matrix(cbind(1, 0.8, -0.5, 0, 0.8, 1, -0.3, 0.3, -0.5, -0.3, 1, 0.6, 0, 0.3,\n    0.6, 1), nrow = 4)\nR\n\n##      [,1] [,2] [,3] [,4]\n## [1,]  1.0  0.8 -0.5  0.0\n## [2,]  0.8  1.0 -0.3  0.3\n## [3,] -0.5 -0.3  1.0  0.6\n## [4,]  0.0  0.3  0.6  1.0\n\n\nSecond, we generate a dataset of random normal variables where each has a defined mean and standard deviation and then bundle these into a dataframe (“original”):\n\nn &lt;- 1000\nk &lt;- 4\noriginal &lt;- NULL\nv &lt;- NULL\nmu &lt;- c(15, 40, 5, 23)  # vector of variable means\ns &lt;- c(5, 20, 4, 15)  # vector of variable SDs\nfor (i in 1:k) {\n    v &lt;- rnorm(n, mu[i], s[i])\n    original &lt;- cbind(original, v)\n}\noriginal &lt;- as.data.frame(original)\nnames(original) = c(\"Y\", \"X1\", \"X2\", \"X3\")\nhead(original)\n\n##           Y       X1        X2        X3\n## 1 20.372498 32.16490  1.641936 25.101063\n## 2 11.035930 46.50011 -2.232105 12.859342\n## 3 19.074992 46.79068  7.553672 18.771677\n## 4 17.467254 76.21649  7.052573 19.358755\n## 5 15.842305 24.18338  7.117205 25.506423\n## 6  7.468329 57.92763 -2.329924  9.514052\n\ncor(original)  # variables are uncorrelated\n\n##               Y           X1          X2          X3\n## Y   1.000000000 -0.009224942  0.02932623  0.03483224\n## X1 -0.009224942  1.000000000 -0.02391135 -0.02447748\n## X2  0.029326229 -0.023911349  1.00000000 -0.08199543\n## X3  0.034832241 -0.024477482 -0.08199543  1.00000000\n\n# make quick bivariate plots for each pair of variables\nplot(original)\n\n\n\n\n\n\n\n# using `pairs(original)` would do the same\n\nNow, let’s normalize and standardize our variables by subtracting the relevant means and dividing by the standard deviation. This converts them to \\(Z\\) scores from a standard normal distribution.\nNow run the following… note how we use the apply() and sweep() functions here. Cool, eh?\n\nmeans &lt;- apply(original, 2, FUN = \"mean\")  # returns a vector of means, where we are taking this across dimension 2 of the array 'orig'\nmeans\n\n##         Y        X1        X2        X3 \n## 14.884200 40.362219  4.788228 23.313678\n\n# or\nmeans &lt;- unlist(summarize(original, ymean = mean(Y), x1mean = mean(X1), x2mean = mean(X2),\n    x3mean = mean(X3)))\nms\n\n## function (..., quiet = FALSE, roll = FALSE) \n## {\n##     out &lt;- .parse_hms(..., order = \"MS\", quiet = quiet)\n##     if (roll) {\n##         hms &lt;- .roll_hms(min = out[\"M\", ], sec = out[\"S\", ])\n##         period(hour = hms$hour, minute = hms$min, second = hms$sec)\n##     }\n##     else {\n##         period(minute = out[\"M\", ], second = out[\"S\", ])\n##     }\n## }\n## &lt;bytecode: 0x12c5adee0&gt;\n## &lt;environment: namespace:lubridate&gt;\n\nstdevs &lt;- apply(original, 2, FUN = \"sd\")\nstdevs\n\n##         Y        X1        X2        X3 \n##  4.967753 20.603313  4.134423 15.078604\n\n# or\nstdevs &lt;- unlist(summarize(original, ySD = sd(Y), x1SD = sd(X1), x2SD = sd(X2), x3SD = sd(X3)))\nstdevs\n\n##       ySD      x1SD      x2SD      x3SD \n##  4.967753 20.603313  4.134423 15.078604\n\nnormalized &lt;- sweep(original, 2, STATS = means, FUN = \"-\")  # 2nd dimension is columns, removing array of means, function = subtract\nnormalized &lt;- sweep(normalized, 2, STATS = stdevs, FUN = \"/\")  # 2nd dimension is columns, scaling by array of sds, function = divide\nhead(normalized)  # now a dataframe of Z scores\n\n##            Y         X1         X2         X3\n## 1  1.1047848 -0.3978642 -0.7609989  0.1185378\n## 2 -0.7746501  0.2979078 -1.6980198 -0.6933226\n## 3  0.8435992  0.3120110  0.6688827 -0.3012216\n## 4  0.5199642  1.7402187  0.5476809 -0.2622871\n## 5  0.1928647 -0.7852544  0.5633136  0.1454210\n## 6 -1.4928021  0.8525529 -1.7216795 -0.9151794\n\nplot(normalized)\n\n\n\n\n\n\n\nM &lt;- as.matrix(normalized)  # define M as our matrix of normalized variables\n\nWith apply(), we apply a function to a specified margin of an array or matrix (1 = row, 2 = column), and with sweep() we then perform whatever function is specified by FUN= on all of the elements in an array specified by the given margin.\nNext, we take what is called the Cholesky decomposition of our correlation matrix, R, and then multiply our normalized data matrix by the decomposition matrix to yield a transformed dataset with the specified correlation among variables. The Cholesky decomposition breaks certain symmetric matrices into two such that:\n\\(R = U \\cdot U^T\\)\n\nU = chol(R)\nnewM = M %*% U\nnew = as.data.frame(newM)\nnames(new) = c(\"Y\", \"X1\", \"X2\", \"X3\")\ncor(new)  # note that is correlation matrix is what we are aiming for!\n\n##              Y         X1         X2         X3\n## Y   1.00000000  0.7980064 -0.4839688 0.03639755\n## X1  0.79800644  1.0000000 -0.2965184 0.32659700\n## X2 -0.48396877 -0.2965184  1.0000000 0.55873249\n## X3  0.03639755  0.3265970  0.5587325 1.00000000\n\nplot(original)\n\n\n\n\n\n\n\nplot(new)  # note the axis scales; using `pairs(new)` would plot the same\n\n\n\n\n\n\n\n\nFinally, we can scale these back out to the mean and distribution of our original random variables.\n\nd &lt;- sweep(new, 2, STATS = stdevs, FUN = \"*\")  # scale back out to original mean...\nd &lt;- sweep(d, 2, STATS = means, FUN = \"+\")  # and standard deviation\nhead(d)\n\n##           Y       X1         X2        X3\n## 1 20.372498 53.65361 -0.4435864 14.440215\n## 2 11.035930 31.27666  0.6287376  3.545882\n## 3 19.074992 58.12405  5.6095032 28.996503\n## 4 17.467254 70.44517  6.8368085 39.015195\n## 5 15.842305 33.83384  5.8276898 23.909793\n## 6  7.468329 26.29613  2.4123689  5.447406\n\ncor(d)\n\n##              Y         X1         X2         X3\n## Y   1.00000000  0.7980064 -0.4839688 0.03639755\n## X1  0.79800644  1.0000000 -0.2965184 0.32659700\n## X2 -0.48396877 -0.2965184  1.0000000 0.55873249\n## X3  0.03639755  0.3265970  0.5587325 1.00000000\n\nplot(d)  # note the change to the axis scales\n\n\n\n\n\n\n\n# using `pairs(d)` would produce the same plot\n\nWe now have our own dataframe, d, comprising correlated random variables in original units!\nLet’s explore this dataset, first with single and then with multivariate regression.\n\n\nCHALLENGE\nStart off by making three bivariate scatterplots in {ggplot2} using the dataframe of three predictor variables (X1, X2, and X3) and one response variable (Y) that we generated above (or download it from “https://raw.githubusercontent.com/difiore/ada-datasets/main/multiple_regression.csv”)\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/multiple_regression.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\n\n## Rows: 1000 Columns: 4\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## dbl (4): Y, X1, X2, X3\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nShow Code\ng1 &lt;- ggplot(data = d, aes(x = X1, y = Y)) + geom_point() + geom_smooth(method = \"lm\",\n    formula = y ~ x)\ng2 &lt;- ggplot(data = d, aes(x = X2, y = Y)) + geom_point() + geom_smooth(method = \"lm\",\n    formula = y ~ x)\ng3 &lt;- ggplot(data = d, aes(x = X3, y = Y)) + geom_point() + geom_smooth(method = \"lm\",\n    formula = y ~ x)\ngrid.arrange(g1, g2, g3, ncol = 3)\n\n\n\n\n\n\n\n\n\nThen, using simple linear regression as implemented with lm(), how does the response variable (Y) vary with each predictor variable (X1, X2, X3)? Are the \\(\\beta_1\\) coefficients for each bivariate significant? How much of the variation in Y does each predictor explain in a simple bivariate linear model?\n\n\nShow Code\nm1 &lt;- lm(data = d, formula = Y ~ X1)\nsummary(m1)\n\n\nShow Output\n## \n## Call:\n## lm(formula = Y ~ X1, data = d)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10.9848  -2.1238   0.0731   2.1175  10.7924 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 7.132691   0.213888   33.35   &lt;2e-16 ***\n## X1          0.198846   0.004785   41.55   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.012 on 998 degrees of freedom\n## Multiple R-squared:  0.6337, Adjusted R-squared:  0.6334 \n## F-statistic:  1727 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\n\nShow Code\nm2 &lt;- lm(data = d, formula = Y ~ X2)\nsummary(m2)\n\n\nShow Output\n## \n## Call:\n## lm(formula = Y ~ X2, data = d)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -16.1071  -3.0301   0.1099   2.9888  15.3328 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 17.91187    0.22465   79.73   &lt;2e-16 ***\n## X2          -0.57531    0.03579  -16.07   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.435 on 998 degrees of freedom\n## Multiple R-squared:  0.2057, Adjusted R-squared:  0.2049 \n## F-statistic: 258.4 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\n\nShow Code\nm3 &lt;- lm(data = d, formula = Y ~ X3)\nsummary(m3)\n\n\nShow Output\n## \n## Call:\n## lm(formula = Y ~ X3, data = d)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -17.2471  -3.2823   0.0706   3.2325  13.9029 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 14.906854   0.288255  51.714   &lt;2e-16 ***\n## X3           0.008163   0.010715   0.762    0.446    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.975 on 998 degrees of freedom\n## Multiple R-squared:  0.0005812,  Adjusted R-squared:  -0.0004202 \n## F-statistic: 0.5804 on 1 and 998 DF,  p-value: 0.4463\n\n\n\nIn simple linear regression, Y has a significant, positive relationship with X1, a signficant negative relationship with X2, and no significant bivariate relationship with X3.\nNow let’s move on to doing actual multiple regression. To review, with multiple regression, we are looking to model a response variable in terms of two or more predictor variables so we can evaluate the effect of each of several explanatory variables while holding the others constant.\nUsing lm() and formula notation, we can fit a model with all three predictor variables. The + sign is used to add additional predictors to our model.\n\nm &lt;- lm(data = d, formula = Y ~ X1 + X2 + X3)\ncoef(m)\n\n## (Intercept)          X1          X2          X3 \n##  9.14885797  0.20049618 -0.19794402 -0.04931104\n\nsummary(m)\n\n## \n## Call:\n## lm(formula = Y ~ X1 + X2 + X3, data = d)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -9.3943 -1.7442 -0.1112  1.8602 10.6242 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  9.148858   0.254643  35.928  &lt; 2e-16 ***\n## X1           0.200496   0.005680  35.297  &lt; 2e-16 ***\n## X2          -0.197944   0.033890  -5.841 7.03e-09 ***\n## X3          -0.049311   0.009235  -5.339 1.15e-07 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.7 on 996 degrees of freedom\n## Multiple R-squared:  0.7062, Adjusted R-squared:  0.7053 \n## F-statistic: 798.1 on 3 and 996 DF,  p-value: &lt; 2.2e-16\n\n\nNext, let’s check if our residuals are random normal…\n\n\nShow Code\nplot(fitted(m), residuals(m))\n\n\n\n\n\n\n\n\n\nShow Code\nhist(residuals(m))\n\n\n\n\n\n\n\n\n\nShow Code\nqqnorm(residuals(m))\nqqline(residuals(m))\n\n\n\n\n\n\n\n\n\nWhat does this output tell us? First off, the results of the omnibus F test tells us that the overall model is significant; using these three variables, we explain signficantly more of the variation in the response variable, Y, than we would using a model with just an intercept, i.e., just that \\(Y\\) = mean(\\(Y\\)).\nFor a multiple regression model, we calculate the F statistic as follows:\n\\[F = \\frac{R^2(n-p-1)}{(1-R^2)p}\\]\nwhere…\n\n\\(R^2\\) = multiple R squared value\n\\(n\\) = number of data points\n\\(p\\) = number of parameters estimated from the data (i.e., the number of \\(\\beta\\) coefficients, not including the intercept)\n\n\nf &lt;- (summary(m)$r.squared * (nrow(df) - (ncol(df) - 1) - 1))/((1 - summary(m)$r.squared) *\n    (ncol(df) - 1))\nf\n\n## numeric(0)\n\n\nIs this F ratio significant?\n\n1 - pf(q = f, df1 = nrow(df) - (ncol(df) - 1) - 1, df2 = ncol(df) - 1)\n\n## numeric(0)\n\n\nSecond, looking at summary() we see that the \\(\\beta\\) coefficient for each of our predictor variables (including X3) is significant. That is, each predictor is significant even when the effects of the other predictor variables are held constant. Recall that in the simple linear regression, the \\(\\beta\\) coefficient for X3 was not significant.\nThird, we can interpret our \\(\\beta\\) coefficients as we did in simple linear regression… for each change of one unit in a particular predictor variable (holding the other predictors constant), our predicted value of the response variable changes \\(\\beta\\) units.\n\n\n21.4.1 Plotting Effects\nWe can use the versatile {effects} package to visualize the results of a variety of model objects, include general and generalized linear models and mixed effects models. The function predictorEffects() with no arguments will display the linear relationship between the response variable and each predictor.\n\nplot(predictorEffects(m))\n\n\n\n\n\n\n\n\nWe can also specify particular predictors we wish to look at by adding the “predictors = ~” argument…\n\nplot(predictorEffects(m, predictors = ~X1))\n\n\n\n\n\n\n\n\nSpecifying the argument “partial.residuals = TRUE” returns the same plots as above, plus the values of the residuals after subtracting off the contribution from all other explanatory variables as well as a loess smoothed line of best fit through the residuals. Such plots are useful for detecting, for example, nonlinear relationships or interactions between predictors that are not obvious from looking at the original data.\n\nplot(predictorEffects(m, partial.residuals = TRUE))\n\n\n\n\n\n\n\n\nThe {ggeffects} package allows us to similarly predict and plot marginal effects from a model object. The predict_response() function takes a model object and one or more focal terms and returns the predicted values for the response variable across values of the focal term, along a specified margin (“mean_reference” by default) for the nonfocal terms. These can then be plotted with the plot() function.\n\nlibrary(ggeffects)\np &lt;- predict_response(m, \"X1 [all]\")\nplot(p)\n\n\n\n\n\n\n\nplot(p, show_data = TRUE, jitter = 0.1)  # superimpose original data\n\n\n\n\n\n\n\nplot(p, show_residuals = TRUE, show_residuals_line = TRUE, jitter = 0.1)  # superimpose residuals and loess smoothed line of best fit\n\n## `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\ndetach(package:ggeffects)\n\n\n\nCHALLENGE\nLoad up the “zombies.csv” dataset with the characteristics of zombie apocalypse survivors again and run a linear model of height as a function of both weight and age. Is the overall model significant? Are both predictor variables significant when the other is controlled for? Make effects plots for each variable, plotting the partial residuals and the loess smoothed relationship between the residuals and each predictor.\n\n\nShow Code\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/zombies.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\n\n\n## Rows: 1000 Columns: 10\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (4): first_name, last_name, gender, major\n## dbl (6): id, height, weight, zombies_killed, years_of_education, age\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nShow Code\nhead(d)\n\n\nShow Output\n## # A tibble: 6 × 10\n##      id first_name last_name gender height weight zombies_killed\n##   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;          &lt;dbl&gt;\n## 1     1 Sarah      Little    Female   62.9   132.              2\n## 2     2 Mark       Duncan    Male     67.8   146.              5\n## 3     3 Brandon    Perez     Male     72.1   153.              1\n## 4     4 Roger      Coleman   Male     66.8   130.              5\n## 5     5 Tammy      Powell    Female   64.7   132.              4\n## 6     6 Anthony    Green     Male     71.2   153.              1\n## # ℹ 3 more variables: years_of_education &lt;dbl&gt;, major &lt;chr&gt;, age &lt;dbl&gt;\n\n\n\nShow Code\nm &lt;- lm(data = d, height ~ weight + age)\nsummary(m)\n\n\nShow Output\n## \n## Call:\n## lm(formula = height ~ weight + age, data = d)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.2278 -1.1782 -0.0574  1.1566  5.4117 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 31.763388   0.470797   67.47   &lt;2e-16 ***\n## weight       0.163107   0.002976   54.80   &lt;2e-16 ***\n## age          0.618270   0.018471   33.47   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.64 on 997 degrees of freedom\n## Multiple R-squared:  0.8555, Adjusted R-squared:  0.8553 \n## F-statistic:  2952 on 2 and 997 DF,  p-value: &lt; 2.2e-16\n\n\n\nShow Code\nplot(predictorEffects(m, partial.residuals = TRUE))",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Multiple Regression and ANCOVA</span>"
    ]
  },
  {
    "objectID": "21-module.html#ancova",
    "href": "21-module.html#ancova",
    "title": "21  Multiple Regression and ANCOVA",
    "section": "21.5 ANCOVA",
    "text": "21.5 ANCOVA\n\nContinous Response Variable with both Continuous and Categorical Predictors\nWe can use the same linear modeling approach to do analysis of covariance, or ANCOVA, where we have a continuous response variable and a combination of continuous and categorical predictor variables. Let’s return to the “zombies.csv” dataset and now include one continuous and one categorical variable in our model… we want to predict height as a function of age (a continuous variable) and gender (a categorical variable), and we want to use Type II regression (because we have an unbalanced design). What is our model formula?\n\n\nShow Code\nd$gender &lt;- factor(d$gender)\nm &lt;- lm(data = d, formula = height ~ gender + age)\nsummary(m)\n\n\nShow Output\n## \n## Call:\n## lm(formula = height ~ gender + age, data = d)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10.1909  -1.7173   0.1217   1.7670   7.6746 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 46.74251    0.56869   82.19   &lt;2e-16 ***\n## genderMale   4.00224    0.16461   24.31   &lt;2e-16 ***\n## age          0.94091    0.02777   33.88   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.603 on 997 degrees of freedom\n## Multiple R-squared:  0.6361, Adjusted R-squared:  0.6354 \n## F-statistic: 871.5 on 2 and 997 DF,  p-value: &lt; 2.2e-16\n\n\n\nShow Code\nm.aov &lt;- Anova(m, type = \"II\")\nm.aov\n\n\nShow Output\n## Anova Table (Type II tests)\n## \n## Response: height\n##           Sum Sq  Df F value    Pr(&gt;F)    \n## gender    4003.9   1  591.15 &lt; 2.2e-16 ***\n## age       7775.6   1 1148.01 &lt; 2.2e-16 ***\n## Residuals 6752.7 997                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nShow Code\nplot(fitted(m), residuals(m))\n\n\n\n\n\n\n\n\n\nShow Code\nhist(residuals(m))\n\n\n\n\n\n\n\n\n\nShow Code\nqqnorm(residuals(m))\nqqline(residuals(m))\n\n\n\n\n\n\n\n\n\nHow do we interpret these results?\n\nThe omnibus F test is significant\nBoth predictors are significant\nHeight is related to age when sex is controlled for\nControlling for age, being male adds ~4 inches to predicted height when compared to being female.\n\n\n\nVisualizing a Parallel Slopes Model\nWe can now write two equations for the relationship between height, on the one hand, and age and gender on the other:\nFor females…\n\\[height = 46.7251 + 0.94091 \\times age\\]\n\nIn this case, females are the first level of our “gender” factor and thus do not have additional regression coefficients associated with them.\n\nFor males…\n\\[height = 46.7251 + 4.00224 + 0.94091 \\times age\\]\n\nHere, the additional 4.00224 added to the intercept term is the coefficient associated with genderMale\n\n\np &lt;- ggplot(data = d, aes(x = age, y = height)) + geom_point(aes(color = factor(gender))) +\n    scale_color_manual(values = c(\"red\", \"blue\"))\np &lt;- p + geom_abline(slope = m$coefficients[3], intercept = m$coefficients[1], color = \"darkred\")\np &lt;- p + geom_abline(slope = m$coefficients[3], intercept = m$coefficients[1] + m$coefficients[2],\n    color = \"darkblue\")\np\n\n\n\n\n\n\n\n\nNote that this model is based on all of the data collectively… we are not doing separate linear models for males and females, which could result in different intercepts and slopes for each sex… rather, we are modeling both sexes as having the same slope but different intercepts. This is what is known as a “parallel slopes” model. Below, we will explore a model where we posit an interaction between age and sex, which would require estimation of four separate parameters (i.e., both a slope and an intercept for males and females rather than, as above, only different intercepts for males and females but the same slope for each sex).\nUsing the confint() function on our ANCOVA model results reveals the confidence intervals for each of the coefficients in our multiple regression, just as it did for simple regression.\n\nconfint(m, level = 0.95)\n\n##                  2.5 %     97.5 %\n## (Intercept) 45.6265330 47.8584809\n## genderMale   3.6792172  4.3252593\n## age          0.8864191  0.9954081\n\n\nWe can also generate effects plots for this ANCOVA model:\n\nplot(predictorEffects(m, partial.residuals = TRUE))",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Multiple Regression and ANCOVA</span>"
    ]
  },
  {
    "objectID": "21-module.html#multicollinearity-and-vifs",
    "href": "21-module.html#multicollinearity-and-vifs",
    "title": "21  Multiple Regression and ANCOVA",
    "section": "21.6 Multicollinearity and VIFs",
    "text": "21.6 Multicollinearity and VIFs\nWhen we have more than one explanatory variable in a regression model, we need to be concerned about possible high inter-correlations those variables. One way to characterize the amount of multicollinearity is by examining the variance inflation factor, or VIF, for each variable. Mathematically, the VIF for a predictor variable i is calculated as:\n\\[\\frac{1}{1-R_i^2}\\] where \\(R_i^2\\) is the R-squared value for the regression of variable i on all other predictors. A high VIF indicates that the predictor is highly collinear with other predictors in the model. As a rule of thumb, a VIF value that exceeds ~5 indicates a problematic amount of multicollinearity. The function vif() from the {car} package provides one way to calculate VIFs.\nBelow, we set up a new model with three predictors, weight, age, and gender, and calculate VIFs:\n\nm &lt;- lm(height ~ weight + age + gender, data = d)\nsummary(m)\n\n## \n## Call:\n## lm(formula = height ~ weight + age + gender, data = d)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.6235 -1.0082  0.0141  0.9845  4.6444 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 33.309791   0.437855   76.08   &lt;2e-16 ***\n## weight       0.140542   0.003083   45.59   &lt;2e-16 ***\n## age          0.662485   0.016953   39.08   &lt;2e-16 ***\n## genderMale   1.609671   0.107436   14.98   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.482 on 996 degrees of freedom\n## Multiple R-squared:  0.8821, Adjusted R-squared:  0.8818 \n## F-statistic:  2484 on 3 and 996 DF,  p-value: &lt; 2.2e-16\n\nvif(m)\n\n##   weight      age   gender \n## 1.463629 1.149158 1.313461\n\n\nWe can also calculate VIFs manually, by running models that regress each individual predictor on all of the others…\n\nw &lt;- lm(weight ~ gender + age, data = d)\nrsq_w &lt;- summary(w)$r.square\n(vif_w &lt;- 1/(1 - rsq_w))\n\n## [1] 1.463629\n\na &lt;- lm(age ~ gender + weight, data = d)\nrsq_a &lt;- summary(a)$r.square\n(vif_a &lt;- 1/(1 - rsq_a))\n\n## [1] 1.149158\n\n# for a categorical predictor variable, we can coerce it to numeric to use it\n# as a response variable\ng &lt;- lm(as.numeric(gender) ~ age + weight, data = d)\nrsq_g &lt;- summary(g)$r.square\n(vif_g &lt;- 1/(1 - rsq_g))\n\n## [1] 1.313461",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Multiple Regression and ANCOVA</span>"
    ]
  },
  {
    "objectID": "21-module.html#confidence-and-prediction-intervals",
    "href": "21-module.html#confidence-and-prediction-intervals",
    "title": "21  Multiple Regression and ANCOVA",
    "section": "21.7 Confidence and Prediction Intervals",
    "text": "21.7 Confidence and Prediction Intervals\nOne common reason for fitting a regression model is to then use that model to predict the value of the response variable under particular combinations of values of predictor variables, i.e., to predict the values of new observations. We can think of the Y value predicted by the regression model (\\(\\hat{y}\\)) as a point estimate of the response variable given particular value(s) of the predictor variable(s). This point estimate is associated with some uncertainty, which we can characterize in terms of either confidence intervals or prediction intervals around each the estimate. CIs represent uncertainty about the mean value of the response variable for a given (combination of) predictor value(s), while PIs represent our uncertainty about actual new predicted values of the response variable at those predictor value(s).\nThe predict() allows us to determine these intervals for individual responses for a given combination of predictor variables. predict() takes as arguments a model object, new values for the predictor(s) to be plugged into the model, an interval type (“confidence” or “prediction”), and a confidence level (e.g., “0.95”).\n\nCHALLENGE\nLet’s return to our model of height as a function of age and gender:\n\nm &lt;- lm(height ~ age + gender, data = d)\n\n\nWhat is the predicted mean height, in inches, for 29-year old males who have survived the zombie apocalypse? What is the 95% confidence interval around this predicted mean height?\n\n\n\nShow Code\nci &lt;- predict(m, newdata = data.frame(age = 29, gender = \"Male\"), interval = \"confidence\",\n    level = 0.95)\nci\n\n\nShow Output\n##        fit      lwr      upr\n## 1 78.03124 77.49345 78.56903\n\n\n\n\nWhat is the 95% prediction interval for the individual heights of 29-year old male zombie apocalypse survivors?\n\n\n\nShow Code\npi &lt;- predict(m, newdata = data.frame(age = 29, gender = \"Male\"), interval = \"prediction\",\n    level = 0.95)\npi\n\n\nShow Output\n##        fit      lwr      upr\n## 1 78.03124 72.89597 83.16651\n\n\n\nIn general, the width of a confidence interval for \\(\\hat{y}\\) increases as the value of our predictor variables moves away from the center of their distribution. Also, although both are intervals are centered at \\(\\hat{y}\\), the predicted mean value of the response variable for the given (combination of) predictor(s), the prediction interval is invariably wider than the confidence interval. This makes intuitive sense, since the prediction interval encapsulates the possibility for individual values of Y to fluctuate away from the predicted mean value, while the confidence interval describes uncertainty in estimates of the predicted mean value. That is, the estimate of the mean of a variable is expected to be less uncertain than estimates of individual variable values, because the mean is already a statistic that summarizes a set of values.\nWe can see this by generating CIs and PIs for a range of data using the predict() function. Below, we pass predict() a new set of 1000 values ranging from the minimum to the maximum male age in our original data set…\n\nmales &lt;- d |&gt;\n    filter(gender == \"Male\")\nnew_x &lt;- seq(from = min(males$age), to = max(males$age), length.out = 1000)\nci &lt;- predict(m, newdata = data.frame(age = new_x, gender = \"Male\"), interval = \"confidence\",\n    level = 0.95)\nci &lt;- tibble(x = new_x, as_tibble(ci))\n\npi &lt;- predict(m, newdata = data.frame(age = new_x, gender = \"Male\"), interval = \"prediction\",\n    level = 0.95)\n\npi &lt;- as_tibble(pi)\npi &lt;- tibble(x = new_x, as_tibble(pi))\n\np &lt;- ggplot(data = d |&gt;\n    filter(gender == \"Male\"), aes(x = age, y = height)) + geom_point() + geom_line(data = ci,\n    aes(x = x, y = fit), color = \"red\") + geom_line(data = ci, aes(x = x, y = lwr),\n    color = \"blue\") + geom_line(data = ci, aes(x = x, y = upr), color = \"blue\") +\n    geom_line(data = pi, aes(x = x, y = lwr), color = \"green\") + geom_line(data = pi,\n    aes(x = x, y = upr), color = \"green\")\np\n\n\n\n\n\n\n\n\nAn easier, alternative way to generate CIs and PIs for across a range of predictor values iswith the augment() function from {broom}, as it returns a “tibble” with fitted values and interval limits appended to the original data frame.\n\nci &lt;- augment(m, data = d, interval = c(\"confidence\"))\npi &lt;- augment(m, data = d, interval = c(\"prediction\"))\n\np &lt;- ggplot(data = ci |&gt;\n    filter(gender == \"Male\"), aes(x = age, y = height)) + geom_point() + geom_line(aes(x = age,\n    y = .fitted), color = \"red\") + geom_ribbon(aes(x = age, ymin = .lower, ymax = .upper),\n    alpha = 0.5) + geom_ribbon(data = pi |&gt;\n    filter(gender == \"Male\"), aes(x = age, ymin = .lower, ymax = .upper), alpha = 0.1)\np",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Multiple Regression and ANCOVA</span>"
    ]
  },
  {
    "objectID": "21-module.html#interactions-between-predictors",
    "href": "21-module.html#interactions-between-predictors",
    "title": "21  Multiple Regression and ANCOVA",
    "section": "21.8 Interactions Between Predictors",
    "text": "21.8 Interactions Between Predictors\nSo far, we have only considered the joint main effects of multiple predictors on a response variable, but often there are interactive effects between our predictors. An interactive effect is an additional change in the response that occurs because of particular combinations of predictors or because the relationship of one continuous variable to a response is contingent on a particular level of a categorical variable. We explored the former case a bit when we looked at ANOVAs involving two discrete predictors. Now, we’ll consider the latter case… is there an interactive effect of sex AND age on height in our population of zombie apocalypse survivors?\nUsing formula notation, it is easy for us to consider interactions between predictors. The colon (:) operator allows us to specify particular interactions we want to consider. We can also use the asterisk (*) operator to specify a full model, i.e., all single terms factors and all their interactions.\nWhat are the formula and results for a linear model of height as a function of age and sex plus the interaction of age and sex?\n\n\nShow Code\nm &lt;- lm(data = d, height ~ age + gender + age:gender)\nsummary(m)\n\n\nShow Output\n## \n## Call:\n## lm(formula = height ~ age + gender + age:gender, data = d)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -9.7985 -1.6973  0.1189  1.7662  7.9473 \n## \n## Coefficients:\n##                Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)    48.18107    0.79839  60.348   &lt;2e-16 ***\n## age             0.86913    0.03941  22.053   &lt;2e-16 ***\n## genderMale      1.15975    1.12247   1.033   0.3018    \n## age:genderMale  0.14179    0.05539   2.560   0.0106 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.595 on 996 degrees of freedom\n## Multiple R-squared:  0.6385, Adjusted R-squared:  0.6374 \n## F-statistic: 586.4 on 3 and 996 DF,  p-value: &lt; 2.2e-16\n\n\n\nShow Code\n# or\nm &lt;- lm(data = d, height ~ age * gender)\nsummary(m)\n\n\nShow Output\n## \n## Call:\n## lm(formula = height ~ age * gender, data = d)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -9.7985 -1.6973  0.1189  1.7662  7.9473 \n## \n## Coefficients:\n##                Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)    48.18107    0.79839  60.348   &lt;2e-16 ***\n## age             0.86913    0.03941  22.053   &lt;2e-16 ***\n## genderMale      1.15975    1.12247   1.033   0.3018    \n## age:genderMale  0.14179    0.05539   2.560   0.0106 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.595 on 996 degrees of freedom\n## Multiple R-squared:  0.6385, Adjusted R-squared:  0.6374 \n## F-statistic: 586.4 on 3 and 996 DF,  p-value: &lt; 2.2e-16\n\n\n\nShow Code\ncoefficients(m)\n\n\nShow Output\n##    (Intercept)            age     genderMale age:genderMale \n##     48.1810741      0.8691284      1.1597481      0.1417928\n\n\n\nHere, when we allow an interaction, there is no main effect of gender, but there is an interaction effect of gender and age.\nIf we want to visualize this…\n\\[ female\\ height = 48.1817041 + 0.8891281 \\times age\\] \\[ male\\ height = 48.1817041 + + 1.1597481 + 0.1417928 \\times age\\]\n\np1 &lt;- ggplot(data = d, aes(x = age, y = height)) + geom_point(aes(color = factor(gender))) +\n    scale_color_manual(values = c(\"red\", \"blue\"))\np1 &lt;- p1 + geom_abline(slope = m$coefficients[2], intercept = m$coefficients[1],\n    color = \"darkred\")\np1 &lt;- p1 + geom_abline(slope = m$coefficients[2] + m$coefficients[4], intercept = m$coefficients[1] +\n    m$coefficients[3], color = \"darkblue\")\np1\n\n\n\n\n\n\n\n# or, using `geom_smooth()`...\np2 &lt;- ggplot(data = d, aes(x = age, y = height)) + geom_point(aes(color = factor(gender))) +\n    scale_color_manual(values = c(\"red\", \"blue\")) + geom_smooth(method = \"lm\", aes(color = factor(gender)))\np2\n\n## `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe predictorEffects() function from {effects} provides and alternative way to visualize interactions…\n\nplot(predictorEffects(m, predictors = ~age))\n\n\n\n\n\n\n\nplot(predictorEffects(m, predictors = ~gender))\n\n\n\n\n\n\n\n\nIn the case of gender, the function plots the difference in predicted height of females and males as a set of levels specified by the argument “xlevels=”. The argument can be set to a particular number of levels (which is 5 by default, as in the example above) or it can be passed a list with elements named for the predictors, with either the number of level or a vector of level values added.\n\nplot(predictorEffects(m, predictors = ~gender, xlevels = list(age = 6)))  # 6 ages, spanning the range of ages in the dataset\n\n\n\n\n\n\n\nplot(predictorEffects(m, predictors = ~gender, xlevels = list(age = c(10, 15, 20))))  # 3 ages, specified by the user\n\n\n\n\n\n\n\n\n\nCHALLENGE\n\nLoad in the “KamilarAndCooper.csv”” dataset we have used previously\nReduce the dataset to the following variables: Family, Brain_Size_Female_Mean, Body_mass_female_mean, MeanGroupSize, DayLength_km, HomeRange_km2, and Move and assign Family to be a factor. Also, rename the brain size, body mass, group size, day length, and home range variables as BrainSize, BodyMass, GroupSize, DayLength, HomeRange\n\n\n\nShow Code\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/KamilarAndCooperData.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\n\n\n## Rows: 213 Columns: 44\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (16): Scientific_Name, Family, Genus, Species, Brain_size_Ref, Mass_Ref,...\n## dbl (28): Brain_Size_Species_Mean, Brain_Size_Female_Mean, Body_mass_male_me...\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nShow Code\nd$Family &lt;- factor(d$Family)\nhead(d)\n\n\nShow Output\n## # A tibble: 6 × 44\n##   Scientific_Name             Family        Genus Species Brain_Size_Species_M…¹\n##   &lt;chr&gt;                       &lt;fct&gt;         &lt;chr&gt; &lt;chr&gt;                    &lt;dbl&gt;\n## 1 Allenopithecus_nigroviridis Cercopitheci… Alle… nigrov…                   58.0\n## 2 Allocebus_trichotis         Cercopitheci… Allo… tricho…                   NA  \n## 3 Alouatta_belzebul           Atelidae      Alou… belzeb…                   52.8\n## 4 Alouatta_caraya             Atelidae      Alou… caraya                    52.6\n## 5 Alouatta_guariba            Atelidae      Alou… guariba                   51.7\n## 6 Alouatta_palliata           Atelidae      Alou… pallia…                   49.9\n## # ℹ abbreviated name: ¹​Brain_Size_Species_Mean\n## # ℹ 39 more variables: Brain_Size_Female_Mean &lt;dbl&gt;, Brain_size_Ref &lt;chr&gt;,\n## #   Body_mass_male_mean &lt;dbl&gt;, Body_mass_female_mean &lt;dbl&gt;,\n## #   Mass_Dimorphism &lt;dbl&gt;, Mass_Ref &lt;chr&gt;, MeanGroupSize &lt;dbl&gt;,\n## #   AdultMales &lt;dbl&gt;, AdultFemale &lt;dbl&gt;, AdultSexRatio &lt;dbl&gt;,\n## #   Social_Organization_Ref &lt;chr&gt;, InterbirthInterval_d &lt;dbl&gt;, Gestation &lt;dbl&gt;,\n## #   WeaningAge_d &lt;dbl&gt;, MaxLongevity_m &lt;dbl&gt;, LitterSz &lt;dbl&gt;, …\n\n\n\nShow Code\nd &lt;- d |&gt;\n    select(Brain_Size_Female_Mean, Family, Body_mass_female_mean, MeanGroupSize,\n        DayLength_km, HomeRange_km2, Move) |&gt;\n    rename(BrainSize = Brain_Size_Female_Mean, BodyMass = Body_mass_female_mean,\n        GroupSize = MeanGroupSize, DayLength = DayLength_km, HomeRange = HomeRange_km2)\n\n\n\nFit a Model I least squares multiple linear regression model using log(HomeRange) as the response variable and log(BodyMass), log(BrainSize), GroupSize, and Move as predictor variables, and view a model summary.\nLook at and interpret the estimated regression coefficients for the fitted model and interpret. Are any of them statistically significant? What can you infer about the relationship between the response and predictors?\nReport and interpret the coefficient of determination and the outcome of the omnibus F test.\nExamine the residuals… are they normally distributed?\n\n\n\nShow Code\nm &lt;- lm(data = d, log(HomeRange) ~ log(BodyMass) + log(BrainSize) + GroupSize + Move)\nsummary(m)\n\n\nShow Output\n## \n## Call:\n## lm(formula = log(HomeRange) ~ log(BodyMass) + log(BrainSize) + \n##     GroupSize + Move, data = d)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.7978 -0.6473 -0.0038  0.8807  2.1598 \n## \n## Coefficients:\n##                 Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)    -6.955853   1.957472  -3.553 0.000865 ***\n## log(BodyMass)   0.315276   0.468439   0.673 0.504153    \n## log(BrainSize)  0.614460   0.591100   1.040 0.303771    \n## GroupSize       0.034026   0.009793   3.475 0.001095 ** \n## Move            0.025916   0.019559   1.325 0.191441    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.127 on 48 degrees of freedom\n##   (160 observations deleted due to missingness)\n## Multiple R-squared:  0.6359, Adjusted R-squared:  0.6055 \n## F-statistic: 20.95 on 4 and 48 DF,  p-value: 4.806e-10\n\n\n\nShow Code\nplot(m$residuals)\n\n\n\n\n\n\n\n\n\nShow Code\nqqnorm(m$residuals)\n\n\n\n\n\n\n\n\n\nShow Code\nshapiro.test(m$residuals)\n\n\nShow Output\n## \n##  Shapiro-Wilk normality test\n## \n## data:  m$residuals\n## W = 0.96517, p-value = 0.1242\n\n\n\nWhen we plot effects for this model, note that the predictor effect plot uses untransformed predictor values on the horizontal axis, not the log-transformed variables that we used in the regression model.\n\nplot(predictorEffects(m, partial.residuals = TRUE))\n\n\n\n\n\n\n\n\nWe can use the “axes=” argument and the “x=” sub-argument to transform the horizontal axis, for example to replace BodyMass by log(BodyMass). The code below specifies a lot of tweaks to the x axis…\n\nplot(predictorEffects(m, ~BodyMass, partial.residuals = TRUE), axes = list(x = list(rotate = 90,\n    BodyMass = list(transform = list(trans = log, inverse = exp), lab = \"log(Body Mass)\",\n        ticks = list(at = c(100, 1000, 10000, 1e+05)), lim = c(100, 1e+05)))))\n\n\n\n\n\n\n\n\n\nWhat happens if we remove the \\(Move\\) term from the model?\n\n\n\nShow Code\nm &lt;- lm(data = d, log(HomeRange) ~ log(BodyMass) + log(BrainSize) + GroupSize)\nsummary(m)\n\n\nShow Output\n## \n## Call:\n## lm(formula = log(HomeRange) ~ log(BodyMass) + log(BrainSize) + \n##     GroupSize, data = d)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.7982 -0.7310 -0.0140  0.8386  3.1926 \n## \n## Coefficients:\n##                 Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)    -5.354845   1.176046  -4.553 1.45e-05 ***\n## log(BodyMass)  -0.181627   0.311382  -0.583 0.560972    \n## log(BrainSize)  1.390536   0.398840   3.486 0.000721 ***\n## GroupSize       0.030433   0.008427   3.611 0.000473 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.228 on 103 degrees of freedom\n##   (106 observations deleted due to missingness)\n## Multiple R-squared:  0.6766, Adjusted R-squared:  0.6672 \n## F-statistic: 71.85 on 3 and 103 DF,  p-value: &lt; 2.2e-16\n\n\n\nShow Code\nplot(m$residuals)\n\n\n\n\n\n\n\n\n\nShow Code\nqqnorm(m$residuals)\n\n\n\n\n\n\n\n\n\nShow Code\nshapiro.test(m$residuals)  # no significant deviation from normal\n\n\nShow Output\n## \n##  Shapiro-Wilk normality test\n## \n## data:  m$residuals\n## W = 0.99366, p-value = 0.9058",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Multiple Regression and ANCOVA</span>"
    ]
  },
  {
    "objectID": "21-module.html#visualizing-model-results",
    "href": "21-module.html#visualizing-model-results",
    "title": "21  Multiple Regression and ANCOVA",
    "section": "21.9 Visualizing Model Results",
    "text": "21.9 Visualizing Model Results\nWe have already seen lots of ways to view linear modeling results, e.g., by using the summary(), glance(), and tidy() functions with model objects as arguments. The {jtools} package provides some interesting additional functions for summarizing and visualizing regression models results.\n\nThe summ() function is an alternative to summary() that provides a concise table of a model and its results. It can be used with standard lm() simple linear regression results (as we work with in this module) as well as with glm() and lmer() results (for generalized linear modeling and mixed effects modeling, respectively), as we cover in Module 23 and Module 24.\nThe effect_plot() function can be used to plot the relationship between (one of) the predictor variables and the response variable while holding the others constant. The function contains lots of possible arguments for customizing the plot, e.g., for specifying whether and what types of interval can be plotted.\nThe plot_summs() function can be used to nicely visualize coefficient estimates and CI values around those terms, including visualizing multiple models on the same plot.\n\nLet’s return to working with the zombie apocaluypse survivors dataset…\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/zombies.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\nlibrary(jtools)\nm1 &lt;- lm(data = d, height ~ age + weight + gender + zombies_killed)\nsumm(m1, confint = TRUE, ci.width = 0.95, digits = 3)\n\n\n\n\n\nObservations\n1000\n\n\nDependent variable\nheight\n\n\nType\nOLS linear regression\n\n\n\n\n\n\n\n\nF(4,995)\n1863.277\n\n\nR²\n0.882\n\n\nAdj. R²\n0.882\n\n\n\n\n\n\n\n\n\nEst.\n2.5%\n97.5%\nt val.\np\n\n\n\n\n(Intercept)\n33.219\n32.340\n34.098\n74.185\n0.000\n\n\nage\n0.662\n0.629\n0.696\n39.060\n0.000\n\n\nweight\n0.141\n0.135\n0.147\n45.586\n0.000\n\n\ngenderMale\n1.609\n1.398\n1.820\n14.974\n0.000\n\n\nzombies_killed\n0.026\n-0.027\n0.079\n0.970\n0.332\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\n\n\neffect_plot(m1, pred = age, interval = TRUE, int.type = \"confidence\", int.width = 0.95,\n    plot.points = TRUE)\n\n\n\n\n\n\n\nm2 &lt;- lm(data = d, height ~ age + weight + gender)\nm3 &lt;- lm(data = d, height ~ age + gender)\nm4 &lt;- lm(data = d, height ~ weight + gender)\nm5 &lt;- lm(data = d, height ~ gender)\n\n# plot with unstandardized beta coefficients\nplot_summs(m1)\n\n\n\n\n\n\n\n# plot with standardized beta coefficients (i.e., mean centers the variables\n# and sets SD to 1)\nplot_summs(m1, scale = TRUE)\n\n\n\n\n\n\n\n# plot with 95% CIs and distributions\nplot_summs(m1, plot.distributions = TRUE)\n\n\n\n\n\n\n\n# plot with 95% CIs and distributions scaled to same height\nplot_summs(m1, plot.distributions = TRUE, rescale.distributions = TRUE)\n\n\n\n\n\n\n\n# plot multiple models\nplot_summs(m1, m2, m3, m4, m5, scale = TRUE)\n\n\n\n\n\n\n\n# plot multiple models\nplot_summs(m1, m2, m3, m4, m5, plot.distributions = TRUE, rescale.distributions = TRUE)\n\n\n\n\n\n\n\ndetach(package:jtools)",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Multiple Regression and ANCOVA</span>"
    ]
  },
  {
    "objectID": "21-module.html#concept-review",
    "href": "21-module.html#concept-review",
    "title": "21  Multiple Regression and ANCOVA",
    "section": "Concept Review",
    "text": "Concept Review\n\nMultiple regression and ANCOVA are further extensions of simple linear regression/ANOVA to cases where we have more than one predictor variable\n“Multiple regression” refers to where we have 2 or more continuous predictor variables\n“ANCOVA” refers to cases where we have one or more continuous predictor variables and one or more categorical predictor variables",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Multiple Regression and ANCOVA</span>"
    ]
  },
  {
    "objectID": "maximum_likelihood.html",
    "href": "maximum_likelihood.html",
    "title": "22  Maximum Likelihood Estimation Example",
    "section": "",
    "text": "22.1 Preliminaries\nInstall and load the {bbmle} library and load the {tidyverse} and {mosaic} libraries\nlibrary(tidyverse)\n\n## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n## ✔ dplyr     1.1.4     ✔ readr     2.1.5\n## ✔ forcats   1.0.0     ✔ stringr   1.5.1\n## ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n## ✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n## ✔ purrr     1.0.4     \n## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\n## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(bbmle)\n\n## Loading required package: stats4\n## \n## Attaching package: 'bbmle'\n## \n## The following object is masked from 'package:dplyr':\n## \n##     slice\n\nlibrary(mosaic)\n\n## Registered S3 method overwritten by 'mosaic':\n##   method                           from   \n##   fortify.SpatialPolygonsDataFrame ggplot2\n## \n## The 'mosaic' package masks several functions from core packages in order to add \n## additional features.  The original behavior of these functions should not be affected by this.\n## \n## Attaching package: 'mosaic'\n## \n## The following object is masked from 'package:Matrix':\n## \n##     mean\n## \n## The following objects are masked from 'package:dplyr':\n## \n##     count, do, tally\n## \n## The following object is masked from 'package:purrr':\n## \n##     cross\n## \n## The following object is masked from 'package:ggplot2':\n## \n##     stat\n## \n## The following objects are masked from 'package:stats':\n## \n##     binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n##     quantile, sd, t.test, var\n## \n## The following objects are masked from 'package:base':\n## \n##     max, mean, min, prod, range, sample, sum",
    "crumbs": [
      "Part III - Miscellany",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Maximum Likelihood Estimation Example</span>"
    ]
  },
  {
    "objectID": "maximum_likelihood.html#probability-density-functions-and-likelihoods",
    "href": "maximum_likelihood.html#probability-density-functions-and-likelihoods",
    "title": "22  Maximum Likelihood Estimation Example",
    "section": "22.2 Probability Density Functions and Likelihoods",
    "text": "22.2 Probability Density Functions and Likelihoods\nA Probability Density Function (PDF) is a function describing the density of a continuous random variable (a probability mass function, or PMF, is an analogous function for discrete random variables). The function’s value can be calculated at any given point in the sample space (i.e., the range of possible values that the random variable might take) and provides the relative likelihood of obtaining that value on a draw from the given distribution.\nThe PDF for a normal distribution defined by the parameters \\(\\mu\\) and \\(\\sigma\\) is…\n\\[f(x | \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^\\frac{-(x-\\mu)^2}{2\\sigma^2}\\]\nWhere: - \\(\\mu\\) is the mean (or expected value) of the distribution - \\(\\sigma^2\\) is the variance of the distribution. - \\(\\sigma\\) the standard deviation. - \\(e\\) is the base of the natural logarithm (approximately 2.71828) - \\(\\pi\\) is a mathematical constant (approximately 3.14159)\nSolving this function for any value of \\(x\\) yields the relative likelihood (\\(L\\)) of drawing that value from the distribution.",
    "crumbs": [
      "Part III - Miscellany",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Maximum Likelihood Estimation Example</span>"
    ]
  },
  {
    "objectID": "maximum_likelihood.html#calculating-likelihoods",
    "href": "maximum_likelihood.html#calculating-likelihoods",
    "title": "22  Maximum Likelihood Estimation Example",
    "section": "22.3 Calculating Likelihoods",
    "text": "22.3 Calculating Likelihoods\nAs an example, we can calculate the relative likelihood of drawing the number 45.6 out of a normal distribution with a mean (\\(\\mu\\)) of 50 and a standard deviation (\\(\\sigma\\)) of 10…\n\nval &lt;- 45.6\nmean &lt;- 50\nsd &lt;- 10\n(l &lt;- 1/sqrt(2 * pi * sd^2) * exp((-(val - mean)^2)/(2 * sd^2)))\n\n## [1] 0.03621349\n\n\nThe dnorm() function returns this relative likelihood directly…\n\n(l &lt;- dnorm(val, mean, sd))\n\n## [1] 0.03621349\n\n\nTo further explore calculating likelihoods, let’s create and plot a sample of 100 random variables from a normal distribution with a \\(\\mu\\) of 50 and a \\(\\sigma\\) of 10:\n\nd &lt;- tibble(val = rnorm(100, mean = 50, sd = 10))\np &lt;- ggplot(d) + geom_histogram(aes(x = val, y = after_stat(density)))\np\n\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThe mean and standard deviation of this sample are pretty are close to the parameters that we pass to the rnorm() function, even though (given the limited size of our sample) the distribution is not smooth…\n\n(mean &lt;- mean(d$val))\n\n## [1] 50.7699\n\n(sd &lt;- sd(d$val))\n\n## [1] 10.39291\n\n\nNow, on top of the histogram of our sample data, let’s plot two different normal distributions, one with the distribution our sample was drawn from (\\(\\mu\\) = 50 and \\(\\sigma\\) = 10, in red) and another with a different \\(\\mu\\) (65) and \\(\\sigma\\) (20).\n\np &lt;- p + stat_function(fun = function(x) dnorm(x, mean = 50, sd = 10), color = \"red\",\n    linewidth = 1) + stat_function(fun = function(x) dnorm(x, mean = 65, sd = 20),\n    color = \"blue\", linewidth = 1)\np\n\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nBased on the figure, it should be pretty clear that our observed sample (our set of 100 random draws) is more likely to have been drawn from the former distribution than the latter.\n\nCHALLENGE\nWhat is the relative likelihood of seeing a value of 41 on a random draw from a normal distribution with \\(\\mu\\) = 50 and \\(\\sigma\\) = 10?\n\nval &lt;- 41\nmean &lt;- 50\nsd &lt;- 10\n# or\n(l1 &lt;- dnorm(val, mean, sd))\n\n## [1] 0.02660852\n\n\n\n\nCHALLENGE\nWhat is the relative likelihood of seeing a value of 41 on a random draw from a normal distribution with \\(\\mu\\) = 65 and \\(\\sigma\\) = 20?\n\nval &lt;- 41\nmean &lt;- 65\nsd &lt;- 10\n(l2 &lt;- dnorm(val, mean, sd))\n\n## [1] 0.002239453\n\n\nNote the difference in these two likelihoods, l1 and l2… there is a much higher likelhood of an observation of 41 coming from the first of the two normal distributions we considered. In this case, the likelihood ratio is l1/l2.\n\nl1/l2\n\n## [1] 11.88171\n\n\nWe can also calculate the relative likelihood of drawing a particular set of observations from a given distribution as the product of the likelihoods of each observation as the probability of n independent events is simply the product (\\(∏\\)) of their independent probabilities. An easy way to calculate the product of a set of numbers is by taking the natural log of each number, summing those values, and then exponentiating the sum. For this reason, likelihood calculations are often operationalized in terms of log likelihoods.\nFor example, the following operations are equivalent…\n\n5 * 8 * 9 * 4\n\n## [1] 1440\n\nexp(log(5) + log(8) + log(9) + log(4))\n\n## [1] 1440\n\nexp(sum(log(c(5, 8, 9, 4))))\n\n## [1] 1440\n\n\n\n\nCHALLENGE\nWhat is the relative likelihood of drawing the set of three numbers 41, 70, and 10 from a normal distribution with \\(\\mu\\) = 50 and \\(\\sigma\\) = 10?\n\nval &lt;- c(41, 70, 10)\nmean &lt;- 65\nsd &lt;- 10\n(l &lt;- dnorm(val, mean, sd))  # vector of likelihoods of each value\n\n## [1] 2.239453e-03 3.520653e-02 1.076976e-08\n\n(l[1] * l[2] * l[3])  # product of likelihoods\n\n## [1] 8.491242e-13\n\n(ll &lt;- log(l))  # log likelihoods of each value\n\n## [1]  -6.101524  -3.346524 -18.346524\n\n(ll &lt;- sum(ll))  # summed log likelihood\n\n## [1] -27.79457\n\n(l &lt;- exp(ll))  # convert back to likelihood\n\n## [1] 8.491242e-13\n\n\n\nNOTE: Likelihoods are always going to be zero or greater, but log likelihoods can be negative. A less negative log likelihood is nonetheless more likely than and more negative one.\n\n\n\nCHALLENGE\nWhat are the log likelihood and likelihood of drawing the sample, d, we constructed above from a normal distribution with \\(\\mu\\) = 50 and \\(\\sigma\\) = 10? How do these compare to the log likelihood and likelihood of that same sample being drawn from a normal distribution with \\(\\mu\\) = 65 and \\(\\sigma\\) = 20?\n\nval &lt;- d$val\nmean &lt;- 50\nsd &lt;- 10\nl &lt;- dnorm(val, mean, sd)\nll &lt;- log(l)\nll &lt;- sum(ll)\n(l &lt;- exp(ll))  # a very tiny number!\n\n## [1] 5.523165e-164\n\nmean &lt;- 65\nsd &lt;- 20\nl &lt;- dnorm(val, mean, sd)\nll &lt;- log(l)\nll &lt;- sum(ll)\n(l &lt;- exp(ll))  # an even tinier number\n\n## [1] 1.549261e-187\n\n\nThe log likelihood and likelihood are both HIGHER (more likely to have been drawn) from the first distribution.",
    "crumbs": [
      "Part III - Miscellany",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Maximum Likelihood Estimation Example</span>"
    ]
  },
  {
    "objectID": "maximum_likelihood.html#maximum-likelihood-estimation",
    "href": "maximum_likelihood.html#maximum-likelihood-estimation",
    "title": "22  Maximum Likelihood Estimation Example",
    "section": "22.4 Maximum Likelihood Estimation",
    "text": "22.4 Maximum Likelihood Estimation\nNow that we have an understanding of how likelihood calculations are done, we can consider the process of maximum likelihood estimation (MLE). MLE allows us to estimate what values of population parameters (e.g., \\(\\mu\\) and \\(\\sigma\\)) are most likely given a dataset and a probability distribution function for the process generating the data (e.g., a Gaussian process).\nFirst, let’s create a function for calculating the negative log likelihood for a set of data under a particular normal distribution. To convert the log likelihood to a negative, we simply multiply it negative 1. The reason we do this is because most optimization algorithms function by searching for parameter values that yield the minimum negative log likelihood rather than the maximum likelihood directly.\n\nverbose_nll &lt;- function(val, mu, sigma, verbose = FALSE) {\n    l &lt;- 0  # set initial likelihood to 0 to define variable\n    ll &lt;- 0  # set initial log likelihood to 0 to define variable\n    for (i in 1:length(val)) {\n        l[[i]] = dnorm(val[[i]], mean = mu, sd = sigma)  # likelihoods\n        ll[[i]] &lt;- log(l[[i]])  # log likelihoods\n        if (verbose == TRUE) {\n            message(paste0(\"x=\", round(val[[i]], 4), \" mu=\", mu, \" sigma=\", sigma,\n                \" L=\", round(l[[i]], 4), \" logL=\", round(ll[[i]], 4)))\n        }\n    }\n    nll &lt;- -1 * sum(ll)\n    return(nll)\n}\n\nTesting our function…\n\n# if we include verbose = TRUE as an argument to `verbose_nll`, the function\n# will print each likelihood and log likelihood value before returning the\n# summed negative log likelihood\nval &lt;- d$val\nmean &lt;- 50\nsd &lt;- 10\nverbose_nll(val, mean, sd)\n\n## [1] 375.915\n\nmean &lt;- 65\nsd &lt;- 20\nverbose_nll(val, mean, sd)\n\n## [1] 430.1456\n\n\n… we see that negative log likelihood of our data under the first distribution is smaller than under the second, indicating that it is more likely that are data are drawn from the first distribution.\nTo use maximum likelihood estimation, we need to create a simpler version of this function that does not have the data as argument, but rather only involves the parameter we want to estimate. The function below does this. Note that this version does not use a loop and has the vector variable val included in the dnorm() function. We also include the argument log = TRUE within the dnorm() function to calculate the log likelihood instead of including that as an extra step.\n\nsimple_nll &lt;- function(mu, sigma, verbose = TRUE) {\n    ll = sum(dnorm(val, mean = mu, sd = sigma, log = TRUE))\n    nll &lt;- -1 * ll\n    return(nll)\n}\n\nTesting our function, we should get the same results as above…\n\nval &lt;- d$val\nmean &lt;- 50\nsd &lt;- 10\nsimple_nll(mean, sd)\n\n## [1] 375.915\n\n\nSo far, we’ve just use our function to calculate (negative log) likelihoods of our data with being generated by a Gaussian (normal) process with specific \\(\\mu\\) and \\(\\sigma\\) parameters… but what we really want to do is estimate values for \\(\\mu\\) and \\(\\sigma\\) that have highest likelihood for generating our data. To do this, we will use the mle2() function from the {bbmle} package. This function takes the following arguments…\n\nminuslogl, which is a user-defined function for generating negative log likelihoods… this is what we created above\nstart, which is a list of initial values for the parameters we want estimate\nmethod, which is the particular algorithm used for optimization of the parameter estimates (mle2() and the optim() function, which it calls, are highly customizable)\n\n\nNOTE: Be aware that to run the function below, we need to have the variable val, a vector of values, specified outside of the function. We previously set val to d, our random sample of 100 draws from a normal distribution with \\(\\mu\\)=50 and \\(\\sigma\\)=10… feel free to play around with alternatives and see how well the MLE process estimates these parameters.\n\n\nval &lt;- rnorm(10000, mean = 50, sd = 10)\nlibrary(bbmle)\nmle_norm &lt;- mle2(minuslogl = simple_nll, start = list(mu = 0, sigma = 1), method = \"SANN\"  # simulated annealing method of optimization\n)\nmle_norm\n\n## \n## Call:\n## mle2(minuslogl = simple_nll, start = list(mu = 0, sigma = 1), \n##     method = \"SANN\")\n## \n## Coefficients:\n##       mu    sigma \n## 49.97748 10.04925 \n## \n## Log-likelihood: -37266.46\n\nsummary(mle_norm)\n\n## Maximum likelihood estimation\n## \n## Call:\n## mle2(minuslogl = simple_nll, start = list(mu = 0, sigma = 1), \n##     method = \"SANN\")\n## \n## Coefficients:\n##        Estimate Std. Error z value     Pr(z)    \n## mu    49.977475   0.100492  497.33 &lt; 2.2e-16 ***\n## sigma 10.049245   0.071036  141.47 &lt; 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## -2 log L: 74532.93\n\n\nAn alternative for MLE is the maxLik() function from the {maxLik} package. To use function, we again first define our own log likelihood function, but this time we do not want an negative log likelihood, so we do negate it, and it should also have only one argument… a vector of parameter values.\n\nlibrary(maxLik)\n\n## Loading required package: miscTools\n\n\n## \n## Attaching package: 'miscTools'\n\n\n## The following object is masked from 'package:bbmle':\n## \n##     stdEr\n\n\n## \n## Please cite the 'maxLik' package as:\n## Henningsen, Arne and Toomet, Ott (2011). maxLik: A package for maximum likelihood estimation in R. Computational Statistics 26(3), 443-458. DOI 10.1007/s00180-010-0217-1.\n## \n## If you have questions, suggestions, or comments regarding the 'maxLik' package, please use a forum or 'tracker' at maxLik's R-Forge site:\n## https://r-forge.r-project.org/projects/maxlik/\n\nsimple_ll &lt;- function(parameters) {\n    # parameters is a vector of parameter values\n    mu &lt;- parameters[1]\n    sigma &lt;- parameters[2]\n    ll = sum(dnorm(val, mean = mu, sd = sigma, log = TRUE))\n    return(ll)\n}\n\n# test...\nsimple_ll(c(50, 10))\n\n## [1] -37266.75\n\n# an alternative version for loglik\nsimple_ll &lt;- function(parameters) {\n    mu &lt;- parameters[1]\n    sigma &lt;- parameters[2]\n    N &lt;- length(val)\n    ll &lt;- -1 * N * log(sqrt(2 * pi)) - N * log(sigma) - 0.5 * sum((val - mu)^2/sigma^2)\n    return(ll)\n}\n\n# test...\nsimple_ll(c(50, 10))\n\n## [1] -37266.75\n\n\nAs for mle2(), for maxLik() we need to specify several arguments:\n\nlogLik, which is our user-defined function for generating (not negative!) log likelihoods\nstart, which is a named vector of initial values for the parameters we want estimate\nmethod, which is the particular algorithm used for optimization of the parameter estimates\n\n\nmle_norm &lt;- maxLik(logLik = simple_ll, start = c(mu = 0, sigma = 1), method = \"NM\")\nsummary(mle_norm)\n\n## --------------------------------------------\n## Maximum Likelihood estimation\n## Nelder-Mead maximization, 91 iterations\n## Return code 0: successful convergence \n## Log-Likelihood: -37266.49 \n## 2  free parameters\n## Estimates:\n##       Estimate Std. error t value Pr(&gt; t)    \n## mu    49.96651    0.11749   425.3  &lt;2e-16 ***\n## sigma 10.06550    0.07747   129.9  &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## --------------------------------------------\n\n\nThe results should be very comparable to those generated using mle2().",
    "crumbs": [
      "Part III - Miscellany",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Maximum Likelihood Estimation Example</span>"
    ]
  },
  {
    "objectID": "maximum_likelihood.html#mle-for-simple-linear-regression",
    "href": "maximum_likelihood.html#mle-for-simple-linear-regression",
    "title": "22  Maximum Likelihood Estimation Example",
    "section": "22.5 MLE for Simple Linear Regression",
    "text": "22.5 MLE for Simple Linear Regression\nLoad in our zombie apocalypse survivors dataset…\n\nlibrary(tidyverse)\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/zombies.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\n\n## Rows: 1000 Columns: 10\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (4): first_name, last_name, gender, major\n## dbl (6): id, height, weight, zombies_killed, years_of_education, age\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Standard OLS linear regression model with our zombie apocalypse survivor\n# dataset\nm &lt;- lm(data = d, height ~ weight)\nsummary(m)\n\n## \n## Call:\n## lm(formula = height ~ weight, data = d)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -7.1519 -1.5206 -0.0535  1.5167  9.4439 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 39.565446   0.595815   66.41   &lt;2e-16 ***\n## weight       0.195019   0.004107   47.49   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.389 on 998 degrees of freedom\n## Multiple R-squared:  0.6932, Adjusted R-squared:  0.6929 \n## F-statistic:  2255 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n# Compare to...  Likelihood based linear regression with `mle2()`... has 3\n# parameters: b0, b1, and sigma, or the standard deviation in (presumably\n# normally distributed!) residuals\nsimple_nll &lt;- function(b0, b1, sigma, verbose = TRUE) {\n    # here, we say how mu is calculated based on parameters\n    mu &lt;- b0 + b1 * d$weight\n    N &lt;- nrow(d)\n    ll &lt;- -1 * N * log(sqrt(2 * pi)) - N * log(sigma) - 0.5 * sum((d$height - mu)^2/sigma^2)\n    nll &lt;- -1 * ll\n    return(nll)\n}\n\nmle_norm &lt;- mle2(minuslogl = simple_nll, start = list(b0 = 10, b1 = 0, sigma = 1),\n    method = \"Nelder-Mead\")\nmle_norm\n\n## \n## Call:\n## mle2(minuslogl = simple_nll, start = list(b0 = 10, b1 = 0, sigma = 1), \n##     method = \"Nelder-Mead\")\n## \n## Coefficients:\n##         b0         b1      sigma \n## 39.5429077  0.1951592  2.3746502 \n## \n## Log-likelihood: -2288.65\n\nsummary(mle_norm)\n\n## Maximum likelihood estimation\n## \n## Call:\n## mle2(minuslogl = simple_nll, start = list(b0 = 10, b1 = 0, sigma = 1), \n##     method = \"Nelder-Mead\")\n## \n## Coefficients:\n##        Estimate Std. Error z value     Pr(z)    \n## b0    39.542908   0.592347  66.756 &lt; 2.2e-16 ***\n## b1     0.195159   0.004083  47.798 &lt; 2.2e-16 ***\n## sigma  2.374650   0.052716  45.046 &lt; 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## -2 log L: 4577.301\n\n# Or compare to...  Likelihood based linear regression with `maxLik()`\nloglik &lt;- function(parameters) {\n    b0 &lt;- parameters[1]\n    b1 &lt;- parameters[2]\n    sigma &lt;- parameters[3]\n    N &lt;- nrow(d)\n    # here, we say how mu is calculated based on parameters\n    mu &lt;- b0 + b1 * d$weight  # estimating mean height as a function of weight\n    # then use this mu in a similar fashion as previously\n    ll &lt;- -1 * N * log(sqrt(2 * pi)) - N * log(sigma) - 0.5 * sum((d$height - mu)^2/sigma^2)\n    return(ll)\n}\n\nmle_norm &lt;- maxLik(loglik, start = c(beta0 = 0, beta1 = 0, sigma = 1), method = \"NM\")\nsummary(mle_norm)\n\n## --------------------------------------------\n## Maximum Likelihood estimation\n## Nelder-Mead maximization, 184 iterations\n## Return code 0: successful convergence \n## Log-Likelihood: -2288.645 \n## 3  free parameters\n## Estimates:\n##        Estimate Std. error t value Pr(&gt; t)    \n## beta0 39.680322   0.950924   41.73  &lt;2e-16 ***\n## beta1  0.194221   0.006522   29.78  &lt;2e-16 ***\n## sigma  2.387501   0.053872   44.32  &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## --------------------------------------------",
    "crumbs": [
      "Part III - Miscellany",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Maximum Likelihood Estimation Example</span>"
    ]
  },
  {
    "objectID": "exercise-01.html",
    "href": "exercise-01.html",
    "title": "Exercise 01",
    "section": "",
    "text": "Send Emails Programmatically",
    "crumbs": [
      "Exercises",
      "Exercise 01"
    ]
  },
  {
    "objectID": "exercise-01.html#learning-objectives",
    "href": "exercise-01.html#learning-objectives",
    "title": "Exercise 01",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nInstalling and loading/attaching packages\nIntroducing functions and arguments\nIntroducing the pipe (%&gt;% or |&gt;) operator",
    "crumbs": [
      "Exercises",
      "Exercise 01"
    ]
  },
  {
    "objectID": "exercise-01.html#preliminary-steps",
    "href": "exercise-01.html#preliminary-steps",
    "title": "Exercise 01",
    "section": "Preliminary Steps",
    "text": "Preliminary Steps\n\nNOTE: The most challenging thing about the exercise below is to get the authentication correct for accessing an SMTP server to send a message. It is often easy to get the R code to work correctly to contact a servers, but then to have the server not be able to send out an email.\n\nThe example below should allow you to send emails via R using credentials associated with a personal or university Google Gmail account.\nIf you have a Google Gmail account (either a personal one, e.g., &lt;username&gt;@gmail.com, or one associated with UT, e.g., a &lt;username&gt;@utexas.edu account), you should follow the steps below to create an “app password” that you can use for authentication.\n\nNOTE: For this process to work smoothly, you should also have “2-Step Verification” established for the account.\n\n\nLog into your Google account or your UT Gmail account on a web browser, click on your user icon and choose “Manage your Google Account”. For example…\n\n\n\n\n\n\n\n\n\n\n… OR …\n\n\n\n\n\n\n\n\n\n\nSelect “Security”…\n\n\n\n\n\n\n\n\n\n\n… and in the window that opens scroll down to the section on “2-Step Verification” and click the arrow at the right.\n\n\n\n\n\n\n\n\n\nAt this point, you may need to provide information to verify your identity, e.g., by entering a password and/or entering a code you receive from Google. Once you have verified your identity, in the “Security” section, scroll down to the bottom of the page to the section on “App passwords” and click the “App passwords” button.\n\n\n\n\n\n\n\n\n\n\nCreate a “new app specific password” by typing a name for the password in the grey field and then click “Create”. The name does not matter, but here I have called it “R”.\n\n\n\n\n\n\n\n\n\n\nGoogle will now create new 16-digit password that you can use in lieu of your actual Google password when sending emails via Gmail in the {emayili} and {blastula} functions below. Copy and save this code in a safe place!\n\n\n\n\n\n\n\n\n\n\nNOTE: The code will contain spaces between each set of 4 letters… regardless of whether you cut out or include those white spaces, your app password should function just the same.\n\n\nFor ease, then set up a variable to hold your app password…\n\n\npwd_gmail &lt;- \"\"  # enter your app password between the quotation marks",
    "crumbs": [
      "Exercises",
      "Exercise 01"
    ]
  },
  {
    "objectID": "exercise-01.html#sending-an-email-version-1-using-the-emayaili-package",
    "href": "exercise-01.html#sending-an-email-version-1-using-the-emayaili-package",
    "title": "Exercise 01",
    "section": "Sending an Email Version 1: Using the {emayaili} package",
    "text": "Sending an Email Version 1: Using the {emayaili} package\nThe {emayili} package makes sending simple text emails very easy.\n\nStep 1\n\nInstall the {emayili} and {tidyverse} packages.\n\n\nNote that {tidyverse} is only needed if we are going to use the %&gt;% operator (see below).\n\nWe can do this at the R console prompt…\n\ninstall.packages(\"emayili\")\ninstall.packages(\"tidyverse\")\n\n# or to install both packages together...\n\ninstall.packages(c(\"emayili\", \"tidyverse\"))\n\nWe can also do this by using the package manager in RStudio.\n\n\n\n\n\n\n\n\n\n\n\nStep 2\n\nLoad and attach these packages to the search path so you can call functions they contain.\n\n\nlibrary(emayili)\nlibrary(tidyverse)\n\n\nNOTE: We can also use require() in lieu of library().\n\n\n\nStep 3\n\nCreate a new email “message” with the envelope() function from the {emayili} package.\n\nThis function takes several intuitive arguments (from, to, subject, and text) that we can assign values to directly within the function using the = operator.\n\nNOTE: from= should typically be the email address you are sending the from, although many SMTP transactions will accept alternatives, such as just a name. You can also use a vector of email addresses for the to= argument to send a message to more than one recipient. The from and to arguments are required… subject and text are optional.\n\n\nmessage_gmail &lt;- envelope(from = \"anthony.difiore@utexas.edu\", to = \"anthony.difiore@gmail.com\",\n    subject = \"Sending a message using {emayili}\", text = \"Hello! This is a plain text message sent to my personal Gmail account from my UT Gmail account.\")\n\n\nNOTE: R essentially ignores all the whitespace (spaces and tab characters) and also allows you to have code continue from one line to the next. It is even pretty forgiving about where you put carriage returns, although it is good form to place them following a comma or before/after a parenthesis or brace.\n\nUsing {tidyverse} syntax, we can set up the same email as follows using the “pipe” operator (%&gt;%):\n\nmessage_gmail &lt;- envelope() %&gt;%\n    from(\"anthony.difiore@utexas.edu\") %&gt;%\n    to(\"anthony.difiore@gmail.com\") %&gt;%\n    subject(\"Sending a message using {emayili}\") %&gt;%\n    text(\"Hello! This is a plain text message sent to my personal Gmail account from my UT Gmail account.\")\n\nAs of R version 4.1, we can alternatively use R’s “native” pipe operator, |&gt;:\n\nmessage_gmail &lt;- envelope() |&gt;\n    from(\"anthony.difiore@utexas.edu\") |&gt;\n    to(\"anthony.difiore@gmail.com\") |&gt;\n    subject(\"Sending a message using {emayili}\") |&gt;\n    text(\"Hello! This is a plain text message sent to my personal Gmail account from my UT Gmail account.\")\n\nThe difference here is that we are first creating an empty “message” object and then “piping” that object using %&gt;% or |&gt; into different helper functions (from(), to(), etc.) to create the details of the message.\n\n\nStep 4\n\nCreate an SMTP (or “Simple Mail Transfer Protocol”) “server” object that includes details about how to send a message, i.e., by specifying the email service’s outgoing server host name, a communications port number to use, and user and password information for authenticating use of the server.\nSMTP transmission can often use any of several standard port numbers (25, 465, 587, and 2525), but 587 and 465 are the most commonly used and support TLS encryption. To use Google’s outgoing email server, set the host to “smtp.gmail.com” and the port to either 465 or 587.\n\n\nNOTE: In the function below, max_times= sets the number of attempts the function will make to send the message. The default (if the argument is omitted) is 5, but I have set it to 1 to exit quickly if the server is not contacted. For username= enter your login ID for the service you are sending from, and for password= either use the variable you set up above to hold the “app password” for your Google account, or enter your password directly here, in quotation marks it, in lieu of the variable gmail_pwd\n\n\nsmtp_gmail &lt;- server(host = \"smtp.gmail.com\", port = 465, max_times = 1, username = \"anthony.difiore@utexas.edu\",\n    password = pwd_gmail)\n\nAlternatively, you can use the gmail() function to create a Gmail server object more easily…\n\nsmtp_gmail &lt;- gmail(username = \"anthony.difiore@utexas.edu\", password = pwd_gmail)\n\n\n\nStep 5\n\nSend your message by passing it as an argument to the server object. To confirm that things are working, send a message to yourself and then CHECK YOUR EMAIL to confirm that you receive the message. A copy of the message should appear in the Sent folder of your email client!\n\n\n# send using Google's Gmail server...\nsmtp_gmail(message_gmail, verbose = TRUE)\n\n\n\nNext Steps?\nUse the RStudio Help tab to browse the documentation associated with the {emayali} package to see how you can customize your message, e.g., by adding cc or bcc arguments, by using a different “reply to” address, by adding attachments, or by encrypting your message.",
    "crumbs": [
      "Exercises",
      "Exercise 01"
    ]
  },
  {
    "objectID": "exercise-01.html#sending-an-email-version-2-using-the-mailr-package",
    "href": "exercise-01.html#sending-an-email-version-2-using-the-mailr-package",
    "title": "Exercise 01",
    "section": "Sending an Email Version 2: Using the {mailR} package",
    "text": "Sending an Email Version 2: Using the {mailR} package\nThe {mailR} package also allows you to easily send simple emails with a single function, send.mail(). Here, we need to specify details about the SMTP server more explicitly than if we use {emayili} (above) or {blastula} (below).\n\nStep 1\n\nInstall the {mailR} package.\n\n\ninstall.packages(\"mailR\")\n\n\n\nStep 2\n\nLoad the {mailR} package.\n\n\nlibrary(mailR)\n\n\n\nStep 3\nCreate and send the email\n\nUsing Google’s Gmail server…\n\nTo use the Gmail server with ports other than 587 (e.g., 465) the argument ssl= must be set to TRUE, which initiates a secure connection and allows required encryption\nIf port 587 is used, either the argument ssl= or tls= should be set to TRUE\n\n\n\nsend.mail(from = \"anthony.difiore@utexas.edu\", to = \"anthony.difiore@gmail.com\",\n    subject = \"Sending a message using {mailR}\", body = \"Hello! This is a plain text message sent from a Gmail account.\",\n    smtp = list(host.name = \"smtp.gmail.com\", port = 587, user.name = \"anthony.difiore@utexas.edu\",\n        passwd = pwd_gmail, ssl = TRUE), authenticate = TRUE, send = TRUE)",
    "crumbs": [
      "Exercises",
      "Exercise 01"
    ]
  },
  {
    "objectID": "exercise-01.html#sending-an-email-version-3-using-the-blastula-package",
    "href": "exercise-01.html#sending-an-email-version-3-using-the-blastula-package",
    "title": "Exercise 01",
    "section": "Sending an Email Version 3: Using the {blastula} package",
    "text": "Sending an Email Version 3: Using the {blastula} package\nThe {blastula} package allows us to create and send more complex HTML and formatted emails using markdown. Using it is a bit more complicated (but more flexible) than using the procedures above.\nSimilar to {emayili}, {blastula} provides two main functions: [1] compose_email() for constructing various parts of a message and [2] smtp_send() for specifying email server settings and passing the message to the server to send. However, {blastula} also adds in some helper functions that allow you to store authentication information in either a separate “credentials” text file that is referenced when you run the function to send a message or in your computer’s keychain. {blastula} also stores some default information on commonly-used email providers and services (e.g., on Google’s Gmail server), which may make configuring the server setup easier than using {emayili} or {mailR}.\n\nStep 1\n\nInstall the {blastula} package.\n\n\ninstall.packages(\"blastula\")\n\n\n\nStep 2\n\nLoad the {blastula} package.\n\n\nlibrary(blastula)\n\n\n\nStep 3\n\nCreate a new email “message” with the compose_email() function.\n\nThis function takes several intuitive arguments that we can assign values to directly within the function.\n\n# Compose the message\nmessage &lt;- compose_email(body = \"Hello! This is a simple HTML message.\")\n\n# Preview the message.  This will open the HTML message in a browser window or\n# in the RStudio Viewer tab\nmessage\n\nWe can add some formatting to our email by using the md() function and markdown syntax. Here, we pipe (%&gt;% or |&gt;) the body argument to the md() function to convert it to markdown.\n\n# Compose the message\nmessage &lt;- compose_email(body = \"# Hello!\\nThis is a simple **HTML** message with some *markdown* syntax.\" |&gt;\n    md())\n\n# Preview the message\nmessage\n\nWe can also spice up our email with an image from a local file. Here, I use an image stored in a folder called “img” inside the my current working directory.\n\n# Create text as html\ntext &lt;- \"# Hello!\\nThis is a simple **HTML** message with some *markdown* syntax... and a cool picture!\"\n\n# Create image as html\nimage &lt;- add_image(file = \"img/batnet.jpg\", width = 300, align = \"center\")\n\n# Compose the message\nmessage &lt;- compose_email(body = c(text, image) |&gt;\n    md())\n\n# Preview the message\nmessage\n\n\n\nStep 4\n\nCreate a credentials file.\n\nThe following code block will create a text file (in JSON format) in the current working directory that contains default information for your outgoing mail server, e.g., Google’s Gmail server (host = \"smtp.gmail.com\"), along with your email address and your password. When you run the following lines of code, you will be asked to enter your password, which will then be stored in the credentials file. For a Gmail account, you can use the app password you created above.\n\nNOTE: Be aware that if you create a credentials file like this, your password will be stored, unencrypted, in the file you create!\n\n\n# credentials file for a Gmail account\ncreate_smtp_creds_file(file = \"my_gmail_creds.txt\", user = \"anthony.difiore@utexas.edu\",\n    provider = \"gmail\")\n\n\nNOTE: If you omit the provider= argument, you should then pass the host=, port=, and use_ssl= arguments yourself to the create_smtp_creds_file() function. The use_ssl= argument allows the use of STARTTLS, which initiates secured (encrypted) TLS or SSL connection, which many email servers require.\n\n\n\nStep 5\n\nSend the message via STMP using a credentials file.\n\n\n# send using Google's servers...\nsmtp_send(email = message, from = \"anthony.difiore@utexas.edu\", to = \"anthony.difiore@gmail.com\",\n    subject = \"Sending a message using {blastula} and a credentials file\", credentials = creds_file(file = \"my_gmail_creds.txt\"))\n\n\n\nStep 6\n\nWe could also send the message by specifying our credentials manually within the smtp_send() function.\n\nThe following will prompt us for our password to send the message:\n\n# send using Google's servers...\nmessage |&gt;\n    smtp_send(from = \"anthony.difiore@utexas.edu\", to = \"anthony.difiore@gmail.com\",\n        subject = \"Sending a message using {blastula} and entering credentials manually\",\n        credentials = creds(user = \"anthony.difiore@utexas.edu\", provider = \"gmail\"))",
    "crumbs": [
      "Exercises",
      "Exercise 01"
    ]
  },
  {
    "objectID": "exercise-01.html#sending-an-email-version-4-using-the-sendmailr-package",
    "href": "exercise-01.html#sending-an-email-version-4-using-the-sendmailr-package",
    "title": "Exercise 01",
    "section": "Sending an Email Version 4: Using the {sendmailR} package",
    "text": "Sending an Email Version 4: Using the {sendmailR} package\nThe {sendmailR} is yet another that you can use to send emails from R.\n\ninstall.packages(\"sendmailR\")\nlibrary(sendmailR)\nsendmail(from = \"anthony.difiore@utexas.edu\", to = \"anthony.difiore@gmail.com\", subject = \"Sending a message using {sendmailR}\",\n    msg = \"Hello! This is a plain text message sent to my personal Gmail account from my UT Gmail account.\",\n    engine = \"curl\", engineopts = list(username = \"anthony.difiore@utexas.edu\", password = pwd_gmail),\n    control = list(smtpServer = \"smtp://smtp.gmail.com\", smtpPortSMTP = 587))\n# can also include the port as part of the smtpServer argument...  e.g.,\n# `control = list(smtpServer='smtp://smtp.gmail.com:587')`\n\n\nNext Steps?\nAgain, use the RStudio Help tab to browse the documentation associated with the {blastula} package to see how you can customize your message, e.g., with attachments.\n\n\nEven More Next Steps?\nFinally, it is a bit more complicated, but note that we can also use the {keyring} package along with {blastula} to set up a credentials “key” in our computer’s keychain and refer to that to specify our credentials for sending a message. When you create the keys, you will be asked to provide your password.\n\ninstall.packages(\"keyring\")\n\n\nlibrary(keyring)\n\n\n# create a Gmail key with a user-specified name, stored with the `id=` argument\ncreate_smtp_creds_key(id = \"my_gmail_key\", user = \"anthony.difiore@utexas.edu\", provider = \"gmail\",\n    overwrite = TRUE  # this argument is only needed if you have an existing key\n)\n\n\n# View all keys\nview_credential_keys()\n\n\n# send a message with credentials from a keychain using Google's servers...\nmessage |&gt;\n    smtp_send(from = \"anthony.difiore@utexas.edu\", to = \"anthony.difiore@gmail.com\",\n        subject = \"Sending a message using {blastula} and credentials from a keychain\",\n        credentials = creds_key(id = \"my_gmail_key\"))",
    "crumbs": [
      "Exercises",
      "Exercise 01"
    ]
  },
  {
    "objectID": "exercise-01.html#clean-up-steps",
    "href": "exercise-01.html#clean-up-steps",
    "title": "Exercise 01",
    "section": "Clean-Up Steps",
    "text": "Clean-Up Steps\n\n# delete all keys\ndelete_all_credential_keys()",
    "crumbs": [
      "Exercises",
      "Exercise 01"
    ]
  },
  {
    "objectID": "exercise-02.html",
    "href": "exercise-02.html",
    "title": "Exercise 02",
    "section": "",
    "text": "Practice Reproducible Research Workflows",
    "crumbs": [
      "Exercises",
      "Exercise 02"
    ]
  },
  {
    "objectID": "exercise-02.html#learning-objectives",
    "href": "exercise-02.html#learning-objectives",
    "title": "Exercise 02",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nIntegrating an RStudio project, a local git repository, and a GitHub remote repository",
    "crumbs": [
      "Exercises",
      "Exercise 02"
    ]
  },
  {
    "objectID": "exercise-02.html#task-1-begin-with-a-remote-repo",
    "href": "exercise-02.html#task-1-begin-with-a-remote-repo",
    "title": "Exercise 02",
    "section": "Task 1: Begin with a remote repo…",
    "text": "Task 1: Begin with a remote repo…\nFollow the instructions outlined as Method 1 in Module 6 to do the following:\n\nStep 1\n\nSet up a new GitHub repo in your GitHub workspace named “exercise-02a”.\n\n\n\n\n\n\n\n\n\n\n\n\nStep 2\n\nAdd me as a collaborator on the repo by going to “Settings”, choosing “Collaborators”, then “Add People”, and searching for my GitHub username (“difiore”).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 3\n\nClone your repo to your local computer.\n\n\n\nStep 4\n\nCreate an new RMarkdown or Quarto document locally with notes you take in class this week - maybe call it “notes.qmd” or “notes.Rmd”? - including at least some expository text written in Markdown and at least one R code block. If you make a “.qmd” document, I recommend unchecking “Use visual markdown editor”, though that is your choice.\n\n\n\n\n\n\n\n\n\n\n… OR …\n\n\n\n\n\n\n\n\n\n\n\nStep 5\n\nUse Render (for “.qmd”) or Knit (for “.Rmd”) to publish your document as HTML.\n\n\n\n\n\n\n\n\n\n\n… OR …\n\n\n\n\n\n\n\n\n\n\n\nStep 6\n\nStage (or “add”) and “commit” the changes to your repo locally and then “push” those up to GitHub.\n\n\n\nStep 7\n\nVisit your repo on GitHub to confirm that you have done the above steps successfully.\n\n\n\nStep 8\n\nSubmit the URL for your repo in Canvas",
    "crumbs": [
      "Exercises",
      "Exercise 02"
    ]
  },
  {
    "objectID": "exercise-02.html#task-2-begin-with-a-local-repo",
    "href": "exercise-02.html#task-2-begin-with-a-local-repo",
    "title": "Exercise 02",
    "section": "Task 2: Begin with a local repo…",
    "text": "Task 2: Begin with a local repo…\nThis is an alternative workflow to that described above, which also works well. It is outlined in more detail in Module 6 as “Option 3” under Connecting a Local Repo to GitHub.\n\nStep 1\n\nFrom within RStudio, create a new project called “exercise-2b” in a brand new directory also called “exercise-2b”.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNOTE Be sure to check the box marked “Create a git repository” pressing the “Create Project” button.\n\n\n\nStep 2\n\nConfigure a GitHub Personal Access Token (PAT) as described in Module 5 and Module 6, making sure that R and RStudio can access your cached credentials.\n\n\n\nStep 3\n\nAs outlined in Module 6 Method 3 Option 1, open your project locally and run usethis::use_github(protocol=\"https\") from the R console within your project’s working directory.\n\n\n\nStep 4\n\nAnswer the question about whether the suggested name for the remote repository is adequate and hit .\n\nThis will create a new remote repository on GitHub, add it as a remote origin/main, and open the GitHub page for the repository in your browser. Switch back to RStudio and continue with Steps 4 to 8 from above.",
    "crumbs": [
      "Exercises",
      "Exercise 02"
    ]
  },
  {
    "objectID": "exercise-03.html",
    "href": "exercise-03.html",
    "title": "Exercise 03",
    "section": "",
    "text": "Explore and Wrangle Data",
    "crumbs": [
      "Exercises",
      "Exercise 03"
    ]
  },
  {
    "objectID": "exercise-03.html#learning-objectives",
    "href": "exercise-03.html#learning-objectives",
    "title": "Exercise 03",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nIntegrating an RStudio project, a local git repository, and a GitHub remote repository\nPerforming basic exploratory data analyses and visualization\nUsing {dplyr} and the pipe operator (|&gt; or %&gt;%) for data wrangling\n\n\nPreliminaries\n\nSet up a new GitHub repo in your GitHub workspace called “exercise-03” and clone that down your computer as a new RStudio project. The instructions outlined as Method 1 in Module 6 will be helpful.\nLoad the “data-wrangling.csv” dataset from this URL as a tabular data structure named d and look at the variables it contains, which are a subset of those in the Kamilar and Cooper dataset on primate ecology, behavior, and life history that we have used previously.\n\n\nHINT: You can use the names() function to see a list of variable names in the dataset\n\n\n\nStep 1\nCreate a new Quarto document called “EDA-challenge.qmd” within the project and use it to complete the following challenges. Be sure to include both descriptive text about what you are doing and code blocks - i.e., follow a “literate programming” approach to be sure that a reader understands what you are doing. For the plots, feel free to use either {base} R graphics or {ggplot2}, as you prefer.\n\nCreate a new variable named BSD (body size dimorphism) which is the ratio of average male to female body mass.\nCreate a new variable named sex_ratio, which is the ratio of the number of adult females to adult males in a typical group.\nCreate a new variable named DI (for “defensibility index”), which is the ratio of day range length to the diameter of the home range.\n\n\nHINT: You will need to calculate the latter for each species from the size of its home range! For the purposes of this assignment, presume that the home range is a circle and use the formula for the area of a circle: \\(AREA = \\pi \\times r^2\\). The variable pi (note, without parentheses!) will return the value of \\(\\pi\\) as a constant built-in to R, and the function sqrt() can be used to calculate the square root (\\(\\sqrt{}\\))\n\n\nPlot the relationship between day range length (y axis) and time spent moving (x axis), for these primate species overall and by family (i.e., a different plot for each family, e.g., by using faceting: + facet_wrap()). Do species that spend more time moving travel farther overall? How about within any particular primate family? Should you transform either of these variables?\nPlot the relationship between day range length (y axis) and group size (x axis), overall and by family. Do species that live in larger groups travel farther overall? How about within any particular primate family? Should you transform either of these variables?\nPlot the relationship between canine size dimorphism (y axis) and body size dimorphism (x axis) overall and by family. Do taxa with greater size dimorphism also show greater canine dimorphism?\nCreate a new variable named diet_strategy that is “frugivore” if fruits make up &gt;50% of the diet, “folivore” if leaves make up &gt;50% of the diet, and “omnivore” if diet data are available, but neither of these is true (i.e., these values are not NA). Then, do boxplots of group size for species with different dietary strategies, omitting the category NA from your plot. Do frugivores live in larger groups than folivores?\n\n\nHINT: To create this new variable, try combining mutate() with ifelse() or case_when() statements… check out the notes on “Conditional Expressions” in Module 11. Take a peak at the code snippet below if you get stuck!\n\n\n\nCode\nmutate(d, diet_strategy = ifelse(Fruit &gt;= 50, \"frugivore\", ifelse(Leaves &gt;= 50, \"folivore\",\n    ifelse(Fruit &lt; 50 & Leaves &lt; 50, \"omnivore\", NA))))\n\nmutate(d, diet_strategy = case_when(Fruit &gt;= 50 ~ \"frugivore\", Leaves &gt;= 50 ~ \"folivore\",\n    Fruit &lt; 50 & Leaves &lt; 50 ~ \"omnivore\", TRUE ~ NA))\n\n\n\nIn one line of code, using {dplyr} verbs and the forward pipe (|&gt; or %&gt;%) operator, do the following:\n\n\nAdd a variable, Binomial to the data frame d, which is a concatenation of the Genus and Species variables…\nTrim the data frame to only include the variables Binomial, Family, Brain_size_species_mean, and Body_mass_male_mean…\nGroup these variables by Family…\nCalculate the average value for Brain_Size_Species_Mean and Body_mass_male_mean per Family (remember, you may need to specify na.rm = TRUE)…\nArrange by increasing average brain size…\nAnd print the output to the console\n\n\n\nStep 2\n\nRender your work to HTML or (if you feel ambitious!) as a PDF. 😃\n\n\n\nStep 3\n\nCommit your changes locally and push your repo, including your “.qmd” file and knitted results, up to GitHub.\n\n\n\nStep 4\n\nSubmit the URL for your repo in Canvas\n\n\n\nOptional Next Steps?\nLoad in a dataset of your own and write R code to do the following:\n\nCreate and print a vector of the variable names in your dataset\nCount the total number of rows (observations) in your dataset\nFor each numeric variable, calculate the number of observations, the mean and standard deviation, and the five-number summary\nMake box-and-whiskers plots for each numerical variable\nFor each categorical variable, generate a table of counts/proportions of alternative variable values",
    "crumbs": [
      "Exercises",
      "Exercise 03"
    ]
  },
  {
    "objectID": "exercise-04.html",
    "href": "exercise-04.html",
    "title": "Exercise 04",
    "section": "",
    "text": "Program a Word Game",
    "crumbs": [
      "Exercises",
      "Exercise 04"
    ]
  },
  {
    "objectID": "exercise-04.html#learning-objectives",
    "href": "exercise-04.html#learning-objectives",
    "title": "Exercise 04",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nLoading data files into R\nBreaking a programming challenge down into discrete steps\nWriting novel functions\nUsing arguments in functions\nUsing set operation functions\nUsing loops and conditional statements\nWorking with different data structures (vectors, tabular data)\nPracticing data wrangling\nDealing with keyboard input",
    "crumbs": [
      "Exercises",
      "Exercise 04"
    ]
  },
  {
    "objectID": "exercise-04.html#wordle-puzzle-challenge",
    "href": "exercise-04.html#wordle-puzzle-challenge",
    "title": "Exercise 04",
    "section": "Wordle Puzzle Challenge",
    "text": "Wordle Puzzle Challenge\nThe rules of Wordle are simple: A player has SIX tries to guess a 5-letter word that has been selected at random from a list of possible words. Guesses need to be valid 5-letter words. After each guess, a player receives feedback about how close their guess was to the word, which provides information they can use to try to solve the puzzle. This feedback lets you know whether each letter your guess is either [1] in the solution word and in the correct spot, [2] in the solution word but in the wrong position, or [3] not in the solution word. In app/browser versions of Wordle, this feedback is provided visually using colors, but that need not be the case.\nThis week’s programming exercise is to work with a partner to get as far along as you can writing R code that allows you to play Wordle to your heart’s content using R/RStudio!\nThe assignment and steps below were inspired by this fun blog post… but before you reference it or other online sites, try to tackle this coding exercise on your own.\n\nPreliminaries\n\nSet up a new GitHub repo in you or your partner’s GitHub workspace named “exercise-04” and clone that down to both of your computers as a new RStudio project. The instructions outlined as Method 1 in Module 6 will be helpful. Make sure that both you and your partner are collaborators on the repo and that you add me as a collaborator as well (my GitHub username is “difiore”).\nGo to https://github.com/difiore/ada-datasets, download the following two data files, and add them to your repo:\n\ncollins-scrabble-words-2019.txt\ngoogle-10000-english-usa-no-swears.txt\n\n\nThe first of these files (279,497 lines long) contains a list of “Official Scrabble Words” in the English language based on the Collins English Dictionary published by HarperCollins. The first line in the file is the word “words” and is used as a header.\nThe second of these files (9885 lines long) contains a list of ~10,000 of the most common words in the English language, based on data compiled by Google, and omitting common swear words. Again, the first line is the word “words” and is used as a header.\n\nCreate a new Quarto or RMarkdown document, called “wordle.qmd” or “wordle.Rmd”. In this, you do your best to recreate all of the wordplay used in Wordle.\n\n\n\nIntroduction\nBefore we begin programming, an important first step is to break down the problem we are trying to down into discrete pieces… What do we need to do to set up a Wordle game? What steps does game play need to follow? What has to be evaluated at each step? How does the game end?\nAs an early step, for example, we need to choose a mystery “solution” word that players will try to guess. We will use the list of the most common words in the English language to do this as a possible source of solution words for the puzzle.\nAdditionally, also need to establish a dictionary of “valid” words that players can guess, and for that we will use the list of Official Scrabble Words.\n\nNOTE: The list of possible “solution” words used in the original Wordle puzzle consists of ~2100 5-letter words, while the list of “valid” words that can be used as guesses totals ~13,000. How many 5-letter words are in each of the two data files you have downloaded?\n\n\n\nStep 1\n\nCreate your own custom function called load_dictionary() that takes a single argument, “filename”, that can be used to read in either of the two data files your downloaded.\nOnce you have created your function, use that function to create two variables, solution_list and valid_list, that, respectively contain vectors of possible solution words and valid words to guess. That is, you should be able to run the following to create these two vectors:\n\n\nvalid_list &lt;- load_dictionary(&lt;filename here&gt;)\nsolution_list &lt;- load_dictionary(&lt;filename here&gt;)\n\nRunning str(valid_list) should return…\n\nchr [1:279496] \"AA\" \"AAH\" \"AAHED\" \"AAHING\"...\n\nRunning str(solution_list) should return…\n\nchr [1:8336] \"THE\" \"OF\" \"AND\" \"TO\"...\n\n\n\nStep 2\n\nWinnow your variable solution_list to only include words that are included in valid_list. There are multiple ways that you could do this, but the set operation function, intersection() is an easy way. Use R help to look at the documentation for the intersection() function to see if you can get that to work. How many words are in your updated solution_list vector?\n\n\n\nStep 3\n\nWrite a custom function called pick_solution() that [1] removes all words from solution_list that are not 5 letters in length, [2] then randomly chooses a single word from those that remain, and [3] then splits that word into a vector of single-character elements. You should be able to pass your solution_list vector as the argument to the function.\n\n\nHINT: For [1], you will want to subset the solution_list vector according to some criterion of word length. For [2], you may find the sample() function useful (use R help to look up documentation on that function). For [3], you may want to look at the functions strsplit() from {base} R or str_split() from the {stringr} package (part of {tidyverse}). Pay attention to the data structures that those functions return, because you will need to carefully reference one of the elements that is returned in order to wind up with a vector!\n\nAs a bonus, you might include a second argument for your pick_solution() function called “word_length” that makes your function flexible enough to select a solution word that is something other than 5 characters long.\nOnce your function works, run it and assign the result to a variable called solution.\n\nsolution &lt;- pick_solution(solution_list)\n\n\n\nStep 4\nNow, to tackle the bulk of the problem, create two more functions. The first should be called play_wordle() and it should take three arguments: [1] the answer to the puzzle (the value of your solution variable), [2] a list of valid guesses (the contents of your valid_list variable), and [3] a value for “number of guesses”, which you should set to the original Wordle game default of 6.\n\nHINT: Here is possible skeleton code for that function.\n\n\nplay_wordle &lt;- function(solution, valid_list, num_guesses=6){\n      &lt;function code here&gt;\n    }\n\nThink carefully about what the play_wordle() function needs to do. It should:\n\nAt the onset, tell the player the rules of the game, e.g., “You have … chances to guess a word of length …”\nDisplay what letters the player has not yet guessed (at the onset, this would be all 26 letters of the alphabet), e.g., “Letters left: …”\n\n\nHINT: There is a built-in dataset in R called LETTERS, which contains the 26 capital letters in the English alphabet and another, letters, that contains all the lowercase letters. You will probably want to use either the toupper() or tolower() functions to ensure that you are always working with the same formatted letters in words and guesses.\n\n\n# using the LETTERS built-in vector\nLETTERS\n\n##  [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\" \"K\" \"L\" \"M\" \"N\" \"O\" \"P\" \"Q\" \"R\" \"S\"\n## [20] \"T\" \"U\" \"V\" \"W\" \"X\" \"Y\" \"Z\"\n\n# using the `toupper()` function\ntoupper(\"change my case\")\n\n## [1] \"CHANGE MY CASE\"\n\n\n\nPrompt the player for a guess, e.g., “Enter guess number …”, read in their guess, and check that their guess is valid (i.e., that it contains the correct number of letters and is a word included in the “valid” word list).\n\n\nHINT: The readline() function, which can take a character string as an argument, will provide a “prompt” entering a line of numeric or character data. Hitting &lt;enter&gt; or &lt;return&gt; signals the end of the line.\n\n\nguess &lt;- readline(\"Enter some data here, then press &lt;enter&gt;: \")\n\n\nCompare the guess to the solution word and generate the necessary feedback, e.g., * for in the word and in the correct position, + for in the word but in the wrong position, and - for not in the word. For this step, try writing a separate “helper” function, evaluate_guess(), called from within play_wordle(). This function should take, as arguments, the player’s guess and the value of the solution variable. This is probably the trickiest part of the problem to program, and there are lots of approaches you might take to evaluating guesses. After you work on this for a while, I can share one solution.\nUpdate the list of letters not yet guessed.\n\n\nHINT: Again, consider using set operations to update the list of letters not yet guessed. setdiff() is a function that returns the difference between two vectors.\n\n\nCheck if the puzzle was solved. If so, the function should indicate that the player WON the game and print out their guess and feedback history. If not, the function should prompt the player for another guess, unless they have already hit the maximum number of guesses allowed.\nIf all guesses are exhausted, the function should indicate that the player LOST the game and, again, print out their guess and feedback history.\n\n\n\nOptional Next Steps?\n\nTry modifying your code to mimic the “hard mode” in Wordle, where information about the letters in the solution and their positions revealed in prior guesses has to be used in subsequent guesses.\nTry spicing up the feedback given using colors or alternative formatting. One way to do this would be to use the {huxtable} package, which is a package for creating text tables that can be styled for display in the R console and can also output to HTML, PDF, and a variety of other formats.\nHave R keep track of the date and not let you play more than one Wordle game per day.\nHave R keep track of your performance across multiple games of Wordle.\nAllow R to post your Wordle results to a social media platform of your choosing. For this, check out, e.g., the {Rfacebook} or {rtweet} packages.\nConvert your code to an interactive {shiny} app to have it run in a web brower. Later modules will introduce you to programming with {shiny}.",
    "crumbs": [
      "Exercises",
      "Exercise 04"
    ]
  },
  {
    "objectID": "exercise-04-solution.html",
    "href": "exercise-04-solution.html",
    "title": "Exercise 04 Solution",
    "section": "",
    "text": "• Solution\nUse the Show Code button to peek at one solution to each of the four functions. Note that needed libraries are specified with the require() at the top of each function. You could use library() as an alternative. You could also have these loaded in the global environment, but these would not be as “portable” across users and systems. Each of these functions includes some commented out print() and/or glimpse() statements that are useful for viewing intermediate values for debugging.\n\nNOTE: The play_wordle() and evaluate_guess() functions include an additional argument, “output_type”, that lets the user specify whether they would like to see simple text output or formatted (“graphic”) output, which is made possible by using the {huxtable} library. A substantial chunk of the play_wordle() and evaluate_guess() functions are devoted to producing this graphic output and are not necessary.\n\n\nFunctions\n\nload_dictionary()\n\n\nShow Code\nload_dictionary &lt;- function(filename) {\n    require(tidyverse)\n    dictionary &lt;- read_csv(filename, col_names = TRUE)\n    dictionary &lt;- dictionary[[\"words\"]]\n    dictionary &lt;- toupper(dictionary)\n    # glimpse(dictionary) # to show the structure of 'dictionary' to confirm\n    # that it is a vector\n    return(dictionary)\n}\n\n\n\n\npick_solution()\n\n\nShow Code\npick_solution &lt;- function(dictionary, word_length = 5) {\n    require(tidyverse)\n    possible_solutions &lt;- dictionary[nchar(dictionary) == word_length]\n    solution &lt;- sample(possible_solutions, 1)\n    # print(solution)\n    solution_vector &lt;- str_split(solution, \"\")[[1]]\n    # glimpse(solution_vector) # to show the structure of the solution vector\n    return(solution_vector)\n}\n\n\n\n\nplay_wordle()\n\n\nShow Code\nplay_wordle &lt;- function(solution, valid_list, num_guesses = 6, output_type = \"text\") {\n    ## ARGUMENT 'output_type=' CAN BE CUT IF GRAPHIC OUTPUT NOT NEEDED\n    require(tidyverse)\n    require(sjmisc)\n    require(huxtable)  ## THIS LIBRARY CALL CAN BE CUT IF GRAPHIC OUTPUT NOT NEEDED\n    word_length &lt;- length(solution)\n    print(paste0(\"You have \", num_guesses, \" chances to guess a word of length \",\n        word_length))\n    letters_left &lt;- LETTERS  # a built-in set of capital letters\n    guess_history &lt;- data.frame(matrix(nrow = num_guesses, ncol = word_length))\n    result_history &lt;- data.frame(matrix(nrow = num_guesses, ncol = word_length))\n    if (output_type == \"graphic\") {\n        guess_history &lt;- as_huxtable(guess_history)\n        result_history &lt;- as_huxtable(result_history)\n    }\n    for (i in 1:num_guesses) {\n        # display 'keyboard'\n        print(paste0(c(\"Letters left: \", letters_left), collapse = \" \"))\n        # read in guess and confirm length and validity\n        guess &lt;- readline(paste0(\"Enter guess \", i, \": \")) %&gt;%\n            toupper()\n        while (nchar(guess) != word_length) {\n            guess &lt;- readline(paste0(\"Guess must have \", word_length, \" characters. Enter guess \",\n                i, \" again : \")) %&gt;%\n                toupper()\n        }\n        while (guess %nin% valid_list) {\n            guess &lt;- readline(paste0(\"Hmm, that word is not in my dictionary of valid words. Enter guess \",\n                i, \" again: \")) %&gt;%\n                toupper()\n        }\n        guess &lt;- str_split(guess, \"\")[[1]]\n        # print(guess) # check output\n\n        # evaluate guess\n        result &lt;- evaluate_guess(guess, solution, output_type)\n\n        # update keyboard\n        letters_left &lt;- setdiff(letters_left, guess)\n\n        # print results\n        guess_history[i, ] &lt;- guess\n        result_history[i, ] &lt;- result\n\n        if (output_type == \"text\")\n            {\n                ## THIS LINE CAN BE CUT IF GRAPHIC OUTPUT NOT NEEDED\n                if (all(result == \"*\")) {\n                  guess_history &lt;- guess_history %&gt;%\n                    na.omit()\n                  result_history &lt;- result_history %&gt;%\n                    na.omit()\n                  print(paste0(\"You won in \", i, \" guesses!\"))\n                  guess_history &lt;- guess_history %&gt;%\n                    unite(everything(), sep = \"\", col = \"guess\", remove = TRUE)\n                  result_history &lt;- result_history %&gt;%\n                    unite(everything(), sep = \"\", col = \"result\", remove = TRUE)\n                  history &lt;- data.frame(guess = guess_history, result = result_history)\n                  print(history)\n                  return(invisible(history))\n                } else {\n                  history &lt;- data.frame(guess = paste0(guess, collapse = \"\"), result = paste0(result,\n                    collapse = \"\"))\n                  print(history)\n                }\n            }  ## THIS LINE CAN BE CUT IF GRAPHIC OUTPUT NOT NEEDED\n\n        ## THIS WHOLE `if() {} else {}` BLOCK BELOW CAN BE CUT IF GRAPHIC\n        ## OUTPUT IS NOT NEEDED\n        if (output_type == \"graphic\") {\n            if (all(background_color(result) == \"#6BA964\")) {\n                history &lt;- result_history %&gt;%\n                  na.omit()\n                print(paste0(\"You won in \", i, \" guesses!\"))\n                print(history, colnames = FALSE)\n                return(invisible(history))\n            } else {\n                print(result, colnames = FALSE)\n            }\n        }\n        ## CLOSE OF BLOCK FOR GRAPHIC OUTPUT\n\n    }\n    print(paste0(\"Sorry, you lost! Solution was \", paste0(solution, collapse = \"\")))\n\n    if (output_type == \"text\")\n        {\n            ## ## THIS LINE CAN BE CUT IF GRAPHIC OUTPUT NOT NEEDED\n            guess_history &lt;- guess_history %&gt;%\n                unite(everything(), sep = \"\", col = \"guess\", remove = TRUE)\n            result_history &lt;- result_history %&gt;%\n                unite(everything(), sep = \"\", col = \"result\", remove = TRUE)\n            history &lt;- data.frame(guess = guess_history, result = result_history)\n            print(history)\n            return(invisible(history))\n        }  ## THIS LINE CAN BE CUT IF GRAPHIC OUTPUT NOT NEEDED\n\n    ## THE WHOLE `if()` BLOCK BELOW CAN BE CUT IF GRAPHIC OUTPUT IS NOT NEEDED\n    if (output_type == \"graphic\") {\n        history &lt;- result_history\n        print(history, colnames = FALSE)\n        return(invisible(history))\n    }\n    ## CLOSE OF BLOCK FOR GRAPHIC OUTPUT\n\n    return()\n}\n\n\n\n\nevaluate_guess()\n\n\nShow Code\nevaluate_guess &lt;- function(guess, solution, output_type) {\n    word_length &lt;- length(solution)\n    text_result &lt;- rep(\"-\", word_length)\n    # the next lines are an ugly hack to deal with repeat letters...  we first\n    # find the number of times letters in the guess appear in the solution\n    # because we will need to clear 'extra' ones away\n    guess_count &lt;- tibble(letter = guess) %&gt;%\n        group_by(letter) %&gt;%\n        summarize(n_in_guess = n())\n    solution_count &lt;- tibble(letter = solution) %&gt;%\n        group_by(letter) %&gt;%\n        summarize(n_in_solution = n())\n    counts &lt;- inner_join(guess_count, solution_count, by = \"letter\") %&gt;%\n        mutate(to_clear = n_in_guess - n_in_solution) %&gt;%\n        filter(to_clear &gt; 0) %&gt;%\n        select(letter, to_clear)\n\n    for (i in 1:word_length) {\n        # these `case_when()` lines are the workhorse of the function...  they\n        # find if each letter in the guess appears in the solution and if it in\n        # the right place\n        text_result[i] &lt;- case_when(guess[i] %in% solution & guess[i] == solution[i] ~\n            \"*\", guess[i] %in% solution & guess[i] != solution[i] ~ \"+\", guess[i] %nin%\n            solution ~ \"-\")\n\n        # this `for()` loop then cycles through cases where the guess contains\n        # more of a particular letter than the solution and clears away the\n        # correct number of matches that are in in the solution but in the\n        # wrong position\n        for (j in counts$letter) {\n            if (guess[i] == j & text_result[i] != \"*\" & counts[counts$letter == j,\n                ]$to_clear &gt; 0) {\n                text_result[i] &lt;- \"-\"\n                counts[counts$letter == j, ]$to_clear &lt;- counts[counts$letter ==\n                  j, ]$to_clear - 1\n            }\n        }\n\n    }\n\n    # format for graphic output\n    graphic_result &lt;- t(data.frame(guess)) %&gt;%\n        as_huxtable() %&gt;%\n        theme_bright() %&gt;%\n        set_all_padding(10) %&gt;%\n        set_text_color(\"#FFFFFF\") %&gt;%\n        set_align(\"center\") %&gt;%\n        set_bold(TRUE) %&gt;%\n        set_all_borders(brdr(4, \"solid\", \"white\")) %&gt;%\n        set_font(\"arial\") %&gt;%\n        set_font_size(18)\n    for (i in 1:word_length) {\n        graphic_result &lt;- set_background_color(graphic_result, 1, i, case_when(text_result[i] ==\n            \"*\" ~ \"#6BA964\", text_result[i] == \"+\" ~ \"#CAB458\", text_result[i] ==\n            \"-\" ~ \"#787C7E\"))\n    }\n    if (output_type == \"text\") {\n        return(text_result)\n    } else {\n        return(graphic_result)\n    }\n}\n\n\n\n\n\nPlaying the Game\n\nCreate Dictionaries\n\nf_solution_list &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/google-10000-english-usa-no-swears.txt\"\nf_valid_list &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/collins-scrabble-words-2019.txt\"\n\nvalid_list &lt;- load_dictionary(f_valid_list)\nsolution_list &lt;- load_dictionary(f_solution_list)\n\n\n\nWinnow Solution Words to Set of Valid Words\n\nsolution_list &lt;- intersect(solution_list, valid_list)\n\n\n\nGame Play and Output\n\nsolution &lt;- pick_solution(solution_list, word_length = 5)\ngame &lt;- play_wordle(solution, valid_list, num_guesses = 6, output = \"graphic\")\nquick_html(game)  # prints results as html\n\n\n\n\nAlternative Functions without Graphic Output\n\n\nShow Code\nplay_wordle &lt;- function(solution, valid_list, num_guesses = 6) {\n    require(tidyverse)\n    require(sjmisc)\n    word_length &lt;- length(solution)\n    print(paste0(\"You have \", num_guesses, \" chances to guess a word of length \",\n        word_length))\n    letters_left &lt;- LETTERS  # a built-in set of capital letters\n    guess_history &lt;- data.frame(matrix(nrow = num_guesses, ncol = word_length))\n    result_history &lt;- data.frame(matrix(nrow = num_guesses, ncol = word_length))\n    for (i in 1:num_guesses) {\n        # display 'keyboard'\n        print(paste0(c(\"Letters left: \", letters_left), collapse = \" \"))\n        # read in guess and confirm length and validity\n        guess &lt;- readline(paste0(\"Enter guess \", i, \": \")) %&gt;%\n            toupper()\n        while (nchar(guess) != word_length) {\n            guess &lt;- readline(paste0(\"Guess must have \", word_length, \" characters. Enter guess \",\n                i, \" again : \")) %&gt;%\n                toupper()\n        }\n        while (guess %nin% valid_list) {\n            guess &lt;- readline(paste0(\"Hmm, that word is not in my dictionary of valid words. Enter guess \",\n                i, \" again: \")) %&gt;%\n                toupper()\n        }\n        guess &lt;- str_split(guess, \"\")[[1]]\n        # print(guess) # check output\n\n        # evaluate guess\n        result &lt;- evaluate_guess(guess, solution)\n\n        # update keyboard\n        letters_left &lt;- setdiff(letters_left, guess)\n\n        # print results\n        guess_history[i, ] &lt;- guess\n        result_history[i, ] &lt;- result\n\n        if (all(result == \"*\")) {\n            guess_history &lt;- guess_history %&gt;%\n                na.omit()\n            result_history &lt;- result_history %&gt;%\n                na.omit()\n            print(paste0(\"You won in \", i, \" guesses!\"))\n            guess_history &lt;- guess_history %&gt;%\n                unite(everything(), sep = \"\", col = \"guess\", remove = TRUE)\n            result_history &lt;- result_history %&gt;%\n                unite(everything(), sep = \"\", col = \"result\", remove = TRUE)\n            history &lt;- data.frame(guess = guess_history, result = result_history)\n            print(history)\n            return(invisible(history))\n        } else {\n            history &lt;- data.frame(guess = paste0(guess, collapse = \"\"), result = paste0(result,\n                collapse = \"\"))\n            print(history)\n        }\n\n    }\n    print(paste0(\"Sorry, you lost! Solution was \", paste0(solution, collapse = \"\")))\n\n    guess_history &lt;- guess_history %&gt;%\n        unite(everything(), sep = \"\", col = \"guess\", remove = TRUE)\n    result_history &lt;- result_history %&gt;%\n        unite(everything(), sep = \"\", col = \"result\", remove = TRUE)\n    history &lt;- data.frame(guess = guess_history, result = result_history)\n    print(history)\n    return(invisible(history))\n\n}\n\nevaluate_guess &lt;- function(guess, solution) {\n    word_length &lt;- length(solution)\n    text_result &lt;- rep(\"-\", word_length)\n    # the next lines are an ugly hack to deal with repeat letters...  we first\n    # find the number of times letters in the guess appear in the solution\n    # because we will need to clear 'extra' ones away\n    guess_count &lt;- tibble(letter = guess) %&gt;%\n        group_by(letter) %&gt;%\n        summarize(n_in_guess = n())\n    solution_count &lt;- tibble(letter = solution) %&gt;%\n        group_by(letter) %&gt;%\n        summarize(n_in_solution = n())\n    counts &lt;- inner_join(guess_count, solution_count, by = \"letter\") %&gt;%\n        mutate(to_clear = n_in_guess - n_in_solution) %&gt;%\n        filter(to_clear &gt; 0) %&gt;%\n        select(letter, to_clear)\n\n    for (i in 1:word_length) {\n        # these `case_when()` lines are the workhorse of the function...  they\n        # find if each letter in the guess appears in the solution and if it in\n        # the right place\n        text_result[i] &lt;- case_when(guess[i] %in% solution & guess[i] == solution[i] ~\n            \"*\", guess[i] %in% solution & guess[i] != solution[i] ~ \"+\", guess[i] %nin%\n            solution ~ \"-\")\n        # this `for()` loop then cycles through cases where the guess contains\n        # more of a particular letter than the solution and clears away the\n        # correct number of matches that are in in the solution but in the\n        # wrong position\n        for (j in counts$letter) {\n            if (guess[i] == j & text_result[i] != \"*\" & counts[counts$letter == j,\n                ]$to_clear &gt; 0) {\n                text_result[i] &lt;- \"-\"\n                counts[counts$letter == j, ]$to_clear &lt;- counts[counts$letter ==\n                  j, ]$to_clear - 1\n            }\n        }\n    }\n    return(text_result)\n}",
    "crumbs": [
      "Exercises",
      "Exercise 04 Solution"
    ]
  },
  {
    "objectID": "exercise-05.html",
    "href": "exercise-05.html",
    "title": "Exercise 05",
    "section": "",
    "text": "Generate Sampling Distributions and CIs",
    "crumbs": [
      "Exercises",
      "Exercise 05"
    ]
  },
  {
    "objectID": "exercise-05.html#learning-objectives",
    "href": "exercise-05.html#learning-objectives",
    "title": "Exercise 05",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nGenerating sampling distributions by simulation\nCalculating standard errors\nPlotting with {ggplot2}\nExamining the distributions of variables\nBootstrapping to derive a CI\n\n\nPreliminaries\n\nSet up a new GitHub repo in your GitHub workspace named “exercise-05” and clone that down to your computer as a new RStudio project. The instructions outlined as Method 1 in Module 6 will be helpful.",
    "crumbs": [
      "Exercises",
      "Exercise 05"
    ]
  },
  {
    "objectID": "exercise-05.html#challenge-1",
    "href": "exercise-05.html#challenge-1",
    "title": "Exercise 05",
    "section": "Challenge 1",
    "text": "Challenge 1\n\nStep 1\n\nUsing the {tidyverse} read_csv() function, load the “IMDB-movies.csv” dataset from this URL as a “tibble” named d\n\n\n\nStep 2\n\nUse a one-line statement to filter the dataset to include just movies from 1920 to 1979 and movies that are between 1 and 3 hours long (runtimeMinutes &gt;= 60 and runtimeMinutes &lt;= 180), and add a new column that codes the startYear into a new variable, decade (“20s”, “30s”, …“70s”). If you do this correctly, there should be 5651 movies remaining in the dataset.\n\n\nHINT: Use {dplyr} functions and the pipe operator!\n\n\n\nStep 3\n\nUse {ggplot2} (which is part of {tidyverse}) to plot histograms of the distribution of runtimeMinutes for each decade.\n\n\nHINT: Try using facet_wrap() to do this!\n\n\n\nStep 4\n\nUse a one-line statement to calculate the population mean and population standard deviation in runtimeMinutes for each decade and save the results in a new dataframe called results.\n\n\n\nStep 5\n\nDraw a single sample of 100 movies, without replacement, from each decade and calculate the single sample mean and single sample standard deviation in runtimeMinutes for each decades. Recall that your single sample mean for each decade is an estimate of the population mean for each decade.\n\n\nHINT: The {dplyr} functions, sample_n() (which is being deprecated) and its replacement, slice_sample(), lets you randomly sample rows from tabular data the same way that sample() lets you sample items from a vector.\n\n\n\nStep 6\n\nCalculate for each decade the standard error around your estimate of the population mean runtimeMinutes based on the standard deviation and sample size (n=100 movies) of your single sample.\n\n\n\nStep 7\n\nCompare these estimates to the actual population mean runtimeMinutes for each decade and to the calculated SE in the population mean for samples of size 100 based on the population standard deviation for each decade.\n\n\n\nStep 8\n\nGenerate a sampling distribution of mean runtimeMinutes for each decade by [a] drawing 1000 random samples of 100 movies from each decade, without replacement, and, for each sample, [b] calculating the mean runtimeMinutes and the standard deviation in runtimeMinutes for each decade. Use either a standard for( ){ } loop, the do(reps) * formulation from {mosaic}, the rerun() function from {purrr}, or the rep_sample_n() workflow from {infer} to generate your these sampling distributions (see Module 16).\n\n\n\nStep 9\n\nThen, calculate the mean and the standard deviation of the sampling distribution of sample means for each decade (the former should be a very good estimate of the population mean, while the latter is another estimate of the standard error in our estimate of the population mean for a particular sample size) and plot a histogram of the sampling distribution for each decade. What shape does it have?\n\n\n\nStep 10\n\nFinally, compare the standard error in runtimeMinutes for samples of size 100 from each decade [1] as estimated from your first sample of 100 movies, [2] as calculated from the known population standard deviations for each decade, and [3] as estimated from the sampling distribution of sample means for each decade.",
    "crumbs": [
      "Exercises",
      "Exercise 05"
    ]
  },
  {
    "objectID": "exercise-05.html#challenge-2",
    "href": "exercise-05.html#challenge-2",
    "title": "Exercise 05",
    "section": "Challenge 2",
    "text": "Challenge 2\n\n\nStep 1\n\nUsing the {tidyverse} read_csv() function, load the “zombies.csv” dataset from this URL as a “tibble” named z. This dataset includes the first and last name and gender of the entire population of 1000 people who have survived the zombie apocalypse and are now ekeing out an existence somewhere on the Gulf Coast, along with several other variables (height, weight, age, number of years of education, number of zombies they have killed, and college major). See here for info on important post-zombie apocalypse majors!\n\n\n\nStep 2\n\nCalculate the population mean and standard deviation for each quantitative random variable in the dataset (height, weight, age, number of zombies killed, and years of education).\n\n\nNOTE: You will not want to use the built in var() and sd() commands as those are for samples.\n\n\n\nStep 3\n\nUse {ggplot} and make boxplots of each of these variables by gender.\n\n\n\nStep 4\n\nUse {ggplot} and make scatterplots of height and weight in relation to age (i.e., use age as the \\(x\\) variable), using different colored points for males versus females. Do these variables seem to be related? In what way?\n\n\n\nStep 5\n\nUsing histograms and Q-Q plots, check whether each of the quantitative variables seem to be drawn from a normal distribution. Which seem to be and which do not?\n\n\nHINT: Not all are drawn from a normal distribution! For those that are not, can you determine what common distribution they are drawn from?\n\n\n\nStep 6\n\nNow use the sample_n() or slice_sample() function from {dplyr} to sample ONE subset of 50 zombie apocalypse survivors (without replacement) from this population and calculate the mean and sample standard deviation for each variable. Also estimate the standard error for each variable based on this one sample and use that to construct a theoretical 95% confidence interval for each mean. You can use either the standard normal or a Student’s t distribution to derive the critical values needed to calculate the lower and upper limits of the CI.\n\n\n\nStep 7\n\nThen draw another 199 random samples of 50 zombie apocalypse survivors out of the population and calculate the mean for each of the these samples. Together with the first sample you drew out, you now have a set of 200 means for each variable (each of which is based on 50 observations), which constitutes a sampling distribution for each variable. What are the means and standard deviations of the sampling distribution for each variable? How do the standard deviations of the sampling distribution for each variable compare to the standard errors estimated from your first sample of size 50?\n\n\n\nStep 8\n\nPlot the sampling distributions for each variable mean. What do they look like? Are they normally distributed? What about for those variables that you concluded were not originally drawn from a normal distribution?\n\n\n\nStep 9\n\nConstruct a 95% confidence interval for each mean directly from the sampling distribution of sample means using the central 95% that distribution (i.e., by setting the lower and upper CI bounds to 2.5% and 97.5% of the way through that distribution).\n\n\nHINT: You will want to use the quantile() function for this!\n\nHow do the various 95% CIs you estimated compare to one another (i.e., the CI based on one sample and the corresponding sample standard deviation versus the CI based on simulation where you created a sampling distribution across 200 samples)?\n\nNOTE: Remember, too, that the standard deviation of the sampling distribution is the standard error. You could use this value to derive yet another estimate for the 95% CI as the shape of the sampling distribution should be normal.\n\n\n\nStep 10\n\nFinally, use bootstrapping to generate a 95% confidence interval for each variable mean by resampling 1000 samples, with replacement, from your original sample (i.e., by setting the lower and upper CI bounds to 2.5% and 97.5% of the way through the sampling distribution generated by bootstrapping). How does this compare to the CIs generated in Step 9?",
    "crumbs": [
      "Exercises",
      "Exercise 05"
    ]
  },
  {
    "objectID": "exercise-05-solution.html",
    "href": "exercise-05-solution.html",
    "title": "Exercise 05 Solution",
    "section": "",
    "text": "• Solution",
    "crumbs": [
      "Exercises",
      "Exercise 05 Solution"
    ]
  },
  {
    "objectID": "exercise-05-solution.html#challenge-1",
    "href": "exercise-05-solution.html#challenge-1",
    "title": "Exercise 05 Solution",
    "section": "Challenge 1",
    "text": "Challenge 1\n\nlibrary(tidyverse)\nlibrary(mosaic)  # for do() *\nlibrary(infer)\nlibrary(ggpubr)  # for ggqqplot()\nlibrary(cowplot)  # for arranging plots\nlibrary(kableExtra)  # for kable_styling()\n\n# define a function for calculating the population sd of a variable\nsd_pop &lt;- function(x, na.rm = TRUE) {\n    # adding an argument for na.rm makes the function more robust\n    if (na.rm == TRUE) {\n        x &lt;- x[!is.na(x)]\n    }\n    sqrt(sum((x - mean(x))^2)/(length(x)))\n}\n\n\nStep 1\n\nUsing the {tidyverse} read_csv() function, load the “IMDB-movies.csv” dataset from this URL as a “tibble” named d.\n\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/IMDB-movies.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\n\n## Rows: 28938 Columns: 10\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (6): tconst, titleType, primaryTitle, genres, nconst, director\n## dbl (4): startYear, runtimeMinutes, averageRating, numVotes\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nStep 2\n\nUse a one-line statement to filter the dataset to include just movies from 1920 to 1979 and movies that are between 1 and 3 hours long (runtimeMinutes &gt;= 60 and runtimeMinutes &lt;= 180), and add a new column that codes the startYear into a new variable, decade.\n\n\nd &lt;- d |&gt;\n    filter(runtimeMinutes &gt;= 60 & runtimeMinutes &lt;= 180 & startYear %in% 1920:1979) |&gt;\n    mutate(decade = case_when(startYear %in% 1920:1929 ~ \"20s\", startYear %in% 1930:1939 ~\n        \"30s\", startYear %in% 1940:1949 ~ \"40s\", startYear %in% 1950:1959 ~ \"50s\",\n        startYear %in% 1960:1969 ~ \"60s\", startYear %in% 1970:1979 ~ \"70s\"))\nnrow(d)\n\n## [1] 5651\n\n\n\n\nStep 3\n\nUse {ggplot2} (which is part of {tidyverse}) to plot histograms of the distribution of runtimeMinutes for each decade.\n\n\n(p &lt;- ggplot(data = d, aes(x = runtimeMinutes)) + geom_histogram(stat = \"bin\", bins = 20,\n    colour = \"black\", fill = \"lightblue\")) + facet_wrap(~decade)\n\n\n\n\n\n\n\n\n\n\nStep 4\n\nUse a one-line statement to calculate the population mean and population standard deviation in runtimeMinutes for each decade and save the results in a new dataframe called results.\n\n\nresults &lt;- d |&gt;\n    group_by(decade) |&gt;\n    dplyr::summarise(pop_n = n(), pop_mean = mean(runtimeMinutes, na.rm = TRUE),\n        pop_sd = sd_pop(runtimeMinutes, na.rm = TRUE))\n# user-defined function or use `sdpop()` from {radiant}\n\nkable(results, digits = 3) |&gt;\n    kable_styling(font_size = 12, full_width = FALSE)\n\n\n\n\ndecade\npop_n\npop_mean\npop_sd\n\n\n\n\n20s\n152\n96.257\n26.115\n\n\n30s\n530\n90.300\n17.272\n\n\n40s\n782\n97.203\n19.111\n\n\n50s\n1081\n98.948\n19.198\n\n\n60s\n1386\n105.586\n21.224\n\n\n70s\n1720\n103.750\n17.954\n\n\n\n\n\n\n\n\n\nStep 5 and 6\n\nDraw a single sample of 100 movies, without replacement, from each decade and calculate the single sample mean and single sample standard deviation in runtimeMinutes for each decades.\nCalculate for each decade the standard error around your estimate of the population mean runtimeMinutes based on the standard deviation and sample size (n=100 movies) of your single sample.\n\n\nn &lt;- 100\nset.seed(1)\ns &lt;- d |&gt;\n    group_by(decade) |&gt;\n    sample_n(n, replace = FALSE) |&gt;\n    # or use `slice_sample()` instead of `sample_n()` if we use\n    # `slice_sample()`, we need to pass it either the number of rows ('n=') or\n    # the proportion of rows ('p=') as an argument, i.e., `slice_sample(n=100,\n    # replace=FALSE)`\ndplyr::summarise(samp_size = n(), samp_1_mean = mean(runtimeMinutes, na.rm = TRUE),\n    samp_1_sd = sd(runtimeMinutes, na.rm = TRUE), samp_1_se = sd(runtimeMinutes,\n        na.rm = TRUE)/sqrt(samp_size))\n\nkable(s, digits = 3) |&gt;\n    kable_styling(font_size = 12, full_width = FALSE)\n\n\n\n\ndecade\nsamp_size\nsamp_1_mean\nsamp_1_sd\nsamp_1_se\n\n\n\n\n20s\n100\n97.87\n27.728\n2.773\n\n\n30s\n100\n88.05\n15.331\n1.533\n\n\n40s\n100\n94.27\n18.406\n1.841\n\n\n50s\n100\n100.60\n18.703\n1.870\n\n\n60s\n100\n107.74\n22.977\n2.298\n\n\n70s\n100\n102.71\n15.252\n1.525\n\n\n\n\n\n\n\n\n\nStep 7\n\nCompare these estimates to the actual population mean runtimeMinutes for each decade and to the calculated SE in the population mean for samples of size 100 based on the population standard deviation for each decade.\n\n\nresults &lt;- inner_join(s, results, by = \"decade\") |&gt;\n    mutate(pop_se = pop_sd/sqrt(n))\n\nkable(results, digits = 3) |&gt;\n    kable_styling(font_size = 12, full_width = FALSE)\n\n\n\n\ndecade\nsamp_size\nsamp_1_mean\nsamp_1_sd\nsamp_1_se\npop_n\npop_mean\npop_sd\npop_se\n\n\n\n\n20s\n100\n97.87\n27.728\n2.773\n152\n96.257\n26.115\n2.612\n\n\n30s\n100\n88.05\n15.331\n1.533\n530\n90.300\n17.272\n1.727\n\n\n40s\n100\n94.27\n18.406\n1.841\n782\n97.203\n19.111\n1.911\n\n\n50s\n100\n100.60\n18.703\n1.870\n1081\n98.948\n19.198\n1.920\n\n\n60s\n100\n107.74\n22.977\n2.298\n1386\n105.586\n21.224\n2.122\n\n\n70s\n100\n102.71\n15.252\n1.525\n1720\n103.750\n17.954\n1.795\n\n\n\n\n\n\n\n\n\nStep 8 and 9\n\nGenerate a sampling distribution of mean runtimeMinutes for each decade by [a] drawing 1000 samples of 100 movies from each decade and, for each sample, [b] calculating the mean runtimeMinutes and the standard deviation in runtimeMinutes for each decade.\n\n\nn &lt;- 100\nreps &lt;- 1000\n# using {mosaic}\ns &lt;- {\n    do(reps) * sample_n(group_by(d, decade), n, replace = FALSE)\n} |&gt;\n    # `do(rep) *` needs to be wrapped in braces in order to introduce the\n    # variable '.index' that we need to then pass as an argument to\n    # `group_by()`\ngroup_by(decade, .index) |&gt;\n    summarise(avg_runtimeMinutes = mean(runtimeMinutes, na.rm = TRUE), sd_runtimeMinutes = sd(runtimeMinutes,\n        na.rm = TRUE))\n\n# or...  using {purrr}\ns &lt;- map(1:reps, ~mean(runtimeMinutes ~ decade, data = sample_n(group_by(d, decade),\n    size = n, replace = FALSE))) |&gt;\n    bind_rows() |&gt;\n    pivot_longer(cols = everything(), names_to = \"decade\", values_to = \"avg_runtimeMinutes\")\n\n# or... (and this runs fastest) using {infer}\ns &lt;- tibble(decade = character(), replicate = numeric(), avg_runtimeMinutes = numeric(),\n    sd_runtimeMinutes = numeric())\n\nfor (i in unique(d$decade)) {\n    df &lt;- d |&gt;\n        filter(decade == i) |&gt;\n        rep_sample_n(size = n, reps = reps, replace = FALSE) |&gt;\n        group_by(replicate) |&gt;\n        summarize(avg_runtimeMinutes = mean(runtimeMinutes, na.rm = TRUE), sd_runtimeMinutes = sd(runtimeMinutes,\n            na.rm = TRUE)) |&gt;\n        mutate(decade = i)\n    s &lt;- bind_rows(s, df)\n}\n\n\n(p &lt;- ggplot(data = s, aes(avg_runtimeMinutes)) + geom_histogram(stat = \"bin\", bins = 20,\n    colour = \"black\", fill = \"lightblue\") + facet_wrap(~decade, scales = \"free_x\"))\n\n\n\n\n\n\n\n# now, distributions are not obviously different from NORMAL :)\n\n\n\nStep 10\n\nFinally, compare the standard error in runtimeMinutes for samples of size 100 from each decade [1] as estimated from your first sample of 100 movies, [2] as calculated from the known population standard deviations for each decade, and [3] as estimated from the sampling distribution of sample means for each decade.\n\n\nsamp_dist_stats &lt;- s |&gt;\n    group_by(decade) |&gt;\n    summarize(samp_dist_mean = mean(avg_runtimeMinutes), samp_dist_sd = sd(avg_runtimeMinutes))\n\ncomparison &lt;- inner_join(results, samp_dist_stats, by = \"decade\") |&gt;\n    dplyr::select(decade, pop_se, samp_1_se, samp_dist_sd)\n\nkable(comparison, digits = 3) |&gt;\n    kable_styling(font_size = 12, full_width = FALSE)\n\n\n\n\ndecade\npop_se\nsamp_1_se\nsamp_dist_sd\n\n\n\n\n20s\n2.612\n2.773\n1.495\n\n\n30s\n1.727\n1.533\n1.512\n\n\n40s\n1.911\n1.841\n1.826\n\n\n50s\n1.920\n1.870\n1.897\n\n\n60s\n2.122\n2.298\n2.095\n\n\n70s\n1.795\n1.525\n1.769\n\n\n\n\n\n\n\nThese different estimates of the SE are all pretty close to one another, except for the “20s”, where the total number of movies in the decade is relatively small.",
    "crumbs": [
      "Exercises",
      "Exercise 05 Solution"
    ]
  },
  {
    "objectID": "exercise-05-solution.html#challenge-2",
    "href": "exercise-05-solution.html#challenge-2",
    "title": "Exercise 05 Solution",
    "section": "Challenge 2",
    "text": "Challenge 2\n\nStep 1\n\nUsing the {tidyverse} read_csv() function, load the “zombies.csv” dataset from this URL as a “tibble” named z. This dataset includes the first and last name and gender of the entire population of 1000 people who have survived the zombie apocalypse and are now ekeing out an existence somewhere on the Gulf Coast, along with several other variables (height, weight, age, number of years of education, number of zombies they have killed, and college major).\n\n\nn &lt;- 50  # set sample size\nalpha &lt;- 0.05  # set alpha\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/zombies.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\n\nsurvivors &lt;- dplyr::select(d, \"gender\", \"height\", \"weight\", \"age\", \"zombies_killed\",\n    \"years_of_education\")\n\nkable(head(survivors), digits = 3) |&gt;\n    kable_styling(font_size = 12, full_width = FALSE)\n\n\n\n\ngender\nheight\nweight\nage\nzombies_killed\nyears_of_education\n\n\n\n\nFemale\n62.890\n132.087\n17.643\n2\n1\n\n\nMale\n67.803\n146.375\n22.590\n5\n3\n\n\nMale\n72.129\n152.937\n21.913\n1\n1\n\n\nMale\n66.785\n129.742\n18.191\n5\n6\n\n\nFemale\n64.718\n132.426\n21.104\n4\n3\n\n\nMale\n71.243\n152.525\n21.484\n1\n4\n\n\n\n\n\n\n\n\n\nStep 2\n\nCalculate the population mean and standard deviation for each quantitative random variable in the dataset (height, weight, age, number of zombies killed, and years of education).\n\n\npop_means &lt;- dplyr::summarise(survivors, mean(height), mean(weight), mean(age), mean(zombies_killed),\n    mean(years_of_education))  # {dplyr} rocks!\n\npop_sds &lt;- dplyr::summarise(survivors, sd_pop(height), sd_pop(weight), sd_pop(age),\n    sd_pop(zombies_killed), sd_pop(years_of_education))  # {dplyr} rocks!\n\n# or, using `sdpop()` from {radiant} pop_SDs &lt;- dplyr::summarise(survivors,\n# sdpop(height), sdpop(weight), sdpop(age), sdpop(zombies_killed),\n# sdpop(years_of_education))\n\n# ALTERNATIVELY...  we can use new dplyr::summarise(across()) construction this\n# takes two arguments, .cols and .fns .cols is a vector of columns to apply the\n# functions to e.g., c(height, weight, age, zombies_killed, years_of_education)\n# .funs is either...  [a] a single function, e.g., `mean`, [b] a purrr style\n# lambda, e.g., ~ mean(.), [c] a list of functions, e.g., `list(mean=mean,\n# sd=sd)` [d] a list of lambdas, e.g., `list(~ mean(.), ~sd(.)) .funs are\n# applied across the set of .cols\n\n# [a]\npop_means_a &lt;- survivors |&gt;\n    dplyr::summarise(across(c(height, weight, age, zombies_killed, years_of_education),\n        mean))\n\n# [b]\npop_means_b &lt;- survivors |&gt;\n    dplyr::summarise(across(c(height, weight, age, zombies_killed, years_of_education),\n        ~mean(.)))\n\n# [c]\npop_means_c &lt;- survivors |&gt;\n    dplyr::summarise(across(c(height, weight, age, zombies_killed, years_of_education),\n        list(mean = mean)))\n\n# [d]\npop_means_d &lt;- survivors |&gt;\n    dplyr::summarise(across(c(height, weight, age, zombies_killed, years_of_education),\n        list(~mean(.))))\n\npop_ses &lt;- pop_sds/sqrt(n)  # not needed yet, but will be later\n\n# make a pretty table by transposing the 1-row dataframes above\nvariables &lt;- c(\"Height\", \"Weight\", \"Age\", \"Kills\", \"Years of Ed\")\n\npop_summary &lt;- bind_cols(variables, t(pop_means), t(pop_sds))\n\nnames(pop_summary) &lt;- c(\"variable\", \"pop_mean\", \"pop_sd\")\n\nkable(pop_summary, digits = 3) |&gt;\n    kable_styling(font_size = 12, full_width = FALSE)\n\n\n\n\nvariable\npop_mean\npop_sd\n\n\n\n\nHeight\n67.630\n4.308\n\n\nWeight\n143.907\n18.392\n\n\nAge\n20.047\n2.964\n\n\nKills\n2.992\n1.748\n\n\nYears of Ed\n2.996\n1.676\n\n\n\n\n\n\n\n\n\nStep 3\n\nUse {ggplot} and make boxplots of each of these variables by gender.\n\n\np1 &lt;- ggplot(data = survivors, aes(x = gender, y = height))\np1 &lt;- p1 + geom_boxplot(aes(colour = factor(gender)))\np1 &lt;- p1 + theme(legend.position = \"none\")\np2 &lt;- ggplot(data = survivors, aes(x = gender, y = weight))\np2 &lt;- p2 + geom_boxplot(aes(colour = factor(gender)))\np2 &lt;- p2 + theme(legend.position = \"none\")\np3 &lt;- ggplot(data = survivors, aes(x = gender, y = age))\np3 &lt;- p3 + geom_boxplot(aes(colour = factor(gender)))\np3 &lt;- p3 + theme(legend.position = \"none\")\np4 &lt;- ggplot(data = survivors, aes(x = gender, y = zombies_killed))\np4 &lt;- p4 + geom_boxplot(aes(colour = factor(gender)))\np4 &lt;- p4 + theme(legend.position = \"none\")\np5 &lt;- ggplot(data = survivors, aes(x = gender, y = years_of_education))\np5 &lt;- p5 + geom_boxplot(aes(colour = factor(gender)))\np5 &lt;- p5 + theme(legend.position = \"none\")\n\n\nplot_grid(p1, p2, p3, p4, p5, nrow = 2)\n\n\n\n\n\n\n\n# or...  (p11 &lt;- ggplot(data=pivot_longer( survivors, cols = c(height, weight,\n# age, zombies_killed, years_of_education), names_to ='variable', values_to =\n# 'value'), aes(x=factor(gender),y=value)) +\n# geom_boxplot(aes(colour=factor(gender)))+ facet_wrap(~ variable, scales =\n# 'free') + xlab('') + ylab('Frequency') + theme(legend.position = 'none'))\n\n\n\nStep 4\n\nUse {ggplot} and make scatterplots of height and weight in relation to age (i.e., use age as the \\(x\\) variable), using different colored points for males versus females. Do these variables seem to be related? In what way?\n\n\np1 &lt;- ggplot(data = survivors, aes(x = age, y = height, colour = factor(gender))) +\n    geom_point() + theme(legend.position = \"bottom\", legend.title = element_blank())\np2 &lt;- ggplot(data = survivors, aes(x = age, y = weight, colour = factor(gender))) +\n    geom_point() + theme(legend.position = \"bottom\", legend.title = element_blank())\n\n\nplot_grid(p1, p2, nrow = 1)\n\n\n\n\n\n\n\n\nBoth seem to be positive linear functions of age.\n\n\nStep 5\n\nUsing histograms and Q-Q plots, check whether the quantitative variables seem to be drawn from a normal distribution.\n\n\n# plot the distributions\np1 &lt;- ggplot(data = survivors, aes(x = height)) + geom_histogram(bins = 20) + ggtitle(\"Height\")\np2 &lt;- ggqqplot(data = survivors, x = \"height\")\np3 &lt;- ggplot(data = survivors, aes(x = weight)) + geom_histogram(bins = 20) + ggtitle(\"Weight\")\np4 &lt;- ggqqplot(data = survivors, x = \"weight\")\np5 &lt;- ggplot(data = survivors, aes(x = age)) + geom_histogram(bins = 20) + ggtitle(\"Age\")\np6 &lt;- ggqqplot(data = survivors, x = \"age\")\np7 &lt;- ggplot(data = survivors, aes(x = zombies_killed)) + geom_histogram(binwidth = 1) +\n    ggtitle(\"Zombies Killed\")\np8 &lt;- ggqqplot(data = survivors, x = \"zombies_killed\")\np9 &lt;- ggplot(data = survivors, aes(x = years_of_education)) + geom_histogram(binwidth = 1) +\n    ggtitle(\"Years of Education\")\np10 &lt;- ggqqplot(data = survivors, x = \"years_of_education\")\n\n\nplot_grid(p1, p3, p5, p2, p4, p6, nrow = 2)\n\n\n\n\n\n\n\nplot_grid(p7, p9, p8, p10, nrow = 2)\n\n\n\n\n\n\n\n\nThe first three are seemingly drawn from normal distributions, but not the latter two. These are discrete variables, and they seem to be drawn from the Poisson distribution.\n\n\nStep 6\n\nNow use the sample_n() or slice_sample() function from {dplyr} to sample ONE subset of 50 zombie apocalypse survivors (without replacement) from this population and calculate the mean and sample standard deviation for each variable. Also estimate the standard error for each variable based on this sample and use that to construct a 95% confidence interval for each mean. You can use either the standard normal or a Student’s t distribution to derive the critical values needed to calculate the lower and upper limits of the CI. [As an additional alternative, you could estimate a CI by bootstrap resampling from that first sample as well.]\n\nFirst, we create some functions for SEs and CIs\n\nse &lt;- function(x, type = \"normal\") {\n    if (type == \"normal\") {\n        se &lt;- sd(x)/sqrt(length(x))\n    }\n    if (type == \"poisson\") {\n        se &lt;- sqrt(mean(x)/length(x))\n        # mean(x) is estimate of lambda\n    }\n    return(se)\n}\n\nci_norm &lt;- function(x, alpha = 0.05) {\n    ci &lt;- (mean(x) + c(-1, 1) * qnorm(1 - alpha/2) * se(x)) |&gt;\n        round(3)\n    # confidence interval based on normal distribution\n    ci &lt;- paste0(\"[\", ci[1], \"-\", ci[2], \"]\")\n    names(ci) &lt;- \"CI\"\n    return(ci)\n}\n\nci_t &lt;- function(x, alpha = 0.05) {\n    ci &lt;- (mean(x) + c(-1, 1) * qt(1 - alpha/2, length(x) - 1) * se(x)) |&gt;\n        round(3)\n    # confidence interval based on t distribution\n    ci &lt;- paste0(\"[\", ci[1], \"-\", ci[2], \"]\")\n    names(ci) &lt;- \"CI\"\n    return(ci)\n}\n\nci_boot &lt;- function(x, alpha = 0.05, n_boot = 1000) {\n    boot &lt;- NULL\n    for (i in 1:n_boot) {\n        boot[i] &lt;- mean(sample(x, length(x), replace = TRUE))\n    }\n    ci &lt;- quantile(boot, c(alpha/2, 1 - alpha/2)) |&gt;\n        round(3)\n    ci &lt;- paste0(\"[\", ci[1], \"-\", ci[2], \"]\")\n    names(ci) &lt;- \"CI\"\n    return(ci)\n}\n\nThen, we get a sample of size n from survivors and calculate statistics…\n\nset.seed(1)  # setting the seed makes the random draws the sample across runs of code\nn &lt;- 50\ns &lt;- survivors |&gt;\n    sample_n(size = n, replace = FALSE)\n# or... `s &lt;- survivors |&gt; slice_sample(n = n, replace = FALSE)` head(s) #\n# uncomment to show start of first sample of size n\n\nsamp_1_means &lt;- s |&gt;\n    dplyr::summarise(across(.cols = c(height, weight, age, zombies_killed, years_of_education),\n        .fns = ~mean(.)))\n\nsamp_1_SDs &lt;- s |&gt;\n    dplyr::summarise(across(.cols = c(height, weight, age, zombies_killed, years_of_education),\n        .fns = ~sd(.)))\n\nsamp_1_SEs &lt;- s |&gt;\n    dplyr::summarise(across(.cols = c(height, weight, age), .fns = ~se(., type = \"normal\")),\n        across(.cols = c(zombies_killed, years_of_education), .fns = ~se(., type = \"poisson\")))\n\n# create a tibble of CIs based on normal distribution\nsamp_1_CI_norm &lt;- s |&gt;\n    dplyr::summarise(across(.cols = c(height, weight, age, zombies_killed, years_of_education),\n        .fns = ~ci_norm(.)))\n\n# create a tibble of CIs based on t distribution\nsamp_1_CI_t &lt;- s |&gt;\n    dplyr::summarise(across(.cols = c(height, weight, age, zombies_killed, years_of_education),\n        .fns = ~ci_t(.)))\n\n# create a tibble of CIs based on bootstrapping\nsamp_1_CI_boot &lt;- s |&gt;\n    dplyr::summarise(across(.cols = c(height, weight, age, zombies_killed, years_of_education),\n        .fns = ~ci_boot(., n_boot = 1000)))\n\n# make a pretty table\nsamp_1_stats &lt;- bind_rows(samp_1_means, samp_1_SDs, samp_1_SEs)\nsamp_1_CIs &lt;- bind_rows(samp_1_CI_norm, samp_1_CI_t, samp_1_CI_boot)\n\nsamp_1_summary &lt;- bind_cols(variables, t(samp_1_stats), t(samp_1_CIs))\nnames(samp_1_summary) &lt;- c(\"Variable\", \"Samp 1 mean\", \"Samp 1 SD\", \"Samp 1 SE\", \"Samp 1 CI norm\",\n    \"Samp 1 CI t\", \"Samp 1 CI boot\")\n\nkable(samp_1_summary, digits = 3) |&gt;\n    kable_styling(font_size = 12, full_width = FALSE)\n\n\n\n\nVariable\nSamp 1 mean\nSamp 1 SD\nSamp 1 SE\nSamp 1 CI norm\nSamp 1 CI t\nSamp 1 CI boot\n\n\n\n\nHeight\n67.302\n4.379\n0.619\n[66.088-68.516]\n[66.057-68.546]\n[66.072-68.535]\n\n\nWeight\n143.466\n20.807\n2.943\n[137.699-149.234]\n[137.553-149.38]\n[137.759-148.981]\n\n\nAge\n20.088\n3.106\n0.439\n[19.227-20.948]\n[19.205-20.97]\n[19.298-20.924]\n\n\nKills\n3.080\n1.850\n0.248\n[2.567-3.593]\n[2.554-3.606]\n[2.62-3.62]\n\n\nYears of Ed\n3.040\n1.564\n0.247\n[2.606-3.474]\n[2.595-3.485]\n[2.639-3.46]\n\n\n\n\n\n\n\n\n\nStep 7\n\nThen draw another 199 random samples of 50 zombie apocalypse survivors out of the population and calculate the mean for each of the these samples. Together with the first sample you drew out, you now have a set of 200 means for each variable (each based on 50 observations), which constitutes a sampling distribution for each variable. What are the means and standard deviations of the sampling distribution for each variable?\n\n\nk &lt;- 199  # additional # of sample sets\n# using {mosaic}\nadditional_samples &lt;- do(k) * sample_n(survivors, size = n, replace = FALSE)\n# or `slice_sample(survivors, n = n, replace = FALSE)` each row will have a\n# single individual (with n = 50 rows per sample) and the .index column\n# contains the replicate number\n\n# now, add a .row and .index column to our original sample\ns &lt;- s |&gt;\n    mutate(.row = 1:n, .index = k + 1)\n\n# and bind the additional + original samples into a single data frame\nall_s &lt;- bind_rows(additional_samples, s)\n\nsamp_dist &lt;- all_s |&gt;\n    group_by(.index) |&gt;\n    dplyr::summarise(across(.cols = c(height, weight, age, zombies_killed, years_of_education),\n        .fns = ~mean(.))) |&gt;\n    dplyr::select(-.index)\n\nsamp_SEs &lt;- all_s |&gt;\n    group_by(.index) |&gt;\n    dplyr::summarise(across(.cols = c(height, weight, age), .fns = ~se(., type = \"normal\")),\n        across(.cols = c(zombies_killed, years_of_education), .fns = ~se(., type = \"poisson\"))) |&gt;\n    dplyr::select(-.index)\n\n# head(sampling_distribution) # uncomment to show start of sampling\n# distributions\n\nsamp_dist_means &lt;- samp_dist |&gt;\n    dplyr::summarise(across(.cols = everything(), .fns = ~mean(.)))\n\nsamp_dist_SDs &lt;- samp_dist |&gt;\n    dplyr::summarise(across(.cols = everything(), .fns = ~sd(.)))\n\n# make a pretty table\nvariables &lt;- c(\"Height\", \"Weight\", \"Age\", \"Kills\", \"Years of Ed\")\nsamp_dist_summary &lt;- bind_cols(variables, t(samp_dist_means), t(samp_dist_SDs))\nnames(samp_dist_summary) &lt;- c(\"Variable\", \"Samp Dist mean\", \"Samp Dist SD\")\n\nkable(samp_dist_summary, digits = 3) |&gt;\n    kable_styling(font_size = 12, full_width = FALSE)\n\n\n\n\nVariable\nSamp Dist mean\nSamp Dist SD\n\n\n\n\nHeight\n67.559\n0.551\n\n\nWeight\n143.747\n2.371\n\n\nAge\n20.039\n0.381\n\n\nKills\n3.016\n0.233\n\n\nYears of Ed\n2.996\n0.240\n\n\n\n\n\n\n\n\nHow do the standard deviations of the sampling distribution for each variable compare to the standard errors estimated from your first sample of size 50?\n\n\nsamp_SE_means &lt;- samp_SEs |&gt;\n    dplyr::summarise(across(.cols = everything(), .fns = ~mean(.)))\n\n# again, make a pretty table\ncompare_SEs &lt;- tibble(Variable = c(\"Height\", \"Weight\", \"Age\", \"Kills\", \"Years of Ed\"),\n    samp_dist_mean = samp_dist_summary$`Samp Dist mean`, `**Samp Dist SD**` = samp_dist_summary$`Samp Dist SD`,\n    `SE from Pop SD` = t(pop_sds/sqrt(n)) |&gt;\n        round(3), `**Samp 1 SE**` = samp_1_summary$`Samp 1 SE`, `Mean SE across Samples` = t(samp_SE_means) |&gt;\n        round(3))\nrownames(compare_SEs) &lt;- NULL  # get rid of rownames to make table pretty\n\nkable(compare_SEs, digits = 3) |&gt;\n    kable_styling(font_size = 12, full_width = FALSE)\n\n\n\n\nVariable\nsamp_dist_mean\n**Samp Dist SD**\nSE from Pop SD\n**Samp 1 SE**\nMean SE across Samples\n\n\n\n\nHeight\n67.559\n0.551\n0.609\n0.619\n0.608\n\n\nWeight\n143.747\n2.371\n2.601\n2.943\n2.587\n\n\nAge\n20.039\n0.381\n0.419\n0.439\n0.413\n\n\nKills\n3.016\n0.233\n0.247\n0.248\n0.245\n\n\nYears of Ed\n2.996\n0.240\n0.237\n0.247\n0.245\n\n\n\n\n\n\n\nThese should all be about the same! As the size of each of the k samples increases, the SD of the sampling distribution for each variable should converge to the population estimate of the standard error, i.e., to SD.pop/sqrt(n), or \\(\\frac{\\sigma}{\\sqrt{n}}\\). The SE for each variable within each sample should be an estimator of this standard error, and the mean SE across samples should be really close to the population estimate.\n\nNOTE: The columns in this table with asterisks (“**“) are those that you were specifically asked to compare.\n\n\n\nStep 8\n\nPlot the sampling distributions for each variable mean. What do they look like? Are they normally distributed? What about for those variables that you concluded were not originally drawn from a normal distribution?\n\n\n# plot the distributions\np1 &lt;- ggplot(data = samp_dist, aes(x = height)) + geom_histogram(bins = 10) + ggtitle(\"Height Means\")\np2 &lt;- ggqqplot(data = samp_dist, x = \"height\")\np3 &lt;- ggplot(data = samp_dist, aes(x = weight)) + geom_histogram(bins = 10) + ggtitle(\"Weight Means\")\np4 &lt;- ggqqplot(data = samp_dist, x = \"weight\")\np5 &lt;- ggplot(data = samp_dist, aes(x = age)) + geom_histogram(bins = 10) + ggtitle(\"Age Means\")\np6 &lt;- ggqqplot(data = samp_dist, x = \"age\")\np7 &lt;- ggplot(data = samp_dist, aes(x = zombies_killed)) + geom_histogram(bins = 10) +\n    ggtitle(\"Zombies Killed Means\")\np8 &lt;- ggqqplot(data = samp_dist, x = \"zombies_killed\")\np9 &lt;- ggplot(data = samp_dist, aes(x = years_of_education)) + geom_histogram(bins = 10) +\n    ggtitle(\"Years of Education Means\")\np10 &lt;- ggqqplot(data = samp_dist, x = \"years_of_education\")\n\n\nplot_grid(p1, p3, p5, p2, p4, p6, nrow = 2)\n\n\n\n\n\n\n\nplot_grid(p7, p9, p8, p10, nrow = 2)\n\n\n\n\n\n\n\n\nThese all look pretty normally distributed, even for those variables that were not drawn from a normal distribution initially! This becomes even more apparent if we set k to a higher number, e.g., 1000.\n\n\nStep 9\n\nConstruct a 95% confidence interval for each mean directly from the sampling distribution of sample means using the central 95% that distribution.\n\n\n# Here, we use the `quantile()` function... first create a function to pull out\n# CI\nci_quant &lt;- function(x, level = 0.95) {\n    ci &lt;- quantile(x, c((1 - level)/2, 1 - (1 - level)/2)) |&gt;\n        round(3)\n    ci &lt;- paste0(\"[\", ci[1], \"-\", ci[2], \"]\")\n    names(ci) &lt;- \"CI\"\n    return(ci)\n}\n\nsamp_dist_CI &lt;- dplyr::summarise(samp_dist, across(.cols = everything(), .fns = ~ci_quant(.,\n    level = 0.95)))\n\nsamp_dist_summary &lt;- bind_cols(samp_dist_summary, t(samp_dist_CI))\n\nnames(samp_dist_summary) &lt;- c(\"Variable\", \"Samp Dist mean\", \"Samp Dist SD\", \"Samp Dist CI\")\n\nkable(samp_dist_summary, digits = 3) |&gt;\n    kable_styling(font_size = 12, full_width = FALSE)\n\n\n\n\nVariable\nSamp Dist mean\nSamp Dist SD\nSamp Dist CI\n\n\n\n\nHeight\n67.559\n0.551\n[66.503-68.626]\n\n\nWeight\n143.747\n2.371\n[139.071-148.556]\n\n\nAge\n20.039\n0.381\n[19.412-20.828]\n\n\nKills\n3.016\n0.233\n[2.579-3.443]\n\n\nYears of Ed\n2.996\n0.240\n[2.479-3.481]\n\n\n\n\n\n\n\n\nHow do the various 95% CIs you estimated compare to one another (i.e., the CIs based on one sample and the corresponding sample standard deviation versus the CI based on simulation where you created a sampling distribution across 200 samples)?\n\n\n# CIs from Sample 1\ncompare_CIs &lt;- dplyr::select(samp_1_summary, -c(\"Samp 1 mean\", \"Samp 1 SD\", \"Samp 1 SE\")) |&gt;\n    bind_cols(`Samp Dist CI` = samp_dist_summary$`Samp Dist CI`)\n\nkable(compare_CIs, digits = 3) |&gt;\n    kable_styling(font_size = 12, full_width = FALSE)\n\n\n\n\nVariable\nSamp 1 CI norm\nSamp 1 CI t\nSamp 1 CI boot\nSamp Dist CI\n\n\n\n\nHeight\n[66.088-68.516]\n[66.057-68.546]\n[66.072-68.535]\n[66.503-68.626]\n\n\nWeight\n[137.699-149.234]\n[137.553-149.38]\n[137.759-148.981]\n[139.071-148.556]\n\n\nAge\n[19.227-20.948]\n[19.205-20.97]\n[19.298-20.924]\n[19.412-20.828]\n\n\nKills\n[2.567-3.593]\n[2.554-3.606]\n[2.62-3.62]\n[2.579-3.443]\n\n\nYears of Ed\n[2.606-3.474]\n[2.595-3.485]\n[2.639-3.46]\n[2.479-3.481]\n\n\n\n\n\n\n\n\n\nStep 10\n\nFinally, use bootstrapping to generate a 95% confidence interval for each variable mean by bootstrapping 1000 samples, with replacement, from your original sample.\n\n\nNOTE: This was already done in Step 6, where we ran the custom CI_boot() function, and the results are included in the Samp 1 CI boot column in the table above.\n\nThe CI based on the sampling distribution generated via resampling is comparable to those based on the first sample using parametric estimates from a normal and a t distribution, as well as that based on bootstrapping using just the first sample for all of the normally distributed variables (age, height, weight). Even for the Poisson-distributed variables (zombies killed, years of education), the lower and upper bounds for the sampling distribution-based CIs are pretty comparable to those estimated from the first sample by either parametric methods or via bootstrap estimation.",
    "crumbs": [
      "Exercises",
      "Exercise 05 Solution"
    ]
  },
  {
    "objectID": "exercise-06.html",
    "href": "exercise-06.html",
    "title": "Exercise 06",
    "section": "",
    "text": "Practice Simulation-Based Inference",
    "crumbs": [
      "Exercises",
      "Exercise 06"
    ]
  },
  {
    "objectID": "exercise-06.html#learning-objectives",
    "href": "exercise-06.html#learning-objectives",
    "title": "Exercise 06",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nUse a real data set to practice…\n\ngenerating confidence intervals around sample statistics by bootstrapping\ndoing permutation-based tests of independence",
    "crumbs": [
      "Exercises",
      "Exercise 06"
    ]
  },
  {
    "objectID": "exercise-06.html#background-on-the-dataset",
    "href": "exercise-06.html#background-on-the-dataset",
    "title": "Exercise 06",
    "section": "Background on the Dataset",
    "text": "Background on the Dataset\nSpider monkeys (genus Ateles) live in large multimale-multifemale social groups containing a total of ~20-30 adult individuals. Association patterns among the members of these group are very flexible. It is rare to see more than a handful of the adult members of the group together at any given time, and, instead, group members organize themselves in multiple smaller subgroups, or “parties”, that travel separately from one another. Individuals and parties may come together (“fuse”), re-assort their membership, and break apart from one another (“fission”) multiple times per day. Each individual, then, shows a different pattern of association with other group members and its own pattern of home range use.\nMy research group has collected data on the ranging patterns of one species of spider monkeys (Ateles belzebuth) in Amazonian Ecuador by following focal individuals and recording their location at regular intervals throughout the day using a GPS. We also record information on the other animals associated with focal individuals at those same intervals. This process yields a large set of location records for each individual based on both when those animals are the focus of focal follows and when they are present in subgroups containing a different focal individual.\nUsing location records collected over several years, we have generated several measures of home range size for 9 adult males and 11 adult females who are members of one social group of Ateles belzebuth.\n\nPreliminaries\n\nUsing the {tidyverse} read_csv() function, load this dataset into R as a “tibble” named d and look at the variables it contains. For this exercise, we are interested in two variables in particular: sex (“M” or “F”, for male versus female) and kernel95, which represents the size of a polygon summarizing the location records for an individual as a 95% utilization density kernel.\n\n\n\nStep 1\n\nReduce the dataset to just the two variables of interest.\n\n\n\nStep 2\n\nDetermine the mean, standard deviation, and standard error in “kernel95” home range size for each sex.\n\n\n\nStep 3\n\nCreate boxplots comparing “kernel95” home range size by sex.\n\n\n\nStep 4\n\nFor each sex, generate a bootstrap distribution for mean kernel95 home range size. To do this, for each sex, you will want to generate a set of 10,000 bootstrap samples (i.e., sampling with replacement), calculate the mean kernel95 home range size for each of these samples, and plot the resulting bootstrap sampling distribution.\n\n\n\nStep 5\n\nPlot an appropriate normal distribution over the bootstrap sampling distribution.\n\n\n\nStep 6\n\nCalculate a 95% confidence interval around for the mean kernel95 home range size for each sex…\n\nUsing the quantile() method applied directly to your bootstrap sampling distribution, and…\nUsing the theory-based “standard error” method, based on qnorm() and the standard deviation of your bootstrap sampling distribution.\n\n\n\n\nStep 7\n\nUse simulation-based permutation to evaluate the difference in mean kernel95 home range size for males versus females. To do this, you will want to shuffle either the variable “sex” or “kernel95” home range size a total of 10,000 times, recalculating mean kernel95 size by sex for each permuted sample and then compare the difference in male and female kernel95 means from your original sample to the permutation distribution for the difference in means.\n\nUnder this approach, what is the “null” hypothesis? What is the test statistic? Is the difference in mean kernel95 home range size “significant”?\n\n\n\n\nStep 8\n\nFinally, use a theory-based parametric test (e.g., a t-test) to also calculate an appropriate test statistic and associated p value for the comparison of male and female mean kernel95 home range size. Under this approach, what is the test statistic? Is the difference in mean kernel95 home range size “significant”?",
    "crumbs": [
      "Exercises",
      "Exercise 06"
    ]
  },
  {
    "objectID": "exercise-06-solution.html",
    "href": "exercise-06-solution.html",
    "title": "Exercise 06 Solution",
    "section": "",
    "text": "• Solution\nLoad in dataset and libraries of interest…\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(infer)\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/tbs-2006-2008-ranges.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\nhead(d)\n\n## # A tibble: 6 × 9\n##   id       sex   sex.code mcp50 mcp80 mcp95 kernel50 kernel80 kernel95\n##   &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n## 1 Ana      F            1 123.   182.  331.     75.1     185.     330.\n## 2 Andreo   M            2 136.   238.  367.    101.      232.     416.\n## 3 Buka     F            1  98.2  202.  234.     60.5     161.     296.\n## 4 Eva      F            1 104.   193.  352.     82.2     202.     355.\n## 5 Evita    F            1 104.   183.  365.     82.6     190.     367.\n## 6 Geronimo M            2 116.   286.  402.    125.      285.     502.\n\n\n\nStep 1\n\nReduce the dataset to just the two variables of interest.\n\n\nd &lt;- d |&gt;\n    select(sex, kernel95)\nhead(d)\n\n## # A tibble: 6 × 2\n##   sex   kernel95\n##   &lt;chr&gt;    &lt;dbl&gt;\n## 1 F         330.\n## 2 M         416.\n## 3 F         296.\n## 4 F         355.\n## 5 F         367.\n## 6 M         502.\n\n\n\n\nStep 2\n\nDetermine the mean, standard deviation, and standard error in “kernel95” home range size for each sex.\n\n\nhr_summary &lt;- d |&gt;\n    group_by(sex) |&gt;\n    summarize(mean = mean(kernel95), sd = sd(kernel95), n = n(), se = sd/sqrt(n))\nhr_summary\n\n## # A tibble: 2 × 5\n##   sex    mean    sd     n    se\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n## 1 F      319.  65.7    11  19.8\n## 2 M      430.  58.3     9  19.4\n\n\n\n\nStep 3\n\nCreate boxplots comparing “kernel95” home range size by sex.\n\n\np &lt;- ggplot(data = d, aes(x = sex, y = kernel95)) + geom_boxplot() + geom_jitter()\np\n\n\n\n\n\n\n\n\n\n\nStep 4\n\nFor each sex, generate a bootstrap distribution for mean kernel95 home range size.\n\n\nNOTE: The code below does this for males… simply replace “M” with “F” in the filter() statement to do the same for females.\n\n\n# ci in mean HR size\nn_boot &lt;- 10000\ns &lt;- d |&gt;\n    filter(sex == \"M\")\n\n# option 1 - using {base} R\nboot &lt;- vector()\n# the size of each bootstrap sample should equivalent to the size our original\n# sample\nfor (i in 1:n_boot) {\n    boot[[i]] &lt;- mean(sample(s$kernel95, length(s$kernel95), replace = TRUE))\n}\n\n# option 2 - using {mosaic}\nboot &lt;- do(n_boot) * mean(sample(s$kernel95, size = length(s$kernel95), replace = TRUE))\nboot &lt;- boot$mean  # pull out mean column as vector\n\n# option 3 - using {infer}\nboot &lt;- s %&gt;%\n    rep_sample_n(replace = TRUE, size = nrow(.), reps = 10000) |&gt;\n    group_by(replicate) |&gt;\n    summarize(mean = mean(kernel95))\nboot &lt;- boot$mean  # pull out mean column as vector\n\n\n\nStep 5\n\nPlot the resulting bootstrap sampling distribution and plot an appropriate normal distribution over the bootstrap sampling distribution.\n\n\nse &lt;- sd(boot)\nhistogram(boot)\nplotDist(\"norm\", mean(boot), se, add = TRUE)\n\n\n\n\n\n\n\n\n\n\nStep 6\n\nCalculate a 95% confidence interval around for the mean kernel95 home range size for each sex…\n\nUsing the quantile() method applied directly to your bootstrap sampling distribution, and\nUsing the theory-based “standard error” method, based on qnorm() and the standard deviation of your bootstrap sampling distribution.\n\n\n\n(ci_boot &lt;- c(quantile(boot, 0.025), quantile(boot, 0.975)))\n\n##     2.5%    97.5% \n## 394.4302 465.6848\n\n(ci_theory &lt;- mean(s$kernel95) + c(-1, 1) * qnorm(0.975) * se)\n\n## [1] 393.9312 465.5502\n\n# or\n\n(ci_theory &lt;- mean(s$kernel95) + qnorm(c(0.025, 0.975)) * se)\n\n## [1] 393.9312 465.5502\n\nladd(panel.abline(v = ci_boot, col = \"red\", lty = 3, lwd = 2))\n\nladd(panel.abline(v = ci_theory, col = \"blue\", lty = 1, lwd = 2))\n\n\n\n\n\n\n\n\nWe can also do all of the above using the {infer} package’s specify() → generate() → calculate() → visualize() workflow…\n\n# option 4 - using {infer}\nboot &lt;- s |&gt;\n    specify(response = kernel95) |&gt;\n    generate(reps = n_boot, type = \"bootstrap\") |&gt;\n    calculate(stat = \"mean\")\n\nci_boot &lt;- boot |&gt;\n    get_confidence_interval(type = \"percentile\", level = 0.95)\n\nci_theory &lt;- boot |&gt;\n    get_confidence_interval(type = \"se\", level = 0.95, point_estimate = mean(s$kernel95))\n\nvisualize(boot) + shade_confidence_interval(endpoints = ci_theory, color = \"blue\",\n    lty = 1, size = 0.5, fill = \"#c0c0c0\") + shade_confidence_interval(endpoints = ci_boot,\n    color = \"red\", lty = 3, size = 0.5, fill = \"#c0c0c0\")\n\n\n\n\n\n\n\n\n\n\nStep 7\n\nUse simulation-based permutation to evaluate the difference in mean kernel95 home range size for males versus females. To do this, you will want to shuffle either the variable “sex” or “kernel95” home range size a total of 10,000 times, recalculating mean kernel95 size by sex for each permuted sample and then compare the difference in male and female kernel95 means from your original sample to the permutation distribution for the difference in means.\n\n\nn_perm &lt;- 10000  # number of permutations\n# create a dummy vector to hold results for each permutation\npermuted_diff &lt;- vector()\npermuted_data &lt;- d\nfor (i in 1:n_perm) {\n    # scramble the sex vector: `sample()` with a vector as an argument yields a\n    # random permutation of the vector\n    permuted_data$sex &lt;- sample(permuted_data$sex)\n    m &lt;- permuted_data[permuted_data$sex == \"M\", ]$kernel95\n    f &lt;- permuted_data[permuted_data$sex == \"F\", ]$kernel95\n    permuted_diff[[i]] &lt;- mean(m) - mean(f)\n}\nhistogram(permuted_diff)\n\n\n\n\n\n\n\nactual_diff &lt;- mean(d[d$sex == \"M\", ]$kernel95) - mean(d[d$sex == \"F\", ]$kernel95)\n\np &lt;- (sum(permuted_diff &gt;= abs(actual_diff)) + sum(permuted_diff &lt;= -abs(actual_diff)))/n_perm\np\n\n## [1] 0.0011\n\nladd(panel.abline(v = actual_diff, col = \"red\", lty = 3, lwd = 2))\n\n\n\n\n\n\n\n\nAgain, we can do this whole process using the {infer} package workflow…\n\nnull_distribution &lt;- d |&gt;\n    specify(formula = kernel95 ~ sex) |&gt;\n    hypothesize(null = \"independence\") |&gt;\n    generate(reps = n_perm, type = \"permute\") |&gt;\n    calculate(stat = \"diff in means\", order = c(\"M\", \"F\"))\n\nactual_diff &lt;- d |&gt;\n    specify(formula = kernel95 ~ sex) |&gt;\n    calculate(stat = \"diff in means\", order = c(\"M\", \"F\"))\nactual_diff\n\n## Response: kernel95 (numeric)\n## Explanatory: sex (factor)\n## # A tibble: 1 × 1\n##    stat\n##   &lt;dbl&gt;\n## 1  111.\n\nnull_distribution |&gt;\n    get_p_value(obs_stat = actual_diff, direction = \"both\")\n\n## # A tibble: 1 × 1\n##   p_value\n##     &lt;dbl&gt;\n## 1  0.0008\n\nvisualize(null_distribution) + shade_p_value(obs_stat = actual_diff, lty = 1, size = 0.5,\n    fill = \"#c0c0c0\", direction = \"both\")\n\n\n\n\n\n\n\n\n\nUnder this approach, what is the “null” hypothesis? What is the test statistic? Is the difference in mean kernel95 home range size “significant”?\n\nThe null hypothesis here is that the difference in mean kernel95 home range size for males versus females is zero. The test statistic is the actual difference in mean kernel95 home range size for males versus females, and it is evaluated relative to a permutation distribution for this statistic. Based on our very low p value (less than 0.001), under a null hypothesis significance testing framework, we would conclude that the difference is “significant” (assuming that p is less than the alpha level we specify).\n\n\nStep 8\n\nFinally, use a theory-based parametric test (e.g., a t-test) to also calculate an appropriate test statistic and associated p value for the comparison of male and female mean kernel95 home range size. Is the difference in mean kernel95 home range size “significant”?\n\n\nf &lt;- d |&gt;\n    filter(sex == \"F\")\nm &lt;- d |&gt;\n    filter(sex == \"M\")\n\n# first, what is ratio of variances in our two samples?\nvar_f &lt;- var(f$kernel95)\nvar_m &lt;- var(m$kernel95)\nvar_f/var_m\n\n## [1] 1.272656\n\n# ratio is less than 2.0, so we can use equal variance version of t test...\n\nnum_f &lt;- nrow(f)\nnum_m &lt;- nrow(m)\n\nmean_f &lt;- mean(f$kernel95)\nmean_m &lt;- mean(m$kernel95)\n\n# hand-calculate the test statistic and p value...\ns2 &lt;- ((num_m - 1) * var_m + (num_f - 1) * var_f)/(num_m + num_f - 2)\nt_stat &lt;- (mean_m - mean_f)/sqrt(s2 * (1/num_m + 1/num_f))\nt_stat\n\n## [1] 3.94816\n\ndf &lt;- num_m + num_f - 2\np &lt;- 2 * (1 - pt(t_stat, df))\np\n\n## [1] 0.0009426349\n\n# or use the t.test() function\nt_test &lt;- t.test(x = m$kernel95, y = f$kernel95, var.equal = TRUE)\nt_test\n\n## \n##  Two Sample t-test\n## \n## data:  m$kernel95 and f$kernel95\n## t = 3.9482, df = 18, p-value = 0.0009426\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##   51.91339 169.99890\n## sample estimates:\n## mean of x mean of y \n##  429.7407  318.7846\n\n\n\nUnder this approach, what is the test statistic? Is the difference in mean kernel95 home range size “significant”?\n\nHere, the null hypothesis again is that the difference in mean kernel95 home range size for males versus females is zero. The test statistic is the actual difference in means scaled by something equivalent to a standard error that takes into account the variance and size of our two samples. This test statistic is then evaluated relative to a t distribution that depends on the number of degrees of freedom, which depends on the sample size of our two samples. Again, based on the very low p value, we would conclude that the difference is “significant” under a null hypothesis significance testing framework (assuming that p is less than the alpha level we specify).",
    "crumbs": [
      "Exercises",
      "Exercise 06 Solution"
    ]
  },
  {
    "objectID": "exercise-07.html",
    "href": "exercise-07.html",
    "title": "Exercise 07",
    "section": "",
    "text": "Explore Distributions and the CLT",
    "crumbs": [
      "Exercises",
      "Exercise 07"
    ]
  },
  {
    "objectID": "exercise-07.html#learning-objectives",
    "href": "exercise-07.html#learning-objectives",
    "title": "Exercise 07",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nPlotting various mathematical distributions\nSampling from distributions that are distinctly non-normal and generating summary statistics\nVisualizing sampling distributions to see the Central Limit Theorem in action\n\n\nlibrary(tidyverse)\nlibrary(infer)  # for rep_sample_n()\nlibrary(ggformula)  # for gf_ functions\nlibrary(cowplot)  # for plot_grid()\nlibrary(mosaic)  # for do() * &lt;function&gt;\nlibrary(kableExtra)  # for kable_styling()\n\nClassic probability and statistical theory - and many parametric statistical tests - assume that the distributions of variables of interest (either things we measure/record about our subjects of study or sample statistics we derived from those measurements) follow certain well-characterized mathematical distributions. For example, when we imagine rolling an unbiased die, we typically assume that we have an equal (i.e., uniform) chance of seeing any given number come up, and when we imagine sampling any particular metric trait from a population, we typically assume that the distribution of that trait in a nature population follows a normal, or Gaussian, distribution (e.g., height). Likewise, we expect or assume that other well-characterized mathematical distributions are appropriate models for the outcomes of other sampling processes. The shape of any of these distributions is governed entirely by a function and one or more tuning parameters for that function.",
    "crumbs": [
      "Exercises",
      "Exercise 07"
    ]
  },
  {
    "objectID": "exercise-07.html#plotting-distributions",
    "href": "exercise-07.html#plotting-distributions",
    "title": "Exercise 07",
    "section": "Plotting Distributions",
    "text": "Plotting Distributions\nBelow is some code we can use to plot some example distributions and visualize how the shape of those distributions changes with different parameter values…\n\n# some continuous distributions - normal, beta, uniform, F, and Chi Square...\nnorm1 &lt;- gf_dist(\"norm\", mean = 2, sd = 1)\nnorm2 &lt;- gf_dist(\"norm\", mean = 30, sd = 15)\nnorm3 &lt;- gf_dist(\"norm\", mean = 100, sd = 15)\nbeta1 &lt;- gf_dist(\"beta\", shape1 = 1, shape2 = 10, xlim = c(-0.1, 1.1))\nbeta2 &lt;- gf_dist(\"beta\", shape1 = 2, shape2 = 10, xlim = c(-0.1, 1.1))\nbeta3 &lt;- gf_dist(\"beta\", shape1 = 3, shape2 = 1, xlim = c(-0.1, 1.1))\nunif1 &lt;- gf_dist(\"unif\", min = 1, max = 2)\nunif2 &lt;- gf_dist(\"unif\", min = 10, max = 25)\nunif3 &lt;- gf_dist(\"unif\", min = 130, max = 240)\nf1 &lt;- gf_dist(\"f\", df1 = 4, df2 = 15)\nf2 &lt;- gf_dist(\"f\", df1 = 4, df2 = 99)\nf3 &lt;- gf_dist(\"f\", df1 = 30, df2 = 199)\nchisq1 &lt;- gf_dist(\"chisq\", df = 2)\nchisq2 &lt;- gf_dist(\"chisq\", df = 3)\nchisq3 &lt;- gf_dist(\"chisq\", df = 20)\n\n# some discrete distributions - Poisson,binomial, negative binomial...\npois1 &lt;- gf_dist(\"pois\", lambda = 3)\npois2 &lt;- gf_dist(\"pois\", lambda = 10)\npois3 &lt;- gf_dist(\"pois\", lambda = 35)\nbinom1 &lt;- gf_dist(\"binom\", size = 10, prob = 0.5, xlim = c(0, 10))\nbinom2 &lt;- gf_dist(\"binom\", size = 20, prob = 0.5, xlim = c(0, 20))\nbinom3 &lt;- gf_dist(\"binom\", size = 20, prob = 0.1, xlim = c(0, 20))\nnbinom1 &lt;- gf_dist(\"nbinom\", size = 5, prob = 0.5, xlim = c(0, 20))\nnbinom2 &lt;- gf_dist(\"nbinom\", size = 10, prob = 0.5, xlim = c(0, 20))\nnbinom3 &lt;- gf_dist(\"nbinom\", size = 15, prob = 0.5, xlim = c(0, 20))\n\n# note that in the code above, we could instead use the `plotDist()` function\n# from {mosaic} this example uses `gf_dist()` simply to create .gg (ggplot)\n# objects rather than .trellis (lattice) objects, which makes visualizing the\n# plots together using {cowplot} look better\n\nrow1 &lt;- plot_grid(norm1, norm2, norm3, nrow = 1) + draw_plot_label(label = \"Normal\",\n    fontface = \"bold\", size = 12, hjust = 0, vjust = -0.5)\nrow2 &lt;- plot_grid(beta1, beta2, beta3, nrow = 1) + draw_plot_label(label = \"Beta\",\n    fontface = \"bold\", size = 12, hjust = 0, vjust = -0.5)\nrow3 &lt;- plot_grid(unif1, unif2, unif3, nrow = 1) + draw_plot_label(label = \"Uniform\",\n    fontface = \"bold\", size = 12, hjust = 0, vjust = -0.5)\nrow4 &lt;- plot_grid(chisq1, chisq2, chisq3, nrow = 1) + draw_plot_label(label = \"Chi Sq\",\n    fontface = \"bold\", size = 12, hjust = 0, vjust = -0.5)\nrow5 &lt;- plot_grid(f1, f2, f3, nrow = 1) + draw_plot_label(label = \"F\", fontface = \"bold\",\n    size = 12, hjust = 0, vjust = -0.5)\n\nblank &lt;- ggplot() + theme_nothing()  # this is just for spacing at the top of the plot\nplot1 &lt;- plot_grid(blank, row1, row2, row3, row4, row5, nrow = 6, rel_heights = c(0.25,\n    1, 1, 1, 1, 1))\nplot1\n\n\n\n\n\n\n\nrow1 &lt;- plot_grid(pois1, pois2, pois3, nrow = 1) + draw_plot_label(label = \"Poisson\",\n    fontface = \"bold\", size = 12, hjust = 0, vjust = -0.5)\nrow2 &lt;- plot_grid(binom1, binom2, binom3, nrow = 1) + draw_plot_label(label = \"Binomial\",\n    fontface = \"bold\", size = 12, hjust = 0, vjust = -0.5)\nrow3 &lt;- plot_grid(nbinom1, nbinom2, nbinom3, nrow = 1) + draw_plot_label(label = \"Negative Binomial\",\n    fontface = \"bold\", size = 12, hjust = 0, vjust = -0.5)\n\nblank &lt;- ggplot() + theme_nothing()  # this is just for spacing at the top of the plot\nplot2 &lt;- plot_grid(blank, row1, row2, row3, nrow = 4, rel_heights = c(0.25, 1, 1,\n    1))\nplot2",
    "crumbs": [
      "Exercises",
      "Exercise 07"
    ]
  },
  {
    "objectID": "exercise-07.html#sampling-distributions-and-the-clt",
    "href": "exercise-07.html#sampling-distributions-and-the-clt",
    "title": "Exercise 07",
    "section": "Sampling Distributions and the CLT",
    "text": "Sampling Distributions and the CLT\nBelow are some examples of [1] drawing random sets of observations from several of these distributions, [2] calculating summary statistics (e.g., means) for each sample, and [3] repeating this process multiple times to generate sampling distributions for these summary statistics. Each of the first three snippets of code below first draws and plots a single sample of size n from a particular distribution, plots a histogram of that sample, and superimposes the distribution is is drawn from (black curve) and the mean of the sample (red line). The last two snippets do the same, but instead plots selected quantile values as example of alternative summary statistics. These are the left-hand plots in the resulting 10-panel figure.\nEach snippet also then draws reps separate samples from the same distributions, calculates the same summary statistics, and plots the resultant sampling distributions of those statistics with a superimposed normal distribution, demonstrating the Central Limit Theorem (CLT). These are the right-hand plots in the resulting 10-panel figure.\nAs a refresher, recall that CLT states that the sampling distribution of a sample mean (and many other sample statistics) is approximately normal if the sample size is large enough, even if the population distribution that the sample is drawn from is not normal.\nAdditionally, the CLT also states that the sampling distribution should have the following properties:\n\nThe mean of the sampling distribution will be equal to the mean of the population distribution.\n\n\\[\\bar{x} = \\mu\\]\n\nThe standard deviation of the sampling distribution will be equal to the standard deviation of the population distribution divided by the square root of the sample size. This is the standard error.\n\n\\[s = \\frac{\\sigma}{\\sqrt{n}}\\]\n\n# sample size\nn &lt;- 10\n# number of replicates\nreps &lt;- 1000\n\n# normal distribution generate 1 sample of size n...\nx &lt;- rnorm(n, mean = 2, sd = 1)\n# and plot it along with the distribution it was drawn from and the mean of the\n# sample\na &lt;- gf_dhistogram(~x) |&gt;\n    gf_dist(\"norm\", mean = 2, sd = 1) |&gt;\n    gf_vline(xintercept = ~c(mean(x)))\n\n# generate a sampling distribution for the sample mean based on *reps* samples\n# of size *n* and put it in a vector...\nn_mean &lt;- tibble(do(reps) * mean(rnorm(n, mean = 2, sd = 1))) |&gt;\n    pull(mean)\n# and plot it along with the sampling distribution and the mean of the sample\nb &lt;- gf_dhistogram(~n_mean, bins = 30) |&gt;\n    gf_dist(\"norm\", mean = mean(n_mean), sd = sd(n_mean)) |&gt;\n    gf_vline(xintercept = ~c(mean(n_mean)))\n\nrow1 &lt;- plot_grid(a, b)\n\n# beta distribution generate 1 sample of size n...\nx &lt;- rbeta(reps, shape1 = 2, shape2 = 10)\n# and plot it along with the distribution it was drawn from and the mean of the\n# sample\na &lt;- gf_dhistogram(~x) |&gt;\n    gf_dist(\"beta\", shape1 = 2, shape2 = 10) |&gt;\n    gf_vline(xintercept = ~c(mean(x)))\n\n# generate a sampling distribution for the sample mean based on *reps* samples\n# of size *n* and put it in a vector...\nb_mean &lt;- tibble(do(reps) * mean(rbeta(reps, shape1 = 2, shape2 = 10))) |&gt;\n    pull(mean)\n# and plot it along with the sampling distribution and the mean of the sample\nb &lt;- gf_dhistogram(~b_mean) |&gt;\n    gf_dist(\"norm\", mean = mean(b_mean), sd = sd(b_mean)) |&gt;\n    gf_vline(xintercept = ~c(mean(b_mean)))\n\nrow2 &lt;- plot_grid(a, b)\n\n# uniform distribution generate 1 sample of size n...\nx &lt;- runif(reps, min = 1, max = 2)\n# and plot it along with the distribution it was drawn from and the mean of the\n# sample\na &lt;- gf_dhistogram(~x) |&gt;\n    gf_dist(\"unif\", min = 1, max = 2) |&gt;\n    gf_vline(xintercept = ~c(mean(x)))\n\n# generate a sampling distribution for the sample mean based on *reps* samples\n# of size *n* and put it in a vector...\nu_mean &lt;- tibble(do(reps) * mean(runif(reps, min = 1, max = 2))) |&gt;\n    pull(mean)\n# and plot it along with the sampling distribution and the mean of the sample\nb &lt;- gf_dhistogram(~u_mean) |&gt;\n    gf_dist(\"norm\", mean = mean(u_mean), sd = sd(u_mean)) |&gt;\n    gf_vline(xintercept = ~c(mean(u_mean)))\n\nrow3 &lt;- plot_grid(a, b)\n\n# normal distribution... with a different statistic generate 1 sample of size\n# n...\nx &lt;- rnorm(n, mean = 2, sd = 1)\n# and plot it along with the distribution it was drawn from and the mean of the\n# sample\na &lt;- gf_dhistogram(~x) |&gt;\n    gf_dist(\"norm\", mean = 2, sd = 1) |&gt;\n    gf_vline(xintercept = ~c(quantile(x, 0.025)))\n\n# generate a sampling distribution for the sample mean based on *reps* samples\n# of size *n* and put it in a vector...\nn_mean &lt;- tibble(do(reps) * quantile(rnorm(n, mean = 2, sd = 1), 0.025)) |&gt;\n    pull(X2.5.)\n# and plot it along with the sampling distribution and the mean of the sample\nb &lt;- gf_dhistogram(~n_mean, bins = 30) |&gt;\n    gf_dist(\"norm\", mean = mean(n_mean), sd = sd(n_mean)) |&gt;\n    gf_vline(xintercept = ~c(mean(n_mean)))\n\nrow4 &lt;- plot_grid(a, b)\n\n# uniform distribution... with a different statistic generate 1 sample of size\n# n...\nx &lt;- runif(reps, min = 1, max = 2)\n# and plot it along with the distribution it was drawn from and the 0.25\n# quantile of the sample\na &lt;- gf_dhistogram(~x) |&gt;\n    gf_dist(\"unif\", min = 1, max = 2) |&gt;\n    gf_vline(xintercept = ~c(quantile(x, 0.25)))\n\n# generate a sampling distribution for the 0.25 quantile based on *reps*\n# samples of size *n* and put it in a vector...\nu_mean &lt;- tibble(do(reps) * quantile(runif(reps, min = 1, max = 2), 0.25)) |&gt;\n    pull(X25.)\n# and plot it along with the sampling distribution and the mean of the sample\nb &lt;- gf_dhistogram(~u_mean) |&gt;\n    gf_dist(\"norm\", mean = mean(u_mean), sd = sd(u_mean)) |&gt;\n    gf_vline(xintercept = ~c(mean(u_mean)))\n\nrow5 &lt;- plot_grid(a, b)\n\nplot3 &lt;- plot_grid(row1, row2, row3, row4, row5, nrow = 5)\nplot3",
    "crumbs": [
      "Exercises",
      "Exercise 07"
    ]
  },
  {
    "objectID": "exercise-07.html#generating-cis-around-a-statistic",
    "href": "exercise-07.html#generating-cis-around-a-statistic",
    "title": "Exercise 07",
    "section": "Generating CIs around a Statistic",
    "text": "Generating CIs around a Statistic\nThe following code draws a single sample of size n from a normal distribution (as above) and calculates the mean and standard deviation of that sample and estimates the standard error of the mean. It then generates several different estimates for a 95% confidence interval around that sample mean:\n\nBased on bootstrap resampling 10,000 times from the original sample and using quantiles from the resulting bootstrap sampling distribution to define the lower and upper bounds of the CI\nUsing the standard deviation of the bootstrap sampling distribution, along with the original sample mean, to generate a theory-based bootstrap CI, presuming that the shape of bootstrap sampling distribution is normal\nUsing the estimate of the standard error generated from the original sample, along with the original sample mean, to generate a different theory-based CI, assuming that the shape of sampling distribution is normal\nUsing the estimate of the standard error generated from the original sample, along with the original sample mean, to generate an alternative theory-based CI, but presuming that the shape of sampling distribution is better modeled as a t-distribution\n\n\n# sample size\nn &lt;- 10\n# number of replicates\nreps &lt;- 10000\n\n# normal distribution generate 1 sample of size n...\nx &lt;- rnorm(n, mean = 2, sd = 1)\nmean(x)\n\n## [1] 2.134202\n\n# close to, but not the same, as the population mean\nse &lt;- sd(x)/sqrt(length(x))\n# estimate of se based on 1 random sample equivalent to...\nse &lt;- sciplot::se(x)\nse_pop &lt;- 1/sqrt(length(x))  # theoretical se calculated from known population sd\n\nboot &lt;- vector()  # set up a dummy variable to hold our bootstrap simulations\n# bootstrap sample size should be the same length as our sample data\nfor (i in 1:reps) {\n    boot[[i]] &lt;- mean(sample(x, length(x), replace = TRUE))\n}\n\n# or...\nboot &lt;- tibble(x) |&gt;\n    rep_sample_n(size = length(x), replace = TRUE, reps = reps) |&gt;\n    group_by(replicate) |&gt;\n    summarize(mean = mean(x)) |&gt;\n    pull(mean)\n\n# or...\nboot &lt;- do(reps) * mean(sample(x, length(x), replace = TRUE))\nboot &lt;- boot$mean\n\n\n# plot a histogram of our bootstrapped sample means with normal curve and\n# original sample mean superimposed\nplot4 &lt;- gf_dhistogram(~boot, title = \"Bootstrap Sampling Distribution\", xlab = \"x\") |&gt;\n    gf_dist(\"norm\", mean = mean(boot), sd = sd(boot)) |&gt;\n    gf_vline(xintercept = ~c(mean(x)))  # mean of our original vector of samples\nplot4\n\n\n\n\n\n\n\n# ci bounds inferred from quantiles of bootstrap distribution\nci.boot.quantiles &lt;- quantile(boot, c(0.025, 0.975))\n\n# ci bounds inferred from original sample mean and sd of bootstrap sampling\n# distribution\nci.boot.theory &lt;- qnorm(c(0.025, 0.975), mean = mean(x), sd = sd(boot))  # 0.025 and 0.975 quantiles of normal with mean and sd of boot\n# equivalent to...  mean + quantiles of standard normal times sd of bootstrap\n# sampling distribution (= standard error)\nci.boot.theory &lt;- mean(x) + qnorm(c(0.025, 0.975), 0, 1) * sd(boot)  # 1.96 SE above and below the mean\n\n# ci bounds inferred from original sample mean + quantiles of standard normal\n# times standard error\nci.norm.theory &lt;- mean(x) + qnorm(c(0.025, 0.975), 0, 1) * se\n# equivalent to...\nci.norm.theory &lt;- mean(x) + c(-1, 1) * qnorm(0.975) * se\n\n# ci bounds inferred from original sample mean + quantiles of t distribution\n# times standard error\nci.t.theory &lt;- mean(x) + qt(c(0.025, 0.975), df = length(x) - 1) * se\n# equivalent to...\nci.t.theory &lt;- mean(x) + c(-1, 1) * qt(0.975, df = length(x) - 1) * se\n\ncomparison &lt;- rbind(ci.boot.quantiles, ci.boot.theory, ci.norm.theory, ci.t.theory)\nkable(comparison, digits = 3) |&gt;\n    kable_styling(font_size = 14, full_width = FALSE)\n\n\n\n\n\n2.5%\n97.5%\n\n\n\n\nci.boot.quantiles\n1.568\n2.595\n\n\nci.boot.theory\n1.611\n2.657\n\n\nci.norm.theory\n1.583\n2.685\n\n\nci.t.theory\n1.498\n2.770\n\n\n\n\n\n\nplot4 &lt;- plot4 |&gt;\n    gf_vline(xintercept = ~ci.boot.quantiles, color = \"blue\") |&gt;\n    gf_vline(xintercept = ~ci.boot.theory, color = \"red\") |&gt;\n    gf_vline(xintercept = ~ci.norm.theory, color = \"green\") |&gt;\n    gf_vline(xintercept = ~ci.t.theory, color = \"purple\")\nplot4",
    "crumbs": [
      "Exercises",
      "Exercise 07"
    ]
  },
  {
    "objectID": "exercise-08.html",
    "href": "exercise-08.html",
    "title": "Exercise 08",
    "section": "",
    "text": "Practice Simple Linear Regression",
    "crumbs": [
      "Exercises",
      "Exercise 08"
    ]
  },
  {
    "objectID": "exercise-08.html#learning-objectives",
    "href": "exercise-08.html#learning-objectives",
    "title": "Exercise 08",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nExplore a published comparative dataset on primate group size, brain size, and life history variables\nConduct simple linear regression analyses with this dataset where you:\n\nGenerate regression coefficients by hand\nUse existing R functions and “formula notation” to generate regression coefficients for simple regression models\nCalculate theory-based standard error estimates and p values for the regression coefficients by hand and also extract these from model summaries\nGenerate theory-based confidence intervals for the regression coefficients by hand and also extract these from model summaries\nGenerate confidence intervals for regression coefficients and p values for regression coefficients using permutation/bootstrapping methods\n\n\n\nData source:\nStreet SE, Navarrete AF, Reader SM, and Laland KN. (2017). Coevolution of cultural intelligence, extended life history, sociality, and brain size in primates. Proceedings of the National Academy of Sciences 114: 7908–7914.\n\n\nPreliminaries\n\nSet up a new GitHub repo in your GitHub workspace named “exercise-09” and clone that down to your computer as a new RStudio project. The instructions outlined as Method 1 in Module 6 will be helpful.\n\n\n\nStep 1\n\nUsing the {tidyverse} read_csv() function, load the “Street_et_al_2017.csv” dataset from this URL as a “tibble” named d.\nDo a quick exploratory data analysis where you generate the five-number summary (median, minimum and maximum and 1st and 3rd quartile values), plus mean and standard deviation, for each quantitative variable.\n\n\nHINT: The skim() function from the package {skimr} makes this very easy!\n\n\n\nStep 2\n\nFrom this dataset, plot brain size (ECV) as a function of social group size (Group_size), longevity (Longevity), juvenile period length (Weaning), and reproductive lifespan (Repro_lifespan).\n\n\n\nStep 3\n\nDerive by hand the ordinary least squares regression coefficients \\(\\beta1\\) and \\(\\beta0\\) for ECV as a function of social group size.\n\n\nHINT: You will need to remove rows from your dataset where one of these variables is missing.\n\n\n\nStep 4\n\nConfirm that you get the same results using the lm() function.\n\n\n\nStep 5\n\nRepeat the analysis above for three different major radiations of primates - “catarrhines”, “platyrrhines”, and “strepsirhines”) separately. These are stored in the variable Taxonomic_group. Do your regression coefficients differ among groups? How might you determine this?\n\n\n\nStep 6\n\nFor your first regression of ECV on social group size, calculate the standard error for the slope coefficient, the 95% CI, and the p value associated with this coefficient by hand. Also extract this same information from the results of running the lm() function.\n\n\n\nStep 7\n\nUse a permutation approach with 1000 permutations to generate a null sampling distribution for the slope coefficient. What is it that you need to permute? What is the p value associated with your original slope coefficient? You can use either the quantile method (i.e., using quantiles from the actual permutation-based null sampling distribution) or a theory-based method (i.e., using the standard deviation of the permutation-based null sampling distribution as the estimate of the standard error, along with a normal or t distribution), or both, to calculate this p value.\n\n\n\nStep 8\n\nUse bootstrapping to generate a 95% CI for your estimate of the slope coefficient using both the quantile method and the theory-based method (i.e., using the standard deviation of the bootstrapped sampling distribution as an estimate of the standard error). Do these CIs suggest that your slope coefficient is different from zero?",
    "crumbs": [
      "Exercises",
      "Exercise 08"
    ]
  },
  {
    "objectID": "data-analysis-replication-assignment.html",
    "href": "data-analysis-replication-assignment.html",
    "title": "Data Analysis Replication",
    "section": "",
    "text": "Objectives\nThe objective of this assignment is to use your skills in R to replicate as closely as you can a set of statistical analyses and results reported in a paper of your choosing from the primary literature in your field.",
    "crumbs": [
      "Assignments",
      "Data Analysis Replication"
    ]
  },
  {
    "objectID": "data-analysis-replication-assignment.html#what-to-do",
    "href": "data-analysis-replication-assignment.html#what-to-do",
    "title": "Data Analysis Replication",
    "section": "What to Do",
    "text": "What to Do\nYou will need to create a new RStudio project and a “.qmd” or “.Rmd” report detailing your work on replicating the results presented in the paper you choose. You should start your reanalysis report with a text description of the study and of the specific data and reanalyses you will be doing, to orient your reader. Outline (briefly!) the goal of the original paper, the data set used, and the analyses conducted, then describe which ones you will replicate. You should also demonstrate how you read in any data file(s) and show a few lines of raw data in your output (e.g., using head()).\nNote that I will be looking for you to clearly take your reader through all of the elements of data manipulation, analysis, and, if appropriate, visualization. You should provide as much coding detail, explanation, and output tables as necessary to compare your results to those published!\nYou do not need to replicate ALL of the analyses presented in the paper (although the more the better)! At a bare minimum, you need to repeat at least three analyses, including at least one descriptive statistical analysis, one visualization, and one inferential statistical analysis.\nAs assessment, I will be looking at several different elements of your report and code for this assignment. Below, I outline some of the main things:\n\nOrganization and Logistics\n\nRepo set up on GitHub, named correctly, and shared with me (URL submitted via Canvas)\nPDF of paper included in the top level of the repository\n“.qmd” or “.Rmd” file for your report stored at the top level of the repo\n“.qmd” or “.Rmd” file includes text and code and embedded images/tables from the paper for comparison\n“.qmd” or “.Rmd” file is well organized into subsections, including blocks of explanatory text and R code blocks, plus output\nR code follows a consistent convention with respect to variable and function names and formatting\nFile(s) with original data are included in a directory (folder) called “data” within the repo\nImages from the original paper that are referenced in the report stored in a separate directory called “images” within the repo\n\nIntroduction and Framing\n\nReport includes a short, introductory description of the goal of the original paper, the data used, the analyses conducted, and the conclusions of the original study\nReport outlines the specific data and reanalyses you will be doing\nCode correctly loads all required packages not included in {base} R\n“.qmd” or “.Rmd” file renders and produces HTML output without requiring edits or additional modifications\n“.qmd” or “.Rmd” file includes a dictionary defining variable names used in dataset and R code\nR code successfully reads in data file(s) from the local “data” directory within the repo and shows a few lines of raw data (e.g., using head())\nR code successfully reads in any image file(s) for comparison from within the local “image” directory within the repo\n\nData Analysis/Visualization Replications\n\nFor each of the analyses/visualizations being done…\n\nText of the report clearly takes the reader through all of the elements of data manipulation/analysis/visualization, from raw data to presentation\nReport text is thorough and the R code is well-documented and provides as much explanation and output (tables, figures, etc.) as necessary to understand how the code works\nReport includes side-by-side comparisons of the results of the replication attempts to the published original results\n\n\nDiscussion and Reflection\n\nReport includes a narrative summary about how successful the analysis replications were or were not - i.e., how well do the replicated results compare to those presented in the original paper?\nReport discusses and addresses any challenges encountered… missing data, unclear information about how data were processed in the original publication, etc.\nReport discusses where and possible reasons why the analysis replications might differ from the authors’ original results",
    "crumbs": [
      "Assignments",
      "Data Analysis Replication"
    ]
  },
  {
    "objectID": "data-analysis-replication-assignment.html#what-to-turn-in",
    "href": "data-analysis-replication-assignment.html#what-to-turn-in",
    "title": "Data Analysis Replication",
    "section": "What to Turn In",
    "text": "What to Turn In\nStart a new GitHub repo and R project and using one of the methods outlined in Module 06. You should call it “data-analysis-replication”.\nThe top level of your repo should include a “.qmd” or “.Rmd” file called “data-analysis-replication.Rmd” where you thoroughly describe and run the code for all of the steps in your reanalysis. You can begin with the standard Quarto or RMarkdown document template created by choosing File &gt; New File and the desired file type and then choosing HTML as the “Default Output Format”. Be sure to remove any extraneous code that is included, by default, in the template.\nThe top level of your repository should also include a PDF copy of the paper you are reanalyzing data from.\nWithin the repository, make a “data” directory in which you include any “.csv” or “.xlsx” or similar files that contain the original data for the paper as you either downloaded them as supplementary material or received them from the paper’s author.\nWithin the repository, also make an “images” directory in which you include “.jpeg” or “.png” or similar files that show figures or tabular results from the original paper. In your “.qmd” or “.Rmd” file, near your own results, you will want to embed some of these results from the original paper that you replicate so that I can see them together. You can include code in one of the following formats to reference files in your “images” folder for inclusion in your document. The first alternative uses raw HTML in markdown, outside of a code block; the second and third accomplish the same thing using {knitr} in an {r} code block in either a Quarto (Alternative 2) or RMarkdown (Alternative 3) document.\nAlternative 1: Raw HTML\nThe line below is included in your markdown file, outside of a code block, positioned where you want the image to go.\n&lt;img src=\"images/imagename.filetype\" width=\"###px\"/&gt;\n\n```{r}\n# some code here...\n```\nAlternative 2: Quarto\nHere, the reference to the image is included within a code block.\n```{r}\n#| out-width: ###px\nknitr::include_graphics(\"images/imagename.filetype\")\n```\n… OR …\n```{r}\n#| out-width: \"##%\"\nknitr::include_graphics(\"images/imagename.filetype\")\n```\nAlternative 3: RMarkdown\nAgain, here, the reference to the image is included within a code block.\n```{r echo=FALSE, width=\"###px\"}\nknitr::include_graphics(\"images/imagename.filetype\")\n```\n… OR …\n```{r echo=FALSE, out.width=\"##%\"}\nknitr::include_graphics(\"images/imagename.filetype\")\n```\nIn each case, you would replace imagename.filetype with the name of your file, e.g., “figure-1.jpeg” and ## or ### with a integer number of pixels (e.g., 200px) or a percent of the window width (e.g., “70%”).\nYou shoudl also add the following line to the initial code chunk in your “.qmd” or “.Rmd” file, which tells {knitr} where to output all of the figures associated with your chunks of code as well as where to find any images that you want to include in your “.html” output.\nknitr::opts_chunk$set(fig.path = \"images/\")\nIf you wish, you can download either the file “data-analysis-replication-template.Rmd” or “data-analysis-replication-template.qmd” from https://github.com/difiore/ada-datasets and use that as a starting point for your file.\nBefore turning in this assignment, you should confirm that you can render your document successfully to HTML and make sure that the entire repo is pushed to GitHub repository as well. I should be able to clone your entire repo to my own computer, open the “.qmd” or “.Rmd” file, and render it to produce a nicely formatted “.html” report describing what you did and seeing your results.\nBy the due date for this assignment, you should push all of the above to your GitHub repository for this assignment. Then, in Canvas, please submit the URL for the repository (grabbed from the green CODE button on the repo’s base page) into the assignment submission text field.\n\nTL/DR: I should be able to CLONE your repository and knit your “.qmd” or “.Rmd” file to show all of your completed work for the data analysis replication assignment. Practically speaking, this means that if your code reads in data from external files, such as “.xlsx” or “.csv” files, it should be general enough to work on ANY machine. Thus, you will want to have your code read any data from local files housed within your repository , i.e., in the “data” folder described above. Your repository should also include a PDF of the original paper with the analyses being replicated at the top level of the repo. The structure of your repo thus should look pretty similar to what is shown below:\n\n\nLocal Repository\n\n\n\n\n\n\n\n\n\n\n\nRemote Repository",
    "crumbs": [
      "Assignments",
      "Data Analysis Replication"
    ]
  },
  {
    "objectID": "packages.html",
    "href": "packages.html",
    "title": "List of Packages Used",
    "section": "",
    "text": "By Module",
    "crumbs": [
      "References",
      "List of Packages Used"
    ]
  },
  {
    "objectID": "packages.html#by-module",
    "href": "packages.html#by-module",
    "title": "List of Packages Used",
    "section": "",
    "text": "Module 03\n\n{easypackages}: Sherman (2016)\n\n\n\nModule 05\n\n{usethis}: Wickham and Bryan (2020)\n\n\n\nModule 07\n\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n\n{ggplot2}: Wickham, Chang, et al. (2020), Wickham (2016)\n{tibble}: Müller and Wickham (2020)\n{tidyr}: Wickham and Henry (2020)\n{readr}: Wickham, Hester, and Francois (2018)\n{purrr}: Henry and Wickham (2020)\n{dplyr}: Wickham, François, Henry, and Müller (2020)\n{stringr}: Wickham (2019a)\n{forcats}: Wickham (2020))\n\n{data.table}: Dowle and Srinivasan (2019)\n\n\n\nModule 08\n\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{readxl}: Wickham and Bryan (2019)\n{XLConnect}: Mirai Solutions GmbH (2020)\n{gdata}: Warnes et al. (2017)\n{xlsx}: Dragulescu and Arendt (2020)\n{curl}: Ooms (2019)\n{rdrop2}: Ram and Yochum (2017)\n{repmis}: Gandrud (2016)\n{googlesheets4}: Bryan (2020)\n{googledrive}: D’Agostino McGowan and Bryan (2019)\n\n\n\nModule 09\n\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{magrittr}: Bache and Wickham (2014)\n{tidylog}: Elbers (2020)\n\n\n\nModule 10\n\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{curl}: Ooms (2019)\n{skimr}: Waring et al. (2020)\n{summarytools}: Comtois (2020)\n{dataMaid}: Petersen and Ekstrøm (2019a), Petersen and Ekstrøm (2019b)\n{psych}: Revelle (2020)\n{pastecs}: Grosjean and Ibanez (2018)\n{Hmisc}: Harrell (2020)\n{ggExtra}: Attali and Baker (2019)\n{car}: Fox, Weisberg, and Price (2020), Fox and Weisberg (2019)\n{GGally}: Schloerke et al. (2020)\n{corrplot}: Wei and Simko (2017a), Wei and Simko (2017b)\n{patchwork}: Pedersen (2019)\n{cowplot}: Wilke (2019)\n{gridExtra}: Auguie (2017)\n\n\n\nModule 11\n\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{sjmisc}: Lüdecke (2020), Lüdecke (2018)\n\n\n\nModule 12\n\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{moments}: Komsta and Novomestky (2022)\n{mosaic}: Pruim, Kaplan, and Horton (2020), Pruim, Kaplan, and Horton (2017)\n{radiant}: Nijs (2020)\n{sciplot}: Morales, with code developed by the R Development Core Team, and with general advice from the R-help listserv community and especially Duncan Murdoch. (2020)\n\n\n\nModule 13\n\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{mosaic}: Pruim et al. (2020), Pruim et al. (2017)\n{cowplot}: Wilke (2019)\n{manipulate}: Allaire (2014)\n\n\n\nModule 14\n\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{mosaic}: Pruim et al. (2020), Pruim et al. (2017)\n{manipulate}: Allaire (2014)\n{boot}: Canty and Ripley (2020), Davison and Hinkley (1997)\n\n\n\nModule 15\n\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{mosaic}: Pruim et al. (2020), Pruim et al. (2017)\n\n\n\nModule 16\n\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{mosaic}: Pruim et al. (2020), Pruim et al. (2017)\n{coin}: Hothorn, Winell, Hornik, van de Wiel, and Zeileis (2019), Hothorn, Hornik, van de Wiel, and Zeileis (2006), Hothorn, Hornik, van de Wiel, and Zeileis (2008)\n{jmuOutlier}: Garren (2019)\n{infer}: Bray, Ismay, Chasnovski, Baumer, and Cetinkaya-Rundel (2019)\n\n\n\nModule 17\n\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{manipulate}: Allaire (2014)\n\n\n\nModule 18\n\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{manipulate}: Allaire (2014)\n{patchwork}: Pedersen (2019)\n{infer}: Bray et al. (2019)\n{broom}: Robinson and Hayes (2020)\n{lmodel2}: Legendre (2018)\n\n\n\nModule 19\n\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{car}: Fox et al. (2020), Fox and Weisberg (2019)\n{ggpubr}: @-R-ggpubr\n\n\n\nModule 20\n\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{car}: Fox et al. (2020), Fox and Weisberg (2019)\n{broom}: Robinson and Hayes (2020)\n{coin}: Hothorn, Winell, et al. (2019), Hothorn et al. (2006), Hothorn et al. (2008)\n{infer}: Bray et al. (2019)\n{permuco}: Frossard and Renaud (2019)\n{dunn.test}: Dinno (2017b)\n{conover.test}: Dinno (2017a)\n{effectsize}: Ben-Shachar, Lüdecke, and Makowski (2020a), Ben-Shachar, Lüdecke, and Makowski (2020b)\n\n\n\nModule 21\n\n{jtools}: Long (2020)\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{broom}: Robinson and Hayes (2020)\n{car}: Fox et al. (2020), Fox and Weisberg (2019)\n{gridExtra}: Auguie (2017)\n{effects}: Fox and Hong (2009a), Fox and Hong (2009b)\n\n\n\nModule 22\n\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{MASS}: Ripley (2019), Venables and Ripley (2002)\n{AICcmodavg}: Mazerolle and portions of code contributed by Dan Linden. (2019)\n{MuMIn}: Bartoń (2020)\n\n\n\nModule 23\n\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{broom}: Robinson and Hayes (2020)\n{patchwork}: Pedersen (2019)\n{lmtest}: Hothorn, Zeileis, Farebrother, and Cummins (2019), Zeileis and Hothorn (2002)\n\n\n\nModule 24\n\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{lmtest}: Hothorn, Zeileis, et al. (2019), Zeileis and Hothorn (2002)\n{AICcmodavg}: Mazerolle and portions of code contributed by Dan Linden. (2019)\n{lme4}: Bates, Maechler, Bolker, and Walker (2020), Bates, Mächler, Bolker, and Walker (2015)\n{redres}: Goode, McClernon, Zhao, Zhang, and Huo. (2024)\n{effects}: Fox and Hong (2009a), Fox and Hong (2009b)\n{cowplot}: Wilke (2019)\n{sjPlots}: Lüdecke (2023)\n{MuMIn}: Bartoń (2020)\n{glmmML}: Broström (2020)\n{MASS}: Ripley (2019)\n\n\n\nModule 25\n\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{devtools}: Wickham, Hester, and Chang (2020)\n{usethis}: Wickham and Bryan (2020)\n{roxygen2}: Wickham, Danenberg, Csárdi, and Eugster (2020)\n{withr}: Hester, Müller, Ushey, Wickham, and Chang (2020)\n{manipulate}: Allaire (2014)\n\n\n\nModule 26\n\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{shiny}: Chang, Cheng, Allaire, Xie, and McPherson (2020)\n{DT}: Xie, Cheng, and Tan (2020)\n\n\n\nModule 27\n\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{broom}: Robinson and Hayes (2020)\n{reticulate}: Ushey, Allaire, and Tang (2020)",
    "crumbs": [
      "References",
      "List of Packages Used"
    ]
  },
  {
    "objectID": "packages.html#in-exercises",
    "href": "packages.html#in-exercises",
    "title": "List of Packages Used",
    "section": "In Exercises",
    "text": "In Exercises\n\n{emayili}: Collier (2021)\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{mailR}: Premraj (2021),\n{blastula}: Iannone and Cheng (2020)\n{usethis}: Wickham and Bryan (2020)\n{emo}: Wickham, François, and D’Agostino McGowan (2019)\n{sjmisc}: Lüdecke (2020)\n{huxtable}: Hugh-Jones (2021)\n{cowplot}: Wilke (2019)\n{mosaic}: Pruim et al. (2020), Pruim et al. (2017)\n{kableExtra}: Zhu (2019)\n{ggpubr}: Kassambara (2020)\n{infer}: Bray et al. (2019)",
    "crumbs": [
      "References",
      "List of Packages Used"
    ]
  },
  {
    "objectID": "packages.html#complete-list",
    "href": "packages.html#complete-list",
    "title": "List of Packages Used",
    "section": "Complete List",
    "text": "Complete List\n\n{AICcmodavg}: Mazerolle and portions of code contributed by Dan Linden. (2019)\n{BBmisc}: Bischl et al. (2017)\n{blastula}: Iannone and Cheng (2020)\n{boot}: Canty and Ripley (2020), Davison and Hinkley (1997)\n{broom}: Robinson and Hayes (2020)\n{car}: Fox et al. (2020), Fox and Weisberg (2019)\n{coin}: Hothorn, Winell, et al. (2019), Hothorn et al. (2006), Hothorn et al. (2008)\n{collape}: Krantz (2020)\n{conover.test}: Dinno (2017a)\n{corrplot}: Wei and Simko (2017a), Wei and Simko (2017b)\n{cowplot}: Wilke (2019)\n{curl}: Ooms (2019)\n{data.table}: Dowle and Srinivasan (2019)\n{dataMaid}: Petersen and Ekstrøm (2019a), Petersen and Ekstrøm (2019b)\n{devtools}: Wickham, Hester, et al. (2020)\n{dplyr}: Wickham, François, et al. (2020)\n{DT}: Xie et al. (2020)\n{dunn.test}: Dinno (2017b)\n{emayili}: Collier (2021)\n{easypackages}: Sherman (2016)\n{effects}: Fox and Hong (2009a), Fox and Hong (2009b)\n{effectsize}: Ben-Shachar et al. (2020a), Ben-Shachar et al. (2020b)\n{emo}: Wickham, François, et al. (2019)\n{forcats}: Wickham (2020)\n{gdata}: Warnes et al. (2017)\n{GGally}: Schloerke et al. (2020)\n{ggExtra}: Attali and Baker (2019)\n{ggplot2}: Wickham, Chang, et al. (2020), Wickham (2016)\n{ggpubr}: Kassambara (2020)\n{glmmML}: Broström (2020)\n{googledrive}: D’Agostino McGowan and Bryan (2019)\n{googlesheets4}: Bryan (2020)\n{gridExtra}: Auguie (2017)\n{Hmisc}: Harrell (2020)\n{huxtable}: Hugh-Jones (2021)\n{infer}: Bray et al. (2019)\n{jmuOutlier}: Garren (2019)\n{jtools}: Long (2020)\n{kableExtra}: Zhu (2019)\n{knitr}: Xie (2020), Xie (2014), Xie (2015)\n{lme4}: Bates et al. (2020), Bates et al. (2015)\n{lmodel2}: Legendre (2018)\n{lmtest}: Hothorn, Zeileis, et al. (2019), Zeileis and Hothorn (2002)\n{magrittr}: Bache and Wickham (2014)\n{manipulate}: Allaire (2014)\n{MASS}: Ripley (2019), Venables and Ripley (2002)\n{moments}: Komsta and Novomestky (2022)\n{mosaic}: Pruim et al. (2020), Pruim et al. (2017)\n{MuMIn}: Bartoń (2020)\n{pastecs}: Grosjean and Ibanez (2018)\n{patchwork}: Pedersen (2019)\n{permuco}: Frossard and Renaud (2019)\n{psych}: Revelle (2020)\n{purrr}: Henry and Wickham (2020)\n{radiant}: Nijs (2020)\n{rdrop2}: Ram and Yochum (2017)\n{readr}: Wickham et al. (2018)\n{redres}: Goode et al. (2024)\n{readxl}: Wickham and Bryan (2019)\n{repmis}: Gandrud (2016)\n{reticulate}: Ushey et al. (2020)\n{roxygen2}: Wickham, Danenberg, et al. (2020)\n{scales}: Wickham and Seidel (2019)\n{sciplot}: Morales et al. (2020)\n{shiny}: Chang et al. (2020)\n{sjmisc}: Lüdecke (2020), Lüdecke (2018)\n{skimr}: Waring et al. (2020)\n{sjPlot}: Lüdecke (2023)\n{stringr}: Wickham (2019a)\n{summarytools}: Comtois (2020)\n{tibble}: Müller and Wickham (2020)\n{tictoc}: Izrailev (2014)\n{tidyr}: Wickham and Henry (2020)\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{usethis}: Wickham and Bryan (2020)\n{withr}: Hester et al. (2020)\n{XLConnect}: Mirai Solutions GmbH (2020)\n{xlsx}: Dragulescu and Arendt (2020)\n\n\n\n\n\n\n\nAllaire J. (2014). manipulate: Interactive plots for RStudio. https://CRAN.R-project.org/package=manipulate\n\n\nAttali D, and Baker C. (2019). ggExtra: Add marginal histograms to ’ggplot2’, and more ’ggplot2’ enhancements. https://CRAN.R-project.org/package=ggExtra\n\n\nAuguie B. (2017). gridExtra: Miscellaneous functions for \"grid\" graphics. https://CRAN.R-project.org/package=gridExtra\n\n\nBache SM, and Wickham H. (2014). magrittr: A forward-pipe operator for R. https://CRAN.R-project.org/package=magrittr\n\n\nBartoń K. (2020). MuMIn: Multi-model inference. https://CRAN.R-project.org/package=MuMIn\n\n\nBates D, Mächler M, Bolker B, and Walker S. (2015). Fitting linear mixed-effects models using lme4. Journal of Statistical Software 67: 1–48. https://doi.org/10.18637/jss.v067.i01\n\n\nBates D, Maechler M, Bolker B, and Walker S. (2020). lme4: Linear mixed-effects models using ’eigen’ and S4. https://CRAN.R-project.org/package=lme4\n\n\nBen-Shachar MS, Lüdecke D, and Makowski D. (2020a). effectsize: Estimation of effect size indices and standardized parameters. https://CRAN.R-project.org/package=effectsize\n\n\nBen-Shachar MS, Lüdecke D, and Makowski D. (2020b). effectsize: Estimation of effect size indices and standardized parameters. Journal of Open Source Software 5: 2815. https://doi.org/10.21105/joss.02815\n\n\nBischl B, Lang M, Bossek J, Horn D, Richter J, and Surmann D. (2017). BBmisc: Miscellaneous helper functions for B. Bischl. https://CRAN.R-project.org/package=BBmisc\n\n\nBray A, Ismay C, Chasnovski E, Baumer B, and Cetinkaya-Rundel M. (2019). infer: Tidy statistical inference. https://CRAN.R-project.org/package=infer\n\n\nBroström G. (2020). glmmML: Generalized linear models with clustering. https://CRAN.R-project.org/package=glmmML\n\n\nBryan J. (2020). googlesheets4: Access Google Sheets using the Sheets API V4. https://CRAN.R-project.org/package=googlesheets4\n\n\nCanty A, and Ripley B. (2020). boot: Bootstrap functions (originally by Angelo Canty for S). https://CRAN.R-project.org/package=boot\n\n\nChang W, Cheng J, Allaire J, Xie Y, and McPherson J. (2020). shiny: Web application framework for R. https://CRAN.R-project.org/package=shiny\n\n\nCollier AB. (2021). Emayili: Send email messages. https://CRAN.R-project.org/package=emayili\n\n\nComtois D. (2020). summarytools: Tools to quickly and neatly summarize data. https://CRAN.R-project.org/package=summarytools\n\n\nD’Agostino McGowan L, and Bryan J. (2019). googledrive: An interface to Google Drive. https://CRAN.R-project.org/package=googledrive\n\n\nDavison AC, and Hinkley DV. (1997). Bootstrap Methods and Their Applications. Cambridge: Cambridge University Press. http://statwww.epfl.ch/davison/BMA/\n\n\nDinno A. (2017a). conover.test: Conover-Iman test of multiple comparisons using rank sums. https://CRAN.R-project.org/package=conover.test\n\n\nDinno A. (2017b). dunn.test: Dunn’s test of multiple comparisons using rank sums. https://CRAN.R-project.org/package=dunn.test\n\n\nDowle M, and Srinivasan A. (2019). data.table: Extension of ’data.frame’. https://CRAN.R-project.org/package=data.table\n\n\nDragulescu A, and Arendt C. (2020). xlsx: Read, write, format Excel 2007 and Excel 97/2000/XP/2003 files. https://CRAN.R-project.org/package=xlsx\n\n\nElbers B. (2020). tidylog: Logging for ’dplyr’ and ’tidyr’ functions. https://CRAN.R-project.org/package=tidylog\n\n\nFox J, and Hong J. (2009a). Effect displays in R for multinomial and proportional-odds logit models: Extensions to the effects package. Journal of Statistical Software. https://CRAN.R-project.org/package=effects\n\n\nFox J, and Hong J. (2009b). Effect displays in R for multinomial and proportional-odds logit models: Extensions to the effects package. Journal of Statistical Software 32: 1–24. https://doi.org/10.18637/jss.v032.i01\n\n\nFox J, and Weisberg S. (2019). An R Companion to Applied Regression (Third Edition). Thousand Oaks CA: Sage. https://socialsciences.mcmaster.ca/jfox/Books/Companion/\n\n\nFox J, Weisberg S, and Price B. (2020). car: Companion to applied regression. https://CRAN.R-project.org/package=car\n\n\nFrossard J, and Renaud O. (2019). permuco: Permutation tests for regression, (repeated measures) ANOVA/ANCOVA and comparison of signals. https://CRAN.R-project.org/package=permuco\n\n\nGandrud C. (2016). repmis: Miscellaneous tools for reproducible research. https://CRAN.R-project.org/package=repmis\n\n\nGarren ST. (2019). jmuOutlier: Permutation tests for nonparametric statistics. https://CRAN.R-project.org/package=jmuOutlier\n\n\nGoode K, McClernon K, Zhao J, Zhang Y, and Huo. Y. (2024). Redres: Residuals and diagnostic plots for mixed models. https://github.com/goodekat/redres.git\n\n\nGrosjean P, and Ibanez F. (2018). pastecs: Package for analysis of space-time ecological series. https://CRAN.R-project.org/package=pastecs\n\n\nHarrell FE Jr. (2020). Hmisc: Harrell miscellaneous. https://CRAN.R-project.org/package=Hmisc\n\n\nHenry L, and Wickham H. (2020). purrr: Functional programming tools. https://CRAN.R-project.org/package=purrr\n\n\nHester J, Müller K, Ushey K, Wickham H, and Chang W. (2020). withr: Run code ’with’ temporarily modified global state. https://CRAN.R-project.org/package=withr\n\n\nHothorn T, Hornik K, van de Wiel MA, and Zeileis A. (2006). A Lego system for conditional inference. The American Statistician 60: 257–263. https://doi.org/10.1198/000313006X118430\n\n\nHothorn T, Hornik K, van de Wiel MA, and Zeileis A. (2008). Implementing a class of permutation tests: The coin package. Journal of Statistical Software 28: 1–23. https://doi.org/10.18637/jss.v028.i08\n\n\nHothorn T, Winell H, Hornik K, van de Wiel MA, and Zeileis A. (2019). coin: Conditional inference procedures in a permutation test framework. https://CRAN.R-project.org/package=coin\n\n\nHothorn T, Zeileis A, Farebrother RW, and Cummins C. (2019). lmtest: Testing linear regression models. https://CRAN.R-project.org/package=lmtest\n\n\nHugh-Jones D. (2021). Huxtable: Easily create and style tables for LaTeX, HTML and other formats. https://CRAN.R-project.org/package=huxtable\n\n\nIannone R, and Cheng J. (2020). Blastula: Easily send HTML email messages. https://CRAN.R-project.org/package=blastula\n\n\nIzrailev S. (2014). tictoc: Functions for timing R scripts, as well as implementations of stack and list structures. https://CRAN.R-project.org/package=tictoc\n\n\nKassambara A. (2020). Ggpubr: ggplot2 based publication ready plots. https://rpkgs.datanovia.com/ggpubr/\n\n\nKomsta L, and Novomestky F. (2022). Moments: Moments, cumulants, skewness, kurtosis and related tests. https://CRAN.R-project.org/package=moments\n\n\nKrantz S. (2020). collapse: Advanced and fast data transformation. https://CRAN.R-project.org/package=collapse\n\n\nLegendre P. (2018). lmodel2: Model II regression. https://CRAN.R-project.org/package=lmodel2\n\n\nLong JA. (2020). jtools: Analysis and presentation of social scientific data. https://CRAN.R-project.org/package=jtools\n\n\nLüdecke D. (2018). sjmisc: Data and variable transformation functions. Journal of Open Source Software 3: 754. https://doi.org/10.21105/joss.00754\n\n\nLüdecke D. (2020). sjmisc: Data and variable transformation functions. https://CRAN.R-project.org/package=sjmisc\n\n\nLüdecke D. (2023). sjPlot: Data visualization for statistics in social science. https://CRAN.R-project.org/package=sjPlot\n\n\nMazerolle MJ, and portions of code contributed by Dan Linden. (2019). AICcmodavg: Model selection and multimodel inference based on (Q)AIC(c). https://CRAN.R-project.org/package=AICcmodavg\n\n\nMirai Solutions GmbH. (2020). XLConnect: Excel connector for R. https://CRAN.R-project.org/package=XLConnect\n\n\nMorales M, with code developed by the R Development Core Team, and with general advice from the R-help listserv community and especially Duncan Murdoch. (2020). sciplot: Scientific graphing functions for factorial designs. https://CRAN.R-project.org/package=sciplot\n\n\nMüller K, and Wickham H. (2020). tibble: Simple data frames. https://CRAN.R-project.org/package=tibble\n\n\nNijs V. (2020). radiant: Business analytics using R and Shiny. https://CRAN.R-project.org/package=radiant\n\n\nOoms J. (2019). curl: A modern and flexible web client for R. https://CRAN.R-project.org/package=curl\n\n\nPedersen TL. (2019). patchwork: The composer of plots. https://CRAN.R-project.org/package=patchwork\n\n\nPetersen AH, and Ekstrøm CT. (2019a). dataMaid: A suite of checks for identification of potential errors in a data frame as part of the data screening process. https://CRAN.R-project.org/package=dataMaid\n\n\nPetersen AH, and Ekstrøm CT. (2019b). dataMaid: Your assistant for documenting supervised data quality screening in R. Journal of Statistical Software 90: 1–38. https://doi.org/10.18637/jss.v090.i06\n\n\nPremraj R. (2021). mailR: A utility to send emails from R. https://CRAN.R-project.org/package=mailR\n\n\nPruim R, Kaplan DT, and Horton NJ. (2017). The mosaic package: Helping students to ’think with data’ using R. The R Journal 9: 77–102. https://journal.r-project.org/archive/2017/RJ-2017-024/index.html\n\n\nPruim R, Kaplan DT, and Horton NJ. (2020). mosaic: Project MOSAIC statistics and mathematics teaching utilities. https://CRAN.R-project.org/package=mosaic\n\n\nRam K, and Yochum C. (2017). rdrop2: Programmatic interface to the ’Dropbox’ API. https://CRAN.R-project.org/package=rdrop2\n\n\nRevelle W. (2020). psych: Procedures for psychological, psychometric, and personality research. https://CRAN.R-project.org/package=psych\n\n\nRipley B. (2019). MASS: Support functions and datasets for Venables and Ripley’s MASS. https://CRAN.R-project.org/package=MASS\n\n\nRobinson D, and Hayes A. (2020). broom: Convert statistical analysis objects into tidy tibbles. https://CRAN.R-project.org/package=broom\n\n\nSchloerke B, Crowley J, Cook D, Briatte F, Marbach M, Thoen E, Elberg A, and Larmarange J. (2020). GGally: Extension to ’ggplot2’. https://CRAN.R-project.org/package=GGally\n\n\nSherman J. (2016). Easypackages: Easy loading and installing of packages. https://CRAN.R-project.org/package=easypackages\n\n\nUshey K, Allaire J, and Tang Y. (2020). reticulate: Interface to ’python’. https://CRAN.R-project.org/package=reticulate\n\n\nVenables WN, and Ripley BD. (2002). Modern Applied Statistics with S (Fourth Edition). New York: Springer. http://www.stats.ox.ac.uk/pub/MASS4\n\n\nWaring E, Quinn M, McNamara A, Arino de la Rubia E, Zhu H, and Ellis S. (2020). skimr: Compact and flexible summaries of data. https://CRAN.R-project.org/package=skimr\n\n\nWarnes GR, Bolker B, Gorjanc G, Grothendieck G, Korosec A, Lumley T, MacQueen D, Magnusson A, Rogers J, and others. (2017). gdata: Various R programming tools for data manipulation. https://CRAN.R-project.org/package=gdata\n\n\nWei T, and Simko V. (2017a). corrplot: Visualization of a correlation matrix. https://CRAN.R-project.org/package=corrplot\n\n\nWei T, and Simko V. (2017b). R package \"corrplot\": Visualization of a correlation matrix. https://github.com/taiyun/corrplot\n\n\nWickham H. (2016). ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org\n\n\nWickham H. (2019a). stringr: Simple, consistent wrappers for common string operations. https://CRAN.R-project.org/package=stringr\n\n\nWickham H. (2019b). tidyverse: Easily install and load the ’tidyverse’. https://CRAN.R-project.org/package=tidyverse\n\n\nWickham H. (2020). forcats: Tools for working with categorical variables (factors). https://CRAN.R-project.org/package=forcats\n\n\nWickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, and Yutani H. (2019). Welcome to the tidyverse. Journal of Open Source Software 4: 1686. https://doi.org/10.21105/joss.01686\n\n\nWickham H, and Bryan J. (2019). readxl: Read Excel files. https://CRAN.R-project.org/package=readxl\n\n\nWickham H, and Bryan J. (2020). usethis: Automate package and project setup. https://CRAN.R-project.org/package=usethis\n\n\nWickham H, Chang W, Henry L, Pedersen TL, Takahashi K, Wilke C, Woo K, Yutani H, and Dunnington D. (2020). ggplot2: Create elegant data visualisations using the grammar of graphics. https://CRAN.R-project.org/package=ggplot2\n\n\nWickham H, Danenberg P, Csárdi G, and Eugster M. (2020). roxygen2: In-line documentation for R. https://CRAN.R-project.org/package=roxygen2\n\n\nWickham H, François R, and D’Agostino McGowan L. (2019). emo: Easily insert ’emoji’. https://github.com/hadley/emo\n\n\nWickham H, François R, Henry L, and Müller K. (2020). dplyr: A grammar of data manipulation. https://CRAN.R-project.org/package=dplyr\n\n\nWickham H, and Henry L. (2020). tidyr: Tidy messy data. https://CRAN.R-project.org/package=tidyr\n\n\nWickham H, Hester J, and Chang W. (2020). devtools: Tools to make developing R packages easier. https://CRAN.R-project.org/package=devtools\n\n\nWickham H, Hester J, and Francois R. (2018). readr: Read rectangular text data. https://CRAN.R-project.org/package=readr\n\n\nWickham H, and Seidel D. (2019). scales: Scale functions for visualization. https://CRAN.R-project.org/package=scales\n\n\nWilke CO. (2019). cowplot: Streamlined plot theme and plot annotations for ’ggplot2’. https://CRAN.R-project.org/package=cowplot\n\n\nXie Y. (2014). knitr: A comprehensive tool for reproducible research in R. In V Stodden, F Leisch, and RD Peng (Eds.), Implementing reproducible computational research. Chapman; Hall/CRC. http://www.crcpress.com/product/isbn/9781466561595\n\n\nXie Y. (2015). Dynamic Documents with R and knitr (Second Edition). Boca Raton, Florida: Chapman; Hall/CRC. https://yihui.org/knitr/\n\n\nXie Y. (2020). knitr: A general-purpose package for dynamic report generation in R. https://CRAN.R-project.org/package=knitr\n\n\nXie Y, Cheng J, and Tan X. (2020). DT: A wrapper of the JavaScript library ’DataTables’. https://CRAN.R-project.org/package=DT\n\n\nZeileis A, and Hothorn T. (2002). Diagnostic checking in regression relationships. R News 2: 7–10. https://CRAN.R-project.org/doc/Rnews/\n\n\nZhu H. (2019). kableExtra: Construct complex table with ’kable’ and pipe syntax. https://CRAN.R-project.org/package=kableExtra",
    "crumbs": [
      "References",
      "List of Packages Used"
    ]
  }
]