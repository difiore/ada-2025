[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Data Analysis",
    "section": "",
    "text": "Overview\n\n\n\n\n\n\n\n\n\n\n\nThis course provides an overview of methods and tools for applied data analysis. It is geared toward research in biological anthropology and evolutionary biology, but the material covered is applicable to a wide range of natural, social science, and humanities disciplines. Students will receive practical, hands-on training in various data science tools and workflows, including data acquisition and wrangling, exploratory data analysis and visualization, statistical analysis and interpretation, and literate programming and version control.\nStatistical topics to be covered include basic descriptive and inferential statistics, hypothesis testing, basic regression and ANOVA, generalized linear modeling, and mixed effects modeling. Statistical inference will be considered from a frequentist perspective, introducing both parametric and resampling techniques. If we have time, I will also introduce a Bayesian perspective, although this approach will not be tackled at a particularly advanced level. Additional methods and tools will also be covered based on time and student interest (e.g., geospatial data analysis, phylogenetic comparative methods, social network analysis, text corpus construction and mining, population genetic analysis) and on how quickly the class feels we can move forward.\nThe course particularly emphasizes the development of solid data science skills, focusing on the practical side of data manipulation, analysis, and visualization. Students will learn to use the statistical programming language R as well as many other useful software tools (e.g., shell scripts, text editors, databases, query languages, and version control systems).\n\n\nThis class is supported by DataCamp, an intuitive online learning platform for data science. Learn R, Python, and SQL the way you learn best, through a combination of short expert videos and hands-on-the-keyboard exercises. Take over 100+ courses by expert instructors on topics such as importing data, data visualization or machine learning and learn faster through immediate and personalised feedback on every exercise.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "00-logistics.html",
    "href": "00-logistics.html",
    "title": "Course Logistics",
    "section": "",
    "text": "Learning Objectives\nAt the conclusion of this course, students will be able to:",
    "crumbs": [
      "Preliminaries",
      "Course Logistics"
    ]
  },
  {
    "objectID": "00-logistics.html#learning-objectives",
    "href": "00-logistics.html#learning-objectives",
    "title": "Course Logistics",
    "section": "",
    "text": "understand and articulate key concepts and methods in applied data science; acquire, manipulate, and manage data from varied sources; conduct exploratory data analyses; test statistical hypotheses; build models to classify and make predictions about data; and evaluate model performance;\nuse modern tools for data analysis (e.g., the Unix command line, version control systems, the R programming environment, web APIs) and apply ‚Äúbest practices‚Äù in data science and data management;\ninteract with both local and remote data sources to store, query, process, and analyze data presented in a variety of common formats (e.g., delimited text files, structured text files, various database systems);\ncomfortably write their own simple computer programs/scripts for data management, statistical analysis, visualization, and more specialized applications;\ndesign and implement reproducible data science workflows that take a project from data acquisition to analysis to presentation and organize their work using a version control system;\nand apply all of these tools to questions of interest in the natural and social sciences.",
    "crumbs": [
      "Preliminaries",
      "Course Logistics"
    ]
  },
  {
    "objectID": "00-logistics.html#prerequisites",
    "href": "00-logistics.html#prerequisites",
    "title": "Course Logistics",
    "section": "Prerequisites",
    "text": "Prerequisites\nAt least one semester of introductory statistics is recommended. Prior programming experience is not expected, but would be helpful!",
    "crumbs": [
      "Preliminaries",
      "Course Logistics"
    ]
  },
  {
    "objectID": "00-structure.html",
    "href": "00-structure.html",
    "title": "Structure",
    "section": "",
    "text": "This course is divided into three main sections.\nIn Part I, we will introduce and practice using the statistical programming software R, the RStudio integrated development environment, and the R package ecosystem. We will also cover programming/scripting fundamentals as implemented in R (functions, flow control) and practice using version control systems (e.g., git and GitHub) as we build up our skills for conducting reproducible research. We will use all of these tools to practice data wrangling and perform exploratory data analysis and visualizations.\nIn Part II, we will cover basic statistical and probability theory and methods of statistical inference. We will discuss classical null hypothesis significance testing and more contemporary methods based on permutation methods and, if time permits, I may also introduce alternative Bayesian approaches to inference. In this section, we will cover a variety of linear modeling topics, including simple and multivariate regression, ANOVA and ANCOVA, generalized linear modeling, and mixed effects modeling, as well as regression diagnostics and tools for model selection.\nFinally, in Part III, I hope to introduce a few additional and more specialized data analysis and visualization topics. Assuming we get there, Part III will introduce a mish-mash of (hopefully useful and interesting!) topics and tools, e.g., working with geospatial data and phylogenetic trees, network analysis, machine learning, natural language processing, image analysis, etc. Past experience suggests that I am proposing an ambitious amount of material to cover, so we likely will not get to some of these more specialized kinds of analyses. Still, if there‚Äôs a topic you are particularly excited about exploring, let me know and I will see what we can do!",
    "crumbs": [
      "Preliminaries",
      "Structure"
    ]
  },
  {
    "objectID": "00-course-schedule.html",
    "href": "00-course-schedule.html",
    "title": "Course Schedule",
    "section": "",
    "text": "Part I - Using R and RStudio",
    "crumbs": [
      "Preliminaries",
      "Course Schedule"
    ]
  },
  {
    "objectID": "00-course-schedule.html#part-i---using-r-and-rstudio",
    "href": "00-course-schedule.html#part-i---using-r-and-rstudio",
    "title": "Course Schedule",
    "section": "",
    "text": "An Introduction to R\n\nModules\n\nModule 01 - Getting Started with R\nModule 02 - Getting Started with RStudio\nModule 03 - Extending the Functionality of R\nModule 04 - Fundamentals of the R Language\n\n\n\nTopics\n\nHistory of R\n\nRelation to other languages and statistics software\n\nInstalling R and RStudio\nUsing R and RStudio in the cloud\nSetting up your RStudio workspace\n\nPanels: Source, Console, Environment/History, Other Views\n\nConfiguration and customization\n\nSetting the working directory\nSaving workspaces\n\nR documentation and getting help\n\nThe ? command\nVignettes\nStack Overflow\n\nR Basics\n\nUsing R interactively\nVariables and assignment\nPackages\n\nInstalling and updating\nDependencies\n\nR objects\n\nObject types - Vectors, simple functions, and environments\nClasses and attributes of objects\nScripting and sourcing scripts\n\n\n\n\n\nSuggested Readings\n\nIntroduction to Data Science\n\nChapter 1 - Introduction\nChapter 2 - R Basics\n\nR in Action, Second Edition\n\nChapter 1 - Getting Started\nChapter 2 - Creating a Dataset\n\n\n\n\nOther Useful Readings\n\nThe Book of R\n\nChapter 1 - Getting Started\nChapter 2 - Numerics, Arithmetic, Assignment, and Vectors\n\nR Programming for Data Science\n\nChapter 3 - History and Overview of R\nChapter 5 - R Nuts and Bolts\n\nStatistics: An Introduction Using R\n\nChapter 1 - Fundamentals\nAppendix: Essentials of the R Language\n\nAdvanced R, First Edition\n\nChapter 2 - Data Structures\n\nModern Data Science with R\n\nAppendix B: An Introduction to R and RStudio\n\n\n\n\n\nVersion Control and Reproducibility\n\nModules\n\nModule 05 - Basics of Version Control\nModule 06 - Reproducible Research Using RStudio\nSome recommendations on Programming Style\n\n\n\nTopics\n\nGood programming practices\n\nVersion control with git and GitHub\nData workflow with R projects using local and remote repositories\nReproducible research using Rmarkdown and Quarto\nProgramming conventions and style\n\n\n\n\nSuggested Readings\n\nIntroduction to Data Science\n\nChapter 39 - Git and GitHub\n\nEssentials of Data Science\n\nChapter 11 - R with Style\n\n\n\n\nOther Useful Readings\n\nHappy Git and GitHub for the useR\nIntroduction to Data Science\n\nChapter 37 - Accessing the terminal and installing Git\nChapter 38 - Organizing with Unix\nChapter 40 - Reproducible projects with RStudio and RMarkdown/Quarto\n\n\n\n\n\nData Science Preliminaries\n\nModules\n\nModule 07 - Additional Data Structures in R\nModule 08 - Getting Data into R\n\n\n\nTopics\n\nWorking with data\n\nThe Tao of text\nMore object types - matrices, n-dimensional arrays, lists, data frames, and other tabular structures (e.g., data tables and ‚Äútibbles‚Äù)\nSubsetting and filtering data structures\n\nSingle bracket ([]) notation\nDouble bracket ([[]]) notation\n$ notation\n\nFactors\nClass coercion and conversion\nSpecial data values - NA, NaN, Inf\nGetting data in and out of R\n\nFrom ‚Äú.csv‚Äù files - {readr}\nFrom Excel - {readxl} and others\nFrom Dropbox - {rdrop2}\nFrom other online resources - {curl}\nFrom databases - {RMySQL}, {RSQLite}, {RPostgreSQL} and others\n\n\n\n\n\nSuggested Readings\n\nThe Book of R\n\nChapter 3 - Matrices and Arrays\nChapter 5 - Lists and Data Frames\n\nR in Action\n\nChapter 4 - Basic Data Management\n\n\n\n\nOther Useful Readings\n\nThe Book of R\n\nChapter 4 - Non-Numeric Values\nChapter 6 - Special Values, Classes, and Coercion\nChapter 8 - Reading and Writing Files\n\nAdvanced R\n\nChapter 4 - Subsetting\n\nR for Data Science\n\nChapter 7 - Data Import\n\n\n\n\n\nExploratory Data Analysis\n\nModules\n\nModule 09 - Exploratory Data Analysis\n\n\n\nTopics\n\nSummarizing and visualizing data\n\nBasic descriptive statistics\nTidying and reshaping data with {tidyr}\nSimple plotting (boxplots, histograms, scatterplots) with {base} R, {ggplot2}, and others\n\n\n\n\nSuggested Readings\n\nIntroduction to Data Science\n\nChapter 5 - The {tidyverse}\n\nR in Action\n\nChapter 6 - Basic Graphs\nChapter 7 - Basic Statistics\n\n\n\n\nOther Useful Readings\n\nThe Book of R\n\nChapter 13 - Elementary Statistics\nChapter 14 - Basic Data Visualization\n\nR for Data Science\n\nChapter 5 - Data Tidying\n\n\n\n\n\nData Wrangling and Programming\n\nModules\n\nModule 10 - Data Wrangling with {dplyr}\nModule 11 - Functions and Flow Control\n\n\n\nTopics\n\nManipulating data\n\n{dplyr} functions - select(), filter(), arrange(), rename(), mutate(), group_by(), summarize()\nChaining and piping data\n\nR programming practices\n\nWriting functions\n\nArgument lists\nDefault values\n\nProgram flow control\n\nConditional statements\nfor() loops\nwhile() loops\n\n\n\n\n\nSuggested Readings\n\nIntroduction to Data Science\n\nChapter 4 - Programming Basics\n\n\n\n\nOther Useful Readings\n\nThe Book of R\n\nChapter 9 - Calling Functions\nChapter 10 - Conditions and Loops\nChapter 11 - Writing Functions\n\nR for Data Science\n\nChapter 3 - Data Transformation\n\nR in Action\n\nChapter 5 - Advanced Data Management",
    "crumbs": [
      "Preliminaries",
      "Course Schedule"
    ]
  },
  {
    "objectID": "00-typography-and-formatting.html",
    "href": "00-typography-and-formatting.html",
    "title": "Typography and Formatting",
    "section": "",
    "text": "To the extent possible, I have tried to follow the following typographic and formatting conventions throughout the course modules.\n\nProgram names are written in bold italics:\n\nR\nRStudio\n\nFunctions, commands, and R code are written as inline code, e.g., x &lt;- 5, or in code blocks:\n\n\nx &lt;- 5\nprint(x)\n\n\nCode output appears in a colored cell, prefaced by ‚Äú##‚Äù:\n\n\n\n## [1] 5\n\n\n\nPackage names are written in {curly braces}:\n\n{tidyverse}\n{lubridate}\n\nVariable names (for vectors, data frames and other tabular data, etc.) are written in bold:\n\nx\ny\nairline_flights\n\nColumn names within a data frame are also written in bold, whether referred to separately or as part of the table:\n\norigin\ndestination\nairline_flights$origin\nairline_flights[[‚Äúdestination‚Äù]]\n\nFilenames and file types are written in ‚Äúquotation marks‚Äù:\n\n‚ÄúmyData.R‚Äù\n‚Äú.csv‚Äù\n\nFull file path names are written in ‚Äúquotation marks‚Äù:\n\n‚Äú/Users/Tony/Desktop/myData.R‚Äù\n‚Äú~/Desktop/myData.R‚Äù\n‚ÄúC:\\Documents and Settings\\Anthony Di Fiore\\Desktop\\myData.R‚Äù\n\nDirectory names are written as inline code followed by a slash:\n\nimg/, src/\n\nImportant concepts are written in italics when first referred to:\n\nworking directory\nenvironment\nnamespace\n\nMenu names and menu commands are written in bold:\n\nFile &gt; New File &gt; R Script\n\nArgument values or values to be replaced are written in lowercase text between &lt;angle brackets&gt;, where that entire text, brackets included, should be replaced with the text being asked for:\n\nsetwd(\"&lt;your working directory&gt;\")\n‚Äú&lt;your file name&gt;.csv‚Äù\n\nConstants are written in italics:\n\npi\n\nArgument names and assignments are written as inline code:\n\nSet na.rm=TRUE\nSet data=df\nSet filename=\"~/Users/Tony/Desktop/output.csv\"\n\nNames of RStudio panes are written in bold:\n\nConsole\nEnvironment/History\n\nNames of RStudio tabs within panes are written in italics:\n\nHistory\nPlots\nHelp\nGit\n\nNames of dialog boxes are written in italics:\n\nGlobal Options\n\nButton names and sections within dialog boxes are written in quotation marks:\n\n‚ÄúOK‚Äù\n‚ÄúCancel‚Äù\n\nCheck box names within dialog boxes are written in inline code:\n\nRestore .RData into workspace at startup\n\nR object class names are written in bold:\n\nnumeric\nfunction\n\ngit repository branch names are written as inline code:\n\nmain\norigin/main\n\nFull URLs/links are written in inline code and may include hyperlinks:\n\nhttps://cran.r-project.org/\nhttps://difiore.github.io/ada-2024/",
    "crumbs": [
      "Preliminaries",
      "Typography and Formatting"
    ]
  },
  {
    "objectID": "00-programming-style-guide.html",
    "href": "00-programming-style-guide.html",
    "title": "Programming Style Guide",
    "section": "",
    "text": "File Type Conventions",
    "crumbs": [
      "Preliminaries",
      "Programming Style Guide"
    ]
  },
  {
    "objectID": "00-programming-style-guide.html#file-type-conventions",
    "href": "00-programming-style-guide.html#file-type-conventions",
    "title": "Programming Style Guide",
    "section": "",
    "text": "Use the uppercase ‚Äú.R‚Äù extension for files containing R code\nUse the ‚Äú.RData‚Äù extension for files that contain binary data\nUse the ‚Äú.Rmd‚Äù extension for RMarkdown documents\nUse the ‚Äú.qmd‚Äù extension for Quarto documents\nUse lowercase file extensions for other standard filetypes (e.g., ‚Äú.csv‚Äù, ‚Äú.jpg‚Äù, ‚Äú.docx‚Äù, ‚Äú.xlsx‚Äù)",
    "crumbs": [
      "Preliminaries",
      "Programming Style Guide"
    ]
  },
  {
    "objectID": "00-programming-style-guide.html#stylistic-conventions",
    "href": "00-programming-style-guide.html#stylistic-conventions",
    "title": "Programming Style Guide",
    "section": "Stylistic Conventions",
    "text": "Stylistic Conventions\n\nUse a space before and after the standard backwards assignment operator &lt;- and other infix operators (except for = used in function arguments), but not around parentheses or brackets:\n\nx &lt;- \"Hello\"\n\nEven though the syntax is valid, avoid using = for assignment, except when assigning values to named arguments in a function:\n\nrnorm(n=1000, mean=50, sd=10)\n\nEven though the syntax is valid, do not abbreviate TRUE and FALSE to T and F\nGenerally avoid using the forward assignment operator -&gt; except at the end of a sequence of piped operations:\n\ndf &lt;- df %&gt;% select(name, age, sex, body_weight)\ndf |&gt; select(name, age, sex, body_weight) -&gt; df\n\nUse a space after a comma when listing the arguments of a function:\n\nx &lt;- c(4, 5, 6, 7)\npaste(\"Data science\", \"is\", \"cool\", sep=\" \")",
    "crumbs": [
      "Preliminaries",
      "Programming Style Guide"
    ]
  },
  {
    "objectID": "00-programming-style-guide.html#programming-conventions",
    "href": "00-programming-style-guide.html#programming-conventions",
    "title": "Programming Style Guide",
    "section": "Programming Conventions",
    "text": "Programming Conventions\n\nUse simple, single characters for temporary variables, like indices:\n\nx &lt;- 1:10\nfor (i in 1:100) {print(i)}\n\nWhenever possible, use short, descriptive names for variables:\n\nrate &lt;- 5.6\nsex &lt;- c(\"M\", \"F\", \"F\", \"M\", \"F\", \"M\" \"F\", \"M\")\n\nFor longer, multi-word variable, function, or argument names, use either camelCase or snake_case:\n\ninterestRate &lt;- 0.45\nsay_hello &lt;- function(x) {print(paste0(\"Hello, \",x))}\nprint_n_rows &lt;- function(x, n_rows=10) {print(x[n_rows,])}\n\nInclude default values in your function definitions:\n\nn_rows=10 in the preceding example\n\nInclude error checking in your functions\nFor support files that contain a single function, name the file to match the name of the function defined in the file:\n\n‚ÄúprettyPrint.R‚Äù for a file that contains the function prettyPrint()\n‚Äúrescale_image.R‚Äù for a file that contains the function rescale_image()",
    "crumbs": [
      "Preliminaries",
      "Programming Style Guide"
    ]
  },
  {
    "objectID": "00-programming-style-guide.html#code-formatting-conventions",
    "href": "00-programming-style-guide.html#code-formatting-conventions",
    "title": "Programming Style Guide",
    "section": "Code Formatting Conventions",
    "text": "Code Formatting Conventions\n\nTry to keep lines of code to less than 80 characters\nUse comments liberally to make notes about what your code does\n\nR ignores lines starting with the hashtag character (#) as well as text after this character (until it encounters a line break)\n\n\n\n# assign `g`, the constant for gravitational acceleration\ng &lt;- 9.80665  # units are m/s^2\n\n\nUse a single # to introduce a comment, and separate comments from code with a single empty line before the comment\n\n\nx &lt;- 3\n\n# Now add 2 to x...\nx &lt;- x + 2\n\n\nIn RStudio, use four dashes ---- at the end of a comment line to indicate a section‚Ä¶ this should allow for code folding in your scripts:\n\n\n# Section 1 ----\nx &lt;- 5\ny &lt;- 3\nz &lt;- x + y^2\n\n\nNOTE: In RStudio, you can highlight several lines and then use ‚åò-SHIFT-C to comment/uncomment multiple lines simultaneously.\n\n\nUse indentation to identify (nested) blocks of code:\n\nUse spaces rather than the invisible tab (\\t) character for indentation\nUse consistent indentation (e.g., 2 spaces, 4 spaces) to keep your code looking neat\n\n\n\n\n\n\n\n\n\n\n\n\nUse a linter (see the ‚ÄúAddins‚Äù section under the Tools menu or in the RStudio toolbar) to catch common style ‚Äúerrors‚Äù\n\n\nNOTE: In RStudio, you can use highlight a chunk of code within an R code block and then use ‚åò-SHIFT-A to try to neatly and consistently reformat your code. Also, when working in the text editor in RStudio, holding the option (‚å•) key while selecting with the cursor allows you to highlight/edit replace text in multiple rows simultaneously.",
    "crumbs": [
      "Preliminaries",
      "Programming Style Guide"
    ]
  },
  {
    "objectID": "00-programming-style-guide.html#version-control-system",
    "href": "00-programming-style-guide.html#version-control-system",
    "title": "Programming Style Guide",
    "section": "Version Control System",
    "text": "Version Control System\n\nFinally, always, always, always use a version control system!!! üòÉ See Module 5 and Module 6 for more details.",
    "crumbs": [
      "Preliminaries",
      "Programming Style Guide"
    ]
  },
  {
    "objectID": "00-resources.html",
    "href": "00-resources.html",
    "title": "Resources",
    "section": "",
    "text": "Texts\nThere are no required texts for this course, but we will be covering useful material from a number of the following books, all of which are excellent resources for learning basic to intermediate level statistics and R programming.\nThese are available in print or electronic format directly from the publishers - e.g., No Starch Press, O‚ÄôReilly Media, Inc., Manning Publications Co. - or from Amazon.com.",
    "crumbs": [
      "Preliminaries",
      "Resources"
    ]
  },
  {
    "objectID": "00-resources.html#texts",
    "href": "00-resources.html#texts",
    "title": "Resources",
    "section": "",
    "text": "Davies, T.M. (2016). The Book of R: A First Course in Programming and Statistics. No Starch Press.\nBaumer, B.S., Kaplan, D.T., & Horton, N.J. (2021). Modern Data Science with R (Second Edition). Chapman & Hall/CRC. (link to web version)[https://mdsr-book.github.io/mdsr2e/]\nIsmay, C. & Kim, A.Y. (2019). Statistical Inference via Data Science: A ModernDive into R and the Tidyverse. Chapman & Hall/CRC. (link to web version)[https://moderndive.com/]\nIrizarry, R.A. (2019). Introduction to Data Science. Lean Publishing.\nKabacoff, R. (2022). R in Action: Data Analysis and Graphics with R (Third Edition). Manning Publications Co.\nWickham, H., √áetinkaya-Rundel, M., & Grolemund, G. (2023). R for Data Science (Second Edition). O‚ÄôReilly Media, Inc.¬†(link to web version)[https://r4ds.hadley.nz/]",
    "crumbs": [
      "Preliminaries",
      "Resources"
    ]
  },
  {
    "objectID": "00-resources.html#other-resources",
    "href": "00-resources.html#other-resources",
    "title": "Resources",
    "section": "Other Resources",
    "text": "Other Resources\n\nCheatsheets\n\nPosit Cheatsheets Resource\nBase R\nAdvanced R\nR Reference Card\nMarkdown and GitHub Flavored Markdown\nRMarkdown 1\nRMarkdown 2\nRMarkdown Reference Guide\nLearning RMarkdown\nRStudio IDE\nData Import\nData Transformation with {dplyr}\nData Wrangling with {dplyr} and {tidyr}\nTypes of Regression (R in Action Table 8.1)\nRegression Syntax (R in Action Table 8.2)\nUseful Functions for Regression Models (R in Action Table 8.3)\n{leaflet} for Interactive Mapping\nBasics of Probability\n{shiny} Tutorial 1\n{shiny} Tutorial 2\n\n\n\nSoftware Tools\n\nProgramming Languages\n\nR (MacOS, Windows, Linux)\nPython (MacOS, Windows, Linux)\nJulia (MacOS, Windows, Linux)\n\n\n\nText and Markdown Editors and Publishing Software\n\nBB Edit (MacOS)\nMarkdownPad2 (Windows)\nNotepad++ (Windows)\nObsidian (MacOS)\nPandoc (MacOS, Windows, Linux)\nQuarto (MacOS, Windows, Linux)\nVisual Studio Code (MacOS, Windows, Linux)\n\n\n\nIDEs\n\nRStudio Desktop (R, Python) (MacOS, Windows, Linux)\nRStudio Cloud/Posit Cloud (R, Python) (browser)\nJupyterLab (R, Python, Julia) (MacOS, Windows, Linux, browser)\nPyCharm (Python) (MacOS)\n\n\n\nVersion Control Tools\n\ngit (MacOS, Windows, Linux)\nGitHub (Website)\nGitHub Desktop (MacOS, Windows)\n\n\n\n\nWeb Resources\n\nCRAN (Comprehensive R Archive Network)\nDataCamp\nSoftware Carpentry\nData Carpentry\nrOpenSci\nStack Overflow\nOnline R Exercises\nR-bloggers\nMockaroo\n\n\n\nBooks\n\nStatistical Modeling in Biology\n\nBolker, B.M. (2008). Ecological Models and Data in R. Princeton University Press.\nIrizarry, R.A. & Love, M.I. (2015). Data Analysis for the Life Sciences. Lean Publishing.\nQuinn, G.P. & Keough, M.J. (2002). Experimental Design and Data Analysis for Biologists. Cambridge University Press.\n\n\n\nR and Basic Statistics\n\nCaffo, B. (2015). Statistical Inference for Data Science. Lean Publishing.\nCaffo, B. (2016). Regression Models for Data Science in R. Lean Publishing.\nChihara, L.M. & Hesterberg, T.C. (2018). Mathematical Statistics with Resampling and R. John Wiley & Sons, Inc.\nCrawley, M.J. (2014). Statistics: An Introduction Using R. (Second Edition). John Wiley & Sons, Inc.\nDalgaard, P. (2008). Introductory Statistics with R (Second Edition). Springer.\nDiez, D., √áetinkaya-Rundel, M., & Barr, C.D. (2019). OpenIntro Statistics (Fourth Edition). OpenIntro.org.\nIrizarry, R.A. (2019). Introduction to Data Science. Lean Publishing.\nIsmay, C. & Kim, A.Y. (2019). Statistical Inference via Data Science: A ModernDive into R and the Tidyverse. Chapman & Hall/CRC.\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning. Springer.\nShahbaba, B. (2012). Biostatistics with R. Springer.\nWolfe, D.A. & Schneider, G. (2017). Intuitive Introductory Statistics. Springer.\n\n\n\nR Programming\n\nDavies, T.M. (2016). The Book of R: A First Course in Programming and Statistics. No Starch Press.\nKabacoff, R. (2022). R in Action: Data Analysis and Graphics with R (Third Edition). Manning Publications Co.\nMatloff, N. (2011). The Art of R Programming. No Starch Press.\nPeng, R. (2020). R Programming for Data Science. Lean Publishing.\nPeng, R. (2016). Exploratory Data Analysis with R. Lean Publishing.\nWickham, H. (2015). Advanced R. Chapman & Hall/CRC.\nWickham, H. (2019). Advanced R. (Second Edition). Chapman & Hall/CRC.\nZuur, A.F., Ieno, E.N., & Meesters, E.H.W.G. (2009). A Beginner‚Äôs Guide to R. Springer.\n\n\n\nR Reference\n\nAdler, J. (2009). R in a Nutshell. O‚ÄôReilly Media, Inc.\nCrawley, M.J. (2012). The R Book (Second Edition). John Wiley & Sons, Inc.\nEkstr√∏m, C. T. (2016). The R Primer (Second Edition). Chapman & Hall/CRC.\nGardener, M. (2012). The Essential R Reference. John Wiley & Sons, Inc.\nLong, J.D. & Teetor, P. (2019). R Cookbook (Second Edition). O‚ÄôReilly Media, Inc.\n\n\n\nR Graphics\n\nChang, W. (2013). R Graphics Cookbook. O‚ÄôReilly Media, Inc.\nWickham, H. (2016). ggplot2: Elegant Graphics for Data Analysis (Second Edition). Springer.\n\n\n\nData Science\n\nBaumer, B.S., Kaplan, D.T., & Horton, N.J. (2017). Modern Data Science with R. Chapman & Hall/CRC.\nBruce, P. & Bruce, A. (2017). Practical Statistics for Data Scientists. O‚ÄôReilly Media, Inc.\nCady, F. (2017). The Data Science Handbook. John Wiley & Sons, Inc.\nGrus, J. (2015). Data Science from Scratch. O‚ÄôReilly Media, Inc.\nIsmay, C. & Kim, A.Y. (2019). Statistical Inference via Data Science: A ModernDive into R and the Tidyverse. Chapman & Hall/CRC.\nLarose, C.D. & Larose D.T. (2019). Data Science Using Python and R. John Wiley & Sons, Inc.\nMaillund, T. (2016). Introduction to Data Science and Statistical Programming in R. Lean Publishing.\nMcNicholas, P.D. & Tait, P.A. (2019). Data Science with Julia. Chapman & Hall/CRC.\nPearson, R.K. (2018). Exploratory Data Analysis Using R. Chapman & Hall/CRC.\nPeng, R.D. & Matsui, E. (2015). The Art of Data Science. Lean Publishing.\nWickham, H. & Grolemund, G. (2017). R for Data Science. O‚ÄôReilly Media, Inc.\nWilliams, G.J. (2017). The Essentials of Data Science. Chapman & Hall/CRC.\nZumel, N. & Mount, J. (2020). Practical Data Science with R, Second Edition. Manning Publications Co.\n\n\n\nData Visualization\n\nDale, K. (2016). Data Visualization with Python and JavaScript. O‚ÄôReilly Media, Inc.\nHealy, K. (2018). Data Visualization: A Practical Introduction. Princeton University Press.\nThomas, S.A. (2015). Data Visualization with JavaScript. No Starch Press.\nWilke, C.O. (2019) Fundamentals of Data Visualization. O‚ÄôReilly Media, Inc.\n\n\n\nSpatial Data Analysis\n\nBivand, R.S., Pebesma, E., & G√≥mez-Rubio, V. (2013). Applied Spatial Data Analysis with R (Second Edition). Springer.\nBrundson, C. & Comber, L. (2019). An Introduction to R for Spatial Analysis and Mapping (Second Edition). SAGE.\nBrunsdon, C. & Singleton, A.D. (Eds.). (2015). Geocomputation: A Practical Primer. Los Angeles: SAGE.\nLovelace, R., Nowosad, J., & Muenchow, J. (2019). Geocomputation with R. Chapman & Hall/CRC.\n\n\n\nR and Bayesian Statistics\n\nBolstad, W.M. & Curran, J.M. (2017). Introduction to Bayesian Statistics (Third Edition). John Wiley & Sons, Inc.\nKruschke, J.K. (2015). Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan (Second Edition). Elsevier.\nMcElreath, R. (2019). Statistical Rethinking: A Bayesian Course with Examples in R and Stan (Second Edition). Chapman & Hall/CRC.\n\n\n\nGeneral and Generalized Regression, Mixed Effects, and Multilevel/Hierarchical Modeling\n\nBurnham, K.P. & Anderson, D.R. (2002). Model Selection and Multimodel Inference. Springer.\nDunn, P.K. & Smyth, G.K. (2018). Generalized Linear Models With Examples in R. Springer.\nFox, J. (2016). Applied Regression Analysis and Generalized Linear Models (Third Edition). SAGE.\nFox, J. & Weisberg, S. (2019). An R Companion to Applied Regression. SAGE.\nGelman, A. & Hill, J. (2007). Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press.\nJames, G,, Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning. Springer.\nHoffman, J.P. (2022). Linear Regression Models: Applications in R. Chapman & Hall/CRC.\nZuur, A.F., Ieno, E.N., Walker, N.J., Savaliev, A.A., & Smith, G.M. (2009). Mixed Effects Models and Extensions in Ecology with R. Springer.\n\n\n\nWeb Scraping, Text Mining, and Text Analysis\n\nFriedl, J.E.F. (2000). Mastering Regular Expressions (Third Edition). O‚ÄôReilly Media, Inc.\nMitchell, R. (2015). Web Scraping with Python. O‚ÄôReilly Media, Inc.\nNolan, D. & Temple Lang, D. (2014). XML and Web Technologies for Data Sciences with R. Springer.\nSilge, J. & Robinson, D. (2017). Text Mining with R: A Tidy Approach. O‚ÄôReilly Media, Inc.\n\n\n\nR Packages, R Markdown, Quarto, and Reproducible Research\n\nGandrud, C. (2020). Reproducible Research with R and RStudio (Third Edition). Chapman & Hall/CRC.\nWickham, H. (2015). R Packages. O‚ÄôReilly Media, Inc.\nXie, Y. (2017). Bookdown: Authoring Books and Technical Documents with RMarkdown. Chapman & Hall/CRC.\nXie, Y., Allaire, J.J., & Grolemund, G. (2018). R Markdown: The Definitive Guide. Chapman & Hall/CRC.\n\n\n\ngit and Unix Shell Tools\n\nAlbing, C., Vossen, J.P., & Newham, C. (2007). Bash Cookbook. O‚ÄôReilly Media, Inc.\nBarrett, D.J. (2016). Linux Pocket Guide: Essential Commands (Third Edition). O‚ÄôReilly Media, Inc.\nChacon, S. & Straub, B. (2014). Pro Git (Second Edition). Apress.\nDougherty, D. & Robbins, A. (1998). Sed and Awk (Second Edition). O‚ÄôReilly Media, Inc.\nNewham, C. & Rosenblatt, B. (2005). Learning the bash Shell (Third Edition). O‚ÄôReilly Media, Inc.\nRobbins, A. (2006). UNIX in a Nutshell (Fourth Edition). O‚ÄôReilly Media, Inc.\n\n\n\nData Science, Statistics, and Programming in Python\n\nBeazley, D. & Jones, B.K. (2013). Python Cookbook (Third Edition). O‚ÄôReilly Media, Inc.\nDowney, A.B. (2012). Think Python. O‚ÄôReilly Media, Inc.\nDowney, A.B. (2014). Think Stats (Second Edition). O‚ÄôReilly Media, Inc.\nDowney, A.B. (2023). Modeling and Simulation in Python. No Starch Press.\nKazil, J. & Jarmul, K. (2016). Data Wrangling with Python. O‚ÄôReilly Media, Inc.\nLubanovic, B. (2014). Introducing Python. O‚ÄôReilly Media, Inc.\nLee, K.D. (2011). Python Programming Fundamentals. Springer.\nLutz, M. (2013). Learning Python (Fifth Edition). O‚ÄôReilly Media, Inc.\nLutz, M. (2014). Python Pocket Reference (Fifth Edition). O‚ÄôReilly Media, Inc.\nMcKinney, W. (2013). Python for Data Analysis. O‚ÄôReilly Media, Inc.\nRogel-Salazar, J. (2023). Statistics and Data Visualisation with Python. Chapman & Hall/CRC.\nVanderPlas, J. (2016). Python Data Science Handbook. O‚ÄôReilly Media, Inc.\nVasiliev, Y. (2022). Python for Data Science. No Starch Press.\nVaughan, L. (2023). Python Tools for Scientists: An Introduction to Coding, Anaconda, Jupyterlab, and the Scientific Libraries. No Starch Press\n\n\n\nData Science, Statistics, and Programming in Julia\n\nMcNicholas, P.D. & Tait, P.A. (2019). Data Science with Julia. Chapman and Hall/CRC.\nPhillips, L. (2024). Practical Julia: A Hands-on Introduction for Scientific Minds. No Starch Press.\n\n\n\nDatabases and SQL\n\nDeBarros, A. (2022). Practical SQL: A Beginner‚Äôs Guide to Storytelling with Data. (Second Edition). No Starch Press.\nKreibich, J.A. (2010). Using SQLite. O‚ÄôReilly Media, Inc.\nObe, R.O. & Hsu, L.S. (2012). PostgreSQL: Up and Running. O‚ÄôReilly Media, Inc.\nRobinson, I., Webber, J., & Eifrem, E. (2015). Graph Databases (Second Edition). O‚ÄôReilly Media, Inc.\n\n\n\nMachine Learning\n\nBoehmke, B. & Greenwall. B. (2020). Hands-On Machine Learning with R. Chapman & Hall/CRC.\nRhys, H.I. (2020). Machine Learning with R, tidyverse, and mlr. Manning Publications Co.",
    "crumbs": [
      "Preliminaries",
      "Resources"
    ]
  },
  {
    "objectID": "01-module.html",
    "href": "01-module.html",
    "title": "1¬† Getting Started with R",
    "section": "",
    "text": "1.1 Objectives",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting Started with ***R***</span>"
    ]
  },
  {
    "objectID": "01-module.html#objectives",
    "href": "01-module.html#objectives",
    "title": "1¬† Getting Started with R",
    "section": "",
    "text": "The goal of this module is to get everyone‚Äôs computers set up with R for the semester and to provide background and an introduction to the R programming language and environment.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting Started with ***R***</span>"
    ]
  },
  {
    "objectID": "01-module.html#backstory",
    "href": "01-module.html#backstory",
    "title": "1¬† Getting Started with R",
    "section": "1.2 Backstory",
    "text": "1.2 Backstory\nThe name R is a nod to the statistical programming language S (for ‚ÄúStatistics‚Äù) that inspired its creation. S was developed at Bell Laboratories by John Chambers and later sold to a small company that further developed it into S-Plus. R was then developed as an alternative to S by Ross Ihaka and Robert Gentleman in the Department of Statistics at the University of Aukland, New Zealand.\nR is an high-level, interpreted language, like Python or Ruby, where commands are executed directly and sequentially, without previously compiling a program into machine-language instructions. Each statement is translated, on the fly, into a sequence of subroutines that have already been compiled into machine code.\nR is open-source software, meaning that the source code for the program is freely available for anyone to modify, extend, and improve upon. R is also FREE (!) for anyone to use and distribution. The large and active community of users and developers is one of the reasons that R has become very popular in academics, science, engineering, and business - any field that requires data analytics. Developers have also built in the capacity for easily making production-quality graphics, making it a great tool for data visualization. There are thus many good reasons to learn and use R.\nHere are a few of the main ones, in a nutshell:\n\nR is high quality software. It is actively developed by an international community of statisticians and software developers with multiple releases and bug fixes every year.\nR is FREE (as in thought). The source code is openly avaialable under the GNU General Public License, which allows others to easily evaluate the quality of the code, contribute new functionality, and quickly fix bugs.\nR is FREE (as in beer). Whereas licenses for other statistical software such as SAS, SPSS, or Stata may cost thousands of dollars, R is available free of charge.\nR is available for multiple platforms. Installers are available for Windows, MacOS, and other Unix based systems and most package are OS agnostic.\nR is extremely extensible. If there is a procedure you want to run that is not included in one of the standard packages, it is likely available in one of the thousands of extensions packages that have been developed and are also freely available. You can also use R to control or interface with external applications, including other programming languages (like Python, SQL, C++, NetLogo), other analysis tools (like GIS software), and databases (like MySQL, PostgreSQL, SQLite, etc). It is also always possible for you to improve R yourself. You can literally do just about anything in R.\nR has a vibrant, intelligent, and generous user community. LOTS of resources are available online for learning and troubleshooting (see, for example, the section on R at the Stack Overflow website.\n\nR can be run in several ways:\n\nInteractively from a console prompt after launching the program from the command line in either a terminal window or command shell.\nIn batch mode, by sourcing commands from an R script file (which is a simple text file).\nFrom within an R graphical user interface (or GUI) or integrated development envrionment (or IDE), which accommodates both of the above.\n\nWe are going to introduce several of these ways of working with R, but the easiest and most convenient is to use an IDE.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting Started with ***R***</span>"
    ]
  },
  {
    "objectID": "01-module.html#installing-the-r-software",
    "href": "01-module.html#installing-the-r-software",
    "title": "1¬† Getting Started with R",
    "section": "1.3 Installing the R Software",
    "text": "1.3 Installing the R Software\n\nDownload and install R from the Compehensive R Archive Network (CRAN) website. Choose the correct version for your operating system.\n\n\n\n\n\n\n\n\n\n\n\nIf you are using MacOS, you should consider also installing XQuartz, which lets you use the X11 X Windows management software.\n\n\n\n\n\n\n\n\n\n\n\nOpen the R program from wherever you installed it (e.g., in MacOS, double-click on the R.app application in your Applications folder; on a PC, search for and open the Rgui.exe application, which should be located somewhere inside your C:\\Program Files\\R\\R-[version] folder‚Ä¶ you should see the console window and the &gt; prompt. Note that your screen may look slightly different from the screenshots below.\n\nAlso, note that you can also run R in a terminal shell (MacOS or Unix) or from the Windows command shell after starting it with the command r. Depending on whether you have set your PATH variable to detect the R executable file, you may or may not need to first navigate into the directory containing the executable file.\n\nOn MacOS, the default GUI will look as follows:\n\n\n\n\n\n\n\n\n\n\n\nOn Windows, the default GUI (RGui) looks like this:",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting Started with ***R***</span>"
    ]
  },
  {
    "objectID": "01-module.html#exploring-the-r-console",
    "href": "01-module.html#exploring-the-r-console",
    "title": "1¬† Getting Started with R",
    "section": "1.4 Exploring the R Console",
    "text": "1.4 Exploring the R Console\n\nR can be used as an interactive calculator from the console prompt, either in a GUI or in the shell.\n\nStandard mathematical order of operations applies (PEMDAS - parentheses, exponents, multiplication/division, addition/subtraction).\n\nThe assignment operator &lt;- can be used to assign a value, the results of an operation, or specific code to an object (e.g., a variable, a function, a complex data structure).\n\nYou can also use =, but I prefer to use that only to assign values to function arguments (more on this later).\n\nYou can set various settings to customize your interactions with R.\n\nTo change the standard prompt, use the function options() with the prompt argument: options(prompt=\"&lt;prompt text&gt;\"), where you supply, between the quotes, text with what you want the prompt to say.\nTo list the current working directory (the default folder where dialog boxes will open and where files and output will be stored unless otherwise specified), use the function getwd(), which has no arguments.\nTo set the current working directory, use the function setwd(\"&lt;path&gt;\"), where you supply, between the quotes, the path to the desired directory.\n\nOn MacOS, these directory options are also available under the Misc menu.\nOn Windows, you can set the working directory with the Change dir command under the File menu.\nIn RStudio, the working directory can be set under the Session menu.\n\n\nWithin the active workspace, R keeps a log of all executed commands, and you can use the arrow keys to scroll through this history. In RStudio, this list is accessible in the History tab.\nCommands and code can also be written in a text file or script and sent to the console for execution.\n\nIn most GUIs/IDEs, you can choose to create a new script document from the File menu, which opens in a text editor of some kind.\nFrom within the text editor, you can send an individual command to the R interpreter by positioning your cursor somewhere in the line you want to execute and hitting ‚åò-RETURN (Mac) or either control-R (for the default GUI that ships with R) or control-ENTER (for the RStudio GUI) (PC).\nTo send a set of commands to the console as a batch, you can highlight the code lines of code you want to execute and then use these same commands.\nYou can include comments in your scripts by prefacing them with #.\nScript files can be saved just like any other type of text file, usually with the ‚Äú.R‚Äù extension by default.\n\nTo view the names of all of the objects in your current workspace, you can use the ls() function. In RStudio, these also all appear in the Environment tab.\nTo clear objects from your workspace, use the rm() function, where an individual object‚Äôs name or a list of object names can be included as the argument to rm().\nTo remove all objects from your workspace, you can use rm(list=ls()).\n\nIn this case, you are passing to rm() a list consisting of all the objects in the workspace, provided by the ls() function.\n\n\n\nCHALLENGE\nFire up R in your mode of choice (by typing ‚ÄúR‚Äù at the console prompt in the Terminal in MacOS or from a cmd or other shell prompt in Windows) and then practice interacting with the software via the command line and console window.\n\nTry doing some math in R by using it to evaluate the following expressions:\n\n8 + 5\n10 - 6 / 2\n(10 - 6) / 2\n10 * 5\n15 / 5\n10 ^ 5\n3 * pi (where pi is a built-in constant)\n\n\n\n\nShow Code\n8 + 5\n\n\nShow Output\n## [1] 13\n\n\n\nShow Code\n10 - 6/2\n\n\nShow Output\n## [1] 7\n\n\n\nShow Code\n(10 - 6)/2\n\n\nShow Output\n## [1] 2\n\n\n\nShow Code\n10 * 5\n\n\nShow Output\n## [1] 50\n\n\n\nShow Code\n15/5\n\n\nShow Output\n## [1] 3\n\n\n\nShow Code\n10^5\n\n\nShow Output\n## [1] 1e+05\n\n\n\nShow Code\n3 * pi\n\n\nShow Output\n## [1] 9.424778\n\n\n\n\nTry working with assignments:\n\nAssign the number 6 to a variable called x.\nAssign the number 5 to a variable called y.\nAssign x * y to a variable called z.\nAssign x^2 to a variable called x2.\n\n\n\n\nShow Code\nx &lt;- 6\nx\n\n\nShow Output\n## [1] 6\n\n\n\nShow Code\ny &lt;- 5\ny\n\n\nShow Output\n## [1] 5\n\n\n\nShow Code\nz &lt;- x * y\nz\n\n\nShow Output\n## [1] 30\n\n\n\nShow Code\nx2 &lt;- x^2\nx2\n\n\nShow Output\n## [1] 36\n\n\n\n\nTry out some of the built-in functions in R:\n\nAssign the number 10 to a variable called x.\nTake the natural log of x using the log() function.\nFind the factorial of x using the factorial() function.\nAssign the number 81 to a variable called y.\nTake the square root of y using the sqrt() function.\nAssign the number -8.349218 to a variable called z.\nUse ?round or help(round) to view the help file for the function round().\nRound z to the 1000ths place.\nUse ?abs() to view the help file for the function abs().\nTake the absolute value of z * y.\n\n\n\n\nShow Code\nx &lt;- 10\nlog(x)\n\n\nShow Output\n## [1] 2.302585\n\n\n\nShow Code\nfactorial(x)\n\n\nShow Output\n## [1] 3628800\n\n\n\nShow Code\ny &lt;- 81\nsqrt(y)\n\n\nShow Output\n## [1] 9\n\n\n\nShow Code\nz &lt;- -8.349218\nround(z, digits = 3)\n\n\nShow Output\n## [1] -8.349\n\n\n\nShow Code\nabs(z * y)\n\n\nShow Output\n## [1] 676.2867\n\n\n\n\nUse the ls() function to list the variables currently stored in your active session.\n\nHow many variables do you have?\n\n\n\n\nShow Code\nls()\n\n\nShow Output\n## [1] \"x\"  \"x2\" \"y\"  \"z\"\n\n\n\n\nUse the command rm(list=ls()) to clear all the variables you have defined.\nWhat happens if you type a function name without including the parentheses?\nWhat happens if you type a function with an invalid or missing argument?",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting Started with ***R***</span>"
    ]
  },
  {
    "objectID": "01-module.html#concept-review",
    "href": "01-module.html#concept-review",
    "title": "1¬† Getting Started with R",
    "section": "Concept Review",
    "text": "Concept Review\n\nInteracting with R from the console prompt\nVariable assignment: &lt;-\nCalling built-in functions: function(&lt;arguments&gt;)\nAccessing R documentation and help files: ?function or help(function)\nWorkspaces and the working directory: getwd(), setwd()\nListing and removing variables from the environment: ls(), rm()\nAccessing the console history",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting Started with ***R***</span>"
    ]
  },
  {
    "objectID": "02-module.html",
    "href": "02-module.html",
    "title": "2¬† Getting Started with RStudio",
    "section": "",
    "text": "2.1 Objectives",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Getting Started with ***RStudio***</span>"
    ]
  },
  {
    "objectID": "02-module.html#objectives",
    "href": "02-module.html#objectives",
    "title": "2¬† Getting Started with RStudio",
    "section": "",
    "text": "The goal of this module is to familiar yourself with the RStudio Integrated Development Environment (IDE).",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Getting Started with ***RStudio***</span>"
    ]
  },
  {
    "objectID": "02-module.html#installing-the-rstudio-software",
    "href": "02-module.html#installing-the-rstudio-software",
    "title": "2¬† Getting Started with RStudio",
    "section": "2.2 Installing the RStudio Software",
    "text": "2.2 Installing the RStudio Software\nApart from the GUIs included in the MacOS and Windows installations of R, there are several IDEs that connect to the R interpreter and provide lots of convenient functionality. One of the most versatile and easy to use (and my favorite) is RStudio, created by the company Posit.\n\nDownload and install the RStudio Integrated Development Environment (IDE)\n\n\n\n\n\n\n\n\n\n\n\nOpen the RStudio program\n\nThe workspace that you see is divided into four separate panes (Source and Console panes on the left, two customizable panes on the right). You can modify the layout and appearance of the RStudio IDE to suit your taste by selecting Preferences from the RStudio menu (MacOS) or by selecting Global Options from the Tools menu (both MacOS and Windows).\n\n\n\n\n\n\n\n\n\nThe Source pane is where you work with and edit various file types (e.g., scripts), while the Console pane is where you run commands in the R interpreter and see the results of those commands. The other two customizable panes provide easy access to useful tools and overviews of your interactions with R. For example, the Environment tab can be used to view all of the objects in the different environments in your current workspace, the History tab shows the log of all of the commands you have sent to the interpreter, and the Packages tab provides a convenient interface for installing and loading packages (see below).\nWithin RStudio, you can change the working directory by going to the Session menu and selecting Set Working Directory.\n\nCHALLENGE\nRepeat the basic maths CHALLENGE from Module 01 using the editor and console in RStudio.\n\nNOTE: In both the base GUI that ships with the R application and in RStudio, the console supports code completion. Pressing TAB after starting to type a function or variable name will give you suggestions as to how to complete what you have begun to type. In RStudio, this functionality is present also when you are typing code in the text editor in the Source pane. Also helpful in RStudio are popup windows that accompany code completion that show, for each function, what possible arguments that function can take and their default values.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Getting Started with ***RStudio***</span>"
    ]
  },
  {
    "objectID": "02-module.html#using-the-rstudio-posit-server",
    "href": "02-module.html#using-the-rstudio-posit-server",
    "title": "2¬† Getting Started with RStudio",
    "section": "2.3 Using the RStudio Posit Server",
    "text": "2.3 Using the RStudio Posit Server\nAn alternative (though likely slower!) way to use R and RStudio is to run them through a browser from a cloud computing server adminstered by Posit, the company that developed and continues to refine RStudio. To use this approach, visit the Posit Cloud website, click the Get Started button, and create or sign up for an account.\n\n\n\n\n\n\n\n\n\nYou can use a Facebook or GitHub account to sign up (I recommend the latter. See Module 05 for more info on signing up for and using GitHub) or create a new account that is specifically for Posit Cloud.\n\n\n\n\n\n\n\n\n\nOnce you are signed up, or if you have already done so, you can use your account to log in. Doing so will bring you to the landing page for your Posit Cloud account.\n\n\n\n\n\n\n\n\n\nYour account lets you access a sandboxed environment on Posit‚Äôs servers that contains R, RStudio, any packages you install, your own files, etc., which you are connecting via a web browser. You can effectively use it for development without storing anything on your local machine.\nThere, if you create a New Project, you will set up a new R project within your account space. Your account space can include multiple projects, each with its own set of associated files.\n\n\n\n\n\n\n\n\n\nRStudio running through your browser looks virtually the same as running it on your own machine. You can can install packages to your workspace, set your own preferences for the IDE, and even run different versions of R. As a free user, your workspace will have some computational limits, so it‚Äôs not going to be useful for production-level computing, but for most things we do in this class, it would be sufficient.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Getting Started with ***RStudio***</span>"
    ]
  },
  {
    "objectID": "02-module.html#customizing-the-rstudio-ide",
    "href": "02-module.html#customizing-the-rstudio-ide",
    "title": "2¬† Getting Started with RStudio",
    "section": "2.4 Customizing the RStudio IDE",
    "text": "2.4 Customizing the RStudio IDE\nIf you open either the Preferences (MacOS) or Global Options (MacOS or PC) dialog box in RStudio you can customize the setup and functionality of your IDE.\nIn the General section, I recommend the settings shown below, particularly about restoring your last workspace into memory upon startup and about asking if you wish to save the contents of your current workspace upon shutdown.\n\n\n\n\n\n\n\n\n\nFeel free to organize the rest of your setup as you would like. For example, you can change lots of options in the Code, Console, Appearance, and Pane Layout sections to set up the IDE as best suits your personal tastes.\n\n\n\n\n\n\n\n\n\nFinally, if you have already installed git on your computer, go to the Git/SVN section‚Ä¶\n\n\n\n\n\n\n\n\n\n‚Ä¶ and make sure that the checkbox ‚ÄúEnable version control interface for RStudio projects‚Äù is selected and that the path to your git executable is filled. If you have installed git successfully, this should be filled with something like ‚Äú/usr/bin/git‚Äù or ‚Äúusr/local/bin/git‚Äù. If it is not, do not worry‚Ä¶ we can set this later after installing and troubleshooting that program. Also make sure that the ‚ÄúSign git commits‚Äù box is unchecked.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Getting Started with ***RStudio***</span>"
    ]
  },
  {
    "objectID": "02-module.html#concept-review",
    "href": "02-module.html#concept-review",
    "title": "2¬† Getting Started with RStudio",
    "section": "Concept Review",
    "text": "Concept Review\n\nInstalling, navigating and customizing the RStudio IDE\nRunning R and RStudio through a browser using RStudio Cloud",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Getting Started with ***RStudio***</span>"
    ]
  },
  {
    "objectID": "03-module.html",
    "href": "03-module.html",
    "title": "3¬† Extending the Functionality of R",
    "section": "",
    "text": "3.1 Objectives",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Extending the Functionality of ***R***</span>"
    ]
  },
  {
    "objectID": "03-module.html#objectives",
    "href": "03-module.html#objectives",
    "title": "3¬† Extending the Functionality of R",
    "section": "",
    "text": "The goal of this module is to show you how to extend the base functionality of R by installing and loading packages.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Extending the Functionality of ***R***</span>"
    ]
  },
  {
    "objectID": "03-module.html#preliminaries",
    "href": "03-module.html#preliminaries",
    "title": "3¬† Extending the Functionality of R",
    "section": "3.2 Preliminaries",
    "text": "3.2 Preliminaries\n\nInstall this package in R: {easypackages}",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Extending the Functionality of ***R***</span>"
    ]
  },
  {
    "objectID": "03-module.html#r-packages",
    "href": "03-module.html#r-packages",
    "title": "3¬† Extending the Functionality of R",
    "section": "3.3 R Packages",
    "text": "3.3 R Packages\nOne of the fantastic things about R, and one of the reasons it is such a flexible tool for so many types of data analysis, is the ability to extend its functionality in a huge variety of ways with packages. Packages are sets of reusable R functions created by the core development team or by users and are akin to libraries in other programming software, like Python. As of January 2020, there are over 15,300 packages that have been contributed to the most common package repository, hosted on the Comprehensive R Archive Network (CRAN) site.\nFrom the console prompt, packages can be installed into R (using the install.packages() function, with the name of the package in parentheses) and then loaded (using the require() or library() functions), which then gives the user access to the functions contained therein. Both RStudio and the base R GUIs for MacOS and Windows have built-in package managers that allow you to search for and install packages.\nEach package includes a namespace associated with the functions contained therein, and those functions are loaded into their own, separate R environments, distinct from the global environment, where the variables we assigned values to in Module 01 were created. An environment can be thought of as a collection of objects (functions, variables etc.) associated either globally, with the R interpreter (the ‚ÄúR_GlobalEnv‚Äù) or with a particular package and its namespace. In RStudio, you can see all of the objects associated with a particular environment in the Environment tab.\nIf a package is loaded that has a function with the same name as one in a previously loaded package or in base R, then the older function will be masked and the newer will be used if called by a user. This is because whenever an object is referenced in a command, the R interpreter searches for that object in various loaded environments in a particular order and operates on the first one it encounters. The global environment is searched first, followed by the environments associated with each loaded package in reverse chronological order of when they are loaded and ending with the base R environment.\n\n\n\n\n\n\n\n\n\n\n\nFROM: Wickham (2019). Advanced R, Second Edition. Chapman & Hall/CRC.\n\n\n\nHowever, functions from different packages with the same name can be called explicitly by using the :: operator, using the construct package-name::function to access the desired namespace and environment. A particular function can be called in this way even if the package as a whole has not been loaded into R using library() or require(). The search() function can be used to return a vector of environment names in the order they will be searched.\n\nInstalling Packages\n\nNOTE: A list of packages we will be using in this course is available here.\n\n\nUsing the base MacOS GUI\n\nSelect the Packages & Data menu.\nChoose Package Installer.\nSelect a package repository to install from, e.g., CRAN (binaries).\nThe first time you go to install packages, you will be prompted to select a mirror site (i.e., the remote server from where you can download any of the packages hosted in the selected package repository).\nSelect an install location. I usually install packages to the system rather than user level.\nCheck the box ‚ÄúInstall dependencies‚Äù.\nSelect a package to install.\n\n\n\nUsing the base Windows GUI\n\nSelect the Packages menu.\nChoose Install package(s)‚Ä¶.\nBy default, the package repository that you will install from is CRAN (binaries).\nThe first time you go to install packages, you will be prompted to select a mirror site (a.k.a., the remote server from where you can download any of the packages hosted in the selected package repository).\nSelect a package to install.\nBy default, packages are installed at the system level (inside of the library folder within your R installation), and any missing dependenices are also installed by default.\n\n\n\nUsing the R console prompt\n\nUse the function install.packages(\"&lt;package name&gt;\"), where you include, between the quotes, the name of the package you want to install. This command installs the package, by default, to the user level, though this can be changed by providing a path to install to using the lib=\"&lt;path&gt; argument. Other arguments for this function can be set to specify the repository to download from, etc.\n\n\n\nUsing RStudio\n\nSelect the Packages tab and then click the ‚ÄúInstall‚Äù button.\nA dialog box will open where you can choose where to install the package from (the central CRAN repository is typically the source you will use) and the install location on your computer.\nYou can install packages either to the user level library (in which case, only the user who is logged in when the package is installed will have access to it) or to the system library (which will make the package available for all users).\nType the name of the package you want to install in the text field in the middle of the dialog box (code completion will list available packages that match what you are typing as you type). You can install multiple packages at the same time by separating them with a space or comma.\nMake sure the ‚ÄúInstall dependencies‚Äù checkbox is selected‚Ä¶ this will automatically check whether other packages that are referenced in the package you want to install are already installed and, if not, will install them as well.\n\n\n\n\n\n\n\n\n\n\n\n\n\nLoading and Attaching Packages\nNote that installing packages simply places them into a standard location on your computer. To actually use the functions they contain in an R session, you need to also load them into your R workspace.\n\nUsing the base MacOS GUI\n\nSelect the Packages & Data menu.\nChoose Package Manager.\nCheck the box next to the package name.\n\n\n\nUsing the base Windows GUI\n\nSelect the Packages menu.\nChoose Load package‚Ä¶.\nSelect the package to load.\n\n\n\nUsing RStudio\n\nYou can load a package interactively in RStudio by clicking the checkbox next to the package name in the Packages tab.\n\n\n\n\n\n\n\n\n\n\n\n\nUsing the console prompt or a script\n\nThe most common way that you will load packages is to do so either interactively at the console prompt or in a script using the command library(&lt;package name&gt;) with the package name, not in quotes, as an argument.\nThe require() function is nearly identical to the library() function except that the former is safer to use inside functions because it will not throw an error if a package is not installed. require() also returns a value of TRUE or FALSE depending on whether the package loaded successfully or not. However, I almost always use the library() function in my scripts. Using library() and require() both load a package‚Äôs named components (its ‚Äúnamespace‚Äù) and attach those to the global environments search list.\n\n\nNOTE: When loading a package with library() or require(), the package name need not be in quotes, although it works if you were to do that.\n\nBe aware that if a named function of a package conflicts with one in an already loaded/attached package, then by default R will warn of the conflict. In that case, it is good form to use explicit function calls, i.e., use the :: operator to specify first the package and then the function (e.g., dplyr::filter()) you wish to call.\n\n\n\nWorking with Packages\n\nYou can use either of the following to list the set of packages you have installed:\n\nlibrary()\ninstalled.packages()\n\nThe command (.packages()) can be used print out the set packages that have been loaded/attached in your current workspace.\n\n\nNOTE: In the example above, the .packages() function is wrapped in parentheses to immediately print the result of the function.\n\nIn RStudio, you can also see a list of all loaded packages by clicking the down arrow next to ‚ÄúGlobal Environment‚Äù in the Environment tab.\n\n\n\n\n\n\n\n\n\n\nThe command detach(package:&lt;package name&gt;), where ‚Äúpackage name‚Äù, not in quotes, is the name of the package you want to unload, will unload a currently loaded package from memory. You can also do this interactively in RStudio by unchecking the box next to the package name in the Packages tab.\nTo update your installed packages to the latest version, you can use the function update.packages(). Using RStudio, you can also select ‚ÄúUpdate‚Äù from the Packages tab to get an interactive dialog box showing you what updates are available from CRAN and letting you install them.\n\n\n\n\n\n\n\n\n\n\n\nTo remove installed packages from your R installation, you can use the function remove.packages() or click the small ‚Äúx‚Äù to the right of the package name in the RStudio packages tab.\n\n\n\n\n\n\n\n\n\n\n\nTo process several package at once, you can pass a vector of package names to many of these functions as an argument, e.g., remove.packages(c(\"abc\",\"citr\"))\nFinally, the {easypackages} packages makes installing and loading multiple packages ‚Äúeasy‚Äù by introducing two helper functions, packages() and libraries(). Both let you specify a vector of package names to either install (e.g., packages(c(\"tidyverse\", \"magrittr\"))) or load (e.g., libraries(c(\"tidyverse\", \"magrittr\"))). For these functions, package names need to be specified in quotation marks.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Extending the Functionality of ***R***</span>"
    ]
  },
  {
    "objectID": "03-module.html#concept-review",
    "href": "03-module.html#concept-review",
    "title": "3¬† Extending the Functionality of R",
    "section": "Concept Review",
    "text": "Concept Review\n\nWorking with packages: install_packages(), library(), require(), detach(), update.packages(), remove.packages(), and {easypackages}\nEnvironments and namespaces: ::, search()",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Extending the Functionality of ***R***</span>"
    ]
  },
  {
    "objectID": "04-module.html",
    "href": "04-module.html",
    "title": "4¬† Fundamentals of the R Language",
    "section": "",
    "text": "4.1 Objectives",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Fundamentals of the ***R*** Language</span>"
    ]
  },
  {
    "objectID": "04-module.html#objectives",
    "href": "04-module.html#objectives",
    "title": "4¬† Fundamentals of the R Language",
    "section": "",
    "text": "The goal of this module is review important conceptual aspects of the R language as well as practices for updating R components of interest.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Fundamentals of the ***R*** Language</span>"
    ]
  },
  {
    "objectID": "04-module.html#r-objects",
    "href": "04-module.html#r-objects",
    "title": "4¬† Fundamentals of the R Language",
    "section": "4.2 R Objects",
    "text": "4.2 R Objects\nAlmost everything in R can be thought of as an object, including variables, functions, complex data structures, and environments.\n\nClasses, Modes, and Types\nObjects in R fall into different classes. There are a few basic (or atomic) classes that pertain to variables: numeric (real numbers), integer (integer numbers), character (for text), logical (Boolean values, i.e., TRUE or FALSE, represented as 1 and 0, respectively), complex (for imaginary numbers), and factor (for defined levels of categorical variables‚Ä¶ we will talk more about factors later on). There are other classes beyond this set of atomic classes relevant to variables. For example, both built-in and user defined functions have the class function. You can ask R to return the class of any object with the class() function, and R objects can have more than one class. You can think of class as being a property of an object that determines how generic functions operate with it.\nExamples:\n\n# class of a variable\nx &lt;- 4\nclass(x)\n\n## [1] \"numeric\"\n\nx &lt;- \"hi there\"\nclass(x)\n\n## [1] \"character\"\n\n# class of a function\nclass(mean)\n\n## [1] \"function\"\n\n\nIn R, environments are objects as well.\n\nWhat is the class of the global environment, where we have been binding values to variable names? To check, use class(globalenv()).\n\n\n\nShow Code\nclass(globalenv())\n\n\nShow Output\n## [1] \"environment\"\n\n\n\nObjects in R also each have a mode and a base type. These are often closely aligned with and similar to the class of an object, but the three terms refer to slightly different things. If an object has no specific class assigned to it, its class is typically the same as its mode. Mode is a mutually exclusive classification of objects, according to their basic structure. When we coerce an object to another basic structure, we are changing its mode but not necessarily the class.\n\n# mode of a variable\nx &lt;- 4\nmode(x)\n\n## [1] \"numeric\"\n\nx &lt;- \"hi there\"\nmode(x)\n\n## [1] \"character\"\n\n# mode of a function\nmode(mean)\n\n## [1] \"function\"\n\n\n\n# type of a variable\nx &lt;- 4\ntypeof(x)\n\n## [1] \"double\"\n\nx &lt;- \"hi there\"\ntypeof(x)\n\n## [1] \"character\"\n\n# type of a function\ntypeof(mean)\n\n## [1] \"closure\"\n\n\n\nNOTE: For more details on the difference between the class, mode, and base type of an object, check out the book Advanced R, Second Edition by Hadley Wickham (RStudio).\n\n\n\nVectors\nR also supports a variety of data structures for variable objects, the most fundamental of which is the vector. Vectors are variables consisting of one or more values of the same type, e.g., student‚Äôs grades on an exam. The class of a vector has to be one of the atomic classes described above. A scalar variable, such as a constant, is simply a vector with only one value.\n\nThere are lots of ways to create vectors‚Ä¶ one of the most common is to use the c() or ‚Äúcombine‚Äù command:\n\n\nx &lt;- c(15, 16, 12, 3, 21, 45, 23)\nx\n\n## [1] 15 16 12  3 21 45 23\n\ny &lt;- c(\"once\", \"upon\", \"a\", \"time\")\ny\n\n## [1] \"once\" \"upon\" \"a\"    \"time\"\n\nz &lt;- \"once upon a time\"\nz\n\n## [1] \"once upon a time\"\n\n\n\nWhat is the class of the vector x? Of z? Use the class() function to check.\n\n\n\nShow Code\nclass(x)\n\n\nShow Output\n## [1] \"numeric\"\n\n\n\nShow Code\nclass(z)\n\n\nShow Output\n## [1] \"character\"\n\n\n\n\nWhat happens if you try the following assignment: x &lt;- c(\"2\", 2, \"zombies\")? What is the class of vector x now?\n\n\n\nShow Code\nx &lt;- c(\"2\", 2, \"zombies\")\nclass(x)\n\n\nShow Output\n## [1] \"character\"\n\n\n\nThis last case is an example of coercion, which happens automatically and often behind the scenes in R. When you attempt to combine different types of elements in the same vector, they are coerced to all be of the same type - the most restrictive type that can accommodate all of the elements. This takes place in a fixed order: logical ‚Üí integer ‚Üí double ‚Üí character. For example, combining a character and an integer yields a character; combining a logical and a double yields a double.\nYou can also deliberately coerce a vector to be represented as a different base type by using an as.*() function, like as.logical(), as.integer(), as.double(), or as.character().\n\nx &lt;- c(3, 4, 5, 6, 7)\nx\n\n## [1] 3 4 5 6 7\n\ny &lt;- as.character(x)\ny\n\n## [1] \"3\" \"4\" \"5\" \"6\" \"7\"\n\n\nAnother way to create vectors is to use the sequence operator, :, which creates a sequence of values from spanning from the left side of the operator to the right, in increments of 1:\n\nx &lt;- 1:10\nx\n\n##  [1]  1  2  3  4  5  6  7  8  9 10\n\nx &lt;- 10:1\nx\n\n##  [1] 10  9  8  7  6  5  4  3  2  1\n\nx &lt;- 1.3:10.5\nx\n\n##  [1]  1.3  2.3  3.3  4.3  5.3  6.3  7.3  8.3  9.3 10.3\n\n\n\nNOTE: Wrapping an assignment in parentheses, as in the code block below, allows simultaneous assignment and printing to the console!\n\n\n(x &lt;- 40:45)\n\n## [1] 40 41 42 43 44 45\n\n\nWe can also create more complex sequences using the seq() function, which takes several arguments:\n\nx &lt;- seq(from = 1, to = 10, by = 2)\n# skips every other value\nx\n\n## [1] 1 3 5 7 9\n\nx &lt;- seq(from = 1, to = 10, length.out = 3)\n# creates 3 evenly spaced values\nx\n\n## [1]  1.0  5.5 10.0\n\n\n\n\nAttributes and Structure\nMany objects in R also have attributes associated with them, which we can think of as metadata, or data describing the object. Some attributes are intrinsic to an object. For example, a useful attribute to know about a vector object is the number of elements in it, which can be queried using the length() command.\n\nlength(x)\n\n## [1] 3\n\n\nWe can also get or assign arbitrary attributes to an object using the function attr(), which takes two arguments: the object whose attributes are being assigned and the name of the attribute.\n\n# we can assign arbitary attributes to the vector x\nattr(x, \"date collected\") &lt;- \"2019-01-22\"\nattr(x, \"collected by\") &lt;- \"Anthony Di Fiore\"\nattributes(x)  # returns a list of attributes of x\n\n## $`date collected`\n## [1] \"2019-01-22\"\n## \n## $`collected by`\n## [1] \"Anthony Di Fiore\"\n\nclass(attributes(x))  # the class of a list is 'list'\n\n## [1] \"list\"\n\n# a 'list' is another R data structure\nattr(x, \"date collected\")  # returns the value of the attribute\n\n## [1] \"2019-01-22\"\n\n\nFinally, every object in R also has a structure, which can be queried using the str() command.\n\nstr(x)  # structure of the variable x\n\n##  num [1:3] 1 5.5 10\n##  - attr(*, \"date collected\")= chr \"2019-01-22\"\n##  - attr(*, \"collected by\")= chr \"Anthony Di Fiore\"\n\nstr(mean)  # struture of the function mean\n\n## function (x, ...)\n\nstr(globalenv())  # structure of the global environment\n\n## &lt;environment: R_GlobalEnv&gt;\n\nstr(attributes(x))  # attribute names are stored as a list\n\n## List of 2\n##  $ date collected: chr \"2019-01-22\"\n##  $ collected by  : chr \"Anthony Di Fiore\"\n\n\n\nNOTE: The glimpse() function from the {dplyr} package also yields information on the structure of an object, sometimes in a more easily-readable format than str().\n\n\n\nCHALLENGE:\nTry some vector math using the console in RStudio:\n\nAssign a sequence of numbers from 15 to 28 to a vector, x.\n\n\nNOTE: There are at least two different ways to do this!\n\n\nThen, assign a sequence of numbers from 1 to 4 to a vector, y.\nFinally, add x and y. What happens?\n\n\n\nShow Code\nx &lt;- 15:28  # or x &lt;- c(15, 16, 17...)\ny &lt;- 1:4\n(x + y)\n\n\nShow Output\n##  [1] 16 18 20 22 20 22 24 26 24 26 28 30 28 30\n\n\n\n\nUse the up arrow to recall the last command from history and modify the command to store the result of the addition to a variable, z. What kind of object is z? Examine it using the class() function. What is the length of z?\n\nNow, think carefully about this output‚Ä¶ there are two important things going on.\nFirst, R has used vectorized addition in creating the new variable. The first element of x was added to the first element of y, the second element of x was added to the second element of y, etc.\nSecond, in performing this new variable assignment, the shorter vector has been recycled. Thus, once we get to the fifth element in x we start over with the first element in y.\nThis means we can very easily do things like adding a constant to all of the elements in a vector or multiplying all the elements by a constant.\n\ny &lt;- 2\n# note that we can wrap a command in parentheses for simultaneous\n# assignment/operation and printing\n(z &lt;- x + y)\n\n##  [1] 17 18 19 20 21 22 23 24 25 26 27 28 29 30\n\n(z &lt;- x * y)\n\n##  [1] 30 32 34 36 38 40 42 44 46 48 50 52 54 56\n\n\nMany function operations in R are also vectorized, meaning that if argument of a function is a vector, but the function acts on a single value, then the function will be applied to each value in the vector and will return a vector of the same length where the function has been applied to each element.\n\nx &lt;- 1:20\n(logx &lt;- log(x))\n\n##  [1] 0.0000000 0.6931472 1.0986123 1.3862944 1.6094379 1.7917595 1.9459101\n##  [8] 2.0794415 2.1972246 2.3025851 2.3978953 2.4849066 2.5649494 2.6390573\n## [15] 2.7080502 2.7725887 2.8332133 2.8903718 2.9444390 2.9957323\n\n(x2 &lt;- x^2)\n\n##  [1]   1   4   9  16  25  36  49  64  81 100 121 144 169 196 225 256 289 324 361\n## [20] 400\n\n(y &lt;- 4 * x + 3)\n\n##  [1]  7 11 15 19 23 27 31 35 39 43 47 51 55 59 63 67 71 75 79 83\n\n\nWe can use the {base} R function plot() to do some quick visualizations.\n\n# `plot()` takes values of x and y values as the first two arguments, and the\n# `type='o'` argument superimposes points and lines\nplot(x, logx, type = \"o\")\n\n\n\n\n\n\n\nplot(x, x2, type = \"o\")\n\n\n\n\n\n\n\nplot(x, y, type = \"o\")\n\n\n\n\n\n\n\n\n\n\nCHALLENGE:\n\nUse the rnorm() function to create a vector, s that contains a set of random numbers drawn from a normal distribution with mean 80 and standard deviation 10. Try doing this with n = 10, n = 100, n = 1000, n = 10000.\n\n\nHINT: Use ?rnorm or help(rnorm) to access the help documentation on how to use the rnorm() function.\n\nThen, use the hist() function to plot a histogram showing the distribution of these numbers.\n\n\nShow Code\ns &lt;- rnorm(n = 10000, mean = 80, sd = 10)\nhist(s)  # hist() plots a simple histogram of values for s\n\n\n\n\n\n\n\n\n\n\nUse the mean() and sd() functions to calculate the mean and standard deviation of s. Here, the whole vector is used as the argument of the function, i.e., the function applies to a set of values not a single value. The function thus returns a vector of length 1.\n\n\n\nShow Code\nmean(s)\n\n\nShow Output\n## [1] 80.06098\n\n\n\nShow Code\nsd(s)\n\n\nShow Output\n## [1] 10.09933",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Fundamentals of the ***R*** Language</span>"
    ]
  },
  {
    "objectID": "04-module.html#scripts-and-functions",
    "href": "04-module.html#scripts-and-functions",
    "title": "4¬† Fundamentals of the R Language",
    "section": "4.3 Scripts and Functions",
    "text": "4.3 Scripts and Functions\nAs mentioned previously, scripts in R are simply text files that store an ordered list of commands, which can be used to link together sets of operations to perform complete analyses and show results.\nFor example, you could enter the lines below into a text editor and then save the script in a file named ‚Äúmy_script.R‚Äù in a folder called src/ inside your working directory.\n\nx &lt;- 1:10\ns &lt;- sum(x)\nl &lt;- length(x)\nm &lt;- s/l\nprint(m)\n\n## [1] 5.5\n\n\nIf you save a script, you can then use the source() function (with the path to the script file of interest as an argument) at the console prompt (or in another script) to read and execute the entire contents of the script file. In RStudio you may also go to Code &gt; Source to run an entire script, or you can run select lines from within a script by opening the script text file, highlighting the lines of interest, and sending those lines to the console using the ‚ÄúRun‚Äù button or the appropriate keyboard shortcut, ‚åò-RETURN (Mac) or control-R (PC).\n\nsource(\"src/my_script.R\")\n\n## [1] 5.5\n\n# assuming the file was saved with the '.R' extension...\n\nIn an R script, you might use several lines of code to accomplish a single analysis, but if you want to be able to flexibly perform that analysis with different input, it is good practice to organize portions of your code within a script into user-defined functions. A function is a bit of code that performs a specific task. It may take arguments or not, and it may return nothing, a single value, or any R object (e.g., a vector or a list, which is another data structure will discuss later on). If care is taken to write functions that work under a wide range of circumstances, then they can be reused in many different places. Novel functions are the basis of the thousands of user-designed packages that are what make R so extensible and powerful.\n\nCHALLENGE:\nTry writing a function!\n\nOpen a new blank document in RStudio\n\nFile &gt; New &gt; R Script\n\nType in the code below to create the say_hi() function, which adds a name to a greeting:\n\n\n# this function takes one argument, `x`, appends the value of that argument to\n# a greeting, and then prints the whole greeting\nsay_hi &lt;- function(x) {\n    hi &lt;- paste(\"Greetings, \", x, \"!\", sep = \"\")\n    return(hi)\n}\n\n\nNOTE: Here, the paste() command allows string concatenation. Alternatively, we could use paste0() and omit the sep= argument.\n\nIn general, the format for a function is as follows: function_name &lt;- function(&lt;arguments&gt;) {&lt;function code&gt;}\nYou can send your new function to the R console by highlighting it in the editor and hitting ‚åò-RETURN (Mac) or control-ENTER (PC). This loads the function as an object into the working environment.\n\n\n\n\n\n\n\n\n\n\nNow we can create some test data and call the function. What are the results?\n\n\nname1 &lt;- \"Rick Grimes\"\nname2 &lt;- \"Ruth Bader Ginsburg\"\nsay_hi(name1)\n\n## [1] \"Greetings, Rick Grimes!\"\n\nsay_hi(name2)\n\n## [1] \"Greetings, Ruth Bader Ginsburg!\"\n\n\nYou can also save the function in a file, e.g., in the src/ folder inside your working directory, and then source(\"&lt;path&gt;\") it in code. Save your function script as ‚Äúsay_hi.R‚Äù and then run the following:\n\nsource(\"src/say_hi.R\")\nname3 &lt;- \"Charles Darwin\"\nsay_hi(name3)\n\n## [1] \"Greetings, Charles Darwin!\"",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Fundamentals of the ***R*** Language</span>"
    ]
  },
  {
    "objectID": "04-module.html#quitting-r-and-saving-your-work",
    "href": "04-module.html#quitting-r-and-saving-your-work",
    "title": "4¬† Fundamentals of the R Language",
    "section": "4.4 Quitting R and Saving your Work",
    "text": "4.4 Quitting R and Saving your Work\nWorking in RStudio, you can save script files (which, again, are just plain text files) using standard dialog boxes.\nWhen you go to quit R (by using the q() function or by trying to close RStudio), you may be asked whether you want to‚Ä¶\n‚ÄúSave workspace image to &lt;path&gt;/.Rdata?‚Äù, where &lt;path&gt; is the path to your working directory.\nSaying ‚ÄúSave‚Äù will store all of the contents of your workspace in a single hidden file, named ‚Äú.Rdata‚Äù. The leading period (‚Äú.‚Äù) makes this invisible to most operating systems, unless you deliberately make it possible to see hidden files.\n\nNOTE: I tend to NOT save my workspace images. You can change the default behavior for this by editing RStudio‚Äôs preferences and choosing ‚ÄúAlways‚Äù, ‚ÄúNever‚Äù, or ‚ÄúAsk‚Äù.\n\n\n\n\n\n\n\n\n\n\nThe next time you start R, the workspace from ‚Äú.RData‚Äù will be loaded again automatically, provided you have not changed your working directory and you have not unchecked ‚ÄúRestore .RData into workspace at startup‚Äù in preferences.\nA second hidden file, ‚Äú.Rhistory‚Äù, will also be stored in the same directory, which will contain a log of all commands you sent to the console, provided you have not unchecked ‚ÄúAlways save history‚Äù.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Fundamentals of the ***R*** Language</span>"
    ]
  },
  {
    "objectID": "04-module.html#updating-r",
    "href": "04-module.html#updating-r",
    "title": "4¬† Fundamentals of the R Language",
    "section": "4.5 Updating R",
    "text": "4.5 Updating R\nR has been under continuous and active development since its inception in the late 1990s, and several updates are made available each year. These update help to fix bugs, improve speed and computational efficiency, and add new functionality to the software. The following information on how to update R is based on this post from Stack Overflow\n\nStep 1: Get the latest version of R\n\nGo to the R Project website.\nClick on CRAN in the sidebar on the left.\nChoose the CRAN Mirror site that you like.\nClick on Download R for‚Ä¶ (choose your operating system).\nFollow the installation procedures for your system.\nRestart RStudio.\n\nStep 2: Relocate your packages\n\nTo ensure that your packages work with your shiny new version of R, you need to:\n\nMove the packages from your old R installation into the new one.\nOn MacOS, this typically means moving all library folders from ‚Äú/Library/Frameworks/R.framework/Versions/3.5/Resources/library‚Äù to ‚Äú/Library/Frameworks/R.framework/Versions/4.3/Resources/library‚Äù\n\n\n\n\nNOTE: You should replace ‚Äú3.5‚Äù and ‚Äú4.3‚Äù with whatever versions you are upgrading from and to, respectively.\n\n\nOn Windows, this typically means moving all library folders from ‚ÄúC:\\Program Files\\R\\R-3.5\\library‚Äù to ‚ÄúC:\\Program Files\\R\\R-4.3\\library‚Äù (if your packages are installed at the system level) or from ‚ÄúC:\\Users\\&lt;user name&gt;\\R\\win-library\\3.5\\‚Äù to ‚ÄúC:\\Users\\&lt;user name&gt;\\R\\win-library\\4.3\\‚Äù (if your packages are installed at the user level)\n\n\nNOTE: You only need to copy whatever packages are not already in the destination directory, i.e., you do not need to overwrite your new {base} package, etc., with your old one.\n\n\nIf those paths do not work for you, try using installed.packages() to find the proper path names. These may vary on your system, depending on where you installed R\nNow you can update your packages by typing update.packages() in your RStudio console, and answering ‚Äúy‚Äù to all of the prompts.\nFinally, to reassure yourself that you have done everything correctly, type these two commands in the RStudio console to see what you‚Äôve got in terms of what version of R you are running, the number of packages you have installed, and what packages you have loaded:\n\n\nversion\npackageStatus()\n(.packages())",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Fundamentals of the ***R*** Language</span>"
    ]
  },
  {
    "objectID": "04-module.html#concept-review",
    "href": "04-module.html#concept-review",
    "title": "4¬† Fundamentals of the R Language",
    "section": "Concept Review",
    "text": "Concept Review\n\nCharacteristics of R objects: class(), mode(), typeof(), str(), attributes(), dplyr::glimpse()\nUsing scripts: source()\n‚Äú.RData‚Äù and ‚Äú.Rhistory‚Äù files\nUpdating R and packages",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Fundamentals of the ***R*** Language</span>"
    ]
  },
  {
    "objectID": "05-module.html",
    "href": "05-module.html",
    "title": "5¬† Basics of Version Control",
    "section": "",
    "text": "5.1 Objectives",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Basics of Version Control</span>"
    ]
  },
  {
    "objectID": "05-module.html#objectives",
    "href": "05-module.html#objectives",
    "title": "5¬† Basics of Version Control",
    "section": "",
    "text": "To introduce the basics of working with version control systems in R, using git and GitHub",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Basics of Version Control</span>"
    ]
  },
  {
    "objectID": "05-module.html#preliminaries",
    "href": "05-module.html#preliminaries",
    "title": "5¬† Basics of Version Control",
    "section": "5.2 Preliminaries",
    "text": "5.2 Preliminaries\n\nInstall this package in R: {usethis}",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Basics of Version Control</span>"
    ]
  },
  {
    "objectID": "05-module.html#version-control-systems",
    "href": "05-module.html#version-control-systems",
    "title": "5¬† Basics of Version Control",
    "section": "5.3 Version Control Systems",
    "text": "5.3 Version Control Systems\nFor any of us who work with data files and associated analyses and documents over a long period of time, it can become very complicated to keep track of the ‚Äúlatest version‚Äù of what we‚Äôre working on. This is especially true if we are collaborating with others on a project and need to share these things back and forth. This is a problem that software developers have been dealing with for a long time, however, and there is a robust ecosystem of ‚Äúversion control systems‚Äù (VCSs) out there for dealing with this problem. The basic idea behind these systems is that all of the work on a particular project is stored in a repository (or repo), and as you work on and modify files and data for the project, you ‚Äúcommit‚Äù your changes periodically. The VCS keeps track of what changes between commits and allows you to roll back to previous versions if need be.\nYou can also branch a repository - basically, make a duplicate copy of it - and work on the new branch and then, later, merge your changes back into the main branch. Multiple people can each work on different branches simultaneously, and the software will take care of looking for and highlighting changes that occur on different branches so that they can be merged back in appropriately. This module will introduce you to one such system.\n\nNOTE: The source for some of the information covered below, along with a host of other valuable information about using R and git, is provided on the web book Happy Git and GitHub for the useR by Jenny Bryan (RStudio).",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Basics of Version Control</span>"
    ]
  },
  {
    "objectID": "05-module.html#first-steps-with-git-and-github",
    "href": "05-module.html#first-steps-with-git-and-github",
    "title": "5¬† Basics of Version Control",
    "section": "5.4 First Steps with git and GitHub",
    "text": "5.4 First Steps with git and GitHub\n\nInstalling git\nOne of the most popular and frequently used VCSs is git.\n\nDownload and install git for your operating system. This will put the appropriate software on your machine.\n\n\n\n\n\n\n\n\n\n\n\nNOTE: To install on MacOS, binary installers may not be available for the newest version of git. Thus, you may need to first install the package manager software Homebrew and then run brew install git at the command line. Visit https://brew.sh/ for instructions on installing Homebrew.\n\nNote that git is simply a bit of software running on your own computer that watches the contents of particular user-specified folders.\nLet us now check if you have installed correctly‚Ä¶\n\nIn RStudio, go to the Tools menu and select Terminal &gt; New Terminal or, if it is visible, click on the Terminal tab in the lower left pane of RStudio interface. In the terminal window, type, which git. On MacOS you should see something like: /usr/bin/git or /usr/local/bin/git, which is the path to your git executable file. On Windows, you should see something like: /cmd/git\n\n\n(base) ad26693 üêµ  $ which git\n/usr/bin/git\n\n\nNOTE: Your terminal prompt may look different than how I have set mine up. My prompt ends with $, which is typical for the bash shell on MacOS. You may see, for example, a prompt that ends in %, which is typical for the zsh shell.\n\n\nIn the same terminal, type git --version to see which version of git you are running. You should see something like git version 2.39.5.\n\n\n(base) ad26693 üêµ  $ git --version\ngit version 2.39.5 (Apple Git-154)\n\nRunning the git --version command or the command git config on MacOS may also prompt you to install a set of developer command line tools‚Ä¶ this is okay (and good!) to accept. Doing so will install the Xcode Command Line Tools. You can also install these using the command xcode-select --install at the terminal prompt.\n\nNOTE: See also Chapter 6 of the web book, Happy Git and GitHub for the useR\n\n\n\nRegistering a GitHub Account\nThe git software we just installed is strictly a command-line tool and is a bit difficult to pick up from scratch. However, the remote repository hosting service GitHub provides an easy to use web-based graphical interface for repository management and version control. GitHub offers the distributed version control and source code management functionality of git plus some additional features. We will get introduced to the main features of git by using GitHub.\n\nGo to GitHub.com, choose Sign up for GitHub and create your own account. You will be asked to provide an username and email address and to select a password.\n\n\n\n\n\n\n\n\n\n\n\n\nTelling git Who You Are\nThe git VCS keeps track of who makes changes to any files in a watched repository. We thus need to tell git (on your local computer) who you are so that when you make changes to a repository, either locally or remotely, they are associated with a particular user name and email. To do this within RStudio, again select Tools &gt; Terminal &gt; New Terminal (which opens a new terminal window in the bottom left pane of the IDE) or Tools &gt; Shell‚Ä¶ (which will open up an external terminal window). Then, type in the following replacing  and  with the name and the email addressed associated with the GitHub account you just set up.\n\ngit config --global user.name \"&lt;your name&gt;\"\ngit config --global user.email \"&lt;your email address&gt;\"\n\n\nNOTE: Your name and email address do not need to be in quotes unless there is a space in one of them.\n\nAlternatively, you can enter these commands directly in a terminal window that you open yourself, rather than one opened from within RStudio (e.g., by opening Applications/Utilities/Terminal.app on MacOS).\n\nNOTE: Under this setup, git is now set to link any commits to the username associated with the email address used to tag your commits, even if you enter a different user name in the --global options here. If you use an email address that is not already associated with a GitHub account, then the username entered here in the local config will appear associated with your commits.\n\nThe command‚Ä¶\n\ngit config --global --list\n\n‚Ä¶ can be used to check if you set things up correctly.\nAlternatively, you can set things up from within R, rather than using a terminal, as follows:\n\n# uncomment the following line, if needed, to install the {usethis} package\n# install.packages('usethis')\n\nlibrary(usethis)\nuse_git_config(user.name = \"your name\", user.email = \"your email address\")\ndetach(package:usethis)\n\n\nNOTE: See also Chapter 7 of the web book Happy Git and GitHub for the useR\n\n\n\nAuthenticating Your GitHub Account\nBefore you can access certain resources and functionality on GitHub, you will need to ‚Äúauthenticate‚Äù your access to GitHub from each local machine that you might work on. Basically, this process involves providing or confirming your access credentials that prove you are who you say you are when accessing GitHub. Access credentials can include a user name and password (often coupled with 2-factor authentication), a ‚Äúpersonal access token‚Äù, and/or an SSH key. Depending on whether we are connecting to GitHub via HTTPS (‚Äúhypertext transfer protocol secure‚Äù) or SSH (‚Äúsecure shell‚Äù), we will need to use different methods of authentication (see further details below).\nTo connect to GitHub via HTTPS (which is the way I typically recommend), we will need to authenticate using a ‚Äúpersonal access token‚Äù, or PAT. When we then try to connect to GitHub from git and are prompted for a password, we would enter this personal access token (PAT) instead. (Password-based authentication for connecting to GitHub was removed a few years back, and using a PAT is more secure.)\nTo create a personal access token, follow the steps below:\n\nStep 1:\nMake sure your email address is verified in GitHub. You probably will have done this already if you have set up a GitHub account.\n\nIn the upper-right corner of any page on GitHub, click your profile photo, then click ‚ÄúSettings‚Äù.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the left sidebar, under the ‚ÄúAccess‚Äù section, click ‚ÄúEmails‚Äù.\nUnder your email address, click ‚ÄúResend verification email‚Äù.\n\nGitHub will then send you an email with a link in it. After clicking that link, you will be taken to your GitHub dashboard and see a confirmation banner.\n\nNOTE: If you have already verified your email, the ‚ÄúResend verification email‚Äù option may not appear.\n\n\n\nStep 2:\nGenerate a new PAT.\n\nAgain, in the upper-right corner of any page on GitHub, click your profile photo, then click ‚ÄúSettings‚Äù\nIn the left sidebar of your profile page, scroll to the bottom and click ‚ÄúDeveloper settings‚Äù\n\n\n\n\n\n\n\n\n\n\n\nIn the left sidebar, expand the section on ‚ÄúPersonal access tokens‚Äù.\n\n\n\n\n\n\n\n\n\n\n\nThere are two types of tokens you can create, ‚Äúfine-grained‚Äù and ‚Äúclassic‚Äù, either of which is fine to create. The former offers the opportunity to finely tune control of what actions on GitHub you would be able to access through R, but the latter is easier to set up. Choose ‚ÄúGenerate New Token‚Äù and select ‚Äúclassic‚Äù‚Ä¶\n\n\n\n\n\n\n\n\n\n\n\nGive your token a descriptive name in the ‚ÄúNote‚Äù field.\nSet an expiration date for the token.\nSelect scope of permissions you would like to grant this token. To use your token to access repositories from the command line and perform other actions with repositories, select the ‚Äúrepo‚Äù checkbox plus any other actions you would like to be able to control.\n\n\n\n\n\n\n\n\n\n\n\nScroll to the bottom of the page and click the green ‚ÄúGenerate token‚Äù button.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe token will be a string of 40 random letters and digits. Once it has been created, copy and store it in a safe place, like a password manager. You should enter the token instead of your git password when performing operations over HTTPS. Personal access tokens can only be used for HTTPS git operations. If you instead want to use SSH, you will have to set that up using an RSA key (see details below for how to do this within RStudio).\n\n\n\nCaching Your GitHub Credentials\nIf you are not prompted for your username and password, your GitHub credentials may already be cached on your computer. If needed, you can update your credentials in your computer‚Äôs keychain to replace your old password with the token.\nIf you are running MacOS, there are several ways you can explicitly cache your credentials if you find that you are asked repeatedly for them when trying to connect to GitHub. Both require that the Homebrew package manager is installed. Visit https://brew.sh/ for instructions on installing Homebrew.\n\nUsing the GitHub CLI (Command Line Interface)\nThe GitHub CLI will automatically store your git credentials for you when you choose HTTPS as your preferred protocol for git operations and answer ‚Äúyes‚Äù to the prompt asking if you would like to authenticate to git with your GitHub credentials.\n\nInstall the GitHub CLI by running brew install gh at a command prompt in a terminal window.\nIn the command line, enter gh auth login, then follow the prompts.\nWhen prompted for your preferred protocol for git operations, select HTTPS\nWhen asked if you would like to authenticate to git with your GitHub credentials, enter ‚ÄúY‚Äù\n\n\n\nUsing the git Credential Manager (GCM)\ngit Credential Manager (GCM) is another way to store your credentials securely and connect to GitHub over HTTPS. With GCM, you do not have to manually create and store a personal access token, as GCM manages authentication on your behalf, including 2FA (two-factor authentication).\n\nInstall GCM by running the following commands at a terminal command prompt:\n\n\nbrew tap microsoft/git\nbrew install --cask git-credential-manager-core\n\nThe next time you clone an HTTPS URL that requires authentication, e.g., for a private repository, git will prompt you to log in using a browser window. You may be asked to authorize an ‚ÄúOAuth app‚Äù. If your account or organization requires two-factor authorization, you will also need to complete the 2FA challenge.\nOnce you have authenticated successfully, your credentials will be stored in your macOS keychain and will be used every time you clone an HTTPS URL. git will not require you to type your credentials in the command line again unless you change your credentials.\nIf you are running Windows, GCM is included with the installer for Git for Windows. During installation you will be asked to select a credential helper, with GCM being set as the default.\nThe information above was pulled from the authentication and account security sections of GitHub‚Äôs documentation. See those links for more details and recommendations for troubleshooting any problems.\n\n\n\n5.4.1 Creating and Registering GitHub Credentials from RStudio\nUse can also use the {usethis} and {gitcreds} packages to generate new PATs and to register your credentials with git from within RStudio.\n\nCreate a GitHub PAT via R by running: usethis::create_github_token(). This command will access a page in your browser for setting up a new (classic) token, pre-populated with appropriate scopes for working in R\nCopy and store the token in a safe place, such as a password manager\nUse the command gitcreds::gitcreds_set() to register your token with git.\nUse the command usethis::git_sitrep() to get the rundown on your current git credentials.\n\nTroubleshooting: If you are having problems, you can try the following two steps, after generating and saving your PAT:\n\nRun credentials::git_credential_forget() to clear the credentials cache\nRun credentials::set_github_pat() and follow the prompts, which may involving pasting your saved token into one of the prompts.\n\n\n\nCreating a Remote Repository\nNow, we will begin demonstrating how a VCS works by first setting up and working with a remote repository (colloquially, a ‚Äúrepo‚Äù) hosted on GitHub. You can do this either by following the instructions laid out in the ‚ÄúHello World‚Äù GitHub guide or as below:\n\nSign in to GitHub in a web browser, navigate to the ‚ÄúRepositories‚Äù tab on your user dashboard, and press the green ‚ÄúNEW‚Äù button.\n\n\n\n\n\n\n\n\n\n\n\nEnter a name for your repository (e.g., ‚Äútest-repo‚Äù). By default, the repository will be designated as a ‚Äúpublic‚Äù one. You can create ‚Äúprivate‚Äù repositories, too, but this typically requires paying a hosting fee.\n\n\nNOTE: Do not include spaces in your repository name!\n\n\nIt is also a good idea to click the ‚ÄúAdd a README file‚Äù checkbox under ‚ÄúInitialize this repository with:‚Äù so that your repository includes at least one file in it. A README file serves as a nice, introductory landing page for your repo. By creating at least one file in your repo, you will also set a default branch, named ‚Äúmain‚Äù, for\nWhen you are done, click the large green ‚ÄúCreate repository‚Äù button.\n\n\nNOTE: It is also good to click the checkbox for ‚ÄúAdd .gitignore‚Äù and select ‚Äú.gitignore template R‚Äù. This will create a hidden file in which you can specify files within your repo that you DO NOT want or need to keep under version control.\n\n\n\n\n\n\n\n\n\n\nCongratulations! You‚Äôve just set up your first GitHub hosted repository!\n\n\nChanges and Commits\n\nAs a first step to working with version control, let‚Äôs make some edits to the remote ‚ÄúREADME‚Äù file in your repository using the browser-based Markdown editor included in GitHub. Clicking the pencil icon at the top right of the ‚ÄúREADME‚Äù file will bring up the editor‚Ä¶\n\n\n\n\n\n\n\n\n\n\n\nYou can then type in edits, using Markdown styling.\n\n\n\n\n\n\n\n\n\n\n\nNOTE: Markdown is essentially a set of rules for how to easily style plain text files in such a way that can be easily converted and rendered in HTML, the structural language of the web. This module, and this entire website, for example, were written in RStudio using a particular version of Markdown called ‚ÄúQuarto Markdown‚Äù. GitHub has a nice, short tutorial that you can follow about ‚ÄúMastering Markdown‚Äù. There is also a nice GitHub guide on ‚ÄúDocumenting your Projects on GitHub‚Äù that provides a useful overview of the benefits of good documentation.\n\n\nWhen you are done editing, commit the changes to your remote ‚ÄúREADME‚Äù file by scrolling to the bottom of the editor window and clicking the ‚ÄúCommit Changes‚Äù button. Note that every commit you make to a repository must have some sort of brief, descriptive message associated with it. Here, GitHub has populated the message field with default text (‚ÄúUpdate README.md‚Äù), but you can change this and, optionally, add a fuller message as well.\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with Branches\nSo far, all of the changes you have made have been to the main branch of your repository, but one of the powerful features of git is that you can create different versions or branches of your repository to try out new things and then merge these back into the main branch. This feature - along with automated checking for merge conflicts created when different modifications have been made on two different branches - is what makes git and other VCSs so valuable for software development.\nHere, we are going to create a new branch of our repository, edit a file in that new branch, and then merge the changes back into the main branch.\n\nCreate a new branch of your repository (e.g., ‚Äúreadme-edits‚Äù) by opening the ‚Äúbranch‚Äù popdown menu, typing a new branch name, and then clicking on the ‚ÄúCreate branch:‚Äù text. This will switch you over to the new branch.\n\n\n\n\n\n\n\n\n\n\n\nUsing the same procedure as above, make some edits to the ‚ÄúREADME‚Äù file on your new branch and then commit those changes. E.g., add a new section to your Markdown file‚Ä¶\n\nTo merge changes from the new branch back into the main branch, we now need to [1] compare changes between branches, [2] create what is called a ‚Äúpull request‚Äù, and [3] pull changes from the new branch back into the main branch. Along the way, if there are any differences between the two branches that cannot be merged without ‚Äúconflicts‚Äù (e.g., cases where the same section of a particular file has been modified in both branches), then these will be highlighted and a merge prevented until the conflict is resolved.\n\nStep 1\n\nReturn to the main branch by selecting the link for the repo and then confirm that you are on the main branch by selecting it from the branches popdown menu.\nThen select ‚ÄúCompare & pull request‚Äù to initiate a new pull request.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 2\n\nSelect the branches to compare. Choose main as the ‚Äúbase‚Äù branch (the branch we are comparing to and merging into) and readme-edits as the ‚Äúcompare‚Äù or ‚Äúhead‚Äù branch (the branch we are currently working in, where our most recent changes have been made).\n\n\nNOTE: You should notice that when we shifted over to the readme-edits branch, GitHub switched the ‚Äúhead‚Äù branch from main to readme-edits.\n\n\n\n\n\n\n\n\n\n\nAfter making these selections, scroll to the bottom of the window‚Ä¶ there you should see you a diffs (for ‚Äúdifferences‚Äù) section that summarizes the differences between the main (base) and compare (head) versions.\n\n\n\n\n\n\n\n\n\n\n\nStep 3\n\nInitialize the pull request by scrolling back up and pressing the green ‚ÄúCreate pull request‚Äù button.\n\n\n\n\n\n\n\n\n\n\n\n\nStep 4\n\nSet up and then confirm the merge by pressing the green ‚ÄúMerge pull request‚Äù and then ‚ÄúConfirm merge‚Äù buttons.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 5\n\nOnce the merge has been completed, you can delete the new branch.\n\n\n\n\n\n\n\n\n\n\n\nNOTE: The ‚ÄúHello World‚Äù GitHub guide also covers basics of working with branches and merging.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Basics of Version Control</span>"
    ]
  },
  {
    "objectID": "05-module.html#connecting-git-and-github",
    "href": "05-module.html#connecting-git-and-github",
    "title": "5¬† Basics of Version Control",
    "section": "5.5 Connecting git and GitHub",
    "text": "5.5 Connecting git and GitHub\nNow that we some familiarity with working with a remotely hosted repository, our next step in developing a git/GitHub workflow is to make a clone of that repository on one or more local computer(s). We can then work on and commit changes locally via git and then, when we are ready, we can ‚Äúpush‚Äù those changes up to the remotely hosted repository on GitHub.\nSimilarly, once a local repository under version control has been created and connected to a remotely hosted version, we can also make changes remotely and then ‚Äúpull‚Äù those changes down to our local repository to keep them in sync.\nIt is very easy to do this process through the RStudio IDE, as we will see below, but we are going to first do it through the command line to show you all of the steps that RStudio facilitates.\n\nCloning a Remote Repository from the Command Line\nThere are two ways to connect between remote and local git repositories, via either HTTPS (which stands for ‚Äúhypertext transfer protocol‚Äù) or SSH (which stands for ‚Äúsecure shell‚Äù). Our first steps will use the first of these approaches and will create a local ‚Äúcloned‚Äù copy of a remote repository we have hosted on GitHub.\n\nOpen a terminal shell. You can either do this directly from your operating system or you can access a shell from within RStudio by choosing Tools &gt; Terminal &gt; New Terminal or Tools &gt; Shell‚Ä¶. Again, the former command opens a new terminal within the RStudio IDE, while the latter opens an external terminal window.\nUsing the UNIX and DOS shell command cd (for ‚Äúchange directories‚Äù), navigate into the folder that you want your cloned repository to be created in. I typically create new repos in a dedicated folder called Repos/ that I use for development work, which I keep in my main user folder on MacOS (e.g., ‚Äú~/Development/Repos‚Äù).\n\n\n(base) ad26693 üêµ  $ cd ~/Development/Repos\n\n\nNOTE: Here, I am using the shortcut ~ operator to access my user home directory. You can, instead, to use the full path to your home directory, e.g., ‚Äú/Users/ad26693/Desktop/Repos‚Äù.\n\n\nNow open a web browser, go to the landing page for your repository on GitHub.com, click the green button that says ‚ÄúCode‚Äù, then select the ‚ÄúHTTPS‚Äù tab and copy the web URL that shows up in the text box by clicking on the tiny clipboard icon.\n\n\n\n\n\n\n\n\n\n\nThe web URL should include your GitHub username and the name of your the repository you are cloning and look something like‚Ä¶\nhttps://github.com/&lt;your user name&gt;/&lt;your repository name&gt;.git\ne.g., https://github.com/difiore/test-repo.git\n\nReturn to the terminal window in RStudio or to the external shell prompt and type: git clone followed by the copied URL. For example‚Ä¶\n\n\n(base) ad26693 üêµ  $ git clone https://github.com/difiore/test-repo.git\n\n\nNOTE: Here, you may be asked for your GitHub username and password‚Ä¶ if so, enter you ‚Äúpersonal access token‚Äù instead of the password.\n\nThe output should look something like:\n\nCloning into 'test-repo'...\nremote: Enumerating objects: 11, done.\nremote: Counting objects: 100% (11/11), done.\nremote: Compressing objects: 100% (10/10), done.\nremote: Total 11 (delta 1), reused 0 (delta 0), pack-reused 0\nReceiving objects: 100% (11/11), done.\nResolving deltas: 100% (1/1), done.\n\nIf you then navigate into the newly created local directory using the cd command, you can list and look at the cloned files. To do so, at the shell prompt type‚Ä¶\n\ncd test-repo (to change into the correct directory)\nls -a (to list all the files in the directory)\nhead README.md (shows the first few lines of the file ‚ÄúREADME.md‚Äù)\ngit remote show origin (shows information about the remote repository, including the branch that ‚Äúpush‚Äù and ‚Äúfetch‚Äù commands will be applied to and which branch is being tracked locally)\n\nThe output should look something like‚Ä¶\n\n(base) ad26693 üêµ  $ cd test-repo/\n(base) ad26693 üêµ  $ ls -a\n.     ..    .git    .gitignore    README.md\n(base) ad26693 üêµ  $ head README.md\n# test-repo\n\nI am making some edits to this README file using Markdown!\n\n## This is a level 2 heading\n\nI can use simple text formating to make **bold** or *italicized* text!\n\n## Added this new section on a branch\n(base) ad26693 üêµ  $ git remote show origin\n* remote origin\n  Fetch URL: https://github.com/difiore/test-repo.git\n  Push  URL: https://github.com/difiore/test-repo.git\n  HEAD branch: main\n  Remote branches:\n    main         tracked\n    readme-edits tracked\n  Local branch configured for 'git pull':\n    main merges with remote main\n  Local ref configured for 'git push':\n    main pushes to main (up to date)\n\n\n\nMaking and Pushing Local Changes from the Command Line\nNow, edit one of the local files in your repo. You can do this by opening the ‚ÄúREADME.md‚Äù file in any text editor (e.g., use the shell command open README.md, make some changes, and save) or by using the shell commands echo and &gt;&gt; (which redirects output to a file instead of the command prompt) to add text to the end of the file (e.g., echo \"Here is some new text I am adding from the shell to update the README file.\" &gt;&gt; README.md).\nYou can also do this by navigating to the ‚ÄúREADME.md‚Äù file through the normal Windows or MacOS file manager, opening it up, and editing it with any plain text text editor like Visual Studio Code or BBEdit.\nIf you now type git status at the shell prompt, you will see a message that the ‚ÄúREADME.md‚Äù file has been changed:\n\n(base) ad26693 üêµ  $ git status\nOn branch main\nYour branch is up to date with 'origin/main'.\n\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n  modified:   README.md\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n\nTo propagate our changes back up to the remote repository, we now need to do three things: [1] First, we need to ‚Äústage‚Äù or ‚Äúadd‚Äù the files to a queue of local changes. [2] We then need to ‚Äúcommit‚Äù those changes to our local repository so that git acknowledges that changes have been made and approved. [3] Finally, we then need to ‚Äúpush‚Äù the committed changes up across the internet to the remote repository hosted on GitHub. These are done with the git commands add, commit, and push in the shell as follows:\n\n(base) ad26693 üêµ  $ git add -A\n(base) ad26693 üêµ  $ git commit -m \"A commit from my local repo\"\n[main e724f84] A commit from my local repo\n 1 file changed, 1 insertion(+)\n \n(base) ad26693 üêµ  $ git push\nEnumerating objects: 5, done.\nCounting objects: 100% (5/5), done.\nDelta compression using up to 4 threads\nCompressing objects: 100% (3/3), done.\nWriting objects: 100% (3/3), 387 bytes | 387.00 KiB/s, done.\nTotal 3 (delta 1), reused 0 (delta 0), pack-reused 0\nremote: Resolving deltas: 100% (1/1), completed with 1 local object.\nTo https://github.com/difiore/test-repo.git\n   280fb0a..e724f84  main -&gt; main\n\n\nNOTE: The -A argument following the git add command means to stage all files in the repo that have changed since the last commit (in this case, we have only one, ‚ÄúREADME.md‚Äù). The -m argument following git commit indicates the message we want to include with our commit. It is necessary to include SOME message with each commit, and it is good practice to include a short description of what the commit includes, e.g., ‚ÄúUpdating README file‚Äù.\n\nTo confirm that your edits have in fact been pushed up successfully, return to the landing page for your repository on GitHub in your web browser and hit refresh‚Ä¶ you should see that the ‚ÄúREADME.md‚Äù file has been updated with new text!\n\n\n\n\n\n\n\n\n\nAnd if you click on the ‚Äúcommits‚Äù history on the right hand site of message block above the updated README file, you should see one with the message ‚ÄúA commit from my local repo‚Äù.\n\n\n\n\n\n\n\n\n\n\nNOTE: See also Chapters 9 to 12 of the web book, Happy Git and GitHub for the useR\n\n\n\nPulling Changes from GitHub from the Command Line\nSimilarly, we can make edits to files in a remote repository on GitHub using the service‚Äôs web-based Markdown editor and then then ‚Äúpull‚Äù those down to our local repository.\n\nIn your web browser, navigate to the page for your repository (e.g., ‚Äútest-repo‚Äù) and again click on the pencil icon at the top right of the ‚ÄúREADME‚Äù file in a remote repository to bring up the editor.\n\n\n\n\n\n\n\n\n\n\n\nAdd some new text to the file and then scroll to the bottom of the page and ‚Äúcommit‚Äù your changes - remember that you need to enter a commit message in the text box (or accept the default ‚ÄúUpdate README.md‚Äù that is auto-filled).\nReturn to the shell prompt inside the local directory for your repository and enter the command git pull. You should see something like the following, indicating that the main branch of the local repository has been ‚Äúfast-forwarded‚Äù to the state of the remote main branch that is being tracked:\n\n\n(base) ad26693 üêµ  $ git pull\nremote: Enumerating objects: 5, done.\nremote: Counting objects: 100% (5/5), done.\nremote: Compressing objects: 100% (3/3), done.\nremote: Total 3 (delta 1), reused 0 (delta 0), pack-reused 0\nUnpacking objects: 100% (3/3), 730 bytes | 182.00 KiB/s, done.\nFrom https://github.com/difiore/test-repo\n   e724f84..8734542  main       -&gt; origin/main\nUpdating e724f84..8734542\nFast-forward\n README.md | 2 ++\n 1 file changed, 2 insertions(+)\n\nIf you run the command git pull and the local repo is in sync with the version hosted on GitHub, you should see something like the following:\n\n(base) ad26693 üêµ  $ git pull\nAlready up to date.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Basics of Version Control</span>"
    ]
  },
  {
    "objectID": "05-module.html#additional-useful-information",
    "href": "05-module.html#additional-useful-information",
    "title": "5¬† Basics of Version Control",
    "section": "5.6 Additional Useful Information",
    "text": "5.6 Additional Useful Information\n\nUsing the Shell within RStudio\nRStudio provides an interface to the most common version control operations including managing changelists, ‚Äúdiffing‚Äù files, committing, and viewing history. While these features cover basic everyday use of git, as we have seen above, we may also occasionally need to use the command line in either a system shell to access all of the underlying functionality of git.\nRStudio includes functionality to make it very straightforward to use various system shells with projects under version control. This includes:\n\nYou can use the Tools &gt; Terminal &gt; New Terminal and Tools &gt; Shell‚Ä¶ commands to open a new terminal within RStudio or a new system shell window with the working directory already initialized to your project‚Äôs root directory.\nWhen using git on Windows, these command by default should open the Git Bash shell, which is a port of the Unix bash shell to Windows that has been specially configured for use with a version of git called MSYS Git. Note that you can disable this behavior and use the standard Windows command prompt instead choosing using Tools &gt; Global Options and selecting an alternative terminal in the Terminal section).\n\n\n\nUsing SSH instead of HTTPS\nRemote repositories under version control can be accessed using a variety of internet file transfer protocols, including HTTPS (the protocol used above) and SSH (‚Äúsecure shell‚Äù), a different file transfer protocol that does not require sending a user name and email address for authentication with every information transfer request. Typically, the authentication for an SSH connection is done using what are know as public/private RSA key pairs. This type of authentication requires two steps:\n\nGenerating a public/private key pair\nProviding the public key to the hosting provider (e.g., GitHub or another service)\n\nWhile Linux and Mac OSX both include SSH as part of the base system, Windows does not. As a result the standard Windows distribution of git (MSYS Git, referenced above) also includes an SSH client.\nIf you are interested in using SSH for connecting with GitHub, you can do the following within RStudio:\n\nStep 1\n\nCreate an RSA Key\n\nIn the ‚ÄúGit/SVN‚Äù tab of the Tools &gt; Global Options dialog box, press the button Create RSA key, which will create a new code that you will need for using SSH to send and receive data from remote servers as an alternative to the somewhat less secure HTTPS protocol we have already seen.\n\n\n\n\n\n\n\n\n\n\n\nStep 2\n\nView and Copy the RSA Key\n\nFrom the ‚ÄúGit/SVN‚Äù tab of the Tools &gt; Global Options dialog box, press the text link View public key, copy the displayed key, and close out of the dialog box.\n\n\n\n\n\n\n\n\n\n\n\nStep 3\n\nSet Up your GitHub Account for SSH\n\nGo to your GitHub account online, open your profile/account settings, and then select the ‚ÄúSSH & GPG keys‚Äù tab.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick ‚ÄúNew SSH key‚Äù, fill out a title for the new key (e.g., ‚ÄúConnect to GitHub from RStudio‚Äù), paste in the public key that you copied from RStudio (see above), and then click ‚ÄúAdd SSH key‚Äù at the bottom of the window.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou should now be set up to use SSH in lieu of HTTPS URLs for cloning repositories and for pushing to/pulling from remote repositories on GitHub. These URLs take the form of git@github.com:&lt;your user name&gt;/&lt;your repository name&gt;.git\n\n\nStep 4\n\nClone the remote repository\n\nThe process is essentially the same as we used above for cloning via HTTPS.\n\nOpen a web browser, go to your repository on GitHub.com, click the green button that says ‚ÄúClone or download‚Äù (but now choose ‚ÄúClone with SSH‚Äù), and copy the URL that shows up in the text box.\n\n\n\n\n\n\n\n\n\n\n\nGo to a shell prompt, cd to the directory that you want the repository to be downloaded into, and type: git clone followed by the copied SSH URL, e.g., git clone git@github.com:&lt;your user name&gt;/&lt;your repository name&gt;.git\n\nThe output should look something like:\n\n(base) ad26693 üêµ  $ git clone git@github.com:difiore/test-repo.git\nCloning into 'test-repo'...\nThe authenticity of host 'github.com (140.82.113.3)' can't be established.\nED25519 key fingerprint is SHA256:+DiY3wvvV6TuJJhbpZisF/zLDA0zPMSvHdkr4UvCOqU.\nThis key is not known by any other names\nAre you sure you want to continue connecting (yes/no/[fingerprint])?\n\nHere, you can answer yes and hit .\n\nWarning: Permanently added 'github.com' (ED25519) to the list of known hosts.\nremote: Enumerating objects: 17, done.\nremote: Counting objects: 100% (17/17), done.\nremote: Compressing objects: 100% (15/15), done.\nremote: Total 17 (delta 3), reused 3 (delta 1), pack-reused 0\nReceiving objects: 100% (17/17), done.\nResolving deltas: 100% (3/3), done.\n\n\n\n\nSwitching Transfer Protocols\nWe can check what protocol you are using to connect to a remote repository by cding into the repository and then typing git remote -v. You should then see something like:\n\n(base) ad26693 üêµ  $ git remote -v\norigin  git@github.com:difiore/test-repo.git (fetch)\norigin  git@github.com:difiore/test-repo.git (push)\n\nTo change the protocol we use, we simply point git to the URL associated with the desired transfer protocol. The following commands switch us from using SSH to HTTPS, then back.\n\n(base) ad26693 üêµ  $ git remote set-url origin https://github.com/difiore/test-repo.git\n(base) ad26693 üêµ  $ git remote -v\norigin  https://github.com/difiore/test-repo.git (fetch)\norigin  https://github.com/difiore/test-repo.git (push)\n(base) ad26693 üêµ  $ git remote set-url origin git@github.com:difiore/test-repo.git\n(base) ad26693 üêµ  $ git remote -v\norigin  git@github.com:difiore/test-repo.git (fetch)\norigin  git@github.com:difiore/test-repo.git (push)\n\n\n\nUsing git with Windows\nSometimes, getting git set up and working properly on Windows machines can be a bit tricky, but I have included information below that might be helpful if you experience difficulties. These notes and procedures were useful for me to install git on a Windows 10 PC and to connect it successfully with RStudio.\n\nSome Notes\n[1] When you install git and you are going through the installation dialog boxes, you can keep the defaults on all of them - just be sure to confirm that‚Ä¶\n\nIn the box about ‚ÄúAdjusting your PATH environment‚Äù you check the radio button to say ‚ÄúGit from the command line and also from 3rd party software‚Äù\n\n\n\n\n\n\n\n\n\n\n\nIn the box about ‚ÄúConfiguring the terminal editor to use with Git Bash‚Äù you check the radio box that says ‚ÄúUse MinTTY (the default terminal of MSYS2)‚Äù\n\n\n\n\n\n\n\n\n\n\n\nNOTE: As part of its installation on a Windows PC, git will also install another piece of software, Git Bash, which is basically on alternative ‚Äúshell‚Äù program you can use to access your computer‚Äôs OS directly. It is analogous (but with somewhat different functionality) to the Windows COMMAND PROMPT and the Windows POWER SHELL.\n\n[2] If you poke around the directory system on your Windows machine, you will find that when you install git it appears to put four, slightly different git.exe files on your computer. Here is a summary of what I think they are and how I think they function, based on searches on StackOverflow\nFile 1. C:\\Program Files\\Git\\bin\\git.exe\nWhen you open RStudio after installing git and then go to Tools &gt; Global Options and select the Git/SVN section, you should see the path above appear in the text box for ‚ÄúGit executable‚Äù.\nI am not entirely sure that this is the case, but I THINK this git.exe file simply links an executable file that is actually stored in a different place (see File 2. below), but this is where many programs will expect to find the git executable by default.\nRStudio should find and fill in this path by itself, but if it does not, then click the ‚ÄúBrowse‚Äù button next to the text box and browse to select this file: C:\\Program Files\\Git\\bin\\git.exe.\nFile 2. C:\\Program Files\\Git\\cmd\\git.exe\nThis is the executable file that is accessible via a PATH environmental variable that should get added to your Windows environment automatically when you installed the git software with the defaults noted above.\nThus, after installing git and restarting your computer, when you then choose Tools &gt; Shell‚Ä¶ or Tools &gt; Terminal &gt; New Terminal from RStudio and type which git in the shell that opens up, it should be THIS executable which is called and returned. That is, if you access the shell or terminal from RStudio and type which git you should get /cmd/git.\n\n\n\n\n\n\n\n\n\nFile 3. C:\\Program Files\\Git\\mingw64\\bin\\git.exe\nThis version of the git executable is what opens if you directly open the Git Bash shell, rather than opening it from within RStudio.\nAfter installing git, you can access Git Bash from the Windows start menu (i.e., the menu that pops up from clicking the Windows icon at the bottom left of your Windows desktop). If you select Git &gt; Git Bash from the start menu, a shell window will open up, and if you then type which git there, it will return /mingw64/bin. Honestly, I have no idea why accessing the Git Bash shell this way rather than through RStudio runs a different instance of git!\n\n\n\n\n\n\n\n\n\nFile 4. C:\\Program Files\\Git\\mingw64\\libexec\\git-core\\git.exe\nThis is another version of the executable that is used by the Git Bash shell. I think it probably just points to the one listed in File 3.\nThe fact that multiple versions of slightly different git executables are installed in different places means that we need to be careful that the right one is used when we try to access git from RStudio. Below, I have tried to distill what we need to know to get things to work for Windows users.\nThus, with the above in mind, if you are having issues with your git installation for Windows, try the following‚Ä¶\n\n\nPreliminaries\nConfirm that your ‚ÄúHOME‚Äù‚Äù environmental variable is set to the root of your user folder. To do this, open the Environmental Variables Control Panel, look in the top panel (‚ÄúUser variables for USERNAME‚Äù) and confirm that there is a ‚ÄúHOME‚Äù variable and it has the path to your user folder. If not, create a new variable called ‚ÄúHOME‚Äù and enter the path or modify the path.\n\nNOTE: When I set up my Windows machine, there was no ‚ÄúHOME‚Äù variable yet specified‚Ä¶ I had to create one. Setting this is important because the git config --global commands you will run below set up and then look for an invisible file, .gitconfig, at the root of your home folder.\n\n\n\nStep 1\nDownload git for Windows from the git website.\nInstall it using the default settings in the installation dialog boxes. This should add C:\\Program Files\\Git\\cmd to your Windows PATH (which contains a list of directories that is loaded into your Windows environment when you either start up Windows or log in, which tells Windows where to search for any installed executable software it is asked to run).\n\n\nStep 2\nLog out of Windows and log in again or completely restart Windows. This is needed to have the PATH environmental variable updated.\n\n\nStep 3\nStart up RStudio and check that the path to the git executable in Tools &gt; Global Options &gt; Git/SVN is set to C:\\Program Files\\Git\\bin\\git.exe I think it should be set automatically, but if not, click the ‚ÄúBrowse‚Äù button next to the text box and find this path and then restart RStudio.\n\n\n\n\n\n\n\n\n\n\nNOTE: The git icon and tab will likely NOT YET APPEAR in your RStudio IDE.\n\n\n\nStep 4\nIn Tools &gt; Global Options &gt; Terminal, make sure that new terminals are set to open with Git Bash rather than some other shell. You can set this in the pop down menu ‚ÄúNew terminals open with‚Ä¶‚Äù There seemed to be an issue that some folks had where new terminal windows were opening at the COMMAND PROMPT, which is not what we want but is easily corrected.\n\n\n\n\n\n\n\n\n\n\n\nStep 5\nChoose Tools &gt; Shell‚Ä¶ or Tools &gt; Terminal &gt; New Terminal and confirm that you are opening in the Git Bash shell. This should be the case if you have done step 4 above. The command prompt will read something like‚Ä¶\n\nIn green: YOUR.USERNAME@YOUR.COMPUTER NAME\nIn purple: MSYS (the default name for the MinTTY Git Bash shell)\nIn orange: The name of the directory/repository you are currently in (i.e., your current working directory)\nThe typical bash command prompt: $\n\n\n\n\n\n\n\n\n\n\n\nNOTE: If your current directory is under version control (e.g., if you are opening a shell for a project that you are already managing with git) then the name of the branch you are on will be listed in in blue parentheses.\n\n\n\n\n\n\n\n\n\n\n\n\nStep 6\nConfirm that git is available by typing which git. It should return /cmd/git\n\n\n\n\n\n\n\n\n\n\n\nStep 7\nSet your git identity as instructed above by typing the following at the command prompt:\n\ngit config --global user.name \"&lt;your name&gt;\"\ngit config --global user.email \"&lt;your email address&gt;\"\n\n\nNOTE: Your name and email address do not need to be in quotes unless there is a space in one of them.\n\n\n\nStep 8\nCheck your configuration by typing the following at the command prompt:\n\ngit config --list\n\nYou should get a list of settings that includes at least two settings, user.name and user.email, that match what you just entered.\n\n\n\n\n\n\n\n\n\n\n\nStep 9\nReturn to RStudio and start a new project (File &gt; New Project) from a ‚ÄúNew Directory‚Äù (e.g., ‚Äútest-repo‚Äù).\nCheck the box indicating that you want to ‚ÄúCreate a git repository‚Äù, RStudio should close and reopen and the git icon and git tab should now appear in the IDE. Also, if you now open a project that is already under version control (e.g., that you clone from GitHub), then these should also appear.\n\n\n\n\n\n\n\n\n\nVo√≠la!!! You should now be ready to go! Try creating a new file in your project and then committing it, as described above.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Basics of Version Control</span>"
    ]
  },
  {
    "objectID": "05-module.html#concept-review",
    "href": "05-module.html#concept-review",
    "title": "5¬† Basics of Version Control",
    "section": "Concept Review",
    "text": "Concept Review\n\nSetting up git\nSetting up GitHub\nWorking in the Terminal/Shell\nCloning a remote repository\ngit basics\n\nStaging and committing changes to a repository\nPushing to and pulling from a remote repository",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Basics of Version Control</span>"
    ]
  },
  {
    "objectID": "06-module.html",
    "href": "06-module.html",
    "title": "6¬† Reproducible Research",
    "section": "",
    "text": "6.1 Objectives",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Reproducible Research</span>"
    ]
  },
  {
    "objectID": "06-module.html#objectives",
    "href": "06-module.html#objectives",
    "title": "6¬† Reproducible Research",
    "section": "",
    "text": "In the last module, we introduced the concept of version control and looked at tools for interfacing between between a repository maintained remotely on GitHub and a local repository, using the version control system, git. Now, we are going to learn how we can use RStudio to manage the git/GitHub version control workflow. An additional objective of this module is to promote the ideas of reproducible research practice and literate programming by introducing you to Quarto and RMarkdown, which are both plain-text document formats that allow us to mix text and code in a flexible way and to generate a variety of nicely rendered outputs (e.g., websites, books, PDFs, presentations, and other media formats).",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Reproducible Research</span>"
    ]
  },
  {
    "objectID": "06-module.html#preliminaries",
    "href": "06-module.html#preliminaries",
    "title": "6¬† Reproducible Research",
    "section": "6.2 Preliminaries",
    "text": "6.2 Preliminaries\nAs a first step, if you haven‚Äôt done so already in Module 02, open the Preferences pane in RStudio (MacOS), go to the Git/SVN section, and make sure that the checkbox ‚ÄúEnable version control interface for RStudio projects‚Äù is selected. Alternatively, you can access the same dialog box by choosing Global Options from the Tools menu (MacOS and PC).\n\n\n\n\n\n\n\n\n\nIn this dialog box, also confirm that the path to your git executable is filled in and correct. If you have already successfully installed git, this should be filled with something like ‚Äú/usr/bin/git‚Äù (on MacOS or Linux) or ‚ÄúC:\\Program Files\\Git\\bin\\git.exe‚Äù on Windows). If it is not, then you can try troubleshooting by following the recommendations in Chapters 13 and 14 of the web book Happy Git and GitHub for the useR.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Reproducible Research</span>"
    ]
  },
  {
    "objectID": "06-module.html#backstory",
    "href": "06-module.html#backstory",
    "title": "6¬† Reproducible Research",
    "section": "6.3 Backstory",
    "text": "6.3 Backstory\nReproducible research refers to the practice of conducting and disseminating scientific research in a way that makes data analysis (and scientific claims more generally) more transparent and repeatable Academics already have means of sharing methods and results generally, through publications (although perhaps typically in less than complete detail), and we can share the data on which those our analyses are based by depositing them in some form of online repository (e.g., via ‚Äúsupplementary information‚Äù that accompanies an article or by posting datasets to repositories like the Dryad Digital Repository or Figshare.\nBut how do we share the details of exactly how we did an analysis? And how can we ensure that it is possible for us to go back, ourselves, and replicate a particular analysis or data transformation? One solution is to integrate detailed text describing a workflow and analytical source code (such as R scripts) together in the same document.\nThis idea of tying together narrative, logic, specific code, and data (or references to them) in a single document stems from the principle of literate programming developed by Donald Knuth. Applied to scientific practice, the concept of literate programming means documenting both the logic behind and analysis and the code used to implement that analysis using computer software. This documentation allows researchers to return to and re-examine their own thought processes at any later time, and also allows them to share their thought processes so that others can understand how an analysis was performed. The upshot is that our scholarship can be better understood, recreated, and independently verified.\nThis is exactly the point of a Quarto or RMarkdown document and of other, similar document formats (e.g., R Notebooks, iPython Notebooks, or Julia Notebooks).\nSo, how does this work?\nFirst, as we saw in the last module, Markdown, is simply a formal way of styling a plain text document so that it can be easily rendered into HTML or PDF files for sharing or publishing. It is based on using some simple formatting and special characters to tag pieces of text such that a parser knows how to convert a plain text document into HTML or PDF. This link takes you to a classic description of Markdown, its syntax, and the philosophy behind it written by John Gruber, Markdown‚Äôs creator. There are now several different ‚Äúdialects‚Äù of Markdown that have been developed, derived from Gruber‚Äôs original suggestions, including a specific one used on GitHub called ‚ÄúGitHub Flavored Markdown‚Äù, or GFM. A guide to this dialect is provided as a PDF here and is available online at this link.\nRMarkdown and Quarto documents are very similar extensions of standard Markdown that allows you to embed chunks of R code (or code blocks of other programming languages, e.g., Python, Latex), along with additional parsing instructions and options for running code, in a plain text file. During the parsing and rendering (or ‚Äúknitting‚Äù) stage, when the text file is being converted to HTML or PDF format, the output of running the embedded code can also be included.\nRMarkdown uses the package {knitr} to produce intermediate files that can then be translated into a variety of formats, including HTML, traditional Markdown, PDFs, MS Word documents, web presentations, and others. A cheatsheet on RMarkdown syntax (which, again, is very similar to Markdown) can be found here. Quarto documents use a separate piece of independent software, quarto (along with the R package {knitr}, in some cases), to do the same thing. An overview and useful definitive guide to all you can do with Quarto can be found here\nIt is important to stress that Markdown (‚Äú.md‚Äù), Quarto (‚Äú.qmd‚Äù), and RMarkdown (‚Äú.Rmd‚Äù) documents ARE JUST PLAIN TEXT files! They are easy to work with, easy to share, easy to edit, easy to put under version control, and robust to changes in proprietary file formats!\nAs a demonstration of reproducible research workflow and best practices, we are going to create a new R project and corresponding repository that we will track with version control using git. Within that project, we then are going to create an Quarto document in which you can take notes or practice coding during class today.\nAs we have seen already, git, at its heart, is a command-line tool, but both local (on your computer) and hosted (e.g., on GitHub) git repositories can be managed using a dedicated git client GUI, such as GitHub Desktop, GitUp, GitKraken, SourceTree, or a host of others. Importantly for us, though, RStudio is designed to also function as a powerful git client. We will explore several ways of setting up RStudio to manage both local and remote copies of a git repository and of keep them in sync.\n\nNOTE: See also Chapters 8 and 12 of the web book Happy Git and GitHub for the useR for more information on managing git and GitHub through RStudio.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Reproducible Research</span>"
    ]
  },
  {
    "objectID": "06-module.html#organizing-work-with-r-projects",
    "href": "06-module.html#organizing-work-with-r-projects",
    "title": "6¬† Reproducible Research",
    "section": "6.4 Organizing Work with R Projects",
    "text": "6.4 Organizing Work with R Projects\nThe easiest way to get RStudio to play nicely with git and GitHub for version control - and a recommended best practice - is to organize your work in RStudio using projects. You can think of an R project as a convenient workspace that is associated with its own working directory, data files, scripts, images, history log, etc. We might, for example, create a separate R project for each manuscript that we are working on.\n\nNOTE: Not surprisingly, the idea of an RStudio ‚Äúproject‚Äù (i.e., an organizing workspace and its associated files) and a ‚Äúrepo‚Äù (a directory that is under version control) go together nicely. It is, however, totally possible to create RStudio projects without having them be under version control, and it is also quite possible to use git or another VCS to track changes to files in a directory without there being an associated RStudio project!\n\nBasically, creating an RStudio project means creating a special text file (‚Äú.Rproj‚Äù) that stores settings for particular RStudio setup and session. When we open an ‚Äú.Rproj‚Äù file, the working directory is automatically set to the root directory for the project (i.e., the directory in which the actual ‚Äú.Rproj‚Äù file is stored), which makes organizing and navigating around the computer‚Äôs filesystem, either from the command line or in R scripts within the project, much easier.\n\nWorkflows for Creating Projects\nFor the sake of security, reproducibility, and collaboration, it makes a lot of sense for us to have all of our data science/data analysis projects both be under version control (e.g., using git) and hosted remotely on a secure and reliable platform (e.g., GitHub) that we and collaborators can access from different locations. There are multiple ways we can accomplish this.\nFor example, we could begin by setting up a new repository remotely, as we did in Module 05, and then ‚Äúclone‚Äù it to our local computer and establish it as a version-controlled RStudio project. Alternatively, we could first create a new version-controlled RStudio project in a local directory and then push it up to GitHub or some other hosting site. [For either of these scenarios, we could also begin with either a new (empty) repository or with one that already has files in it.] We will go through all of these methods below, but I personally think the first process - beginning with a remote repository - is the easiest and most intuitive, but I describe all of these approaches further below.\n\nMethod 1: Create a new RStudio project by cloning from a remote repository\nPerhaps the easiest way to get RStudio working nicely with git and GitHub is by creating a new repository on GitHub and then ‚Äúcloning‚Äù this remote repository to a local computer and placing it under version control.\n\nNOTE: Before completing the following, you should make sure to delete any existing local version of the remote repository that you might have cloned previously, e.g., by moving it to the Trash on MacOS or the Recycle Bin on Windows. This is because if you try to clone a remote repository into an existing directory, you will get an error!\n\n\nStep 1\n\nFirst, as described in Module 05, go to your user profile in GitHub and create a new repository.\n\nYou will need to specify a repository name (with no spaces), whether it is a public or private repository, and whether to initialize the repository with one or more files already in it. I recommend initializing with both a ‚ÄúREADME‚Äù file and with a ‚Äú.gitignore‚Äù file. You can also choose to use an R template to follow for the ‚Äú.gitignore‚Äù file. The ‚Äú.gitignore‚Äù file is simply a text document the provides git with a list of files that you ask it NOT to track changes to. These are typically either very large files (e.g., data files) or various hidden files that are either unnecessary or undesirable to track every single change to.\n\nOnce your repository is created, go to the green ‚ÄúCode‚Äù popdown menu and click the clipboard icon to copy the repository‚Äôs HTTPS URL. This is the same process we used in Module 05 for cloning a remote repository, and the URL is likely to be https://github.com/ followed by &lt;your user name&gt;/&lt;your repository name&gt;).\n\n\n\nStep 2\n\nFrom the File menu in RStudio, choose New Project and select to ‚ÄúCheckout a project from a version control repository‚Äù‚Ä¶\n\n\nNOTE: This step can also be done from the popdown menu located at the top right of the RStudio IDE.\n\n\n\n\n\n\n\n\n\n\n\nChoose the top item, ‚ÄúClone a project from a Git repository‚Äù‚Ä¶\n\n\n\n\n\n\n\n\n\n\n\nIn the subsequent dialog box, paste in the ‚ÄúRepository URL:‚Äù you copied in Step 1.\n\nYou can choose what parent folder to clone the repository into using the ‚ÄúCreate project as a subdirectory of:‚Äù field (e.g., ~/Development/Repos). In most cases, the ‚ÄúProject directory name:‚Äù field will be filled in automatically, as the name of the remote repository you are cloning.\n\n\n\n\n\n\n\n\n\nHitting the ‚ÄúCreate Project‚Äù button will download the repository from GitHub into a new directory on your local computer. RStudio will close and reopen, after which the working directory will be set to the new local repository directory, which you can confirm by typing getwd() at the R console prompt. The local directory is now set up to be tracked by git on your local computer and should be connected to GitHub.\nSelecting the Files tab in RStudio will then show you all of the files in the newly created local repository, which should now contain a ‚Äú.gitignore‚Äù file and an ‚Äú.Rproj‚Äù file with the name of your project (e.g., ‚Äútest-repo.Rproj‚Äù), in addition to the ‚ÄúREADME.md‚Äù file you created on GitHub.\nYou should now also see a ‚ÄúGit‚Äù tab in the upper right pane of the RStudio IDE, and that tab should list the ‚Äú.Rproj‚Äù file associated with the status ?, indicating that it has not yet been added, committed, or synced with the remote repository. The other two files (‚Äú.gitignore‚Äù and ‚ÄúREADME.md‚Äù) are already in sync.\n\n\n\n\n\n\n\n\n\n\n\nOther Notes\nThis process works identically if you want to clone a repo from GitHub that already has other files in it besides ‚Äú.gitignore‚Äù and ‚ÄúREADME.md‚Äù. In that case, RStudio will simply just create the ‚Äú.Rproj‚Äù file and modify any existing ‚Äú.gitignore‚Äù file to also include one or more other files that it recommends that git not track, e.g., ‚Äú.Rproj.user‚Äù. The new ‚Äú.Rproj‚Äù file and modified ‚Äú.gitignore‚Äù file, which were created locally, should appear under the ‚ÄúGit‚Äù tab with with the status ? until you add, commit, and push them.\nThe process also works the same if you set up an empty repo on GitHub, in which case you will have seen the window below immediately upon creating the repository:\n\n\n\n\n\n\n\n\n\nHere, grabbing the URL from the ‚ÄúQuick setup‚Äù section and using that to set up a new project is RStudio is exactly the same process as we followed above.\n\nNOTE: You could also, in a terminal window, navigate to where you want the remote project to be cloned and follow either the ‚Äú‚Ä¶ or create‚Äù or ‚Äú‚Ä¶ or push‚Äù instructions listed. Doing so will link a local directory to the remote repo you created on GitHub BUT you will then not have created that as an RStudio project and would need to follow up with other instructions below about creating a new project from an existing local directory.\n\n\n\n\nMethod 2: Creating a new RStudio project under version control in a new directory\n\nIn RStudio, select File &gt; New project and click New Directory.\n\n\n\n\n\n\n\n\n\n\n\nChoose the project type as ‚ÄúNew Project‚Äù, then name the directory and choose where you would like it to be stored, checking the box marked Create a git repository, and pressing ‚ÄúCreate Project‚Äù.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRStudio will create a new directory for you with an ‚Äú.Rproj‚Äù file (with the name of your project) and a ‚Äú.gitignore‚Äù file inside of it. This directory is now being tracked by git and RStudio. You can now create and edit files in this new directory and stage and commit them locally directly from RStudio. See the section below on ‚ÄúModifying Files Locally‚Äù in RStudio.\n\n\nNOTE: It is important to remember that this project is still only under local version control‚Ä¶ you will not yet be able to push changes up to GitHub. To do that, see the section below on ‚ÄúConnecting a Local Repo to GitHub‚Äù.\n\n\n\nMethod 3: Creating a new RStudio project in an existing directory\nIf you have an existing directory on your local computer that is already under git version control, then you can simply create a new RStudio project for that directory, and version control features will be automatically enabled. To do this:\n\nExecute the New Project command (from the File menu)\nChoose to create a new project from an Existing Directory\n\n\n\n\n\n\n\n\n\n\n\nSelect the appropriate directory and then click Create Project\n\nNew ‚Äú.Rproj‚Äù and ‚Äú.gitignore‚Äù files will be created inside of that directory, and RStudio‚Äôs version control features will then be available for that directory. Now, you can edit files currently in the directory or create new ones, as well as stage and commit them to the local repository directly from RStudio. See the section below on ‚ÄúModifying Files Locally‚Äù in RStudio.\nIf you create a new project for a directory that was not already under version control, you can enable version control within RStudio by choosing Tools &gt; Project Options to open the Project Options dialog box. Once there, go to the Git/SVN section and choose ‚ÄúGit‚Äù from the ‚ÄúVersion control system‚Äù popdown menu, and then confirm that you want to set up version control for the project. You can also enable version control for a new project in a directory not formerly under version control by running the use_git() command from the {usethis} package. In either case, again, new ‚Äú.Rproj‚Äù and ‚Äú.gitignore‚Äù files will be created inside that directory, and RStudio‚Äôs version control features will then be available.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNOTE: Again, it is important to remember that local projects created via Method 2 and Method 3 are still only under local version control‚Ä¶ you will not yet be able to push any additional files you add to the project directory or changes to local files up to GitHub. To do that, see the section below on ‚ÄúConnecting a Local Repo to GitHub‚Äù.\n\n\n\n\nConnecting a Local Repo to GitHub\nTo create a new remote repository on GitHub into which you can push the contents of an existing local repository, e.g., one created by Method 2 or Method 3 above, so that it is also backed-up off site and accessible to you or collaborators working at different locations, you have a couple of different options (see also Chapter 17 of the web book Happy Git and GitHub for the userR).\n\nOption 1: Use the {usethis} package to create a new remote repository on GitHub from within RStudio\n\nStep 1\n\nMake sure you have configured a GitHub Personal Access Token (PAT) using the process outlined in Module 5. Briefly‚Ä¶\n\nLogin to your GitHub account and go to the Settings tab. Then select Developer Settings and then Personal Access Tokens.\nSelect Generate new token, create a new ‚Äúclassic‚Äù token, and give it a nickname that reminds you of the intended purpose (e.g., ‚ÄúGitHub Access from R‚Äù).\nPick a scope that confers the privileges you need, e.g., repo, and then press the green ‚ÄúGenerate Token‚Äù button.\nCopy this token to a safe place, and register this credential with git using one of the methods outlined in Module 5\n\n\n\n\nStep 2\n\nCreate a remote repository on GitHub and push a local repository to it:\n\nOpen the RStudio project that you want to create a remote repository for. This project needs to be under version control already and include at least one commit.\nAt the R console prompt within the project‚Äôs working directory, type usethis::use_github(protocol=\"https\")\nAnswer the question about whether the suggested name for the remote repository is adequate.\nThis should create a new repository on GitHub, add it as a remote origin/main, set up a tracking branch, and open it in your browser.\n\n\nFrom within RStudio, you can now add or edit files to the project, locally commit any changes you make to those files, and push them up to GitHub as described above.\n\n\n\nOption 2: Set up a dummy remote repository on GitHub and push to it from within RStudio\n\nGo to the GitHub website and create a new, empty repository. You can use the same name as that of the RStudio project you want to push up, or you can create a different one.\n\n\n\n\n\n\n\n\n\n\n\nNOTE: In setting up the repository on GitHub, you should choose not to initialize the remote repository you are going to be pushing to with either a ‚ÄúREADME‚Äù or ‚Äú.gitignore‚Äù file!\n\n\nFrom the Git tab in RStudio, select the New Branch icon.\n\n\n\n\n\n\n\n\n\n\n\nClick ‚ÄúAdd remote‚Äù, paste in the URL for your dummy remote repository in the text field, and type in ‚Äúorigin‚Äù for the name of the remote branch.\nClick ‚ÄúAdd‚Äù. We should be back in the ‚ÄúNew Branch‚Äù dialog box. Enter main as the branch name (to push from the local main branch to a remote main), and make sure ‚ÄúSync branch with remote‚Äù button is checked.\nClick ‚ÄúCreate‚Äù and in the next dialog box, choose ‚ÄúOverwrite‚Äù. This should push your local repository main branch up to the remote main branch.\n\n\n\nOption 3: Set up a dummy remote repository on GitHub and push to it from a terminal shell\n\nAs in Option 2, go to the GitHub website and create a new, empty repository. You can use the same name as that of the RStudio project you want to push up, or you can create a different one.\n\n\n\n\n\n\n\n\n\n\n\nNOTE: Here, again, you should choose not to initialize the remote repository you are going to be pushing to with either a ‚ÄúREADME‚Äù or ‚Äú.gitignore‚Äù file!\n\n\nFrom the next screen, select the desired transfer protocol you want to use under the Quick setup... option at the top (HTTPS or SSH). Then, scroll down to the option: ...or push an existing repository from the command line.\n\n\n\n\n\n\n\n\n\n\n\nCopy the lines of code listed there and then return to RStudio.\nIn RStudio, open the project that you want to push to GitHub and click Tools &gt; Terminal &gt; New Terminal or Tools &gt; Shell‚Ä¶ to open a terminal window. Alternatively, open a separate terminal window and navigate to the root of the directory of the repository you wish to push.\n\n\nNOTE: It is important that you run these lines of code from within the directory that you wish to push to GitHub. When you open a new terminal or shell from within RStudio, you should be in the correct directory, as those processes open the shell in the current working directory. If not, though, use shell commands (i.e., cd) to navigate into the correct directory.\n\nAt the shell prompt, enter the lines of code you copied.\n\ngit remote add origin https://github.com/&lt;your user name&gt;/&lt;your repository name&gt;.git\ngit branch -M main\ngit push -u origin main\n\nThe first line tells git the remote URL that you are going to push to, the second makes sure you are on the main branch of the repository, and the third pushes your local repository main branch up to the remote main branch.\nCongratulations! You have now pushed commits from a local repository to GitHub, and you should be able to see those files in your remote GitHub repository online. The ‚ÄúPull‚Äù (blue down arrow) and ‚ÄúPush‚Äù (green up arrow) buttons in RStudio should now also work.\n\nIMPORTANT: Remember, after each commit you do via RStudio (or via the command line), you will have to push to GitHub manually. This does not happen automatically!\n\n\nNOTE: Additional information on using projects in RStudio is available here.\n\n\n\n\nTroubleshooting\nMost of the time, the installation and setup instructions provided in Module 05 and this module for getting git and RStudio to work together work just fine, but sometimes you may have issues. The most common problem I have seen is when RStudio is unable to find your proper git installation, either because the path to the correct git executable did not get written into your shell profile when you installed git or because the correct path to git has not be set properly in RStudio.\nChapters 13 and 14 of the web book Happy Git and GitHub R offer a number of suggestions for how to troubleshoot these problems, but some things to check are:\n\nIs git installed correctly? An easy way to test this is to enter git in a terminal shell. If you get a complaint about git not being found, it means either installation was unsuccessful or that the path to the git program is not on your PATH in your shell profile. Try reinstalling git and then either logging out of your computer and logging in again or restarting.\nIs the proper path to git set in RStudio? InRStudio, go to Tools &gt; Global Options and select the ‚ÄúGit/SVN‚Äù tab and make sure that the box ‚ÄúGit executable‚Äù points to the executably file for your git installation. On macOS and Linux operating systems, the path usually is ‚Äú/usr/bin/git‚Äù (on MacOS or Linux) and on Windows operating systems it is usually ‚ÄúC:\\Program Files\\Git\\bin\\git.exe‚Äù. Sometimes, git will be installed in a slightly different place, most commonly (on MacOS) in ‚Äú/usr/local/bin/git‚Äù, in which case you will need to change the path in the dialog box. To find the correct path, type which git in a terminal shell and then enter that in the ‚Äú‚ÄúGit executable‚Äù box.\n\n\nNOTE: If you make any changes, e.g., if you reinstall git, you will likely need to open a new shell window (so that your updated PATH gets read and your computer can find git). You may also need to also either log out of your commputer and log in again or restart your computer. If you make changes in the RStudio IDE, e.g., if you change the path to your git executable via the Tools &gt; Global Options &gt; Git/SVN tab, you will need to completely RESTART RStudio, and you may need to also log out and back in or restart your computer.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Reproducible Research</span>"
    ]
  },
  {
    "objectID": "06-module.html#working-with-projects-in-rstudio",
    "href": "06-module.html#working-with-projects-in-rstudio",
    "title": "6¬† Reproducible Research",
    "section": "6.5 Working with Projects in RStudio",
    "text": "6.5 Working with Projects in RStudio\nOnce you have a project set up in RStudio and under version control, git will be watching your local project directory for any changes that you might make to what is contained in that directory or to the files therein. For example, if you add a file to the directory, it will show up in the Files tab, and if you delete a file, it will disappear from that list. Thus, you can drag and drop files and directories into and out of the project repo and git will keep track of them.\nNow, you can edit existing files in the repository (e.g., the ‚ÄúREADME.md‚Äù file), create one or more new scripts (‚Äú.R‚Äù), Quarto (‚Äú.qmd‚Äù) documents, RMarkdown (‚Äú.Rmd‚Äù) documents, or add other files and folders (e.g., data files) to your repository and git will keept track of them.\n\nModifying Files Locally\nWithin RStudio, click on the ‚ÄúREADME.md‚Äù file to open it in the text editor panel of the IDE. Make some changes to this file (e.g., add the line, ‚ÄúAnd here is some new text I am adding in the RStudio IDE.‚Äù) and then save your changes. When you do, the ‚ÄúREADME.md‚Äù file should show up in the Git tab with a blue ‚ÄúM‚Äù next to it, indicating that it has been ‚Äúmodified‚Äù.\nWe can commit these changes to the local repository and push them up to GitHub as follows:\n\nClick the Git tab in upper right pane.\nCheck the Staged box next to the ‚ÄúREADME.md‚Äù file, which will get the file ready to commit to your local repository. If you have not already done so, you should also stage the ‚Äú.gitignore‚Äù and ‚Äú.Rproj‚Äù files.\n\n\nNOTE: The ‚Äú.gitignore‚Äù file that was created when you activated version control is basically a list of files that you want git to ignore and not worry about tracking changes to. By default, it includes the names of several files, e.g., ‚Äú.Rhistory‚Äù and ‚Äú.RData‚Äù, which can be quite large and are not necessarily that important to track all changes to. You can also add to the ‚Äú.gitignore‚Äù document the names of any other files in your working directory that you do not want git to track. These files can sit in your local working directory, unstaged and uncommitted, with no problems.\n\nAfter being staged, the status of the files should turn to a green A (for ‚Äúadded‚Äù).\n\nClick the ‚ÄúCommit‚Äù button. You will see a new, ‚ÄúReview Changes‚Äù window open up with the names of the files in your directory in the upper left pane. Selecting any of these will show you, in the lower pane, an overview of the differences between current version of the file and the version that was most recently previously committed.\n\n\n\n\n\n\n\n\n\n\n\nEnter an identifying ‚ÄúCommit message‚Äù in the box in the upper right pane, e.g., ‚ÄúFirst commit from RStudio‚Äù and then click ‚ÄúCommit‚Äù.\n\nA window will pop up confirming what you have just done, which you can then close.\n\n\n\n\n\n\n\n\n\nIf you now select the History tab in the ‚ÄúReview Changes‚Äù window, you can see the history of commits. Selecting any of the nodes in the commit history will show you (in the lower pane) the files involved in the commit and how the content of those files has changed since your last commit. For example, the node associated with your initial commit will show you the initial file contents, while subsequent nodes highlight where a new version differs from the previous one.\n\n\nPushing Changes to a Remote Repo\n\nWith your commits completed, the files disappear from the ‚ÄúReview Changes‚Äù window and from the Git tab in RStudio window (meaning they have all been committed locally). You should now be able to pass all of your local changes up to GitHub by clicking the green ‚ÄúPush‚Äù (up arrow) button. This updates the remote copy of your repository and makes it available to collaborators or to you, working on a different local computer.\n\n\n\n\n\n\n\n\n\n\nYou should see a dialog box like the following indicating success:\n\n\n\n\n\n\n\n\n\n\n\nConfirming Changes\nFinally, you can confirm that the local changes were indeed sent up to the remote repository on GitHub by going back to the page for the repository in your web browser and hitting refresh‚Ä¶ again, you should see that the ‚ÄúREADME.md‚Äù file has been updated with new text and, if you click on the ‚Äúcommits‚Äù tab, you should see one with the message ‚ÄúCommit from RStudio‚Äù.\n\n\n\n\n\n\n\n\n\n\n\nCHALLENGE\nCreate and save a completely new Quarto (‚Äú.qmd‚Äù), RMarkdown (‚Äú.Rmd‚Äù), markdown, or plain text document in your current working directory/test repository by selecting File &gt; New File and then picking the file type of your choice. If you create a markdown or plain text file, a blank document will open, while if you create a Quarto or RMarkdown (‚Äú.Rmd‚Äù) document, a dialog box will open that allows you set such things as the title and author for your document and a choice of output format for when you choose to ‚Äúrender‚Äù (for Quarto documents) or ‚Äúknit‚Äù (for RMarkdown documents) the file you create.\n\nNOTE: The process of rendering/knitting converts your file, with notes and code, into nicely formatted output (e.g., a web page, PDF document, or MS Word document) that can include the output of running your code.\n\nThe image below shows the dialog box for creating a new Quarto document. Minimally, provide a title for your document. I also always uncheck the box for ‚ÄúUse visual markdown editor‚Äù because I prefer to work on my documents in ‚ÄúSource‚Äù mode (you can toggle between these options using the buttons at the top left of the document pane in RStudio).\n\n\n\n\n\n\n\n\n\nOnce you have created a document, keep the header section (the piece at the top of the file between the --- lines) but feel free to erase the rest of the pre-populated content that RStudio provides as a template and to fill it with your own notes and/or code blocks. The content does not matter‚Ä¶ be creative and try some markdown formatting! Code blocks begin and end with three backticks, and the the programming language for the block (R, python, bash) is indicated in braces immediately following the opening st of backticks. Anything outside of code blocks can be formatted using markdown styling.\nFor example‚Ä¶\n---\ntitle: \"My document\"\n---\n\n# This is **markdown** formatted content.\n\nThe block below is an **R** code block.\n\n```{r}\n\n```\n\nHere is some more **markdown** content\nAfter you have made some edits to your file, save it, commit it to the local clone of your repository, and push it from your local repository up to the main branch of your repository on GitHub.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Reproducible Research</span>"
    ]
  },
  {
    "objectID": "06-module.html#deleting-repositories",
    "href": "06-module.html#deleting-repositories",
    "title": "6¬† Reproducible Research",
    "section": "6.6 Deleting Repositories",
    "text": "6.6 Deleting Repositories\nIf you want to get rid of a local repository, you can simply send it to the Trash (on MacOS) or Recycle Bin (on Windows) and throw it away. It is just a regular, self-contained directory, and your local git executable will no longer track it.\nIf you want to get rid of a remote repository on GitHub, navigate to the repository‚Äôs web page in a browser, click on ‚ÄúSettings‚Äù tab, scroll down to bottom of the page in the ‚ÄúDanger Zone‚Äù section, and select the option ‚ÄúDelete this repository‚Äù. You will be asked to type in the repository name to confirm that you want to delete it and will likely have to enter your GitHub password.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Reproducible Research</span>"
    ]
  },
  {
    "objectID": "06-module.html#additional-resources",
    "href": "06-module.html#additional-resources",
    "title": "6¬† Reproducible Research",
    "section": "6.7 Additional Resources",
    "text": "6.7 Additional Resources\nThe web book Happy Git and GitHub for the useR by Dr.¬†Jenny Bryan (an important contributor to a number of key R packages) is an excellent source of information about how to set up and troubleshoot your RStudio/git/GitHub workflow‚Ä¶ I encourage you to check it out!",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Reproducible Research</span>"
    ]
  },
  {
    "objectID": "06-module.html#customizing-execution-of-quarto-and-rmardown-documents",
    "href": "06-module.html#customizing-execution-of-quarto-and-rmardown-documents",
    "title": "6¬† Reproducible Research",
    "section": "6.8 Customizing Execution of Quarto and RMardown Documents",
    "text": "6.8 Customizing Execution of Quarto and RMardown Documents\nIt is possible to set various options for how Quarto and RMardown documents are rendered, either for an entire document or on a code chunk-by-code chunk basis. For example, for Quarto documents, we can set the following options to either TRUE or FALSE by including these either in the header of the document or at the start of a chunk:\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\neval\nEvaluate the code (TRUE) or just echo code (FALSE) in the output\n\n\necho\nInclude (TRUE) or omit (FALSE) the code in output\n\n\noutput\nInclude (TRUE) or omit (FALSE) the results of executing the code in the output\n\n\nwarnings\nInclude (TRUE) or suppress warnings (FALSE) in the output\n\n\nerror\nInclude (TRUE) or suppress (FALSE) errors in the output (If TRUE, errors executing code will not halt rendering of the document)\n\n\ninclude\nFALSE prevents any output (code or results) from being included\n\n\n\nFor example, to apply to the whole document, we place the option under execute in the header‚Ä¶\n---\ntitle: \"My Document\"\nexecute:\n  echo: false\n---\n‚Ä¶ and to apply to a specific code chunk, we preface the line with #|‚Ä¶\n```{r}\n#| echo: false\n```\nWe can also set options in either document header to or in a given code chunk to include code folding. For example, to produce an HTML document that shows code as folded, we can use‚Ä¶\n---\ntitle: \"My Document\"\nformat:\n  html:\n    code-fold: TRUE\n---\n‚Ä¶ and for a specific chunk, we can use‚Ä¶\n```{r}\n#| code-fold: TRUE\n```\nTypically, when rendering to HTML, Quarto will produce a separate folder of output images that is linked to from the HTML file that is produced, but it is possible to create a self-contained HTML file that embeds those (and any other) resources by specifying embed-resource: TRUE in the header. Note that this can result in very large files being created!\n---\ntitle: \"My Document\"\nformat:\n  html:\n    embed-resources: TRUE\n---\nAdditional information on Quarto HTML options can be found at https://quarto.org/docs/reference/formats/html.html#rendering\nDetails on rendering and chunk options for knitting RMarkdown documents can be found at https://yihui.org/knitr/options/.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Reproducible Research</span>"
    ]
  },
  {
    "objectID": "06-module.html#concept-review",
    "href": "06-module.html#concept-review",
    "title": "6¬† Reproducible Research",
    "section": "Concept Review",
    "text": "Concept Review\n\nQuarto and RMarkdown documents and execution options\nCreating R Projects (3 ways)\n\nCreating a project from a remote repository under version control (Method 1)\nCreating a brand new project locally (Method 2)\nCreating a project from an existing local repository under version control (Method 3)\n\nConnecting a local repository and a remote repository on GitHub (2 ways)\n\nLESS COMPLICATED (Corresponding to Method 1): Start by setting up a repository on GitHub and then clone the repository locally (see above). Once created, you can copy files into it, stage and commit those files, and push them to GitHub\nMORE COMPLICATED (Corresponding to Method 2 and Method 3): Start by setting up a local repository under version control and then set up and connect to a remote repository on GitHub. See Chapter 17 of the web book Happy Git and GitHub for the useR for additional instructions to use for following this workflow.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Reproducible Research</span>"
    ]
  },
  {
    "objectID": "07-module.html",
    "href": "07-module.html",
    "title": "7¬† Additional Data Structures in R",
    "section": "",
    "text": "7.1 Objectives",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Additional Data Structures in ***R***</span>"
    ]
  },
  {
    "objectID": "07-module.html#objectives",
    "href": "07-module.html#objectives",
    "title": "7¬† Additional Data Structures in R",
    "section": "",
    "text": "The objective of this module is to introduce additional fundamental data structures in R (matrices, arrays, lists, data frames, and the like) and to learn how to extract, filter, and subset data from them.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Additional Data Structures in ***R***</span>"
    ]
  },
  {
    "objectID": "07-module.html#preliminaries",
    "href": "07-module.html#preliminaries",
    "title": "7¬† Additional Data Structures in R",
    "section": "7.2 Preliminaries",
    "text": "7.2 Preliminaries\n\nGO TO: https://github.com/difiore/ada-datasets, select the ‚Äúrandom-people.csv‚Äù file, then press the ‚ÄúDownload‚Äù button and save the file to your local computer (e.g., on your desktop).\n\n\n\n\n\n\n\n\n\n\nAlternatively, you can press the ‚ÄúRaw‚Äù button, highlight, and copy the text to a text editor, and save it. RStudio, as we have seen, has a powerful built-in text editor. There are also a number of other excellent text editors that you download for FREE (e.g., BBEdit for MacOS, Notepad++ for Windows, or Visual Studio Code for either operating system). - Install and load these packages in R: {tidyverse} (which includes {ggplot2}, {dplyr}, {readr}, {tibble}, and {tidyr}, plus others, so they do not need to be installed separately) and {data.table}",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Additional Data Structures in ***R***</span>"
    ]
  },
  {
    "objectID": "07-module.html#matrices-and-arrays",
    "href": "07-module.html#matrices-and-arrays",
    "title": "7¬† Additional Data Structures in R",
    "section": "7.3 Matrices and Arrays",
    "text": "7.3 Matrices and Arrays\nSo far, we have seen several way of creating vectors, which are the most fundamental data structures in R. Today, we will explore and learn how to manipulate other fundamental data structures, including matrices, arrays, lists, and data frames, as well as variants on data frames (e.g., data tables and ‚Äútibbles‚Äù.)\n\nNOTE: The kind of vectors we have been talking about so far are also sometimes referred to as atomic vectors, and all of the elements of a vector have to have the same data type. We can think of lists (see below) as a different kind of vector, where the elements can have different types, but I prefer to consider lists as a different kind of data structure. Wickham (2019) Advanced R, Second Edition discusses the nuances of various R data structures in more detail.\n\nMatrices and arrays are extensions of the basic vector data structure, and like vectors, all of the elements in an array or matrix have to be of the same atomic type.\nWe can think of a matrix as a two-dimensional structure consisting of several atomic vectors stored together, but, more accurately, a matrix is essentially a single atomic vector that is split either into multiple columns or multiple rows of the same length. Matrices are useful constructs for performing many mathematical and statistical operations. Again, like 1-dimensional atomic vectors, matrices can only store data of one atomic class (e.g., numerical or character). Matrices are created using the matrix() function.\n\nm &lt;- matrix(data = c(1, 2, 3, 4), nrow = 2, ncol = 2)\nm\n\n##      [,1] [,2]\n## [1,]    1    3\n## [2,]    2    4\n\n\nMatrices are typically filled column-wise, with the argument, byrow=, set to FALSE by default (note that FALSE is not in quotation marks). This means that the first column of the matrix will be filled first, the second column second, etc.\n\nm &lt;- matrix(data = c(1, 2, 3, 4, 5, 6), nrow = 2, ncol = 3, byrow = FALSE)\nm\n\n##      [,1] [,2] [,3]\n## [1,]    1    3    5\n## [2,]    2    4    6\n\n\nThis pattern can be changed by specifying the byrow= argument as TRUE.\n\nm &lt;- matrix(data = c(1, 2, 3, 4, 5, 6), nrow = 2, ncol = 3, byrow = TRUE)\nm\n\n##      [,1] [,2] [,3]\n## [1,]    1    2    3\n## [2,]    4    5    6\n\n\nYou can also create matrices by binding vectors of the same length together either row-wise (with the function rbind()) or column-wise (with the function cbind()).\n\nv1 &lt;- c(1, 2, 3, 4)\nv2 &lt;- c(6, 7, 8, 9)\nm1 &lt;- rbind(v1, v2)\nm1\n\n##    [,1] [,2] [,3] [,4]\n## v1    1    2    3    4\n## v2    6    7    8    9\n\n\n\nm2 &lt;- cbind(v1, v2)\nm2\n\n##      v1 v2\n## [1,]  1  6\n## [2,]  2  7\n## [3,]  3  8\n## [4,]  4  9\n\n\nStandard metadata about a matrix can be extracted using the class(), dim(), names(), rownames(), colnames() and other commands. The dim() command returns an vector containing the number of rows at index position 1 and the number of columns at index position 2.\n\nclass(m1)\n\n## [1] \"matrix\" \"array\"\n\nclass(m2)\n\n## [1] \"matrix\" \"array\"\n\n\n\ndim(m1)\n\n## [1] 2 4\n\ndim(m2)\n\n## [1] 4 2\n\n\n\ncolnames(m1)\n\n## NULL\n\nrownames(m1)\n\n## [1] \"v1\" \"v2\"\n\n\n\nNOTE: In this example, colnames are not defined for m1 since rbind() was used to create the matrix.\n\n\ncolnames(m2)\n\n## [1] \"v1\" \"v2\"\n\nrownames(m2)\n\n## NULL\n\n\n\nNOTE: Similarly, in this example, rownames are not defined for m2, since cbind() was used to create the matrix.\n\nAs we saw with vectors, the structure (str()) and glimpse (dplyr::glimpse()) commands can be applied to any data structure to provide details about that object. These are incredibly useful functions that you will find yourself using over and over again.\n\nstr(m1)\n\n##  num [1:2, 1:4] 1 6 2 7 3 8 4 9\n##  - attr(*, \"dimnames\")=List of 2\n##   ..$ : chr [1:2] \"v1\" \"v2\"\n##   ..$ : NULL\n\nstr(m2)\n\n##  num [1:4, 1:2] 1 2 3 4 6 7 8 9\n##  - attr(*, \"dimnames\")=List of 2\n##   ..$ : NULL\n##   ..$ : chr [1:2] \"v1\" \"v2\"\n\n\nThe attributes (attributes()) command can be used to list the attributes of a data structure.\n\nattributes(m1)\n\n## $dim\n## [1] 2 4\n## \n## $dimnames\n## $dimnames[[1]]\n## [1] \"v1\" \"v2\"\n## \n## $dimnames[[2]]\n## NULL\n\nattr(m1, which = \"dim\")\n\n## [1] 2 4\n\nattr(m1, which = \"dimnames\")[[1]]\n\n## [1] \"v1\" \"v2\"\n\nattr(m1, which = \"dimnames\")[[2]]\n\n## NULL\n\n\nAn array is a more general atomic data structure, of which a vector (with 1 implicit dimension) and a matrix (with 2 defined dimensions) are but examples. Arrays can include additional dimensions, but (like vectors and matrices) they can only include elements that are all of the same atomic data class (e.g., numeric, character). The example below shows the construction of a 3 dimensional array with 5 rows, 6 columns, and 3 ‚Äúlevels‚Äù). Visualizing higher and higher dimension arrays, obviously, becomes challenging!\n\na &lt;- array(data = 1:90, dim = c(5, 6, 3))\na\n\n## , , 1\n## \n##      [,1] [,2] [,3] [,4] [,5] [,6]\n## [1,]    1    6   11   16   21   26\n## [2,]    2    7   12   17   22   27\n## [3,]    3    8   13   18   23   28\n## [4,]    4    9   14   19   24   29\n## [5,]    5   10   15   20   25   30\n## \n## , , 2\n## \n##      [,1] [,2] [,3] [,4] [,5] [,6]\n## [1,]   31   36   41   46   51   56\n## [2,]   32   37   42   47   52   57\n## [3,]   33   38   43   48   53   58\n## [4,]   34   39   44   49   54   59\n## [5,]   35   40   45   50   55   60\n## \n## , , 3\n## \n##      [,1] [,2] [,3] [,4] [,5] [,6]\n## [1,]   61   66   71   76   81   86\n## [2,]   62   67   72   77   82   87\n## [3,]   63   68   73   78   83   88\n## [4,]   64   69   74   79   84   89\n## [5,]   65   70   75   80   85   90\n\n\n\nSubsetting\nYou can reference or extract select elements from vectors, matrices, and arrays by subsetting them using their index position(s) in what is knows as bracket notation ([ ]). For vectors, you would specify an index value in one dimension. For matrices, you would give the index values in two dimensions. For arrays generally, you would give index values for each dimension in the array.\nFor example, suppose you have the following vector:\n\nv &lt;- 1:100\nv\n\n##   [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n##  [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n##  [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n##  [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n##  [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n##  [91]  91  92  93  94  95  96  97  98  99 100\n\n\nYou can select the first 15 elements using bracket notation as follows:\n\nv[1:15]\n\n##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15\n\n\nYou can also supply a vector of index values as the argument to [ ] to use for subsetting:\n\nv[c(2, 4, 6, 8, 10)]\n\n## [1]  2  4  6  8 10\n\n\nSimilarly, you can also use a function or a calculation to subset a vector. What does the following return?\n\nv &lt;- 101:200\nv[seq(from = 1, to = 100, by = 2)]\n\n##  [1] 101 103 105 107 109 111 113 115 117 119 121 123 125 127 129 131 133 135 137\n## [20] 139 141 143 145 147 149 151 153 155 157 159 161 163 165 167 169 171 173 175\n## [39] 177 179 181 183 185 187 189 191 193 195 197 199\n\n\nAs an example for a matrix, suppose you have the following:\n\nm &lt;- matrix(data = 1:80, nrow = 8, ncol = 10, byrow = FALSE)\nm\n\n##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n## [1,]    1    9   17   25   33   41   49   57   65    73\n## [2,]    2   10   18   26   34   42   50   58   66    74\n## [3,]    3   11   19   27   35   43   51   59   67    75\n## [4,]    4   12   20   28   36   44   52   60   68    76\n## [5,]    5   13   21   29   37   45   53   61   69    77\n## [6,]    6   14   22   30   38   46   54   62   70    78\n## [7,]    7   15   23   31   39   47   55   63   71    79\n## [8,]    8   16   24   32   40   48   56   64   72    80\n\n\nYou can extract the element in row 4, column 5 and assign it to a new variable, x, as follows:\n\nx &lt;- m[4, 5]\nx\n\n## [1] 36\n\n\nYou can also extract an entire row or an entire column (or set of rows or set of columns) from a matrix by specifying the desired row or column number(s) and leaving the other value blank.\n\nx &lt;- m[4, ]  # extracts 4th row\nx\n\n##  [1]  4 12 20 28 36 44 52 60 68 76\n\n\n\n\nCHALLENGE\n\nGiven the matrix, m, above, extract the 2nd, 3rd, and 6th columns and assign them to the variable x\n\n\n\nShow Code\nx &lt;- m[, c(2, 3, 6)]\nx\n\n\nShow Output\n##      [,1] [,2] [,3]\n## [1,]    9   17   41\n## [2,]   10   18   42\n## [3,]   11   19   43\n## [4,]   12   20   44\n## [5,]   13   21   45\n## [6,]   14   22   46\n## [7,]   15   23   47\n## [8,]   16   24   48\n\n\n\n\nGiven the matrix, m, above, extract the 6th to 8th row and assign them to the variable x\n\n\n\nShow Code\nx &lt;- m[6:8, ]\nx\n\n\nShow Output\n##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n## [1,]    6   14   22   30   38   46   54   62   70    78\n## [2,]    7   15   23   31   39   47   55   63   71    79\n## [3,]    8   16   24   32   40   48   56   64   72    80\n\n\n\n\nGiven the matrix, m, above, extract the elements from row 2, column 2 to row 6, column 9 and assign them to the variable x\n\n\n\nShow Code\nx &lt;- m[2:6, 2:9]\nx\n\n\nShow Output\n##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n## [1,]   10   18   26   34   42   50   58   66\n## [2,]   11   19   27   35   43   51   59   67\n## [3,]   12   20   28   36   44   52   60   68\n## [4,]   13   21   29   37   45   53   61   69\n## [5,]   14   22   30   38   46   54   62   70\n\n\n\n\n\nOverwriting Elements\nYou can replace elements in a vector or matrix, or even entire rows or columns, by identifying the elements to be replaced and then assigning them new values.\nStarting with the matrix, m, defined above, explore what will be the effects of operations below. Pay careful attention to row and column index values, vector recycling, and automated conversion/recasting among data classes.\n\nm[7, 1] &lt;- 564\nm[, 8] &lt;- 2\nm[2:5, 4:8] &lt;- 1\nm[2:5, 4:8] &lt;- c(20, 19, 18, 17)\nm[2:5, 4:8] &lt;- matrix(data = c(20:1), nrow = 4, ncol = 5, byrow = TRUE)\nm[, 8] &lt;- c(\"a\", \"b\")",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Additional Data Structures in ***R***</span>"
    ]
  },
  {
    "objectID": "07-module.html#lists-and-data-frames",
    "href": "07-module.html#lists-and-data-frames",
    "title": "7¬† Additional Data Structures in R",
    "section": "7.4 Lists and Data Frames",
    "text": "7.4 Lists and Data Frames\nUnlike vectors, matrices, and arrays, two other data structures - lists and data frames - can be used to group together a heterogeneous mix of R structures and objects. A single list, for example, could contain a matrix, vector of character strings, vector of factors, an array, even another list.\nLists are created using the list() function where the elements to add to the list are given as arguments to the function, separated by commas. Type in the following example:\n\ns &lt;- c(\"this\", \"is\", \"a\", \"vector\", \"of\", \"strings\")\n# this is a vector of character strings\nm &lt;- matrix(data = 1:40, nrow = 5, ncol = 8)  # this is a matrix\nb &lt;- FALSE  # this is a boolean variable\nl &lt;- list(s, m, b)\nl\n\n## [[1]]\n## [1] \"this\"    \"is\"      \"a\"       \"vector\"  \"of\"      \"strings\"\n## \n## [[2]]\n##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n## [1,]    1    6   11   16   21   26   31   36\n## [2,]    2    7   12   17   22   27   32   37\n## [3,]    3    8   13   18   23   28   33   38\n## [4,]    4    9   14   19   24   29   34   39\n## [5,]    5   10   15   20   25   30   35   40\n## \n## [[3]]\n## [1] FALSE\n\n\n\nSubsetting Lists\nYou can reference or extract elements from a list similarly to how you would from other data structure, except that you use double brackets ([[ ]]) to reference a single element in the list.\n\nl[[2]]\n\n##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n## [1,]    1    6   11   16   21   26   31   36\n## [2,]    2    7   12   17   22   27   32   37\n## [3,]    3    8   13   18   23   28   33   38\n## [4,]    4    9   14   19   24   29   34   39\n## [5,]    5   10   15   20   25   30   35   40\n\n\nAn extension of this notation can be used to access elements contained within an element in the list. For example:\n\nl[[2]][2, 6]\n\n## [1] 27\n\nl[[2]][2, ]\n\n## [1]  2  7 12 17 22 27 32 37\n\nl[[2]][, 6]\n\n## [1] 26 27 28 29 30\n\n\nTo reference or extract multiple elements from a list, you would use single bracket ([ ]) notation, which would itself return a list. This is called ‚Äúlist slicing‚Äù.\n\nl[2:3]\n\n## [[1]]\n##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n## [1,]    1    6   11   16   21   26   31   36\n## [2,]    2    7   12   17   22   27   32   37\n## [3,]    3    8   13   18   23   28   33   38\n## [4,]    4    9   14   19   24   29   34   39\n## [5,]    5   10   15   20   25   30   35   40\n## \n## [[2]]\n## [1] FALSE\n\n\n\nl[c(1, 3)]\n\n## [[1]]\n## [1] \"this\"    \"is\"      \"a\"       \"vector\"  \"of\"      \"strings\"\n## \n## [[2]]\n## [1] FALSE\n\n\nUsing class() and str() (or dplyr::glimpse()) provides details about the our list and its three elements:\n\nclass(l)\n\n## [1] \"list\"\n\nstr(l)\n\n## List of 3\n##  $ : chr [1:6] \"this\" \"is\" \"a\" \"vector\" ...\n##  $ : int [1:5, 1:8] 1 2 3 4 5 6 7 8 9 10 ...\n##  $ : logi FALSE\n\ndplyr::glimpse(l)\n\n## List of 3\n##  $ : chr [1:6] \"this\" \"is\" \"a\" \"vector\" ...\n##  $ : int [1:5, 1:8] 1 2 3 4 5 6 7 8 9 10 ...\n##  $ : logi FALSE\n\n\nYou can name the elements in a list using the names() function, which adds a name attribute to each list item.\n\nnames(l) &lt;- c(\"string\", \"matrix\", \"logical\")\nnames(l)\n\n## [1] \"string\"  \"matrix\"  \"logical\"\n\n\nYou can also use the name of an item in the list to refer to it using the shortcut $ notation. This is the equivalent of using [[ ]] with either the column number or the name of the column in quotation marks as the argument inside of the double bracket.\n\n# all of the following are equivalent!\nl$string\n\n## [1] \"this\"    \"is\"      \"a\"       \"vector\"  \"of\"      \"strings\"\n\nl[[1]]\n\n## [1] \"this\"    \"is\"      \"a\"       \"vector\"  \"of\"      \"strings\"\n\nl[[\"string\"]]\n\n## [1] \"this\"    \"is\"      \"a\"       \"vector\"  \"of\"      \"strings\"\n\n\n\n# all of the following are equivalent\nl$matrix[3, 5]\n\n## [1] 23\n\nl[[2]][3, 5]\n\n## [1] 23\n\nl[[\"matrix\"]][3, 5]\n\n## [1] 23\n\n\nThe data frame is the perhaps the most useful (and most familiar) data structure that we can operate with in R as it most closely aligns with how we tend to represent tabular data, with rows as cases or observations and columns as variables describing those observations (e.g., a measurement of a particular type). Variables tend to be measured using the same units and thus fall into the same data class and can be thought of as analogous to vectors, so a data frame is essentially a list of atomic vectors that all have the same length.\nThe data.frame() command can be used to create data frames from scratch.\n\ndf &lt;- data.frame(firstName = c(\"Rick\", \"Negan\", \"Dwight\", \"Maggie\", \"Michonne\"),\n    community = c(\"Alexandria\", \"Saviors\", \"Saviors\", \"Hiltop\", \"Alexandria\"), sex = c(\"M\",\n        \"M\", \"M\", \"F\", \"F\"), age = c(42, 40, 33, 28, 31))\ndf\n\n##   firstName  community sex age\n## 1      Rick Alexandria   M  42\n## 2     Negan    Saviors   M  40\n## 3    Dwight    Saviors   M  33\n## 4    Maggie     Hiltop   F  28\n## 5  Michonne Alexandria   F  31\n\n\nMore commonly we read tabular data into R from some external data source (see Module 08, which typically results in the table being represented as a data frame. The following code, for example, will read from the file ‚Äúrandom-people.csv‚Äù stored in a folder called ‚Äúdata‚Äù (a data/ directory) located inside a user‚Äôs working directory.\n\ndf &lt;- read.csv(file = \"data/random-people.csv\", sep = \",\", header = TRUE, stringsAsFactors = FALSE)\n# only print select columns of this data frame head() means we will also only\n# print the first several rows\nhead(df[, c(1, 3, 4, 11, 12)])\n\n##   gender name.first name.last login.password           dob\n## 1   male        ted    wright          rolex  11/8/73 1:33\n## 2   male    quentin   schmitt         norton  5/24/51 3:16\n## 3 female      laura  johansen        stevens 5/22/77 21:03\n## 4   male     ismael   herrero         303030   8/1/58 9:13\n## 5 female     susana    blanco          aloha 12/18/55 3:21\n## 6   male      mason    wilson         topdog  6/23/60 9:19\n\n\n\nNOTE: To run the example code above, you may need to replace the string in file=\"&lt;string&gt;\" with the path to where you stored the file on your local computer.\n\n\nstr(df)\n\n## 'data.frame':    20 obs. of  17 variables:\n##  $ gender           : chr  \"male\" \"male\" \"female\" \"male\" ...\n##  $ name.title       : chr  \"mr\" \"mr\" \"ms\" \"mr\" ...\n##  $ name.first       : chr  \"ted\" \"quentin\" \"laura\" \"ismael\" ...\n##  $ name.last        : chr  \"wright\" \"schmitt\" \"johansen\" \"herrero\" ...\n##  $ location.street  : chr  \"2020 royal ln\" \"2433 rue dubois\" \"2142 elmelunden\" \"3897 calle del barquillo\" ...\n##  $ location.city    : chr  \"coffs harbour\" \"vitry-sur-seine\" \"silkeboeg\" \"gandia\" ...\n##  $ location.state   : chr  \"tasmania\" \"indre-et-loire\" \"hovedstaden\" \"ceuta\" ...\n##  $ location.postcode: chr  \"4126\" \"99856\" \"16264\" \"61349\" ...\n##  $ email            : chr  \"ted.wright@example.com\" \"quentin.schmitt@example.com\" \"laura.johansen@example.com\" \"ismael.herrero@example.com\" ...\n##  $ login.username   : chr  \"organicleopard402\" \"bluegoose191\" \"orangebird528\" \"heavyswan518\" ...\n##  $ login.password   : chr  \"rolex\" \"norton\" \"stevens\" \"303030\" ...\n##  $ dob              : chr  \"11/8/73 1:33\" \"5/24/51 3:16\" \"5/22/77 21:03\" \"8/1/58 9:13\" ...\n##  $ date.registered  : chr  \"5/5/07 20:26\" \"4/11/11 7:05\" \"5/16/14 15:53\" \"2/17/06 16:53\" ...\n##  $ phone            : chr  \"01-0349-5128\" \"05-72-65-32-21\" \"81616775\" \"974-117-403\" ...\n##  $ cell             : chr  \"0449-989-455\" \"06-83-24-92-41\" \"697-993-20\" \"665-791-673\" ...\n##  $ picture.large    : chr  \"https://randomuser.me/api/portraits/men/48.jpg\" \"https://randomuser.me/api/portraits/men/53.jpg\" \"https://randomuser.me/api/portraits/women/70.jpg\" \"https://randomuser.me/api/portraits/men/79.jpg\" ...\n##  $ nat              : chr  \"AU\" \"FR\" \"DK\" \"ES\" ...\n\ndplyr::glimpse(df)\n\n## Rows: 20\n## Columns: 17\n## $ gender            &lt;chr&gt; \"male\", \"male\", \"female\", \"male\", \"female\", \"male\", ‚Ä¶\n## $ name.title        &lt;chr&gt; \"mr\", \"mr\", \"ms\", \"mr\", \"ms\", \"mr\", \"mr\", \"miss\", \"m‚Ä¶\n## $ name.first        &lt;chr&gt; \"ted\", \"quentin\", \"laura\", \"ismael\", \"susana\", \"maso‚Ä¶\n## $ name.last         &lt;chr&gt; \"wright\", \"schmitt\", \"johansen\", \"herrero\", \"blanco\"‚Ä¶\n## $ location.street   &lt;chr&gt; \"2020 royal ln\", \"2433 rue dubois\", \"2142 elmelunden‚Ä¶\n## $ location.city     &lt;chr&gt; \"coffs harbour\", \"vitry-sur-seine\", \"silkeboeg\", \"ga‚Ä¶\n## $ location.state    &lt;chr&gt; \"tasmania\", \"indre-et-loire\", \"hovedstaden\", \"ceuta\"‚Ä¶\n## $ location.postcode &lt;chr&gt; \"4126\", \"99856\", \"16264\", \"61349\", \"29445\", \"91479\",‚Ä¶\n## $ email             &lt;chr&gt; \"ted.wright@example.com\", \"quentin.schmitt@example.c‚Ä¶\n## $ login.username    &lt;chr&gt; \"organicleopard402\", \"bluegoose191\", \"orangebird528\"‚Ä¶\n## $ login.password    &lt;chr&gt; \"rolex\", \"norton\", \"stevens\", \"303030\", \"aloha\", \"to‚Ä¶\n## $ dob               &lt;chr&gt; \"11/8/73 1:33\", \"5/24/51 3:16\", \"5/22/77 21:03\", \"8/‚Ä¶\n## $ date.registered   &lt;chr&gt; \"5/5/07 20:26\", \"4/11/11 7:05\", \"5/16/14 15:53\", \"2/‚Ä¶\n## $ phone             &lt;chr&gt; \"01-0349-5128\", \"05-72-65-32-21\", \"81616775\", \"974-1‚Ä¶\n## $ cell              &lt;chr&gt; \"0449-989-455\", \"06-83-24-92-41\", \"697-993-20\", \"665‚Ä¶\n## $ picture.large     &lt;chr&gt; \"https://randomuser.me/api/portraits/men/48.jpg\", \"h‚Ä¶\n## $ nat               &lt;chr&gt; \"AU\", \"FR\", \"DK\", \"ES\", \"ES\", \"NZ\", \"DE\", \"US\", \"TR\"‚Ä¶\n\n\nAs for other data structures, you can select and subset data frames using single bracket notation ([ ]). You can also select named columns from a data frame using the $ operator or the equivalent double bracket notation ([[ ]]).\n\n# single bracket notation\ndf[, 4]\n\n##  [1] \"wright\"     \"schmitt\"    \"johansen\"   \"herrero\"    \"blanco\"    \n##  [6] \"wilson\"     \"strauio\"    \"gordon\"     \"limoncuocu\" \"perrin\"    \n## [11] \"lopez\"      \"waisanen\"   \"brewer\"     \"brown\"      \"baettner\"  \n## [16] \"wallace\"    \"gonzalez\"   \"neva\"       \"barnaby\"    \"moser\"\n\nstr(df[, 4])\n\n##  chr [1:20] \"wright\" \"schmitt\" \"johansen\" \"herrero\" \"blanco\" \"wilson\" ...\n\n# returns the vector of data stored in column 4\n\nThe following are all equivalent‚Ä¶\n\n# using the $ operator with the column name\ndf$name.last\n\n##  [1] \"wright\"     \"schmitt\"    \"johansen\"   \"herrero\"    \"blanco\"    \n##  [6] \"wilson\"     \"strauio\"    \"gordon\"     \"limoncuocu\" \"perrin\"    \n## [11] \"lopez\"      \"waisanen\"   \"brewer\"     \"brown\"      \"baettner\"  \n## [16] \"wallace\"    \"gonzalez\"   \"neva\"       \"barnaby\"    \"moser\"\n\nstr(df$name.last)\n\n##  chr [1:20] \"wright\" \"schmitt\" \"johansen\" \"herrero\" \"blanco\" \"wilson\" ...\n\n# returns the vector of data stored in column `name.last`\n\n\n# using double bracket notation and a column index\ndf[[4]]\n\n##  [1] \"wright\"     \"schmitt\"    \"johansen\"   \"herrero\"    \"blanco\"    \n##  [6] \"wilson\"     \"strauio\"    \"gordon\"     \"limoncuocu\" \"perrin\"    \n## [11] \"lopez\"      \"waisanen\"   \"brewer\"     \"brown\"      \"baettner\"  \n## [16] \"wallace\"    \"gonzalez\"   \"neva\"       \"barnaby\"    \"moser\"\n\nstr(df[[4]])\n\n##  chr [1:20] \"wright\" \"schmitt\" \"johansen\" \"herrero\" \"blanco\" \"wilson\" ...\n\n# returns the vector of data stored in column 4\n\n\n# using double bracket notation with the column name\ndf[[\"name.last\"]]\n\n##  [1] \"wright\"     \"schmitt\"    \"johansen\"   \"herrero\"    \"blanco\"    \n##  [6] \"wilson\"     \"strauio\"    \"gordon\"     \"limoncuocu\" \"perrin\"    \n## [11] \"lopez\"      \"waisanen\"   \"brewer\"     \"brown\"      \"baettner\"  \n## [16] \"wallace\"    \"gonzalez\"   \"neva\"       \"barnaby\"    \"moser\"\n\nstr(df[[\"name.last\"]])\n\n##  chr [1:20] \"wright\" \"schmitt\" \"johansen\" \"herrero\" \"blanco\" \"wilson\" ...\n\n# returns the vector of data stored in column `name.last`\n\nNote that the following return data structures that are not quite the same as those returned above. Instead, these return data frames rather than vectors!\n\n# using single bracket notation with a column index and no row index\nhead(df[4])\n\n##   name.last\n## 1    wright\n## 2   schmitt\n## 3  johansen\n## 4   herrero\n## 5    blanco\n## 6    wilson\n\nstr(df[4])\n\n## 'data.frame':    20 obs. of  1 variable:\n##  $ name.last: chr  \"wright\" \"schmitt\" \"johansen\" \"herrero\" ...\n\n# returns a data frame of the data from column 4\n\n\n# using single bracket notation with a column name\nhead(df[\"name.last\"])\n\n##   name.last\n## 1    wright\n## 2   schmitt\n## 3  johansen\n## 4   herrero\n## 5    blanco\n## 6    wilson\n\nstr(df[\"name.last\"])\n\n## 'data.frame':    20 obs. of  1 variable:\n##  $ name.last: chr  \"wright\" \"schmitt\" \"johansen\" \"herrero\" ...\n\n# returns a data frame of the data from column `name.last`\n\nAs with matrixes, you can add rows (additional cases) or columns (additional variables) to a data frame using rbind() and cbind().\n\ndf &lt;- cbind(df, id = c(1:20))\ndf &lt;- cbind(df, school = c(\"UT\", \"UT\", \"A&M\", \"A&M\", \"UT\", \"Rice\", \"Texas Tech\",\n    \"UT\", \"UT\", \"Texas State\", \"A&M\", \"UT\", \"Rice\", \"UT\", \"A&M\", \"Texas Tech\", \"A&M\",\n    \"UT\", \"Texas State\", \"A&M\"))\nhead(df)\n\n##   gender name.title name.first name.last          location.street\n## 1   male         mr        ted    wright            2020 royal ln\n## 2   male         mr    quentin   schmitt          2433 rue dubois\n## 3 female         ms      laura  johansen          2142 elmelunden\n## 4   male         mr     ismael   herrero 3897 calle del barquillo\n## 5 female         ms     susana    blanco  2208 avenida de america\n## 6   male         mr      mason    wilson         4576 wilson road\n##     location.city location.state location.postcode                       email\n## 1   coffs harbour       tasmania              4126      ted.wright@example.com\n## 2 vitry-sur-seine indre-et-loire             99856 quentin.schmitt@example.com\n## 3       silkeboeg    hovedstaden             16264  laura.johansen@example.com\n## 4          gandia          ceuta             61349  ismael.herrero@example.com\n## 5        mastoles    extremadura             29445   susana.blanco@example.com\n## 6         dunedin       taranaki             91479    mason.wilson@example.com\n##      login.username login.password           dob date.registered          phone\n## 1 organicleopard402          rolex  11/8/73 1:33    5/5/07 20:26   01-0349-5128\n## 2      bluegoose191         norton  5/24/51 3:16    4/11/11 7:05 05-72-65-32-21\n## 3     orangebird528        stevens 5/22/77 21:03   5/16/14 15:53       81616775\n## 4      heavyswan518         303030   8/1/58 9:13   2/17/06 16:53    974-117-403\n## 5    silverkoala701          aloha 12/18/55 3:21   10/3/02 17:55    917-199-202\n## 6    organicduck470         topdog  6/23/60 9:19    12/1/08 8:31 (137)-326-5772\n##             cell                                    picture.large nat id school\n## 1   0449-989-455   https://randomuser.me/api/portraits/men/48.jpg  AU  1     UT\n## 2 06-83-24-92-41   https://randomuser.me/api/portraits/men/53.jpg  FR  2     UT\n## 3     697-993-20 https://randomuser.me/api/portraits/women/70.jpg  DK  3    A&M\n## 4    665-791-673   https://randomuser.me/api/portraits/men/79.jpg  ES  4    A&M\n## 5    612-612-929 https://randomuser.me/api/portraits/women/18.jpg  ES  5     UT\n## 6 (700)-060-1523   https://randomuser.me/api/portraits/men/60.jpg  NZ  6   Rice\n\n\nAlternatively, you can extend a data frame by adding a new variable directly using the $ operator, like this:\n\ndf$school &lt;- c(\"UT\", \"UT\", \"A&M\", \"A&M\", \"UT\", \"Rice\", \"Texas Tech\", \"UT\", \"UT\",\n    \"Texas State\", \"A&M\", \"UT\", \"Rice\", \"UT\", \"A&M\", \"Texas Tech\", \"A&M\", \"UT\", \"Texas State\",\n    \"A&M\")\nhead(df)\n\n##   gender name.title name.first name.last          location.street\n## 1   male         mr        ted    wright            2020 royal ln\n## 2   male         mr    quentin   schmitt          2433 rue dubois\n## 3 female         ms      laura  johansen          2142 elmelunden\n## 4   male         mr     ismael   herrero 3897 calle del barquillo\n## 5 female         ms     susana    blanco  2208 avenida de america\n## 6   male         mr      mason    wilson         4576 wilson road\n##     location.city location.state location.postcode                       email\n## 1   coffs harbour       tasmania              4126      ted.wright@example.com\n## 2 vitry-sur-seine indre-et-loire             99856 quentin.schmitt@example.com\n## 3       silkeboeg    hovedstaden             16264  laura.johansen@example.com\n## 4          gandia          ceuta             61349  ismael.herrero@example.com\n## 5        mastoles    extremadura             29445   susana.blanco@example.com\n## 6         dunedin       taranaki             91479    mason.wilson@example.com\n##      login.username login.password           dob date.registered          phone\n## 1 organicleopard402          rolex  11/8/73 1:33    5/5/07 20:26   01-0349-5128\n## 2      bluegoose191         norton  5/24/51 3:16    4/11/11 7:05 05-72-65-32-21\n## 3     orangebird528        stevens 5/22/77 21:03   5/16/14 15:53       81616775\n## 4      heavyswan518         303030   8/1/58 9:13   2/17/06 16:53    974-117-403\n## 5    silverkoala701          aloha 12/18/55 3:21   10/3/02 17:55    917-199-202\n## 6    organicduck470         topdog  6/23/60 9:19    12/1/08 8:31 (137)-326-5772\n##             cell                                    picture.large nat id school\n## 1   0449-989-455   https://randomuser.me/api/portraits/men/48.jpg  AU  1     UT\n## 2 06-83-24-92-41   https://randomuser.me/api/portraits/men/53.jpg  FR  2     UT\n## 3     697-993-20 https://randomuser.me/api/portraits/women/70.jpg  DK  3    A&M\n## 4    665-791-673   https://randomuser.me/api/portraits/men/79.jpg  ES  4    A&M\n## 5    612-612-929 https://randomuser.me/api/portraits/women/18.jpg  ES  5     UT\n## 6 (700)-060-1523   https://randomuser.me/api/portraits/men/60.jpg  NZ  6   Rice\n\n\nUsing the [[ ]] operator with a new variable name in quotation marks works, too:\n\ndf[[\"school\"]] &lt;- c(\"UT\", \"UT\", \"A&M\", \"A&M\", \"UT\", \"Rice\", \"Texas Tech\", \"UT\", \"UT\",\n    \"Texas State\", \"A&M\", \"UT\", \"Rice\", \"UT\", \"A&M\", \"Texas Tech\", \"A&M\", \"UT\", \"Texas State\",\n    \"A&M\")\nhead(df)\n\n##   gender name.title name.first name.last          location.street\n## 1   male         mr        ted    wright            2020 royal ln\n## 2   male         mr    quentin   schmitt          2433 rue dubois\n## 3 female         ms      laura  johansen          2142 elmelunden\n## 4   male         mr     ismael   herrero 3897 calle del barquillo\n## 5 female         ms     susana    blanco  2208 avenida de america\n## 6   male         mr      mason    wilson         4576 wilson road\n##     location.city location.state location.postcode                       email\n## 1   coffs harbour       tasmania              4126      ted.wright@example.com\n## 2 vitry-sur-seine indre-et-loire             99856 quentin.schmitt@example.com\n## 3       silkeboeg    hovedstaden             16264  laura.johansen@example.com\n## 4          gandia          ceuta             61349  ismael.herrero@example.com\n## 5        mastoles    extremadura             29445   susana.blanco@example.com\n## 6         dunedin       taranaki             91479    mason.wilson@example.com\n##      login.username login.password           dob date.registered          phone\n## 1 organicleopard402          rolex  11/8/73 1:33    5/5/07 20:26   01-0349-5128\n## 2      bluegoose191         norton  5/24/51 3:16    4/11/11 7:05 05-72-65-32-21\n## 3     orangebird528        stevens 5/22/77 21:03   5/16/14 15:53       81616775\n## 4      heavyswan518         303030   8/1/58 9:13   2/17/06 16:53    974-117-403\n## 5    silverkoala701          aloha 12/18/55 3:21   10/3/02 17:55    917-199-202\n## 6    organicduck470         topdog  6/23/60 9:19    12/1/08 8:31 (137)-326-5772\n##             cell                                    picture.large nat id school\n## 1   0449-989-455   https://randomuser.me/api/portraits/men/48.jpg  AU  1     UT\n## 2 06-83-24-92-41   https://randomuser.me/api/portraits/men/53.jpg  FR  2     UT\n## 3     697-993-20 https://randomuser.me/api/portraits/women/70.jpg  DK  3    A&M\n## 4    665-791-673   https://randomuser.me/api/portraits/men/79.jpg  ES  4    A&M\n## 5    612-612-929 https://randomuser.me/api/portraits/women/18.jpg  ES  5     UT\n## 6 (700)-060-1523   https://randomuser.me/api/portraits/men/60.jpg  NZ  6   Rice\n\n\n\nNOTE: In the above examples, cbind() results in school being added as a factor while using the $ operator results in school being added as a character vector. You can see this by using the str() command.\n\nA factor is another atomic data class for R for dealing efficiently with nominal variables, usually character strings. Internally, R assigns integer values to each unique string (e.g., 1 for ‚Äúfemale‚Äù, 2 for ‚Äúmale‚Äù, etc.).\n\n\nFiltering Rows of a Data Frame\nAn expression that evaluates to a logical vector also be used to subset data frames. Here, we filter the data frame for only those rows where the variable school is ‚ÄúUT‚Äù.\n\nnew_df &lt;- df[df$school == \"UT\", ]\nnew_df\n\n##    gender name.title name.first  name.last         location.street\n## 1    male         mr        ted     wright           2020 royal ln\n## 2    male         mr    quentin    schmitt         2433 rue dubois\n## 5  female         ms     susana     blanco 2208 avenida de america\n## 8  female       miss     kaylee     gordon         5475 camden ave\n## 9    male         mr     baraek limoncuocu          2664 baedat cd\n## 12   male         mr   valtteri   waisanen          9850 hemeentie\n## 14 female       miss   kimberly      brown         8654 manor road\n## 18 female         ms       ella       neva          4620 visiokatu\n##      location.city location.state location.postcode\n## 1    coffs harbour       tasmania              4126\n## 2  vitry-sur-seine indre-et-loire             99856\n## 5         mastoles    extremadura             29445\n## 8            flint         oregon             84509\n## 9            siirt          tokat             86146\n## 12          halsua  south karelia             58124\n## 14          bangor        borders          HI92 8RY\n## 18          kerava finland proper             26385\n##                              email    login.username login.password\n## 1           ted.wright@example.com organicleopard402          rolex\n## 2      quentin.schmitt@example.com      bluegoose191         norton\n## 5        susana.blanco@example.com    silverkoala701          aloha\n## 8        kaylee.gordon@example.com beautifulgoose794       atlantis\n## 9  baraek.limoncuoculu@example.com whitebutterfly599         tobias\n## 12   valtteri.waisanen@example.com        redswan919       nocturne\n## 14      kimberly.brown@example.com  crazyelephant996       nebraska\n## 18           ella.neva@example.com  orangegorilla786       f00tball\n##               dob date.registered          phone           cell\n## 1    11/8/73 1:33    5/5/07 20:26   01-0349-5128   0449-989-455\n## 2    5/24/51 3:16    4/11/11 7:05 05-72-65-32-21 06-83-24-92-41\n## 5   12/18/55 3:21   10/3/02 17:55    917-199-202    612-612-929\n## 8   3/24/48 12:22     5/5/13 8:14 (817)-962-1275 (831)-325-1142\n## 9    5/8/92 22:01    9/12/04 0:56 (023)-879-4331 (837)-014-1113\n## 12 12/24/80 10:40   9/22/03 20:47     02-227-661  042-153-83-79\n## 14    1/9/86 8:54    12/3/11 0:41   017684 80873   0799-553-944\n## 18  7/18/91 14:30    3/17/14 7:13     02-351-279  043-436-42-30\n##                                       picture.large nat id school\n## 1    https://randomuser.me/api/portraits/men/48.jpg  AU  1     UT\n## 2    https://randomuser.me/api/portraits/men/53.jpg  FR  2     UT\n## 5  https://randomuser.me/api/portraits/women/18.jpg  ES  5     UT\n## 8  https://randomuser.me/api/portraits/women/65.jpg  US  8     UT\n## 9    https://randomuser.me/api/portraits/men/94.jpg  TR  9     UT\n## 12   https://randomuser.me/api/portraits/men/80.jpg  FI 12     UT\n## 14 https://randomuser.me/api/portraits/women/49.jpg  GB 14     UT\n## 18 https://randomuser.me/api/portraits/women/68.jpg  FI 18     UT\n\n\nIn this case, R evaluates the expression df$school == \"UT\" and returns a logical vector equal in length to the number of rows in df.\n\ndf$school == \"UT\"\n\n##  [1]  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE  TRUE\n## [13] FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE\n\n\nIt then subsets the original df based on that vector, returning only rows that evaluate to ‚ÄúTRUE‚Äù.\nThe Boolean operators & (for ‚ÄúAND‚Äù) and | (for ‚ÄúOR‚Äù) can be used to create more complex filtering criteria. Here, we filter the data frame for only those rows where the variable school is ‚ÄúUT‚Äù AND the variable gender is ‚Äúfemale‚Äù.\n\nnew_df &lt;- df[df$school == \"UT\" & df$gender == \"female\", ]\nnew_df\n\n##    gender name.title name.first name.last         location.street location.city\n## 5  female         ms     susana    blanco 2208 avenida de america      mastoles\n## 8  female       miss     kaylee    gordon         5475 camden ave         flint\n## 14 female       miss   kimberly     brown         8654 manor road        bangor\n## 18 female         ms       ella      neva          4620 visiokatu        kerava\n##    location.state location.postcode                      email\n## 5     extremadura             29445  susana.blanco@example.com\n## 8          oregon             84509  kaylee.gordon@example.com\n## 14        borders          HI92 8RY kimberly.brown@example.com\n## 18 finland proper             26385      ella.neva@example.com\n##       login.username login.password           dob date.registered\n## 5     silverkoala701          aloha 12/18/55 3:21   10/3/02 17:55\n## 8  beautifulgoose794       atlantis 3/24/48 12:22     5/5/13 8:14\n## 14  crazyelephant996       nebraska   1/9/86 8:54    12/3/11 0:41\n## 18  orangegorilla786       f00tball 7/18/91 14:30    3/17/14 7:13\n##             phone           cell\n## 5     917-199-202    612-612-929\n## 8  (817)-962-1275 (831)-325-1142\n## 14   017684 80873   0799-553-944\n## 18     02-351-279  043-436-42-30\n##                                       picture.large nat id school\n## 5  https://randomuser.me/api/portraits/women/18.jpg  ES  5     UT\n## 8  https://randomuser.me/api/portraits/women/65.jpg  US  8     UT\n## 14 https://randomuser.me/api/portraits/women/49.jpg  GB 14     UT\n## 18 https://randomuser.me/api/portraits/women/68.jpg  FI 18     UT\n\n\nHere, we filter the data frame for only rows where either the school is ‚ÄúUT‚Äù OR the variable gender is ‚Äúfemale‚Äù, using the | operator. We also select only the columns gender, name.first, and name.last by passing a vector to the second argument of the [ ] function.\n\nnew_df &lt;- df[df$school == \"UT\" | df$gender == \"female\", c(\"gender\", \"name.first\",\n    \"name.last\")]\nnew_df\n\n##    gender name.first  name.last\n## 1    male        ted     wright\n## 2    male    quentin    schmitt\n## 3  female      laura   johansen\n## 5  female     susana     blanco\n## 8  female     kaylee     gordon\n## 9    male     baraek limoncuocu\n## 12   male   valtteri   waisanen\n## 13 female    vanessa     brewer\n## 14 female   kimberly      brown\n## 15 female     loreen   baettner\n## 16 female      becky    wallace\n## 18 female       ella       neva\n\n\n\n\nSelecting Columns of a Data Frame\nWe can also select to only return particular columns when we filter. Here, we return only the columns name.last, name.first, and school.\n\nnew_df &lt;- df[df$school == \"UT\", c(\"name.last\", \"name.first\", \"school\")]\nnew_df\n\n##     name.last name.first school\n## 1      wright        ted     UT\n## 2     schmitt    quentin     UT\n## 5      blanco     susana     UT\n## 8      gordon     kaylee     UT\n## 9  limoncuocu     baraek     UT\n## 12   waisanen   valtteri     UT\n## 14      brown   kimberly     UT\n## 18       neva       ella     UT\n\n\nHere, we return all rows from the data frame, but only the ‚Äúname.last‚Äù, ‚Äúname.first‚Äù, and ‚Äúschool‚Äù columns.\n\nnew_df &lt;- df[, c(\"name.last\", \"name.first\", \"school\")]\nnew_df\n\n##     name.last name.first      school\n## 1      wright        ted          UT\n## 2     schmitt    quentin          UT\n## 3    johansen      laura         A&M\n## 4     herrero     ismael         A&M\n## 5      blanco     susana          UT\n## 6      wilson      mason        Rice\n## 7     strauio       lutz  Texas Tech\n## 8      gordon     kaylee          UT\n## 9  limoncuocu     baraek          UT\n## 10     perrin     basile Texas State\n## 11      lopez      ruben         A&M\n## 12   waisanen   valtteri          UT\n## 13     brewer    vanessa        Rice\n## 14      brown   kimberly          UT\n## 15   baettner     loreen         A&M\n## 16    wallace      becky  Texas Tech\n## 17   gonzalez     hector         A&M\n## 18       neva       ella          UT\n## 19    barnaby      simon Texas State\n## 20      moser        max         A&M\n\n\nWe can also refer to columns by their positions and return them in a select order, thereby restructuring the data frame. Here, we return all rows from the data frame, but include only columns 1, 3, and 4, flipping the order of the latter two columns:\n\nnew_df &lt;- df[, c(1, 4, 3)]\nnew_df\n\n##    gender  name.last name.first\n## 1    male     wright        ted\n## 2    male    schmitt    quentin\n## 3  female   johansen      laura\n## 4    male    herrero     ismael\n## 5  female     blanco     susana\n## 6    male     wilson      mason\n## 7    male    strauio       lutz\n## 8  female     gordon     kaylee\n## 9    male limoncuocu     baraek\n## 10   male     perrin     basile\n## 11   male      lopez      ruben\n## 12   male   waisanen   valtteri\n## 13 female     brewer    vanessa\n## 14 female      brown   kimberly\n## 15 female   baettner     loreen\n## 16 female    wallace      becky\n## 17   male   gonzalez     hector\n## 18 female       neva       ella\n## 19   male    barnaby      simon\n## 20   male      moser        max\n\n\nWe can use a minus sign - in front of a vector of indices to instead indicate columns we do not want to return:\n\nnew_df &lt;- df[, -c(1, 2, 5:18)]\nnew_df\n\n##    name.first  name.last      school\n## 1         ted     wright          UT\n## 2     quentin    schmitt          UT\n## 3       laura   johansen         A&M\n## 4      ismael    herrero         A&M\n## 5      susana     blanco          UT\n## 6       mason     wilson        Rice\n## 7        lutz    strauio  Texas Tech\n## 8      kaylee     gordon          UT\n## 9      baraek limoncuocu          UT\n## 10     basile     perrin Texas State\n## 11      ruben      lopez         A&M\n## 12   valtteri   waisanen          UT\n## 13    vanessa     brewer        Rice\n## 14   kimberly      brown          UT\n## 15     loreen   baettner         A&M\n## 16      becky    wallace  Texas Tech\n## 17     hector   gonzalez         A&M\n## 18       ella       neva          UT\n## 19      simon    barnaby Texas State\n## 20        max      moser         A&M",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Additional Data Structures in ***R***</span>"
    ]
  },
  {
    "objectID": "07-module.html#factors",
    "href": "07-module.html#factors",
    "title": "7¬† Additional Data Structures in R",
    "section": "7.5 Factors",
    "text": "7.5 Factors\nWe were introduced to the factor data class above. Again, factors are numeric codes that R can use internally that correspond to character value ‚Äúlevels‚Äù.\nWhen we load in data from an external source (as we do in Module 08), {base} R tends to import character string data as factors, assigning to each unique string to an integer numeric code and assigning the string as a ‚Äúlabel‚Äù for that code. Using factors can make some code run much more quickly (e.g., ANOVA, ANCOVA, and other forms of regression using categorical variables).",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Additional Data Structures in ***R***</span>"
    ]
  },
  {
    "objectID": "07-module.html#variable-conversion-and-coercion",
    "href": "07-module.html#variable-conversion-and-coercion",
    "title": "7¬† Additional Data Structures in R",
    "section": "7.6 Variable Conversion and Coercion",
    "text": "7.6 Variable Conversion and Coercion\nYou can convert factor to character data (and vice versa) using the as.character() or as.factor() commands. You can also convert/coerce any vector to a different class using similar constructs (e.g., as.numeric()), although not all such conversions are really meaningful. Converting factor data to numeric results in the the converted data having the value of R‚Äôs internal numeric code for the factor level, while converting character data to numeric results in the data being coerced into the special data value of NA (see below) for missing data.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Additional Data Structures in ***R***</span>"
    ]
  },
  {
    "objectID": "07-module.html#special-data-values",
    "href": "07-module.html#special-data-values",
    "title": "7¬† Additional Data Structures in R",
    "section": "7.7 Special Data Values",
    "text": "7.7 Special Data Values\nFinally, R has three special data values that it uses in a variety of situations.\n\nNA (for not available) is used for missing data. Many statistical functions offer the possibility to include as an argument na.rm=TRUE (‚Äúremove NAs‚Äù) so that NAs are excluded from a calculation.\nInf (and -Inf) is used when the result of a numerical calculation is too extreme for R to express\nNaN (for not a number) is used when R cannot express the results of a calculation , e.g., when you try to take the square root of a negative number\n\n\nCHALLENGE\n\nStore the following vector of numbers as a 5 x 3 matrix: 3, 0, 1 ,23, 1, 2, 33, 1, 1, 42, 0, 1, 41, 0, 2\nBe sure to fill the matrix ROWWISE\n\n\n\nShow Code\nm &lt;- matrix(c(3, 0, 1, 23, 1, 2, 33, 1, 1, 42, 0, 1, 41, 0, 2), nrow = 5, ncol = 3,\n    byrow = TRUE)\nm\n\n\nShow Output\n##      [,1] [,2] [,3]\n## [1,]    3    0    1\n## [2,]   23    1    2\n## [3,]   33    1    1\n## [4,]   42    0    1\n## [5,]   41    0    2\n\n\n\n\nThen, coerce the matrix to a data frame\n\n\n\nShow Code\n(d &lt;- as.data.frame(m))\n\n\nShow Output\n##   V1 V2 V3\n## 1  3  0  1\n## 2 23  1  2\n## 3 33  1  1\n## 4 42  0  1\n## 5 41  0  2\n\n\n\n\nAs a data frame, coerce the second column to be logical (i.e., Boolean)\n\n\n\nShow Code\n(d[, 2] &lt;- as.logical(d[, 2]))\n\n\nShow Output\n## [1] FALSE  TRUE  TRUE FALSE FALSE\n\n\n\n\nAs a data frame, coerce the third column to be a factor\n\n\n\nShow Code\n(d[, 3] &lt;- as.factor(d[, 3]))\n\n\nShow Output\n## [1] 1 2 1 1 2\n## Levels: 1 2\n\n\n\n\nWhen you are done, use the str() command to show the data type for each variable in your dataframe.\n\n\nstr(d)\n\n## 'data.frame':    5 obs. of  3 variables:\n##  $ V1: num  3 23 33 42 41\n##  $ V2: logi  FALSE TRUE TRUE FALSE FALSE\n##  $ V3: Factor w/ 2 levels \"1\",\"2\": 1 2 1 1 2",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Additional Data Structures in ***R***</span>"
    ]
  },
  {
    "objectID": "07-module.html#other-data-frame-like-structures",
    "href": "07-module.html#other-data-frame-like-structures",
    "title": "7¬† Additional Data Structures in R",
    "section": "7.8 Other Data Frame-Like Structures",
    "text": "7.8 Other Data Frame-Like Structures\n\nData Tables\nA ‚Äúdata table‚Äù is a structure introduced in the package {data.table} that provides an enhancements to the data frame structure, which is the standard data structure for storing tabular data in {base} R. We use the same syntax for creating a data table‚Ä¶\n\ndt &lt;- data.table(firstName = c(\"Rick\", \"Negan\", \"Dwight\", \"Maggie\", \"Michonne\"),\n    community = c(\"Alexandria\", \"Saviors\", \"Saviors\", \"Hiltop\", \"Alexandria\"), sex = c(\"M\",\n        \"M\", \"M\", \"F\", \"F\"), age = c(42, 40, 33, 28, 31))\ndt\n\n##    firstName  community    sex   age\n##       &lt;char&gt;     &lt;char&gt; &lt;char&gt; &lt;num&gt;\n## 1:      Rick Alexandria      M    42\n## 2:     Negan    Saviors      M    40\n## 3:    Dwight    Saviors      M    33\n## 4:    Maggie     Hiltop      F    28\n## 5:  Michonne Alexandria      F    31\n\nstr(dt)\n\n## Classes 'data.table' and 'data.frame':   5 obs. of  4 variables:\n##  $ firstName: chr  \"Rick\" \"Negan\" \"Dwight\" \"Maggie\" ...\n##  $ community: chr  \"Alexandria\" \"Saviors\" \"Saviors\" \"Hiltop\" ...\n##  $ sex      : chr  \"M\" \"M\" \"M\" \"F\" ...\n##  $ age      : num  42 40 33 28 31\n##  - attr(*, \".internal.selfref\")=&lt;externalptr&gt;\n\n# versus...\n\ndf &lt;- data.frame(firstName = c(\"Rick\", \"Negan\", \"Dwight\", \"Maggie\", \"Michonne\"),\n    community = c(\"Alexandria\", \"Saviors\", \"Saviors\", \"Hiltop\", \"Alexandria\"), sex = c(\"M\",\n        \"M\", \"M\", \"F\", \"F\"), age = c(42, 40, 33, 28, 31))\ndf\n\n##   firstName  community sex age\n## 1      Rick Alexandria   M  42\n## 2     Negan    Saviors   M  40\n## 3    Dwight    Saviors   M  33\n## 4    Maggie     Hiltop   F  28\n## 5  Michonne Alexandria   F  31\n\nstr(df)\n\n## 'data.frame':    5 obs. of  4 variables:\n##  $ firstName: chr  \"Rick\" \"Negan\" \"Dwight\" \"Maggie\" ...\n##  $ community: chr  \"Alexandria\" \"Saviors\" \"Saviors\" \"Hiltop\" ...\n##  $ sex      : chr  \"M\" \"M\" \"M\" \"F\" ...\n##  $ age      : num  42 40 33 28 31\n\n\nNote that printing a data table results in a slightly different output than printing a data frame (e.g., row numbers are printed followed by a ‚Äú:‚Äù) and the structure (str()) looks a bit different. Also, different from data frames, when we read in data, columns of character type are never converted to factors by default (i.e., we do not need to specify anything like stringsAsFactors=FALSE when we read in data‚Ä¶ that‚Äôs the opposite default as we see for data frames).\nThe big advantage of using data tables over data frames is that they support a different, easier syntax for filtering rows and selecting columns and for grouping output.\n\ndt[sex == \"M\"]  # filter for sex = 'M' in a data table\n\n##    firstName  community    sex   age\n##       &lt;char&gt;     &lt;char&gt; &lt;char&gt; &lt;num&gt;\n## 1:      Rick Alexandria      M    42\n## 2:     Negan    Saviors      M    40\n## 3:    Dwight    Saviors      M    33\n\ndf[df$sex == \"M\", ]  # filter for sex = 'M' in a data frame\n\n##   firstName  community sex age\n## 1      Rick Alexandria   M  42\n## 2     Negan    Saviors   M  40\n## 3    Dwight    Saviors   M  33\n\ndt[1:2]  # return the first two rows of the data table\n\n##    firstName  community    sex   age\n##       &lt;char&gt;     &lt;char&gt; &lt;char&gt; &lt;num&gt;\n## 1:      Rick Alexandria      M    42\n## 2:     Negan    Saviors      M    40\n\ndf[1:2, ]  # return the first two rows of the data table\n\n##   firstName  community sex age\n## 1      Rick Alexandria   M  42\n## 2     Negan    Saviors   M  40\n\ndt[, sex]  # returns the variable 'sex'\n\n## [1] \"M\" \"M\" \"M\" \"F\" \"F\"\n\nstr(dt[, sex])  # sex is a CHARACTER vector\n\n##  chr [1:5] \"M\" \"M\" \"M\" \"F\" \"F\"\n\ndf[, c(\"sex\")]  # returns the variable 'sex'\n\n## [1] \"M\" \"M\" \"M\" \"F\" \"F\"\n\nstr(df[, c(\"sex\")])  # sex is a FACTOR with 2 levels\n\n##  chr [1:5] \"M\" \"M\" \"M\" \"F\" \"F\"\n\n\nThe data table structure also allows us more straightforward syntax - and implements much faster algorithms - for computations on columns and for perform aggregations of data by a grouping variable.\n\n\n‚ÄúTibbles‚Äù\nA ‚Äútibble‚Äù is another newer take on the data frame structure, implemented in the package {tibble} (which is loaded as part of {tidyverse}). The structure was created to keep the best features of data frames and correct some of the more frustrating aspects associated with the older structure. For example, like data tables, tibbles do not by default change the input type of a variable from character to factor when the tibble is created.\n\nt &lt;- tibble(firstName = c(\"Rick\", \"Negan\", \"Dwight\", \"Maggie\", \"Michonne\"), community = c(\"Alexandria\",\n    \"Saviors\", \"Saviors\", \"Hiltop\", \"Alexandria\"), sex = c(\"M\", \"M\", \"M\", \"F\", \"F\"),\n    age = c(42, 40, 33, 28, 31))\nt\n\n## # A tibble: 5 √ó 4\n##   firstName community  sex     age\n##   &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt; &lt;dbl&gt;\n## 1 Rick      Alexandria M        42\n## 2 Negan     Saviors    M        40\n## 3 Dwight    Saviors    M        33\n## 4 Maggie    Hiltop     F        28\n## 5 Michonne  Alexandria F        31\n\nstr(t)\n\n## tibble [5 √ó 4] (S3: tbl_df/tbl/data.frame)\n##  $ firstName: chr [1:5] \"Rick\" \"Negan\" \"Dwight\" \"Maggie\" ...\n##  $ community: chr [1:5] \"Alexandria\" \"Saviors\" \"Saviors\" \"Hiltop\" ...\n##  $ sex      : chr [1:5] \"M\" \"M\" \"M\" \"F\" ...\n##  $ age      : num [1:5] 42 40 33 28 31\n\n\nNote that the output of printing a tibble again looks slightly different than that for data frames or data tables‚Ä¶ e.g., the data type of each column is included in the header row, for example. str() also shows us that characters were not converted to factors.\nAdditionally, subsetting tibbles with the single bracket operator ([ ]) always returns a tibble, whereas doing the same with a data frame can return either a data frame or a vector.\n\nt[, \"age\"]\n\n## # A tibble: 5 √ó 1\n##     age\n##   &lt;dbl&gt;\n## 1    42\n## 2    40\n## 3    33\n## 4    28\n## 5    31\n\nclass(t[, \"age\"])\n\n## [1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\ndf[, \"age\"]\n\n## [1] 42 40 33 28 31\n\nclass(df[, \"age\"])\n\n## [1] \"numeric\"\n\n\nThere are some other subtle differences regarding the behavior of tibbles versus data frames that are also worthwhile to note. Data frames support ‚Äúpartial matching‚Äù in variable names when the $ operator is used, thus df$a will return the variable df$age. Tibbles are stricter and will never do partial matching.\n\ndf$a  # returns df$age\n\n## [1] 42 40 33 28 31\n\nt$a  # returns NULL and gives a warning\n\n## Warning: Unknown or uninitialised column: `a`.\n\n\n## NULL\n\n\nFinally, tibbles are careful about recycling. When creating a tibble, columns have to have consistent lengths and only values of length 1 are recycled. Thus, in the following‚Ä¶\n\nt &lt;- tibble(a = 1:4, c = 1)\n# this works fine... c is recycled\nt\n\n## # A tibble: 4 √ó 2\n##       a     c\n##   &lt;int&gt; &lt;dbl&gt;\n## 1     1     1\n## 2     2     1\n## 3     3     1\n## 4     4     1\n\n# t &lt;- tibble(a=1:4, c=1:2) but this would throw an error... c is not recycled\n# even though it could fit evenly into the number of rows\n\ndf &lt;- data.frame(a = 1:4, c = 1:2)\ndf\n\n##   a c\n## 1 1 1\n## 2 2 2\n## 3 3 1\n## 4 4 2\n\n\n\n# | include: false\ndetach(package:tidyverse)\ndetach(package:data.table)",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Additional Data Structures in ***R***</span>"
    ]
  },
  {
    "objectID": "07-module.html#concept-review",
    "href": "07-module.html#concept-review",
    "title": "7¬† Additional Data Structures in R",
    "section": "Concept Review",
    "text": "Concept Review\n\nCreating matrices and arrays: matrix(data=, nrow=, ncol=, byrow=), array(data=, dim=), rbind(), cbind()\nCreating lists: list()\nCreating data frames: data.frame()\nSubsetting: single bracket ([ ]), double bracket ([[ ]]), and $ notation\nVariable coercion: as.numeric(), as.character(), as.data.frame()\nReading .csv data: read.csv(file=, header=, stringsAsFactors=)\nFiltering rows of a data frame using {base} R: df[df$&lt;variable name&gt; == \"&lt;criterion&gt;\", ], df[df[[\"&lt;variable name&gt;\"]] == \"&lt;criterion&gt;\", ]\nSelecting/excluding columns of a data frame: df[ , c(\"&lt;variable name 1&gt;\", \"&lt;variable name 2&gt;\",...)],df[ , c(&lt;column index 1&gt;, &lt;column index 2&gt;,...)]",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Additional Data Structures in ***R***</span>"
    ]
  },
  {
    "objectID": "08-module.html",
    "href": "08-module.html",
    "title": "8¬† Getting Data into R",
    "section": "",
    "text": "8.1 Objectives",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Getting Data into ***R***</span>"
    ]
  },
  {
    "objectID": "08-module.html#objectives",
    "href": "08-module.html#objectives",
    "title": "8¬† Getting Data into R",
    "section": "",
    "text": "The objective of this module to learn how to download data sets from various local and online sources.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Getting Data into ***R***</span>"
    ]
  },
  {
    "objectID": "08-module.html#preliminaries",
    "href": "08-module.html#preliminaries",
    "title": "8¬† Getting Data into R",
    "section": "8.2 Preliminaries",
    "text": "8.2 Preliminaries\n\nGO TO: https://github.com/difiore/ada-datasets, download the ‚Äú.txt‚Äù and ‚Äú.csv‚Äù versions of ‚ÄúCountry-Data-2016‚Äù, and save them locally.\n\nThis data set consists of basic statistics (area, current population size, birth rate, death rate, life expectancy, and form of government) for 249 countries taken from WorldData.info that I have combined with data from the International Union for the Conservation of Nature (IUCN)‚Äôs Red List Summary Statistics about the number of threatened animal species by country.\n\nFrom the same page, download each of the ‚Äú.txt‚Äù, ‚Äú.csv‚Äù, and ‚Äú.xlsx‚Äù versions of ‚ÄúCPDS-1960-2014-reduced‚Äù.\n\nThese files contain a version of the Comparative Political Data Set (CPDS), which is ‚Äúa collection of political and institutional country-level data provided by Prof.¬†Dr.¬†Klaus Armingeon and collaborators at the University of Berne. The dataset consists of annual data for 36 democratic countries for the period of 1960 to 2014 or since their transition to democracy‚Äù (Armingeon et al.¬†2016). The full dataset consists of 300 variables, which I have pared down to a smaller set of economical and population size variables.\n\nCITATION: Armingeon K, Isler C, Kn√∂pfel L, Weisstanner D, and Engler S. 2016. Comparative Political Data Set 1960-2014. Bern: Institute of Political Science, University of Berne.\n\n\nInstall these packages in R: {readxl}, {XLConnect}, {gdata}, {xlsx}, {curl}, {rdrop2}, {repmis}, {googlesheets4}, and {googledrive}\nLoad {tidyverse}\n\n\nNOTE: Some of these packages (e.g., {XLConnect}, {xlsx}) require that your computer has a Java Runtime Environment (JRE) installed. A JRE is a bundle of software contaiing an interpreter and compiler for Java code, which is used to implement some of the functionality in the package. If your computer does not already have a JRE installed, you will need to also install one before being able to use these packages. You can download a JRE from Oracle‚Äôs Java website.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Getting Data into ***R***</span>"
    ]
  },
  {
    "objectID": "08-module.html#the-tao-of-text",
    "href": "08-module.html#the-tao-of-text",
    "title": "8¬† Getting Data into R",
    "section": "8.3 The Tao of Text",
    "text": "8.3 The Tao of Text\nSo far, we have seen how to create a variety of data structures by hand (e.g., using the c() function), but for larger data sets we will need mechanisms to import data into R. There are many methods for importing tabular data, stored in various formats (like text files, spreadsheets, and databases).\nPlain text files are, arguably, the very best way to store data (and scripts and other documents) as they are a standard format that has been around longer than most operating systems and are unlikely to change anytime soon. Some of the benefits of using text files are listed below:\n\nPlain text does not have a version and does not age.\nPlain text files are platform and software agnostic.\nPlain text files can be opened by a wide variety of programs.\nPlain text can easily be copied and pasted into a wide range of software.\nPlain text files tend to be smaller and quicker to open then proprietary formats.\nPlain text files are easy to transmit over the web.\nMany mature and efficient software tools exist for indexing, parsing, searching, and modifying text.\nThe content of plain text files looks the same on any system.\nVarious flavors of Markdown can be used for styling plain text files, if needed.\nPlain text remains itself outside of the digital context.\nCAVEAT: With text, we do have to think about the sometimes gnarly issue of text encoding. See this article or this pdf for a nice overview of the issues.\n\n\nTL/DR: Work with UTF-8 encoding whenever you can!",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Getting Data into ***R***</span>"
    ]
  },
  {
    "objectID": "08-module.html#working-with-local-files",
    "href": "08-module.html#working-with-local-files",
    "title": "8¬† Getting Data into R",
    "section": "8.4 Working with Local Files",
    "text": "8.4 Working with Local Files\n\nSetting the Path to a File\nThe file.choose() command is a useful command for interactive engagement with R. It gives you a familiar operating system-specific dialog box and allows you to select a file. You can use this to specify the path to a locally-stored file. The code below will assign the variable f to the full path to the file you choose.\n\nf &lt;- file.choose()\n\nAlternatively, you can directly assign a variable, e.g., f, to be the path to a locally-stored file. The file paths below refer to where I have saved the data files I downloaded - in a folder called data/ within my working directory. You may need to change this path if you have saved downloaded data to a different location on your computer.\n\n\nLoading Data from Text Files\nIn R, we can load a data set from a several types of plain text file stored on a local computer using the read.table() function from the {base} package, with the path to the file as the first (file=\"&lt;path&gt;\") argument for the function. An additional argument (header=&lt;boolean&gt;) can be used to specify whether the first row of the data file consists of column/field names.\nThe generic read.table() function can be used to read data files where columns are separated by tabs, commas, white space, or some other delimiter. The sep= argument tells R what character is used as a delimiter. The skip= argument can be used to start reading a file after a set number of rows.\nThere are format-specific variants of read.table() (e.g., read.csv()) that have different defaults and may be quicker for certain file types. Note that, as mentioned in Module 07, when using this function from the {base} package, the argument stringsAsFactors= is set to be TRUE by default, and we need to set it as FALSE if we want character strings to be loaded as actual strings.\nAs an example, we will read in some of the data sets that you have copied and stored locally in the files ‚ÄúCPDS-1960-2014-reduced.csv‚Äù and ‚ÄúCPDS-1960-2014-reduced.txt‚Äù.\n\nReading comma-separated (‚Äú.csv‚Äù) text files with {base} R\n\nf &lt;- \"data/CPDS-1960-2014-reduced.csv\"\nd &lt;- read.table(f, header = TRUE, sep = \",\", stringsAsFactors = FALSE)\nhead(d)  # shows the first 6 lines of data\n\n##   year   country gov_right1 gov_cent1 gov_left1 gov_party      elect vturn\n## 1 1960 Australia        100         0         0         1             95.5\n## 2 1961 Australia        100         0         0         1 09/12/1961  95.3\n## 3 1962 Australia        100         0         0         1             95.3\n## 4 1963 Australia        100         0         0         1 30/11/1963  95.7\n## 5 1964 Australia        100         0         0         1             95.7\n## 6 1965 Australia        100         0         0         1             95.7\n##   womenpar realgdpgr inflation debt_hist deficit ttl_labf labfopar unemp\n## 1        0        NA      3.73     40.15    0.46  4215.00       NA  1.25\n## 2        0     -0.64      2.29     38.62   -0.36  4286.00       NA  2.46\n## 3        0      5.77     -0.32     38.75   -0.79  4382.00       NA  2.32\n## 4        0      6.01      0.64     37.34   -0.51  4484.00       NA  1.87\n## 5        0      6.26      2.87     35.31   -0.08  4610.80    67.24  1.45\n## 6        0      4.99      3.41     53.99   -0.73  4745.95    67.66  1.36\n##       pop pop15_64 pop65 elderly\n## 1 10275.0   6296.5 874.9    8.51\n## 2 10508.2   6428.6 894.6    8.51\n## 3 10700.5   6571.5 913.6    8.54\n## 4 10906.9   6710.9 933.0    8.55\n## 5 11121.6   6857.3 948.1    8.52\n## 6 11340.9   7014.6 966.3    8.52\n\n\n\nNOTE: You can use a second argument to the head() function to return a specified number of lines, e.g., head(d, 10). You can also use bracket notation to display a certain range of lines, e.g., head(d)[11:20].\n\nOr, alternatively‚Ä¶\n\nd &lt;- read.csv(f, header = TRUE, stringsAsFactors = FALSE)\nhead(d)\n\n##   year   country gov_right1 gov_cent1 gov_left1 gov_party      elect vturn\n## 1 1960 Australia        100         0         0         1             95.5\n## 2 1961 Australia        100         0         0         1 09/12/1961  95.3\n## 3 1962 Australia        100         0         0         1             95.3\n## 4 1963 Australia        100         0         0         1 30/11/1963  95.7\n## 5 1964 Australia        100         0         0         1             95.7\n## 6 1965 Australia        100         0         0         1             95.7\n##   womenpar realgdpgr inflation debt_hist deficit ttl_labf labfopar unemp\n## 1        0        NA      3.73     40.15    0.46  4215.00       NA  1.25\n## 2        0     -0.64      2.29     38.62   -0.36  4286.00       NA  2.46\n## 3        0      5.77     -0.32     38.75   -0.79  4382.00       NA  2.32\n## 4        0      6.01      0.64     37.34   -0.51  4484.00       NA  1.87\n## 5        0      6.26      2.87     35.31   -0.08  4610.80    67.24  1.45\n## 6        0      4.99      3.41     53.99   -0.73  4745.95    67.66  1.36\n##       pop pop15_64 pop65 elderly\n## 1 10275.0   6296.5 874.9    8.51\n## 2 10508.2   6428.6 894.6    8.51\n## 3 10700.5   6571.5 913.6    8.54\n## 4 10906.9   6710.9 933.0    8.55\n## 5 11121.6   6857.3 948.1    8.52\n## 6 11340.9   7014.6 966.3    8.52\n\n\n\ntail(d)  # shows the last 6 lines of data\n\n##      year country gov_right1 gov_cent1 gov_left1 gov_party      elect vturn\n## 1573 2009     USA      16.36     83.64         0         1             53.2\n## 1574 2010     USA      11.76     88.24         0         1 02/11/2010  39.8\n## 1575 2011     USA       8.80     91.20         0         1             39.8\n## 1576 2012     USA       5.88     94.12         0         1 06/11/2012  50.9\n## 1577 2013     USA       5.88     94.12         0         1             50.9\n## 1578 2014     USA       8.40     91.60         0         1 04/11/2014  35.6\n##      womenpar realgdpgr inflation debt_hist deficit ttl_labf labfopar unemp\n## 1573     16.8     -2.78     -0.36     93.47  -12.83 155454.0    75.49   9.3\n## 1574     16.8      2.53      1.64    102.66  -12.18 155220.3    74.80   9.6\n## 1575     16.8      1.60      3.16    108.25  -10.75 154949.3    74.12   8.9\n## 1576     18.0      2.22      2.07    111.48   -9.00 156368.6    74.53   8.1\n## 1577     17.8      1.49      1.46    111.45   -5.49 156761.2    74.41   7.4\n## 1578     19.3      2.43      1.62    111.70   -5.13 157268.8       NA   6.2\n##           pop pop15_64    pop65 elderly\n## 1573 306771.5 206060.8 39623.18   12.92\n## 1574 309347.1 207665.3 40479.35   13.09\n## 1575 311721.6 209179.2 41366.63   13.27\n## 1576 314112.1 209823.0 43164.91   13.74\n## 1577 316498.0 210673.5 44723.04   14.13\n## 1578 318857.0 211545.9 46243.21   14.50\n\nclass(d)  # shows that tables are typically loaded in as data frames\n\n## [1] \"data.frame\"\n\n\n\n\nReading tab-separated (‚Äú.tsv‚Äù, ‚Äú.txt‚Äù) text files with {base} R\n\nNOTE: In the following snippet, you can change the sep= argument as needed to use other delimiters\n\n\nf &lt;- \"data/CPDS-1960-2014-reduced.txt\"  # specfies a local path\nd &lt;- read.table(f, header = TRUE, sep = \"\\t\", stringsAsFactors = FALSE, fill = TRUE)\n# if fill is left as the default (FALSE) then this will throw an error...  if\n# TRUE then if the rows have unequal length, blank fields are implicitly added\nhead(d)  # shows the first 6 lines of data\n\n##   year   country gov_right1 gov_cent1 gov_left1 gov_party      elect vturn\n## 1 1960 Australia        100         0         0         1             95.5\n## 2 1961 Australia        100         0         0         1 09/12/1961  95.3\n## 3 1962 Australia        100         0         0         1             95.3\n## 4 1963 Australia        100         0         0         1 30/11/1963  95.7\n## 5 1964 Australia        100         0         0         1             95.7\n## 6 1965 Australia        100         0         0         1             95.7\n##   womenpar realgdpgr inflation debt_hist deficit ttl_labf labfopar unemp\n## 1        0        NA      3.73     40.15    0.46  4215.00       NA  1.25\n## 2        0     -0.64      2.29     38.62   -0.36  4286.00       NA  2.46\n## 3        0      5.77     -0.32     38.75   -0.79  4382.00       NA  2.32\n## 4        0      6.01      0.64     37.34   -0.51  4484.00       NA  1.87\n## 5        0      6.26      2.87     35.31   -0.08  4610.80    67.24  1.45\n## 6        0      4.99      3.41     53.99   -0.73  4745.95    67.66  1.36\n##       pop pop15_64 pop65 elderly\n## 1 10275.0   6296.5 874.9    8.51\n## 2 10508.2   6428.6 894.6    8.51\n## 3 10700.5   6571.5 913.6    8.54\n## 4 10906.9   6710.9 933.0    8.55\n## 5 11121.6   6857.3 948.1    8.52\n## 6 11340.9   7014.6 966.3    8.52\n\n\nOr, alternatively‚Ä¶\n\nd &lt;- read.delim(f, header = TRUE, stringsAsFactors = FALSE)\n# for the `read.delim()` function, fill=TRUE by default\nhead(d)\n\n##   year   country gov_right1 gov_cent1 gov_left1 gov_party      elect vturn\n## 1 1960 Australia        100         0         0         1             95.5\n## 2 1961 Australia        100         0         0         1 09/12/1961  95.3\n## 3 1962 Australia        100         0         0         1             95.3\n## 4 1963 Australia        100         0         0         1 30/11/1963  95.7\n## 5 1964 Australia        100         0         0         1             95.7\n## 6 1965 Australia        100         0         0         1             95.7\n##   womenpar realgdpgr inflation debt_hist deficit ttl_labf labfopar unemp\n## 1        0        NA      3.73     40.15    0.46  4215.00       NA  1.25\n## 2        0     -0.64      2.29     38.62   -0.36  4286.00       NA  2.46\n## 3        0      5.77     -0.32     38.75   -0.79  4382.00       NA  2.32\n## 4        0      6.01      0.64     37.34   -0.51  4484.00       NA  1.87\n## 5        0      6.26      2.87     35.31   -0.08  4610.80    67.24  1.45\n## 6        0      4.99      3.41     53.99   -0.73  4745.95    67.66  1.36\n##       pop pop15_64 pop65 elderly\n## 1 10275.0   6296.5 874.9    8.51\n## 2 10508.2   6428.6 894.6    8.51\n## 3 10700.5   6571.5 913.6    8.54\n## 4 10906.9   6710.9 933.0    8.55\n## 5 11121.6   6857.3 948.1    8.52\n## 6 11340.9   7014.6 966.3    8.52\n\n\n\n\nReading text files with the {readr} package\nThe {readr} package, which is loaded as part of {tidyverse} provides easy alternative functions to read in delimited text files. It runs faster than the {base} package functions. It begins by reading in an initial set of rows (a default number of 1000) from the table and then tries to impute the data class of each column. If you want, you can also directly specify the data class of each column with the col_types() function. The col_names=&lt;boolean&gt; argument is used to specify if your data has a header row.\n\nNOTE: There are variants of the main read_&lt;type&gt;() function for different types of files, e.g., tab-separated values (read_tsv()), comma-separated values (read_csv()), those with some other delimiter (read_delim()). A few common delimiters that might be used in text files are commas (,), tabs (\\t), semicolons (;), and end-of-line characters, e.g., ‚Äúnew lines‚Äù (\\n) or ‚Äúcarriage returns‚Äù (\\r).\n\n\nf &lt;- \"data/CPDS-1960-2014-reduced.txt\"\nd &lt;- read_tsv(f, col_names = TRUE)  # for tab-separated value files\nhead(d)\n\n## # A tibble: 6 √ó 20\n##    year country   gov_right1 gov_cent1 gov_left1 gov_party elect  vturn womenpar\n##   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n## 1  1960 Australia        100         0         0         1 &lt;NA&gt;    95.5        0\n## 2  1961 Australia        100         0         0         1 09/12‚Ä¶  95.3        0\n## 3  1962 Australia        100         0         0         1 &lt;NA&gt;    95.3        0\n## 4  1963 Australia        100         0         0         1 30/11‚Ä¶  95.7        0\n## 5  1964 Australia        100         0         0         1 &lt;NA&gt;    95.7        0\n## 6  1965 Australia        100         0         0         1 &lt;NA&gt;    95.7        0\n## # ‚Ñπ 11 more variables: realgdpgr &lt;dbl&gt;, inflation &lt;dbl&gt;, debt_hist &lt;dbl&gt;,\n## #   deficit &lt;dbl&gt;, ttl_labf &lt;dbl&gt;, labfopar &lt;dbl&gt;, unemp &lt;dbl&gt;, pop &lt;dbl&gt;,\n## #   pop15_64 &lt;dbl&gt;, pop65 &lt;dbl&gt;, elderly &lt;dbl&gt;\n\nclass(d)\n\n## [1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\"\n\n# returns d as a data frame, but also as a 'tibble' note the output is more\n# verbose and the problems() function highlights where there might have been\n# parsing errors\n\nOr, alternatively‚Ä¶\n\nd &lt;- read_delim(f, delim = \"\\n\", col_names = TRUE)\n# for generic delimited files, where the delimiter is a tab ('\\t')\nhead(d)\n\n## # A tibble: 6 √ó 1\n##   year\\tcountry\\tgov_right1\\tgov_cent1\\tgov_left1\\tgov_party\\telect\\tvturn\\two‚Ä¶¬π\n##   &lt;chr&gt;                                                                         \n## 1 \"1960\\tAustralia\\t100.00\\t0.00\\t0.00\\t1\\t\\t95.5\\t0.0\\t\\t3.73\\t40.15\\t0.46\\t42‚Ä¶\n## 2 \"1961\\tAustralia\\t100.00\\t0.00\\t0.00\\t1\\t09/12/1961\\t95.3\\t0.0\\t-0.64\\t2.29\\t‚Ä¶\n## 3 \"1962\\tAustralia\\t100.00\\t0.00\\t0.00\\t1\\t\\t95.3\\t0.0\\t5.77\\t-0.32\\t38.75\\t-0.‚Ä¶\n## 4 \"1963\\tAustralia\\t100.00\\t0.00\\t0.00\\t1\\t30/11/1963\\t95.7\\t0.0\\t6.01\\t0.64\\t3‚Ä¶\n## 5 \"1964\\tAustralia\\t100.00\\t0.00\\t0.00\\t1\\t\\t95.7\\t0.0\\t6.26\\t2.87\\t35.31\\t-0.0‚Ä¶\n## 6 \"1965\\tAustralia\\t100.00\\t0.00\\t0.00\\t1\\t\\t95.7\\t0.0\\t4.99\\t3.41\\t53.99\\t-0.7‚Ä¶\n## # ‚Ñπ abbreviated name:\n## #   ¬π‚Äã`year\\tcountry\\tgov_right1\\tgov_cent1\\tgov_left1\\tgov_party\\telect\\tvturn\\twomenpar\\trealgdpgr\\tinflation\\tdebt_hist\\tdeficit\\tttl_labf\\tlabfopar\\tunemp\\tpop\\tpop15_64\\tpop65\\telderly`\n\n\n\nf &lt;- \"data/CPDS-1960-2014-reduced.csv\"\nd &lt;- read_csv(f, col_names = TRUE)  # for comma-separated value files\n\n## Rows: 1578 Columns: 20\n## ‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n## Delimiter: \",\"\n## chr  (2): country, elect\n## dbl (18): year, gov_right1, gov_cent1, gov_left1, gov_party, vturn, womenpar...\n## \n## ‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n## ‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(d)\n\n## # A tibble: 6 √ó 20\n##    year country   gov_right1 gov_cent1 gov_left1 gov_party elect  vturn womenpar\n##   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n## 1  1960 Australia        100         0         0         1 &lt;NA&gt;    95.5        0\n## 2  1961 Australia        100         0         0         1 09/12‚Ä¶  95.3        0\n## 3  1962 Australia        100         0         0         1 &lt;NA&gt;    95.3        0\n## 4  1963 Australia        100         0         0         1 30/11‚Ä¶  95.7        0\n## 5  1964 Australia        100         0         0         1 &lt;NA&gt;    95.7        0\n## 6  1965 Australia        100         0         0         1 &lt;NA&gt;    95.7        0\n## # ‚Ñπ 11 more variables: realgdpgr &lt;dbl&gt;, inflation &lt;dbl&gt;, debt_hist &lt;dbl&gt;,\n## #   deficit &lt;dbl&gt;, ttl_labf &lt;dbl&gt;, labfopar &lt;dbl&gt;, unemp &lt;dbl&gt;, pop &lt;dbl&gt;,\n## #   pop15_64 &lt;dbl&gt;, pop65 &lt;dbl&gt;, elderly &lt;dbl&gt;\n\n\nOr, alternatively‚Ä¶\n\nd &lt;- read_delim(f, delim = \",\", col_names = TRUE)\n\n## Rows: 1578 Columns: 20\n## ‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n## Delimiter: \",\"\n## chr  (2): country, elect\n## dbl (18): year, gov_right1, gov_cent1, gov_left1, gov_party, vturn, womenpar...\n## \n## ‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n## ‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# for generic delimited files, where the delimiter is a comma\nhead(d)\n\n## # A tibble: 6 √ó 20\n##    year country   gov_right1 gov_cent1 gov_left1 gov_party elect  vturn womenpar\n##   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n## 1  1960 Australia        100         0         0         1 &lt;NA&gt;    95.5        0\n## 2  1961 Australia        100         0         0         1 09/12‚Ä¶  95.3        0\n## 3  1962 Australia        100         0         0         1 &lt;NA&gt;    95.3        0\n## 4  1963 Australia        100         0         0         1 30/11‚Ä¶  95.7        0\n## 5  1964 Australia        100         0         0         1 &lt;NA&gt;    95.7        0\n## 6  1965 Australia        100         0         0         1 &lt;NA&gt;    95.7        0\n## # ‚Ñπ 11 more variables: realgdpgr &lt;dbl&gt;, inflation &lt;dbl&gt;, debt_hist &lt;dbl&gt;,\n## #   deficit &lt;dbl&gt;, ttl_labf &lt;dbl&gt;, labfopar &lt;dbl&gt;, unemp &lt;dbl&gt;, pop &lt;dbl&gt;,\n## #   pop15_64 &lt;dbl&gt;, pop65 &lt;dbl&gt;, elderly &lt;dbl&gt;\n\n\n\n\n\nLoading Data from Excel Files\nWhile you should never need to use Excel, sometimes you will no doubt be given a spreadsheet file with some data in it that you want to read in R. There are several packages available that provide functions for loading data into R from Excel spreadsheet files: {readxl}, {XLConnect}, and {xlsx}. The first two of these are fast, easy to use, and work well. Both require that you have successfully installed {rJava} on your computer, which both packages import as a dependency. Be warned, sometimes installing {rJava} can be tricky! {xlsx} also may be slower than the other methods, but its read.xlsx2() function may improve the speed.\n\nNOTE: You shoud always use str() to check if your variables come in as the correct data class!\n\n\nUsing the {readxl} package\n\nlibrary(readxl)\nf &lt;- \"data/CPDS-1960-2014-reduced.xlsx\"\nd &lt;- read_excel(f, sheet = 1, col_names = TRUE)\nhead(d)\n\n## # A tibble: 6 √ó 20\n##    year country   gov_right1 gov_cent1 gov_left1 gov_party elect              \n##   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dttm&gt;             \n## 1  1960 Australia        100         0         0         1 NA                 \n## 2  1961 Australia        100         0         0         1 1961-12-09 00:00:00\n## 3  1962 Australia        100         0         0         1 NA                 \n## 4  1963 Australia        100         0         0         1 1963-11-30 00:00:00\n## 5  1964 Australia        100         0         0         1 NA                 \n## 6  1965 Australia        100         0         0         1 NA                 \n## # ‚Ñπ 13 more variables: vturn &lt;dbl&gt;, womenpar &lt;dbl&gt;, realgdpgr &lt;dbl&gt;,\n## #   inflation &lt;dbl&gt;, debt_hist &lt;dbl&gt;, deficit &lt;dbl&gt;, ttl_labf &lt;dbl&gt;,\n## #   labfopar &lt;dbl&gt;, unemp &lt;dbl&gt;, pop &lt;dbl&gt;, pop15_64 &lt;dbl&gt;, pop65 &lt;dbl&gt;,\n## #   elderly &lt;dbl&gt;\n\nstr(d)  # `read_excel()` yields a 'tibble'\n\n## tibble [1,578 √ó 20] (S3: tbl_df/tbl/data.frame)\n##  $ year      : num [1:1578] 1960 1961 1962 1963 1964 ...\n##  $ country   : chr [1:1578] \"Australia\" \"Australia\" \"Australia\" \"Australia\" ...\n##  $ gov_right1: num [1:1578] 100 100 100 100 100 100 100 100 100 100 ...\n##  $ gov_cent1 : num [1:1578] 0 0 0 0 0 0 0 0 0 0 ...\n##  $ gov_left1 : num [1:1578] 0 0 0 0 0 0 0 0 0 0 ...\n##  $ gov_party : num [1:1578] 1 1 1 1 1 1 1 1 1 1 ...\n##  $ elect     : POSIXct[1:1578], format: NA \"1961-12-09\" ...\n##  $ vturn     : num [1:1578] 95.5 95.3 95.3 95.7 95.7 95.7 95.1 95.1 95.1 95 ...\n##  $ womenpar  : num [1:1578] 0 0 0 0 0 0 0.8 0.8 0.8 0 ...\n##  $ realgdpgr : num [1:1578] NA -0.643 5.767 6.009 6.258 ...\n##  $ inflation : num [1:1578] 3.729 2.288 -0.319 0.641 2.866 ...\n##  $ debt_hist : num [1:1578] 40.1 38.6 38.7 37.3 35.3 ...\n##  $ deficit   : num [1:1578] 0.4582 -0.3576 -0.7938 -0.5062 -0.0804 ...\n##  $ ttl_labf  : num [1:1578] 4215 4286 4382 4484 4611 ...\n##  $ labfopar  : num [1:1578] NA NA NA NA 67.2 ...\n##  $ unemp     : num [1:1578] 1.25 2.46 2.32 1.87 1.45 ...\n##  $ pop       : num [1:1578] 10275 10508 10700 10907 11122 ...\n##  $ pop15_64  : num [1:1578] 6296 6429 6572 6711 6857 ...\n##  $ pop65     : num [1:1578] 875 895 914 933 948 ...\n##  $ elderly   : num [1:1578] 8.51 8.51 8.54 8.55 8.52 ...\n\ndetach(package:readxl)\n\n\n\nUsing the {XLConnect} package\n\nlibrary(XLConnect)\nf &lt;- \"data/CPDS-1960-2014-reduced.xlsx\"\nd &lt;- readWorksheetFromFile(f, sheet = 1, header = TRUE)\nhead(d)\nstr(d)\n\nThe {XLConnect} package can also write data frames back out to Excel worksheets. If the file does not exist, it is created. If it does exist, data is cleared and overwritten. The second process is MUCH slower. In the following, I have included a conditional statement (if(){}) which will implement the file.remove() command here, if needed.\n\nf &lt;- \"output.xlsx\"\nif (file.exists(f)) {\n    file.remove(f)\n}\nwriteWorksheetToFile(f, d, sheet = \"myData\", clearSheets = TRUE)\ndetach(package:XLConnect)\n\nFor futher information on using {XLConnect} check out this blog post.\n\n\nUsing the {xlsx} package\n\nlibrary(xlsx)\nf &lt;- \"data/CPDS-1960-2014-reduced.xlsx\"\nd &lt;- read.xlsx(f, sheetIndex = 1)\n# or pass a named sheet using the argument `sheetIndex=` the function\n# `read.xlsx2()` is an updated alternative from the same package and may run\n# faster\nhead(d)\ndetach(package:xlsx)\n\n\nTL/DR: {readxl} seems to be the best package for reading Excel data, but you might need other packages and functions to write native Excel files (‚Äú.xls‚Äù or ‚Äú.xlsx‚Äù).",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Getting Data into ***R***</span>"
    ]
  },
  {
    "objectID": "08-module.html#working-with-remote-files",
    "href": "08-module.html#working-with-remote-files",
    "title": "8¬† Getting Data into R",
    "section": "8.5 Working with Remote Files",
    "text": "8.5 Working with Remote Files\nWe can also use R as an interface to work with data and files stored on a server elsewhere on the web, e.g., Dropbox, GitHub, or Google Drive.\nTo read ‚Äú.csv‚Äù or ‚Äú.txt‚Äù files directly from GitHub, use the {curl} or {readr} packages.\nGO TO: https://github.com/difiore/ada-datasets, select the ‚Äú.csv‚Äù version of the ‚ÄúCPDS-1960-2014-reduced‚Äù file, then press ‚ÄúRAW‚Äù and copy the URL from the address box of your browser window‚Ä¶ this is what you need to use as an argument for the functions below (you will repeat this for the ‚Äú.txt‚Äù version later on)\n\nUsing the {curl} Package\nThe {curl} package lets us open connection across the internet to read data from a URL, which we can then couple with one of the {base} read.table() functions.\nFor a comma-separated value (‚Äú.csv‚Äù) text file‚Ä¶\n\nlibrary(curl)\nf &lt;- curl(\"https://raw.githubusercontent.com/difiore/ada-datasets/main/CPDS-1960-2014-reduced.csv\")\nd &lt;- read.csv(f, header = TRUE, sep = \",\", stringsAsFactors = FALSE)\nhead(d)\n\n##   year   country gov_right1 gov_cent1 gov_left1 gov_party      elect vturn\n## 1 1960 Australia        100         0         0         1             95.5\n## 2 1961 Australia        100         0         0         1 09/12/1961  95.3\n## 3 1962 Australia        100         0         0         1             95.3\n## 4 1963 Australia        100         0         0         1 30/11/1963  95.7\n## 5 1964 Australia        100         0         0         1             95.7\n## 6 1965 Australia        100         0         0         1             95.7\n##   womenpar realgdpgr inflation debt_hist deficit ttl_labf labfopar unemp\n## 1        0        NA      3.73     40.15    0.46  4215.00       NA  1.25\n## 2        0     -0.64      2.29     38.62   -0.36  4286.00       NA  2.46\n## 3        0      5.77     -0.32     38.75   -0.79  4382.00       NA  2.32\n## 4        0      6.01      0.64     37.34   -0.51  4484.00       NA  1.87\n## 5        0      6.26      2.87     35.31   -0.08  4610.80    67.24  1.45\n## 6        0      4.99      3.41     53.99   -0.73  4745.95    67.66  1.36\n##       pop pop15_64 pop65 elderly\n## 1 10275.0   6296.5 874.9    8.51\n## 2 10508.2   6428.6 894.6    8.51\n## 3 10700.5   6571.5 913.6    8.54\n## 4 10906.9   6710.9 933.0    8.55\n## 5 11121.6   6857.3 948.1    8.52\n## 6 11340.9   7014.6 966.3    8.52\n\n# returns a data frame\n\nFor a tab-delimited (‚Äú.tsv‚Äù or .‚Äùtxt‚Äù) text file‚Ä¶\n\nf &lt;- curl(\"https://raw.githubusercontent.com/difiore/ada-datasets/main/CPDS-1960-2014-reduced.txt\")\nd &lt;- read.table(f, header = TRUE, sep = \"\\t\", stringsAsFactors = FALSE)\nhead(d)\n\n##   year   country gov_right1 gov_cent1 gov_left1 gov_party      elect vturn\n## 1 1960 Australia        100         0         0         1             95.5\n## 2 1961 Australia        100         0         0         1 09/12/1961  95.3\n## 3 1962 Australia        100         0         0         1             95.3\n## 4 1963 Australia        100         0         0         1 30/11/1963  95.7\n## 5 1964 Australia        100         0         0         1             95.7\n## 6 1965 Australia        100         0         0         1             95.7\n##   womenpar realgdpgr inflation debt_hist deficit ttl_labf labfopar unemp\n## 1        0        NA      3.73     40.15    0.46  4215.00       NA  1.25\n## 2        0     -0.64      2.29     38.62   -0.36  4286.00       NA  2.46\n## 3        0      5.77     -0.32     38.75   -0.79  4382.00       NA  2.32\n## 4        0      6.01      0.64     37.34   -0.51  4484.00       NA  1.87\n## 5        0      6.26      2.87     35.31   -0.08  4610.80    67.24  1.45\n## 6        0      4.99      3.41     53.99   -0.73  4745.95    67.66  1.36\n##       pop pop15_64 pop65 elderly\n## 1 10275.0   6296.5 874.9    8.51\n## 2 10508.2   6428.6 894.6    8.51\n## 3 10700.5   6571.5 913.6    8.54\n## 4 10906.9   6710.9 933.0    8.55\n## 5 11121.6   6857.3 948.1    8.52\n## 6 11340.9   7014.6 966.3    8.52\n\n# returns a data frame\ndetach(package:curl)\n\n\n\nUsing the {readr} Package\nUsing {readr}, filenames beginning with ‚Äúhttp://‚Äù, ‚Äúhttps://‚Äù, ‚Äúftp://‚Äù, or ‚Äúfttps://‚Äù can be read without having to set up a curl connection interface.\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/CPDS-1960-2014-reduced.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\n\n## Rows: 1578 Columns: 20\n## ‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n## Delimiter: \",\"\n## chr  (2): country, elect\n## dbl (18): year, gov_right1, gov_cent1, gov_left1, gov_party, vturn, womenpar...\n## \n## ‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n## ‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(d)\n\n## # A tibble: 6 √ó 20\n##    year country   gov_right1 gov_cent1 gov_left1 gov_party elect  vturn womenpar\n##   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n## 1  1960 Australia        100         0         0         1 &lt;NA&gt;    95.5        0\n## 2  1961 Australia        100         0         0         1 09/12‚Ä¶  95.3        0\n## 3  1962 Australia        100         0         0         1 &lt;NA&gt;    95.3        0\n## 4  1963 Australia        100         0         0         1 30/11‚Ä¶  95.7        0\n## 5  1964 Australia        100         0         0         1 &lt;NA&gt;    95.7        0\n## 6  1965 Australia        100         0         0         1 &lt;NA&gt;    95.7        0\n## # ‚Ñπ 11 more variables: realgdpgr &lt;dbl&gt;, inflation &lt;dbl&gt;, debt_hist &lt;dbl&gt;,\n## #   deficit &lt;dbl&gt;, ttl_labf &lt;dbl&gt;, labfopar &lt;dbl&gt;, unemp &lt;dbl&gt;, pop &lt;dbl&gt;,\n## #   pop15_64 &lt;dbl&gt;, pop65 &lt;dbl&gt;, elderly &lt;dbl&gt;\n\n# returns a 'tibble', a new version of a data frame\n\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/CPDS-1960-2014-reduced.txt\"\nd &lt;- read_tsv(f, col_names = TRUE)\n\n## Rows: 1578 Columns: 20\n## ‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n## Delimiter: \"\\t\"\n## chr  (2): country, elect\n## dbl (18): year, gov_right1, gov_cent1, gov_left1, gov_party, vturn, womenpar...\n## \n## ‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n## ‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(d)\n\n## # A tibble: 6 √ó 20\n##    year country   gov_right1 gov_cent1 gov_left1 gov_party elect  vturn womenpar\n##   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n## 1  1960 Australia        100         0         0         1 &lt;NA&gt;    95.5        0\n## 2  1961 Australia        100         0         0         1 09/12‚Ä¶  95.3        0\n## 3  1962 Australia        100         0         0         1 &lt;NA&gt;    95.3        0\n## 4  1963 Australia        100         0         0         1 30/11‚Ä¶  95.7        0\n## 5  1964 Australia        100         0         0         1 &lt;NA&gt;    95.7        0\n## 6  1965 Australia        100         0         0         1 &lt;NA&gt;    95.7        0\n## # ‚Ñπ 11 more variables: realgdpgr &lt;dbl&gt;, inflation &lt;dbl&gt;, debt_hist &lt;dbl&gt;,\n## #   deficit &lt;dbl&gt;, ttl_labf &lt;dbl&gt;, labfopar &lt;dbl&gt;, unemp &lt;dbl&gt;, pop &lt;dbl&gt;,\n## #   pop15_64 &lt;dbl&gt;, pop65 &lt;dbl&gt;, elderly &lt;dbl&gt;\n\n# returns a 'tibble', a new version of a data frame\n\n\n\nAccessing Files on Dropbox\nTo load data from a ‚Äú.csv‚Äù file located in a personal Dropbox account you can use the {rdrop2} package.\n\nNOTE: The {rdrop2} package has been deprecated because one of the dependencies upon which it relies - {assertive} - and, in turn, many of the dependencies upon which {assertive} relies, have not been maintained for some time. Still, it is possible with some care to install an older versions of {rdrop2} and all of its recursive dependencies from Package Archive Files downloaded from CRAN and thus get it working. It takes some time, but may be worthwhile.\n\n\nAlso note that the following code block cannot be rendered to show you the output because it requires an interactive R environment for drop_auth(), drop_search(), etc.\n\n\nlibrary(rdrop2)\ndrop_auth(new_user = TRUE)  # opens a browser dialog box to ask for authorization...\ndrop_dir()  # lists the contents of your dropbox folder\nf &lt;- \"CPDS-1960-2014-reduced.csv\"  # name of the file to read from\nf &lt;- drop_search(query = f, mode = \"filename\")\n# searches your dropbox directory for file or directory names... this can be\n# slow!\nfilenames &lt;- vector()\nfor (i in 1:length(f$matches)) {\n    filenames &lt;- c(filenames, f$matches[[i]]$metadata$path_display)\n    # this is the location of the results returned above the [[i]] returns each\n    # encountered file with a matching filename and puts them into a vector\n}\nd &lt;- drop_read_csv(filenames[1], header = TRUE, sep = \",\", stringsAsFactors = FALSE)\n# here the [1] reads only the first file from filenames, but this can be\n# modified this to read more than one file\ndetach(package:rdrop2)\n\nThis same process can be done to load data from other types of delimited files in Dropbox by setting the appropriate sep= argument.\nYou can also read text files from a Dropbox account (e.g., your own or someone else‚Äôs) using a direct link that can be created for any file. To create this link, you will need to choose to share the file (via the dropdown menu activated by clicking the three dots icon to the right of the filename) and then create and copy the direct link to the file.\n\nlink &lt;- \"https://www.dropbox.com/s/hes2loy1x4tikh9/CPDS-1960-2014-reduced.csv?dl=0\"\n\nYou can read text files from a Dropbox account (e.g., your own or someone else‚Äôs) using direct link for the file. To find this link, hover over the filename on Dropbox and then select ‚ÄúCopy link‚Äù and grab the direct link to the file.\n\n\n\n\n\n\n\n\n\nYou can then assign a variable to hold the link, which likely will end in dl=0.\n\nlink &lt;- \"https://www.dropbox.com/scl/fi/xyhwpfzhdo42mj840pv2e/CPDS-1960-2014-reduced.csv?rlkey=3r4ck0thhgb3p8t3zwmonund5&dl=0\"\n# NOTE: enter your link between quotation marks in lieu of this dummy link!\n\n\nNOTE: Following a shared Dropbox link like this one will take you to a webpage that has the data embedded‚Ä¶ to get R to access the raw data, you will need to change the characters at end of the link from dl=0 to dl=1 or to raw=1. That is what the gsub() command in the first line of code below does. Then, you can load data from that file with read.csv() or read_csv(). This\n\n\nlink &lt;- gsub(pattern = \"dl=0\", replacement = \"dl=1\", x = link)\nd &lt;- read.csv(link, header = TRUE, sep = \",\", stringsAsFactors = FALSE)\n# or, using {tidyverse}, `d &lt;-read_csv(link, col_names = TRUE)`\nhead(d)\nstr(d)\n\nYou can also use the source_data() function from the {repmis} package (‚ÄúMiscellaneous Tools for Reproducible Research‚Äù) to load data from a file on Dropbox. This function detects column types and gives a few more warnings than others if it encounters somthing odd.\n\nlibrary(repmis)\nd &lt;- source_data(link, header = TRUE, sep = \",\")\n# use the same updated link to the raw data as above\nhead(d)\nstr(d)\n\n\n\nAccessing Files on Box\nYou can load tabular data from Box sites (e.g., UT Box) with {base} R read.table() functions using a direct link that someone has shared with you. To get such a link, hover over the filename on Box and click the link button to ‚ÄúCopy Shared Link‚Äù.\n\n\n\n\n\n\n\n\n\nIn the dialog box that opens, select ‚ÄúLink Settings‚Äù‚Ä¶\n\n\n\n\n\n\n\n\n\n‚Ä¶ and then copy the direct link, which will include the file type extension for the file. It is important to get the direct link to the file rather than just copying the shared link without the file type extension!\n\n\n\n\n\n\n\n\n\nYou can then assign this link to a variable and read data from it using read.csv() or read_csv().\n\nlink &lt;- \"https://utexas.box.com/shared/static/gnu54b6qt9e6t1b93ydyevanus7m7frj.csv\"\n# NOTE: enter the direct link between quotation marks in lieu of this dummy\n# link!\nd &lt;- read.csv(link, sep = \",\", header = TRUE, stringsAsFactors = FALSE)\n# or, using {tidyverse}, `d &lt;-read_csv(link, col_names = TRUE)`\n\nOr, alternatively, using {repmis}‚Ä¶\n\nd &lt;- source_data(link, header = TRUE, sep = \",\")\ndetach(package:repmis)\n\n\nNOTE: The {boxr} package also provides lots of functionality to interact with files on Box directly, but setup requires an authentication process that I have not yet been able to get working!\n\n\n\nImporting Data from Google Sheets\nFinally, you can also load data directly from a Google Sheets spreadsheet into R using the {googledrive} and {googlesheets4} packages. Try saving one of the ‚ÄúCPDS-1960-2014-reduced‚Äù file as a Google Sheet and then extracting it into R using the code below.\n\nNOTE: The following code block cannot be rendered to show you the output because it requires an interactive R environment for several functions (e.g., drive_auth()).\n\n\nlibrary(googledrive)\nlibrary(googlesheets4)\ndrive_auth()  # authenticate access to Google Drive\n# usually only needed once opens a web browser and has you sign in\ngs4_auth(token = drive_token())  # apply that authentication to Google Sheets\nf &lt;- gs4_find() %&gt;%\n    filter(name == \"CPDS-1960-2014-reduced\")\n# first find all Google Sheets in your drive and then filter for one of\n# interest\ngs4_get(f)\n# get info on the Google Sheets file selected, including the number and names\n# of the different worksheets within the spreadsheet\nd &lt;- read_sheet(f)  # read data from the first sheet in the spreadsheet\ndetach(package:googlesheets4)\ndetach(package:googledrive)\n\nFollow the links below for more information on the basics of using {googledrive} and {googlesheets4}.\n\n\nDownloading Remote Files\n\nDropbox\nThe {rdrop2} package, if you are able to install it, can also be used to download a file from a personal Dropbox account to your local computer, rather than just connecting to a Dropbox file to read the data stored there. This should work with any file type. The {rdrop2} package may not be available to install via CRAN, so you may need to install the package directly from the developer‚Äôs GitHub site https://github.com/karthik/rDrop2. To do so, install the package {devtools} and then run devtools::install_github(\"karthik/rdrop2\")\n\nNOTE: Again, the following code block cannot be rendered to show you the output because it requires an interactive R environment for drop_search(), etc.\n\n\nlibrary(rdrop2)\nf &lt;- \"CPDS-1960-2014-reduced.csv\"  # name of file to download\nf &lt;- drop_search(query = f, mode = \"filename\")\n# searches your dropbox directory for that file or directory name\nfilenames &lt;- vector()\nfor (i in 1:length(f$matches)) {\n    filenames &lt;- c(filenames, f$matches[[i]]$metadata$path_display)\n    # this is the location of the results returned above the [[i]] returns each\n    # encountered file with a matching filename and puts them into a vector\n}\n\ndrop_download(filenames[1], local_path = paste0(\"data\", filenames), overwrite = TRUE,\n    progress = TRUE)\n# here the [1] reads only the first encountered file... this can be modified\n# this to read more than one file\n\n# this function will save the file to a folder called 'data' inside the current\n# working directory\n\nThe progress=TRUE argument gives you a reassuring progress bar. By default, this argument is set to FALSE.\n\nNOTE: This process also works for other file types, e.g., Excel files:\n\n\nfilename &lt;- \"CPDS-1960-2014-reduced.xlsx\"  # name of file to download\nf &lt;- drop_search(filename)\n# searches your dropbox directory for that file or directory name\nfilenames &lt;- vector()\nfor (i in 1:length(f$matches)) {\n    filenames &lt;- c(filenames, f$matches[[i]]$metadata$path_display)\n    # this is the location of the results returned above the [[i]] returns each\n    # encountered file with a matching filename and puts them into a vector\n}\ndrop_download(filenames[1], local_path = paste0(\"data\", filenames), overwrite = TRUE,\n    progress = TRUE)\n# here the [1] reads only the first file...  need to modify this to read more\n# than one file\n\n# this will save the file to a folder called 'data' inside the current working\n# directory\ndetach(package:rdrop2)\n\n\n\nGoogle Drive\nThe {googledrive} package allows you to interact with a Google Drive account to search for, download, upload, and manipulate files.\n\nlibrary(googledrive)\nf &lt;- \"CPDS-1960-2014-reduced.xlsx\"  # name of the file to download\ndrive_auth()  # authenticate access to Google Drive\n\n# to download the file...\ndrive_download(f, path = paste0(\"data/\", f), overwrite = TRUE)\n\n# this will save the file to a folder called 'data' inside the current working\n# directory\n\n# to search for a file and get info about it\nfiles &lt;- drive_find(pattern = f, n_max = 1)\n# this example finds a single file, but this might return a tibble\n\nprint(files)  # prints a list of files matching the pattern\n\nfiles$drive_resource  # shows metadata about file\n\nid &lt;- files$id  # get the Google file id for the file\nid\n\n# to remove a file\ndrive_rm(files)\n\n# to upload a file...\ndrive_upload(paste0(\"data/\", f), name = \"CPDS-1960-2014-reduced.csv\", overwrite = TRUE)\ndetach(package:googledrive)\n\nMore on the basics of using {googledrive} and its functionality can be found here.\n\n# | include: false\ndetach(package:tidyverse)",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Getting Data into ***R***</span>"
    ]
  },
  {
    "objectID": "08-module.html#concept-review",
    "href": "08-module.html#concept-review",
    "title": "8¬† Getting Data into R",
    "section": "Concept Review",
    "text": "Concept Review\n\nThere are lots and lots of ways to get data into R from a variety of sources!\nThe file.choose() command will allow you to browse the directory structure on your local machine\nThe {readr} and {readxl} packages contain probably the most useful functions for reading in most types of delimited data (‚Äú.csv‚Äù, ‚Äú.txt‚Äù, ‚Äú.tsv‚Äù, ‚Äú.xlsx‚Äù)\nWe can read in or download data from remote sites on the web with {curl} or specific packages designed to work with particular hosting sites (e.g., GitHub, Dropbox, Box, Google Sheets, Google Drive)",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Getting Data into ***R***</span>"
    ]
  },
  {
    "objectID": "09-module.html",
    "href": "09-module.html",
    "title": "9¬† Exploratory Data Analysis",
    "section": "",
    "text": "9.1 Objectives",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "09-module.html#objectives",
    "href": "09-module.html#objectives",
    "title": "9¬† Exploratory Data Analysis",
    "section": "",
    "text": "The objective of this module to begin exploring data using the summary functions and graphing abilities of R.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "09-module.html#preliminaries",
    "href": "09-module.html#preliminaries",
    "title": "9¬† Exploratory Data Analysis",
    "section": "9.2 Preliminaries",
    "text": "9.2 Preliminaries\n\nGO TO: https://github.com/difiore/ada-datasets, select the ‚Äú.csv‚Äù version of the ‚ÄúCountry-Data-2016‚Äù file, then press the ‚ÄúRAW‚Äù button, highlight, and copy the text to a text editor and save it locally. Do the same for the ‚ÄúKamilarAndCooperData‚Äù file.\nInstall these packages in R: {skimr}, {summarytools}, {dataMaid}, {psych}, {pastecs}, {Hmisc}, {ggExtra}, {car}, {GGally}, {corrplot}, {patchwork}, {cowplot}, and {gridExtra}\nLoad {curl} and {tidyverse}",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "09-module.html#backstory",
    "href": "09-module.html#backstory",
    "title": "9¬† Exploratory Data Analysis",
    "section": "9.3 Backstory",
    "text": "9.3 Backstory\nExploratory data analysis (EDA) is typically the first step in any kind of data analysis in which you examine your dataset, often using visual methods, to summarize the main characteristics of your variables (central tendency, distributions) and check for various issues that may complicate further analysis (e.g., missing data). The influential statistician, John Tukey, wrote a classic book on the subject, Exploratory Data Analysis (Addison-Wesley) in 1977, in which he advocated the use of the five-number summary to quickly understand the distributions of numeric variables. These include:\n\nthe sample minimum (i.e., the value of the smallest observation)\nthe lower or first quartile\nthe median (i.e., the middle value in the distribution of observation)\nthe upper quartile or third quartile\nthe sample maximum (i.e., the value of the largest observation)\n\nR has some very easy to use functions for taking a quick tour of your data. We have seen some of these already (e.g., head(), tail(), and str()), and you should always use these right after loading in a dataset to work with. Also useful are dim() to return the number of rows and columns in a data frame, names(), colnames(), and sometimes rownames().\n\nNOTE: You can use the attach() function to make variables within data frames accessible in R with fewer keystrokes. The attach() function binds the variables from data frame named as an argument to the local namespace so that as long as the data frame is attached, variables can be called by their names without explicitly referring to the data frame. That is, if you attach() a data frame, then you do not need to use the $ operator or double bracket notation to refer to a particular variable or column vector from your data frame.\nIt is important to remember to detach() data frames when finished. It is also possible to attach multiple data frames (or the same data frame multiple times), and, if these share variable names, then the more recently attached one will mask the other. Thus, it is best to attach only one data frame at a time. You can also attach() and detach() packages from the namespace. In general, I do not recommend using attach() and detach() for dataframes.\n\nEXAMPLE:\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/Country-Data-2016.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\nnames(d)\n\n##  [1] \"country\"        \"population\"     \"area\"           \"govt_form\"     \n##  [5] \"birthrate\"      \"deathrate\"      \"life_expect\"    \"mammals\"       \n##  [9] \"birds\"          \"reptiles\"       \"amphibians\"     \"fishes\"        \n## [13] \"mollucs\"        \"other_inverts\"  \"plants\"         \"fungi_protists\"\n\nattach(d)\nmean(life_expect, na.rm = TRUE)\n\n## [1] 72.19083\n\ndetach(d)\n# mean(life_expect, na.rm=TRUE) # this throws an error!\nmean(d$life_expect, na.rm = TRUE)\n\n## [1] 72.19083\n\n\n\nNOTE: The with() function accomplishes much the same thing as attach() but is self-contained and cleaner, especially for use in functions. If you use with(), however, all code to be run should be included as an argument of the function.\n\nEXAMPLE:\n\nwith(d, mean(life_expect, na.rm = TRUE))\n\n## [1] 72.19083",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "09-module.html#tidy-data",
    "href": "09-module.html#tidy-data",
    "title": "9¬† Exploratory Data Analysis",
    "section": "9.4 ‚ÄúTidy‚Äù Data",
    "text": "9.4 ‚ÄúTidy‚Äù Data\nThe tabular data we have worked with thus far has all been presented in a format where each row represents a single case or observation or record, and each column contains a different variable that is scored for each of these observations. Data in this format is referred to as ‚Äútidy‚Äù, and the {tidyverse} set of R packages (which we have used a bit already) provides many tools for manipulating tabular data in this format.\n\n\n\n\n\n\n\n\n\nHaving data in a ‚Äútidy‚Äù format is useful for many kinds of analyses (although ‚Äútidy‚Äù data is not the ONLY useful data format). Often, then, the first manipulations we need to do with data in order to work with it efficiently are to reformat it to make it ‚Äútidy‚Äù.\nThere are multiple ways that data can be ‚Äúnon-tidy‚Äù, but one common one way is because data is organized in such a way to make data entry in a spreadsheet easier. For example, consider the following table of data representing body weight in grams for three individual titi monkeys, a species of South American primate, collected in each of two years:\n\n\n\nIndividual\nYear_1\nYear_2\n\n\n\n\nLily\n580\n600\n\n\nLeia\n550\n575\n\n\nLoki\n600\n620\n\n\n\nThis data is easy to ENTER, but it is non-tidy‚Ä¶ Why? Because the latter two columns actually represent a combinations of two variables (‚ÄúYear‚Äù and ‚ÄúWeight‚Äù) that pertain to a given individual. Each row thus represents two observations.\nBelow is the same data in tidy format, where each row represents a single observation:\n\n\n\nIndividual\nYear\nWeight\n\n\n\n\nLily\nYear_1\n580\n\n\nLily\nYear_2\n600\n\n\nLeia\nYear_1\n550\n\n\nLeia\nYear_2\n575\n\n\nLoki\nYear_1\n600\n\n\nLoki\nYear_2\n620\n\n\n\nWe can convert data from the former format to the latter using the function pivot_longer(), where the first argument is the tabular data of interest, the next argument is a vector of columns to collect data from, the names_to= argument is the name we will assign to a new variable that indicates from which column values are being collected from in the non-tidy dataset, and the final argument, values_to=, is the name for the new variable into which we are collecting values. This process has the effect, often, of making ‚Äúwide‚Äù tables narrower and longer, thus is sometime referred to as converting data to ‚Äúlong‚Äù format, thus the name for the function, pivot_longer(). Note that this function is essentially an update of an older {tidyr} function, gather(), which had a less intuitive name and set of argument names.\n\nd &lt;- data.frame(Individual = c(\"Lily\", \"Leia\", \"Loki\"), Year_1 = c(580, 550, 600),\n    Year_2 = c(600, 575, 620))\nd\n\n##   Individual Year_1 Year_2\n## 1       Lily    580    600\n## 2       Leia    550    575\n## 3       Loki    600    620\n\nd &lt;- pivot_longer(d, c(\"Year_1\", \"Year_2\"), names_to = \"Year\", values_to = \"Weight\")\nd\n\n## # A tibble: 6 √ó 3\n##   Individual Year   Weight\n##   &lt;chr&gt;      &lt;chr&gt;   &lt;dbl&gt;\n## 1 Lily       Year_1    580\n## 2 Lily       Year_2    600\n## 3 Leia       Year_1    550\n## 4 Leia       Year_2    575\n## 5 Loki       Year_1    600\n## 6 Loki       Year_2    620\n\n\nAlternatively, sometimes we will have data that is non-tidy because variables for the same observation are presented in different rows. Consider the following example of ‚Äúnon-tidy‚Äù data:\n\n\n\nSpecies\nVariable\nValue\n\n\n\n\nOrangutans\nBody Size (kg)\n37\n\n\nOrangutans\nBrain Size (cc)\n340\n\n\nChimpanzees\nBody Size (kg)\n38\n\n\nChimpanzees\nBrain Size (cc)\n350\n\n\nGorillas\nBody Size (kg)\n80\n\n\nGorillas\nBrain Size (cc)\n470\n\n\n\nThe same data could be represented in ‚Äútidy‚Äù format as:\n\n\n\nSpecies\nBody Size (kg)\nBrain Size (cc)\n\n\n\n\nOrangutans\n37\n340\n\n\nChimpanzees\n38\n350\n\n\nGorillas\n80\n470\n\n\n\nThe pivot_wider() function lets us easily reformat. Here, names_from= is the column containing the different variables, and values_from= is the column containing the measured values of those variables. Note that this function is essentially an update of an older {tidyr} function, spread().\n\nd &lt;- data.frame(Species = c(\"Orangutans\", \"Orangutans\", \"Chimpanzees\", \"Chimpanzees\",\n    \"Gorillas\", \"Gorillas\"), Variable = rep(c(\"Body Size (kg)\", \"Brain Size (cc)\"),\n    3), Value = c(37, 340, 38, 350, 80, 470))\nd\n\n##       Species        Variable Value\n## 1  Orangutans  Body Size (kg)    37\n## 2  Orangutans Brain Size (cc)   340\n## 3 Chimpanzees  Body Size (kg)    38\n## 4 Chimpanzees Brain Size (cc)   350\n## 5    Gorillas  Body Size (kg)    80\n## 6    Gorillas Brain Size (cc)   470\n\nd &lt;- pivot_wider(d, names_from = \"Variable\", values_from = \"Value\")\nd\n\n## # A tibble: 3 √ó 3\n##   Species     `Body Size (kg)` `Brain Size (cc)`\n##   &lt;chr&gt;                  &lt;dbl&gt;             &lt;dbl&gt;\n## 1 Orangutans                37               340\n## 2 Chimpanzees               38               350\n## 3 Gorillas                  80               470\n\n\nThis process has the effect, often, of making long and narrow tables wider, thus is sometime referred to as converting data to ‚Äúwide‚Äù format.\nThe image below, pulled from a cheatsheet on Data Wrangling with {dplyr} and {tidyr}, summarizes the basics of these processes for ‚Äúreshaping‚Äù data.\n\n\n\n\n\n\n\n\n\nFor the remainder of this module, all of the example datasets will comprise data already in the ‚Äútidy‚Äù format.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "09-module.html#exploring-single-variables",
    "href": "09-module.html#exploring-single-variables",
    "title": "9¬† Exploratory Data Analysis",
    "section": "9.5 Exploring Single Variables",
    "text": "9.5 Exploring Single Variables\n\nVariable Summaries\nThe summary() function from {base} R provides a quick overview of each variable in a data frame. For numeric variables, this includes the five-number summary and the mean, as well as a count of NA (missing values). For factors, it includes a count of each factor.\n\n\nCHALLENGE\n\nLoad the ‚ÄúCountry-Data-2016‚Äù dataset into a data frame variable, d, and summarize the variables in that data frame. You can load the file any way you want, e.g., load from a local file or access the data straight from GitHub, as in the code below.\n\n\n# using {base} R\nf &lt;- curl(\"https://raw.githubusercontent.com/difiore/ada-datasets/main/Country-Data-2016.csv\")\nd &lt;- read.csv(f, header = TRUE, sep = \",\", stringsAsFactors = FALSE)\nd &lt;- as_tibble(d)  # I like tibbles!\nhead(d)\n\n## # A tibble: 6 √ó 16\n##   country    population   area govt_form birthrate deathrate life_expect mammals\n##   &lt;chr&gt;           &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;   &lt;int&gt;\n## 1 Afghanist‚Ä¶   32564342 6.52e5 islamic ‚Ä¶      38.6      13.9        50.9      11\n## 2 Albania       3029278 2.87e4 republic       12.9       6.6        78.1       3\n## 3 Algeria      39542166 2.38e6 republic       23.7       4.3        76.6      14\n## 4 American ‚Ä¶      54343 1.99e2 territor‚Ä¶      22.9       4.8        75.1       1\n## 5 Andorra         85580 4.68e2 constitu‚Ä¶       8.1       7          82.7       2\n## 6 Angola       19625353 1.25e6 republic       38.8      11.5        55.6      17\n## # ‚Ñπ 8 more variables: birds &lt;int&gt;, reptiles &lt;int&gt;, amphibians &lt;int&gt;,\n## #   fishes &lt;int&gt;, mollucs &lt;int&gt;, other_inverts &lt;int&gt;, plants &lt;int&gt;,\n## #   fungi_protists &lt;int&gt;\n\nnames(d)\n\n##  [1] \"country\"        \"population\"     \"area\"           \"govt_form\"     \n##  [5] \"birthrate\"      \"deathrate\"      \"life_expect\"    \"mammals\"       \n##  [9] \"birds\"          \"reptiles\"       \"amphibians\"     \"fishes\"        \n## [13] \"mollucs\"        \"other_inverts\"  \"plants\"         \"fungi_protists\"\n\n# or, using {readr}\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/Country-Data-2016.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\n\n## Rows: 248 Columns: 16\n## ‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n## Delimiter: \",\"\n## chr  (2): country, govt_form\n## dbl (14): population, area, birthrate, deathrate, life_expect, mammals, bird...\n## \n## ‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n## ‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(d)\n\n## # A tibble: 6 √ó 16\n##   country    population   area govt_form birthrate deathrate life_expect mammals\n##   &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;   &lt;dbl&gt;\n## 1 Afghanist‚Ä¶   32564342 6.52e5 islamic ‚Ä¶      38.6      13.9        50.9      11\n## 2 Albania       3029278 2.87e4 republic       12.9       6.6        78.1       3\n## 3 Algeria      39542166 2.38e6 republic       23.7       4.3        76.6      14\n## 4 American ‚Ä¶      54343 1.99e2 territor‚Ä¶      22.9       4.8        75.1       1\n## 5 Andorra         85580 4.68e2 constitu‚Ä¶       8.1       7          82.7       2\n## 6 Angola       19625353 1.25e6 republic       38.8      11.5        55.6      17\n## # ‚Ñπ 8 more variables: birds &lt;dbl&gt;, reptiles &lt;dbl&gt;, amphibians &lt;dbl&gt;,\n## #   fishes &lt;dbl&gt;, mollucs &lt;dbl&gt;, other_inverts &lt;dbl&gt;, plants &lt;dbl&gt;,\n## #   fungi_protists &lt;dbl&gt;\n\nnames(d)\n\n##  [1] \"country\"        \"population\"     \"area\"           \"govt_form\"     \n##  [5] \"birthrate\"      \"deathrate\"      \"life_expect\"    \"mammals\"       \n##  [9] \"birds\"          \"reptiles\"       \"amphibians\"     \"fishes\"        \n## [13] \"mollucs\"        \"other_inverts\"  \"plants\"         \"fungi_protists\"\n\n\n\nWhat are the median area and population size of all countries in the dataset?\n\n\nHINT: There are a couple of ways to do this‚Ä¶ try summary() and median() (for the latter, you‚Äôll need to use the na.rm=TRUE argument).\n\n\n\nShow Code\nsummary(d)\n\n\nShow Output\n##    country            population             area            govt_form        \n##  Length:248         Min.   :3.000e+01   Min.   :1.000e-01   Length:248        \n##  Class :character   1st Qu.:2.991e+05   1st Qu.:1.769e+03   Class :character  \n##  Mode  :character   Median :4.912e+06   Median :6.970e+04   Mode  :character  \n##                     Mean   :2.999e+07   Mean   :6.110e+05                     \n##                     3rd Qu.:1.803e+07   3rd Qu.:3.988e+05                     \n##                     Max.   :1.367e+09   Max.   :1.710e+07                     \n##                     NA's   :6           NA's   :1                             \n##    birthrate       deathrate      life_expect       mammals      \n##  Min.   : 0.00   Min.   : 0.00   Min.   :49.80   Min.   :  0.00  \n##  1st Qu.:11.40   1st Qu.: 5.65   1st Qu.:67.40   1st Qu.:  3.00  \n##  Median :16.40   Median : 7.40   Median :74.70   Median :  8.00  \n##  Mean   :18.95   Mean   : 7.61   Mean   :72.19   Mean   : 13.85  \n##  3rd Qu.:24.35   3rd Qu.: 9.40   3rd Qu.:78.40   3rd Qu.: 15.00  \n##  Max.   :45.50   Max.   :14.90   Max.   :89.50   Max.   :188.00  \n##  NA's   :17      NA's   :17      NA's   :19      NA's   :3       \n##      birds           reptiles         amphibians          fishes      \n##  Min.   :  0.00   Min.   :  0.000   Min.   :  0.000   Min.   :  0.00  \n##  1st Qu.:  6.00   1st Qu.:  2.000   1st Qu.:  0.000   1st Qu.: 11.00  \n##  Median : 12.00   Median :  5.000   Median :  0.000   Median : 25.00  \n##  Mean   : 17.82   Mean   :  8.331   Mean   :  9.849   Mean   : 32.84  \n##  3rd Qu.: 19.00   3rd Qu.:  8.000   3rd Qu.:  4.000   3rd Qu.: 43.00  \n##  Max.   :165.00   Max.   :139.000   Max.   :215.000   Max.   :249.00  \n##  NA's   :3        NA's   :3         NA's   :3         NA's   :3       \n##     mollucs       other_inverts        plants        fungi_protists   \n##  Min.   :  0.00   Min.   :  0.00   Min.   :   0.00   Min.   : 0.0000  \n##  1st Qu.:  0.00   1st Qu.:  3.00   1st Qu.:   2.00   1st Qu.: 0.0000  \n##  Median :  1.00   Median : 11.00   Median :  10.00   Median : 0.0000  \n##  Mean   :  9.62   Mean   : 32.57   Mean   :  60.78   Mean   : 0.6082  \n##  3rd Qu.:  6.00   3rd Qu.: 33.00   3rd Qu.:  44.00   3rd Qu.: 0.0000  \n##  Max.   :301.00   Max.   :340.00   Max.   :1856.00   Max.   :12.0000  \n##  NA's   :3        NA's   :3        NA's   :3         NA's   :3\n\n\n\nShow Code\nmedian(d$area, na.rm = TRUE)\n\n\nShow Output\n## [1] 69700\n\n\n\nShow Code\nmedian(d$population, na.rm = TRUE)\n\n\nShow Output\n## [1] 4911766\n\n\n\n\nCreate a new density variable in your data frame which is population / area. What are the 10 most dense countries? The 10 least dense?\n\n\nHINT: Check out the order() function.\n\n\n\nShow Code\nd$density &lt;- d$population/d$area\nd &lt;- d[order(d$density, decreasing = TRUE), ]  # or\nd &lt;- d[order(-d$density), ]\nd[1:10, ]\n\n\nShow Output\n## # A tibble: 10 √ó 17\n##    country   population   area govt_form birthrate deathrate life_expect mammals\n##    &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;   &lt;dbl&gt;\n##  1 Macau         592731   28   special ‚Ä¶       8.9       4.2        84.5       0\n##  2 Monaco         30535    2   constitu‚Ä¶       6.7       9.2        89.5       3\n##  3 Holy See‚Ä¶        842    0.1 monarchy       NA        NA          NA         1\n##  4 Singapore    5674472  697   republic        8.3       3.4        84.7      13\n##  5 Hong Kong    7141106 1108   special ‚Ä¶       9.2       7.1        82.9       3\n##  6 Gibraltar      29258    7   British ‚Ä¶      14.1       8.4        79.3       4\n##  7 Bahrain      1346613  760   constitu‚Ä¶      13.7       2.7        78.7       3\n##  8 Maldives      393253  298   republic       15.8       3.9        75.4       2\n##  9 Malta         413965  316   republic       10.2       9.1        80.2       2\n## 10 Bermuda        70196   54   British ‚Ä¶      11.3       8.2        81.2       4\n## # ‚Ñπ 9 more variables: birds &lt;dbl&gt;, reptiles &lt;dbl&gt;, amphibians &lt;dbl&gt;,\n## #   fishes &lt;dbl&gt;, mollucs &lt;dbl&gt;, other_inverts &lt;dbl&gt;, plants &lt;dbl&gt;,\n## #   fungi_protists &lt;dbl&gt;, density &lt;dbl&gt;\n\n\n\nShow Code\nd &lt;- d[order(d$density), ]\nd[1:10, ]\n\n\nShow Output\n## # A tibble: 10 √ó 17\n##    country   population   area govt_form birthrate deathrate life_expect mammals\n##    &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;   &lt;dbl&gt;\n##  1 South Ge‚Ä¶         30 3.90e3 British ‚Ä¶      NA        NA          NA         3\n##  2 Greenland      57733 2.17e6 autonomo‚Ä¶      14.5       8.5        72.1       9\n##  3 Falkland‚Ä¶       3361 1.22e4 British ‚Ä¶      NA        NA          NA         4\n##  4 Pitcairn‚Ä¶         48 4.7 e1 British ‚Ä¶      NA        NA          NA         1\n##  5 Mongolia     2992908 1.56e6 republic       20.3       6.4        69.3      11\n##  6 Western ‚Ä¶     570866 2.66e5 autonomo‚Ä¶      30.2       8.3        62.6      10\n##  7 French G‚Ä¶     181000 8.35e4 overseas‚Ä¶       0         0          76.1       8\n##  8 Namibia      2212307 8.24e5 republic       19.8      13.9        51.6      14\n##  9 Australia   22751014 7.74e6 parliame‚Ä¶      12.2       7.1        82.2      63\n## 10 Iceland       331918 1.03e5 republic       13.9       6.3        83         6\n## # ‚Ñπ 9 more variables: birds &lt;dbl&gt;, reptiles &lt;dbl&gt;, amphibians &lt;dbl&gt;,\n## #   fishes &lt;dbl&gt;, mollucs &lt;dbl&gt;, other_inverts &lt;dbl&gt;, plants &lt;dbl&gt;,\n## #   fungi_protists &lt;dbl&gt;, density &lt;dbl&gt;\n\n\n\n\nUse subsetting or filtering to extract data from the 20 largest countries into a new variable, s. What are the median area and population size of these countries?\n\n\n\nShow Code\ns &lt;- d[order(-d$population), ]\ns &lt;- s[1:20, ]\nmed_area &lt;- median(s$area, na.rm = TRUE)\nmed_area\n\n\nShow Output\n## [1] 1052875\n\n\n\nShow Code\nmed_pop &lt;- median(s$population, na.rm = TRUE)\nmed_pop\n\n\nShow Output\n## [1] 124328234\n\n\n\n\nExtract data from all countries beginning with the letters ‚ÄúA‚Äù through ‚ÄúF‚Äù. What are the mean area and population size of these countries?\n\n\nNOTE: Single bracket notation subsetting is used here to return the rows of the d data frame where the country name (d$country) begins with the capital letters ‚ÄúA‚Äù through ‚ÄúF‚Äù. The grep() function is a pattern recognition function that uses a regular expression to pull out the country names of interest.\n\n\n\nShow Code\ns &lt;- d[grep(pattern = \"^[A-F]\", d$country), ]\nsummary(s)\n\n\nShow Output\n##    country            population             area           govt_form        \n##  Length:78          Min.   :5.960e+02   Min.   :      14   Length:78         \n##  Class :character   1st Qu.:2.991e+05   1st Qu.:    4066   Class :character  \n##  Mode  :character   Median :4.785e+06   Median :   51148   Mode  :character  \n##                     Mean   :3.507e+07   Mean   :  918248                     \n##                     3rd Qu.:1.469e+07   3rd Qu.:  466498                     \n##                     Max.   :1.367e+09   Max.   :14000000                     \n##                     NA's   :4                                                \n##    birthrate       deathrate       life_expect       mammals    \n##  Min.   : 0.00   Min.   : 0.000   Min.   :49.80   Min.   : 0.0  \n##  1st Qu.:11.65   1st Qu.: 5.850   1st Qu.:68.75   1st Qu.: 3.0  \n##  Median :15.90   Median : 7.700   Median :75.50   Median : 7.0  \n##  Mean   :18.77   Mean   : 7.861   Mean   :72.25   Mean   :13.4  \n##  3rd Qu.:23.30   3rd Qu.: 9.500   3rd Qu.:78.40   3rd Qu.:14.0  \n##  Max.   :42.00   Max.   :14.400   Max.   :82.70   Max.   :81.0  \n##  NA's   :7       NA's   :7        NA's   :7                     \n##      birds           reptiles        amphibians         fishes      \n##  Min.   :  0.00   Min.   : 0.000   Min.   :  0.00   Min.   :  0.00  \n##  1st Qu.:  6.00   1st Qu.: 2.000   1st Qu.:  0.00   1st Qu.: 10.00  \n##  Median : 11.00   Median : 5.000   Median :  0.00   Median : 24.50  \n##  Mean   : 18.62   Mean   : 7.397   Mean   : 11.86   Mean   : 29.54  \n##  3rd Qu.: 18.00   3rd Qu.: 8.000   3rd Qu.:  3.00   3rd Qu.: 41.50  \n##  Max.   :165.00   Max.   :43.000   Max.   :215.00   Max.   :133.00  \n##                                                                     \n##     mollucs       other_inverts        plants        fungi_protists  \n##  Min.   :  0.00   Min.   :  0.00   Min.   :   0.00   Min.   :0.0000  \n##  1st Qu.:  0.00   1st Qu.:  4.00   1st Qu.:   2.25   1st Qu.:0.0000  \n##  Median :  1.00   Median : 11.00   Median :  10.00   Median :0.0000  \n##  Mean   : 10.17   Mean   : 23.63   Mean   :  70.64   Mean   :0.6026  \n##  3rd Qu.:  5.00   3rd Qu.: 25.25   3rd Qu.:  41.75   3rd Qu.:0.0000  \n##  Max.   :174.00   Max.   :340.00   Max.   :1856.00   Max.   :8.0000  \n##                                                                      \n##     density         \n##  Min.   :   0.2761  \n##  1st Qu.:  24.5932  \n##  Median :  75.0297  \n##  Mean   : 162.4785  \n##  3rd Qu.: 140.7140  \n##  Max.   :1771.8592  \n##  NA's   :4\n\n\n\nDoing the same thing with {dplyr} verbs (see Module 10) is easier‚Ä¶\n\n\nShow Code\ns &lt;- filter(d, grepl(pattern = \"^[A-F]\", country))\nsummary(s)\n\n\nShow Output\n##    country            population             area           govt_form        \n##  Length:78          Min.   :5.960e+02   Min.   :      14   Length:78         \n##  Class :character   1st Qu.:2.991e+05   1st Qu.:    4066   Class :character  \n##  Mode  :character   Median :4.785e+06   Median :   51148   Mode  :character  \n##                     Mean   :3.507e+07   Mean   :  918248                     \n##                     3rd Qu.:1.469e+07   3rd Qu.:  466498                     \n##                     Max.   :1.367e+09   Max.   :14000000                     \n##                     NA's   :4                                                \n##    birthrate       deathrate       life_expect       mammals    \n##  Min.   : 0.00   Min.   : 0.000   Min.   :49.80   Min.   : 0.0  \n##  1st Qu.:11.65   1st Qu.: 5.850   1st Qu.:68.75   1st Qu.: 3.0  \n##  Median :15.90   Median : 7.700   Median :75.50   Median : 7.0  \n##  Mean   :18.77   Mean   : 7.861   Mean   :72.25   Mean   :13.4  \n##  3rd Qu.:23.30   3rd Qu.: 9.500   3rd Qu.:78.40   3rd Qu.:14.0  \n##  Max.   :42.00   Max.   :14.400   Max.   :82.70   Max.   :81.0  \n##  NA's   :7       NA's   :7        NA's   :7                     \n##      birds           reptiles        amphibians         fishes      \n##  Min.   :  0.00   Min.   : 0.000   Min.   :  0.00   Min.   :  0.00  \n##  1st Qu.:  6.00   1st Qu.: 2.000   1st Qu.:  0.00   1st Qu.: 10.00  \n##  Median : 11.00   Median : 5.000   Median :  0.00   Median : 24.50  \n##  Mean   : 18.62   Mean   : 7.397   Mean   : 11.86   Mean   : 29.54  \n##  3rd Qu.: 18.00   3rd Qu.: 8.000   3rd Qu.:  3.00   3rd Qu.: 41.50  \n##  Max.   :165.00   Max.   :43.000   Max.   :215.00   Max.   :133.00  \n##                                                                     \n##     mollucs       other_inverts        plants        fungi_protists  \n##  Min.   :  0.00   Min.   :  0.00   Min.   :   0.00   Min.   :0.0000  \n##  1st Qu.:  0.00   1st Qu.:  4.00   1st Qu.:   2.25   1st Qu.:0.0000  \n##  Median :  1.00   Median : 11.00   Median :  10.00   Median :0.0000  \n##  Mean   : 10.17   Mean   : 23.63   Mean   :  70.64   Mean   :0.6026  \n##  3rd Qu.:  5.00   3rd Qu.: 25.25   3rd Qu.:  41.75   3rd Qu.:0.0000  \n##  Max.   :174.00   Max.   :340.00   Max.   :1856.00   Max.   :8.0000  \n##                                                                      \n##     density         \n##  Min.   :   0.2761  \n##  1st Qu.:  24.5932  \n##  Median :  75.0297  \n##  Mean   : 162.4785  \n##  3rd Qu.: 140.7140  \n##  Max.   :1771.8592  \n##  NA's   :4\n\n\n\n\nNOTE: Here, we use grepl() instead of grep() because we want to return a logical vector for all of the elements of ‚Äúcountry‚Äù to filter() on the basis of. grep() returns a vector of index positions.\n\nOr, instead of summary(), we can use mean() with the na.rm=TRUE argument‚Ä¶\n\n\nShow Code\nmean(s$population, na.rm = TRUE)\n\n\nShow Output\n## [1] 35065172\n\n\n\nShow Code\nmean(s$area, na.rm = TRUE)\n\n\nShow Output\n## [1] 918247.7\n\n\n\nThere are a number of other packages and associated functions we might also use to generate nice summaries of our data.\nFor example, with the skim() function from {skimr}, you can get for numeric variables a count of observations, the number of missing observations, the mean and standard deviation, and the five-number summary. The select(), kable() and kable_styling() functions will format the output nicely!\n\nlibrary(skimr)\nlibrary(kableExtra)\n\n\ns &lt;- skim(d)  # the main `skimr()` function\ns %&gt;%\n    filter(skim_type == \"numeric\") %&gt;%\n    rename(variable = skim_variable, missing = n_missing, mean = numeric.mean, sd = numeric.sd,\n        min = numeric.p0, p25 = numeric.p25, median = numeric.p50, p75 = numeric.p75,\n        max = numeric.p100, hist = numeric.hist) %&gt;%\n    select(variable, missing, mean, sd, min, median, max, hist) %&gt;%\n    # drop p25 and p75 for purposes of display\nkable() %&gt;%\n    kable_styling(font_size = 10, full_width = FALSE)\n\n\n\n\nvariable\nmissing\nmean\nsd\nmin\nmedian\nmax\nhist\n\n\n\n\npopulation\n6\n2.998647e+07\n1.241345e+08\n30.0000000\n4.911766e+06\n1.367485e+09\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\narea\n1\n6.109518e+05\n1.927077e+06\n0.1000000\n6.970000e+04\n1.709824e+07\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nbirthrate\n17\n1.894892e+01\n9.925912e+00\n0.0000000\n1.640000e+01\n4.550000e+01\n‚ñÇ‚ñá‚ñÖ‚ñÇ‚ñÅ\n\n\ndeathrate\n17\n7.610390e+00\n3.135197e+00\n0.0000000\n7.400000e+00\n1.490000e+01\n‚ñÅ‚ñÖ‚ñá‚ñÉ‚ñÇ\n\n\nlife_expect\n19\n7.219083e+01\n8.587024e+00\n49.8000000\n7.470000e+01\n8.950000e+01\n‚ñÇ‚ñÇ‚ñÉ‚ñá‚ñÇ\n\n\nmammals\n3\n1.385306e+01\n2.058178e+01\n0.0000000\n8.000000e+00\n1.880000e+02\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nbirds\n3\n1.781633e+01\n2.213342e+01\n0.0000000\n1.200000e+01\n1.650000e+02\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nreptiles\n3\n8.330612e+00\n1.408903e+01\n0.0000000\n5.000000e+00\n1.390000e+02\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\namphibians\n3\n9.848980e+00\n2.869408e+01\n0.0000000\n0.000000e+00\n2.150000e+02\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nfishes\n3\n3.283673e+01\n3.552576e+01\n0.0000000\n2.500000e+01\n2.490000e+02\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n\n\nmollucs\n3\n9.620408e+00\n2.734627e+01\n0.0000000\n1.000000e+00\n3.010000e+02\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nother_inverts\n3\n3.256735e+01\n5.362966e+01\n0.0000000\n1.100000e+01\n3.400000e+02\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nplants\n3\n6.077551e+01\n1.618624e+02\n0.0000000\n1.000000e+01\n1.856000e+03\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nfungi_protists\n3\n6.081633e-01\n1.847000e+00\n0.0000000\n0.000000e+00\n1.200000e+01\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\ndensity\n6\n4.230267e+02\n1.882632e+03\n0.0076864\n8.389234e+01\n2.116896e+04\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\n\n\n\n\n\n\ndetach(package:kableExtra)\ndetach(package:skimr)\n\nWith descr(), dfSummary(), and view() from the {summarytools} package, you can also return nicely formatted tables of summary statistics. Note that the output of the code below is not shown.\n\nCAUTION: Q1 and Q3 (the equivalent of ‚Äúnumeric.p25‚Äù and ‚Äúnumeric.p75‚Äù) are calculated slightly differently for descr() and dfSummary() than for summary() and skim()!\n\n\nlibrary(summarytools)\ns &lt;- descr(d, style = \"rmarkdown\", transpose = TRUE)\n# %&gt;% to view() print nicely formatted table to viewer\ns %&gt;%\n    summarytools::view()\ns &lt;- dfSummary(d, style = \"grid\", plain.ascii = FALSE)\ns %&gt;%\n    summarytools::view()\ndetach(package:summarytools)\n\nWith makeDataReport() from the {dataMaid} package, you can generate a nicely formated report that gets written out to a location of your choice. Again, the output of this code is not shown as it generates a report in an external file.\n\nlibrary(dataMaid)\n# this code below produces a formated report, with the type of report specified\n# by `output=`\nmakeDataReport(d, output = \"html\", file = \"~/Desktop/dataMaid-output.Rmd\", replace = TRUE)\ndetach(package:dataMaid)\n\nAdditionally, the packages {psych}, {pastecs}, and {Hmisc} also all contain functions for producing variable summaries:\npsych::describe(d)\npastecs::stat.desc(d)\nHmisc::describe(d)\n\n\nBoxplots, Barplots, and Dotcharts\nThe boxplot() function from {base} R provides a box-and-whiskers visual representation of the five-number summary, sometimes noting outliers that go beyond the bulk of the data. The function balks if you pass it nonnumeric data, so you will want to reference columns of interest specifically using either double bracket notation or the $ operator.\n\nNOTE: Actually, it is technically not true that boxplot() always shows you the five-number summary. One of the default arguments for boxplot() includes range=1.5, which means that the ‚Äúwhiskers‚Äù of the boxplot extend to 1.5 times the interquartile range, and then values more extreme than that are indicated as single points. If range=0, then, the ‚Äúwhiskers‚Äù extend to the minimum and maximum values of the data.\n\nThe barplot() function is also useful taking a quick look at crude data, with bar height proportional to the value of the variable. The function dotchart() provides a similar graphical summary.\n\n\nCHALLENGE\n\nMake boxplots, barplots, and dotcharts of the raw population and area data, then log() transform the variables and repeat the boxplots.\n\n\nNOTE: The par() command in the code below lets you set up a grid of panels in which to plot. Here, we set up a 1 row x 2 column grid.\n\n\n\nShow Code\npar(mfrow = c(1, 2))\nd$log_population &lt;- log(d$population)\nd$log_area &lt;- log(d$area)\nboxplot(d$population, ylab = \"Population\")\nboxplot(d$area, ylab = \"Area\")\n\n\n\n\n\n\n\n\n\nShow Code\nbarplot(d$population, xlab = \"Case\", ylab = \"Population\")\ndotchart(d$population, xlab = \"Population\", ylab = \"Case\")\n\n\n\n\n\n\n\n\n\nShow Code\nbarplot(d$area, xlab = \"Case\", ylab = \"Area\")\ndotchart(d$area, xlab = \"Area\", ylab = \"Case\")\n\n\n\n\n\n\n\n\n\nShow Code\nboxplot(d$log_population, ylab = \"log(Population)\")\nboxplot(d$log_area, ylab = \"log(Area)\")\n\n\n\n\n\n\n\n\n\n\n\nPlotting with {ggplot2}\nAs an alternative for plotting, we can use the ‚Äúgrammar of graphics‚Äù {ggplot2} package:\n\n# first, we use `tidyr::pivot_longer()` to convert our data from wide to long\n# format this is so we can use `facet.grid()`\n\n# uncomment this to look at a different set of variables d_long &lt;-\n# pivot_longer(d, c('log_population','log_area'), names_to='Variable',\n# values_to='Value')\n\n# uncomment this to look at a different set of variables d_long &lt;-\n# pivot_longer(d, c('mammals','birds','reptiles','fishes'),\n# names_to='Variable', values_to='Value')\n\nd_long &lt;- pivot_longer(d, c(\"birthrate\", \"deathrate\", \"life_expect\"), names_to = \"Variable\",\n    values_to = \"Value\")\n\np &lt;- ggplot(data = d_long, aes(x = factor(0), y = Value)) + geom_boxplot(na.rm = TRUE,\n    outlier.shape = NA) + theme(axis.title.x = element_blank(), axis.text.x = element_blank(),\n    axis.ticks.x = element_blank()) + geom_dotplot(binaxis = \"y\", stackdir = \"center\",\n    stackratio = 0.2, alpha = 0.3, dotsize = 0.5, color = NA, fill = \"red\", na.rm = TRUE) +\n    facet_grid(. ~ Variable) + geom_rug(sides = \"l\")\np\n\n\n\n\n\n\n\np &lt;- ggplot(data = d_long, aes(x = factor(0), y = Value)) + geom_violin(na.rm = TRUE,\n    draw_quantiles = c(0.25, 0.5, 0.75)) + theme(axis.title.x = element_blank(),\n    axis.text.x = element_blank(), axis.ticks.x = element_blank()) + geom_dotplot(binaxis = \"y\",\n    stackdir = \"center\", stackratio = 0.2, alpha = 0.3, dotsize = 0.5, color = NA,\n    fill = \"red\", na.rm = TRUE) + facet_grid(. ~ Variable) + geom_rug(sides = \"l\")\np\n\n\n\n\n\n\n\n\n\n\nHistograms\nThe hist() function returns a histogram showing the complete empirical distribution of the data in binned categories, which is useful for checking skewwness of the data, symmetry, multi-modality, etc. Setting the argument freq=FALSE will scale the Y axis to represent the proportion of observations falling into each bin rather than the count.\n\n\nCHALLENGE\n\nMake histograms of the log() transformed population and area data from the ‚ÄúCountry-Data-2016‚Äù file. Explore what happens if you set freq=FALSE versus the default of freq=TRUE. Try looking at other variables as well.\n\n\n\nShow Code\npar(mfrow = c(1, 2))  # sets up two panels side by side\nattach(d)  # lets us use variable names without specifying the data frame!\nhist(log(population), freq = FALSE, col = \"red\", main = \"Plot 1\", xlab = \"log(population size)\",\n    ylab = \"density\", ylim = c(0, 0.2))\nhist(log(area), freq = FALSE, col = \"red\", main = \"Plot 2\", xlab = \"log(area)\", ylab = \"density\",\n    ylim = c(0, 0.2))\n\n\n\n\n\n\n\n\n\n\nNOTE: You can add lines to your histograms (e.g., to show the mean value for a variable) using the abline() command, with arguments. For example, to show a single vertical line representing the mean log(population size), you would add the argument v=mean(log(area), na.rm=TRUE). We need to set na.rm to be TRUE otherwise the mean() function will not run the way we expect.\n\n\n\nShow Code\nhist(log(area), freq = FALSE, col = \"red\", main = \"Plot 2\", xlab = \"log(area)\", ylab = \"density\",\n    ylim = c(0, 0.2))\nabline(v = mean(log(area), na.rm = TRUE))\n\n\n\n\n\n\n\n\n\n\n\nDensity Plots\nThe density() function computes a non-parametric estimate of the distribution of a variable, which can be combined with other plots to also yield a graphical view of the distribution of the data. If your data have missing values, then you need to add the argument na.rm=TRUE to the density() function. To superimpose a density() curve on a histogram, you can use the lines(density()) function.\n\npar(mfrow = c(1, 1))  # set up one panel and redraw the log(population) histogram\nhist(log(population), freq = FALSE, col = \"white\", main = \"Density Plot with Mean\",\n    xlab = \"log(population size)\", ylab = \"density\", ylim = c(0, 0.2))\nabline(v = mean(log(population), na.rm = TRUE), col = \"blue\")\nlines(density(log(population), na.rm = TRUE), col = \"green\")\n\n\n\n\n\n\n\ndetach(d)\n\n\n\nTables\nThe table() function can be used to summarize counts and proportions for categorical variables in your dataset.\n\n\nCHALLENGE\n\nUsing the table() function, find what is the most common form of government in the ‚ÄúCountry-Data-2016‚Äù dataset. How many countries have that form?\n\n\nHINT: We can combine table() with sort() and the argument decreasing=TRUE to get the desired answered straight away.\n\n\n\nShow Code\nt &lt;- sort(table(d$govt_form), decreasing = TRUE)\nt\n\n\nShow Output\n## \n##                                republic                 constitutional monarchy \n##                                     127                                      33 \n##              British overseas territory            overseas territory of France \n##                                      12                                       7 \n##           presidential federal republic                                monarchy \n##                                       7                                       6 \n##                  parliamentary monarchy          territory of the United States \n##                                       5                                       5 \n##                 parliamentary democracy                  territory of Australia \n##                                       4                                       4 \n## autonomous territory of the Netherlands                        federal republic \n##                                       3                                       3 \n##                        islamic republic                      socialist republic \n##                                       3                                       3 \n##                       absolute monarchy            autonomous region of Denmark \n##                                       2                                       2 \n##     autonomous territory of New Zealand            overseas community of France \n##                                       2                                       2 \n##          parliamentary federal republic  special administrative region of China \n##                                       2                                       2 \n##                     territory of Norway                       autonomous region \n##                                       2                                       1 \n##             autonomous region of France            autonomous region of Morocco \n##                                       1                                       1 \n##                              federation         foreign-administrated territory \n##                                       1                                       1 \n##      islamic-socialist peoples republic                  parliamentary republic \n##                                       1                                       1 \n##                        peoples republic                     territory of France \n##                                       1                                       1 \n##                territory of New Zealand            territory of the Netherlands \n##                                       1                                       1\n\n\n\nDoing the same thing with {dplyr} verbs (see Module 10) provides nicer output‚Ä¶\n\nt &lt;- group_by(d, govt_form) %&gt;%\n    summarize(count = n()) %&gt;%\n    arrange(desc(count))\nt\n\n## # A tibble: 33 √ó 2\n##    govt_form                      count\n##    &lt;chr&gt;                          &lt;int&gt;\n##  1 republic                         127\n##  2 constitutional monarchy           33\n##  3 British overseas territory        12\n##  4 overseas territory of France       7\n##  5 presidential federal republic      7\n##  6 monarchy                           6\n##  7 parliamentary monarchy             5\n##  8 territory of the United States     5\n##  9 parliamentary democracy            4\n## 10 territory of Australia             4\n## # ‚Ñπ 23 more rows",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "09-module.html#exploring-multiple-variables",
    "href": "09-module.html#exploring-multiple-variables",
    "title": "9¬† Exploratory Data Analysis",
    "section": "9.6 Exploring Multiple Variables",
    "text": "9.6 Exploring Multiple Variables\n\nPlotting Several Variables\n\nMultiple boxplots or histograms can be laid out side-by-side or overlaid.\n\n\n\nCHALLENGE\nRead in the dataset ‚ÄúKamilarAndCooperData‚Äù, which contains a host of summary information about 213 primate species.\nSpend some time exploring the data on your own and then make boxplots of log(female body mass) ~ family. Try doing this with {base} graphics and then look at how we might do in in {ggplot2}, which provides a standard ‚Äúgrammar of graphics‚Äù (see the {ggplot2} documentation)\n\n\nShow Code\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/KamilarAndCooperData.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\nhead(d)\n\n\nShow Output\n## # A tibble: 6 √ó 45\n##   Scientific_Name        Superfamily Family Genus Species Brain_Size_Species_M‚Ä¶¬π\n##   &lt;chr&gt;                  &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;                    &lt;dbl&gt;\n## 1 Allenopithecus_nigrov‚Ä¶ Cercopithe‚Ä¶ Cerco‚Ä¶ Alle‚Ä¶ nigrov‚Ä¶                   58.0\n## 2 Allocebus_trichotis    Cercopithe‚Ä¶ Cerco‚Ä¶ Allo‚Ä¶ tricho‚Ä¶                   NA  \n## 3 Alouatta_belzebul      Ceboidea    Ateli‚Ä¶ Alou‚Ä¶ belzeb‚Ä¶                   52.8\n## 4 Alouatta_caraya        Ceboidea    Ateli‚Ä¶ Alou‚Ä¶ caraya                    52.6\n## 5 Alouatta_guariba       Ceboidea    Ateli‚Ä¶ Alou‚Ä¶ guariba                   51.7\n## 6 Alouatta_palliata      Ceboidea    Ateli‚Ä¶ Alou‚Ä¶ pallia‚Ä¶                   49.9\n## # ‚Ñπ abbreviated name: ¬π‚ÄãBrain_Size_Species_Mean\n## # ‚Ñπ 39 more variables: Brain_Size_Female_Mean &lt;dbl&gt;, Brain_size_Ref &lt;chr&gt;,\n## #   Body_mass_male_mean &lt;dbl&gt;, Body_mass_female_mean &lt;dbl&gt;,\n## #   Mass_Dimorphism &lt;dbl&gt;, Mass_Ref &lt;chr&gt;, MeanGroupSize &lt;dbl&gt;,\n## #   AdultMales &lt;dbl&gt;, AdultFemale &lt;dbl&gt;, AdultSexRatio &lt;dbl&gt;,\n## #   Social_Organization_Ref &lt;chr&gt;, InterbirthInterval_d &lt;dbl&gt;, Gestation &lt;dbl&gt;,\n## #   WeaningAge_d &lt;dbl&gt;, MaxLongevity_m &lt;dbl&gt;, LitterSz &lt;dbl&gt;, ‚Ä¶\n\n\n\n\nlibrary(skimr)\nlibrary(kableExtra)\n\n\n\nShow Code\ns &lt;- skim(d)  # formats results to a wide table\n# here we make use of the `%&gt;%` operator and {dplyr} verbs... see below\ns %&gt;%\n    filter(skim_variable == \"Scientific_Name\" | skim_type == \"numeric\") %&gt;%\n    rename(variable = skim_variable, missing = n_missing, mean = numeric.mean, sd = numeric.sd,\n        min = numeric.p0, p25 = numeric.p25, median = numeric.p50, p75 = numeric.p75,\n        max = numeric.p100, hist = numeric.hist) %&gt;%\n    select(variable, missing, mean, sd, min, median, max, hist) %&gt;%\n    # drop p25 and p75 for purposes of display\nkable() %&gt;%\n    kable_styling(font_size = 10, full_width = FALSE)\n\n\n\n\n\nvariable\nmissing\nmean\nsd\nmin\nmedian\nmax\nhist\n\n\n\n\nScientific_Name\n0\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nBrain_Size_Species_Mean\n42\n68.109649\n7.768403e+01\n1.630\n61.45000\n491.2700\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n\n\nBrain_Size_Female_Mean\n48\n65.235879\n7.371151e+01\n1.660\n57.20000\n480.1500\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n\n\nBody_mass_male_mean\n18\n8111.800513\n1.920858e+04\n31.000\n4290.00000\n170400.0000\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nBody_mass_female_mean\n18\n5396.473846\n1.008896e+04\n30.000\n3039.00000\n97500.0000\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nMass_Dimorphism\n18\n1.246005\n3.212141e-01\n0.841\n1.10900\n2.6880\n‚ñá‚ñÉ‚ñÇ‚ñÅ‚ñÅ\n\n\nMeanGroupSize\n60\n15.055065\n1.759245e+01\n1.000\n7.80000\n90.0000\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n\n\nAdultMales\n68\n2.515621\n2.373255e+00\n0.900\n1.50000\n16.0000\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nAdultFemale\n68\n5.048655\n5.428615e+00\n1.000\n2.90000\n25.2000\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n\n\nAdultSexRatio\n84\n2.304729\n2.192575e+00\n0.500\n1.45000\n15.6000\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nInterbirthInterval_d\n103\n572.058546\n3.549477e+02\n144.470\n476.93000\n2007.5000\n‚ñá‚ñÖ‚ñÅ‚ñÅ‚ñÅ\n\n\nGestation\n75\n163.465362\n3.740063e+01\n59.990\n165.04000\n256.0000\n‚ñÅ‚ñÉ‚ñá‚ñÉ‚ñÇ\n\n\nWeaningAge_d\n95\n310.042881\n2.546776e+02\n40.000\n237.73500\n1260.8100\n‚ñá‚ñÖ‚ñÇ‚ñÅ‚ñÅ\n\n\nMaxLongevity_m\n66\n327.331701\n1.259496e+02\n103.000\n303.60000\n720.0000\n‚ñÖ‚ñá‚ñÜ‚ñÇ‚ñÅ\n\n\nLitterSz\n47\n1.180542\n3.601098e-01\n0.990\n1.01000\n2.5200\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nGR_MidRangeLat_dd\n38\n-1.796457\n1.362270e+01\n-24.500\n-0.76000\n35.8800\n‚ñÜ‚ñÜ‚ñá‚ñÇ‚ñÅ\n\n\nPrecip_Mean_mm\n38\n1543.134286\n5.158597e+02\n419.000\n1541.90000\n2794.3000\n‚ñÇ‚ñÖ‚ñá‚ñÖ‚ñÇ\n\n\nTemp_Mean_degC\n38\n23.132000\n3.405405e+00\n2.600\n24.30000\n27.4000\n‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñá\n\n\nAET_Mean_mm\n38\n1253.112000\n2.635696e+02\n453.100\n1291.10000\n1828.3000\n‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñÇ\n\n\nPET_Mean_mm\n38\n1553.158857\n1.567096e+02\n842.500\n1566.90000\n1927.3000\n‚ñÅ‚ñÅ‚ñÇ‚ñá‚ñÇ\n\n\nHomeRange_km2\n65\n1.937905\n5.210604e+00\n0.002\n0.27500\n28.2400\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nDayLength_km\n104\n1.551449\n1.439277e+00\n0.250\n1.21200\n11.0000\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nTerritoriality\n109\n2.228885\n2.201259e+00\n0.225\n1.59235\n15.5976\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nFruit\n98\n47.736956\n2.408200e+01\n1.000\n48.00000\n97.0000\n‚ñÖ‚ñá‚ñá‚ñá‚ñÉ\n\n\nCanine_Dimorphism\n92\n1.617438\n6.722173e-01\n0.880\n1.56000\n5.2630\n‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÅ\n\n\nFeed\n141\n33.075833\n1.366899e+01\n9.000\n33.30000\n63.9000\n‚ñá‚ñá‚ñá‚ñá‚ñÇ\n\n\nMove\n143\n21.668857\n1.059124e+01\n3.000\n21.00000\n70.6000\n‚ñÖ‚ñá‚ñÇ‚ñÅ‚ñÅ\n\n\nRest\n143\n34.259571\n1.967175e+01\n4.000\n30.48000\n78.5000\n‚ñá‚ñá‚ñÖ‚ñÜ‚ñÇ\n\n\nSocial\n136\n7.368701\n5.242077e+00\n0.900\n5.40000\n23.5000\n‚ñá‚ñÉ‚ñÉ‚ñÅ‚ñÅ\n\n\n\n\n\n\n\n\ndetach(package:kableExtra)\ndetach(package:skimr)\n\nWe can also make boxplots summarizing a numerical variable in relation to another categorical variable.\nPlotting using {base} graphics‚Ä¶ the ~ operator can be read as ‚Äúby‚Äù.\n\nboxplot(log(d$Body_mass_female_mean) ~ d$Family)\n\n\n\n\n\n\n\n\nAlternatively, plotting using {ggplot2}‚Ä¶\n\np &lt;- ggplot(data = d, aes(x = Family, y = log(Body_mass_female_mean)))\np &lt;- p + geom_boxplot(na.rm = TRUE)\np &lt;- p + theme(axis.text.x = element_text(angle = 90))\np &lt;- p + ylab(\"log(Female Body Mass)\")\np\n\n\n\n\n\n\n\n\n\n\nBivariate Scatterplots\nScatterplots are a natural tool for visualizing two continuous variables and can be made easily with the plot(x=&lt;variable 1&gt;, y=&lt;variable 2&gt;) function in {base} graphics (where &lt;variable 1&gt; and &lt;variable 2&gt; denote the names of the two variables you wish to plot). Transformations of the variables, e.g., log or square root (sqrt()), may be necessary for effective visualization.\n\n\nCHALLENGE\n\nAgain using data from the ‚ÄúKamilarAndCooperData‚Äù dataset, plot the relationship between female body size and female brain size. Then, play with log transforming the data and plot again.\n\n\n\nShow Code\npar(mfrow = c(1, 2))  # sets up two panels side by side\nplot(x = d$Body_mass_female_mean, y = d$Brain_Size_Female_Mean)\nplot(x = log(d$Body_mass_female_mean), y = log(d$Brain_Size_Female_Mean))\n\n\n\n\n\n\n\n\n\nThe grammar for {ggplot2} is a bit more complicated‚Ä¶ see if you can follow it in the example below.\n\np &lt;- ggplot(data = d, aes(x = log(Body_mass_female_mean), y = log(Brain_Size_Female_Mean),\n    color = factor(Family)))  # first, we build a plot object and color points by Family\n# then we modify the axis labels\np &lt;- p + xlab(\"log(Female Body Mass)\") + ylab(\"log(Female Brain Size)\")\n# then we make a scatterplot\np &lt;- p + geom_point(na.rm = TRUE)\n# then we modify the legend\np &lt;- p + theme(legend.position = \"bottom\", legend.title = element_blank())\n# and, finally, we plot the object\np\n\n\n\n\n\n\n\n\nWe can use the cool package {ggExtra} to add marginal univariate plots to our bivariate scatterplots, too.\n\nlibrary(ggExtra)\nggMarginal(p, type = \"densigram\")  # try with other types, too!\n\n## Warning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\n## ‚Ñπ Please use `after_stat(density)` instead.\n## ‚Ñπ The deprecated feature was likely used in the ggExtra package.\n##   Please report the issue at &lt;https://github.com/daattali/ggExtra/issues&gt;.\n\n\n\n\n\n\n\n\ndetach(package:ggExtra)\n\nUsing {ggplot2}, we can also easily set up a grid for ‚Äúfaceting‚Äù‚Äù by a grouping variable‚Ä¶\n\np &lt;- p + facet_wrap(~Family, ncol = 4)  # wrap data 'by' family into 4 columns\np &lt;- p + theme(legend.position = \"none\")\np\n\n\n\n\n\n\n\n\nIn {ggplot2} can easily add regression lines to our plot. Here, we add a linear model to each facet.\n\np &lt;- p + geom_smooth(method = \"lm\", fullrange = FALSE, na.rm = TRUE)\np\n\n\n\n\n\n\n\n\nThe scatterplot() function from the {car} package also produces nice bivariate plots and includes barplots along the margins.\n\nlibrary(car)\n\n\nscatterplot(data = d, log(Brain_Size_Female_Mean) ~ log(Body_mass_female_mean), xlab = \"log(Female Body Mass\",\n    ylab = \"log(Female Brain Size\", boxplots = \"xy\", regLine = list(method = lm,\n        lty = 1, lwd = 2, col = \"red\"))\n\n\n\n\n\n\n\ndetach(package:car)\n\n\n\nCHALLENGE\n\nBuild your own bivariate scatterplot of log(MaxLongevity_m) by log(Body_mass_female_mean) using the ‚ÄúKamilarAndCooperData‚Äù dataset.\n\n\n\nShow Code\np &lt;- ggplot(data = d, aes(x = log(Body_mass_female_mean), y = log(MaxLongevity_m)))\np &lt;- p + geom_point(na.rm = TRUE)\np &lt;- p + geom_smooth(method = \"lm\", na.rm = TRUE)\np\n\n\n## `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe can also use either the pairs() function from {base} R, the scatterplotMatrix() function from the {car} package, or the pairs.panel() function from the {psych} package to do multiple scatterplots simulaneously. The ggpairs() function from the {GGally} package can also be used, but it is slower.\n\n\nCHALLENGE\nSelect the variables Brain_Size_Female_Mean, Body_mass_female_mean, MeanGroupSize, WeaningAge_d, MaxLongevity_m, HomeRange_km2, and DayLength_km from data frame d and plot scatterplots of all pairs of variables.\n\nNOTE: Here we are using the select() function from the {dplyr} package, which we cover in more detail in Module 10.\n\n\n\nShow Code\ns &lt;- select(d, c(\"Brain_Size_Female_Mean\", \"Body_mass_female_mean\", \"MeanGroupSize\",\n    \"WeaningAge_d\", \"MaxLongevity_m\", \"HomeRange_km2\", \"DayLength_km\"))\npairs(s[, 1:ncol(s)])  # or\n\n\n\n\n\n\n\n\n\nShow Code\npairs(data = s, ~.)  # NAs are ignored by default\n# adding argument `upper.panel=NULL` will suppress plots above the diagonal\n\n\nThe {car} package function scatterplotMatrix() provides a more customizable interface to the pairs() function from {base} R. It includes a neat rug plot for each variable.\n\nlibrary(car)\n\n\nscatterplotMatrix(s, smooth = TRUE, regLine = list(method = lm, lty = 1, lwd = 2),\n    ellipse = TRUE, upper.panel = NULL)\n\n\n\n\n\n\n\ndetach(package:car)\n\nThe {psych} package also makes it easy to construct a scatterplot matrix with customizable output using the pairs.panel() function. Here, method= specifies the correlation method, density= specifies whether to superimpose a density curve on the histogram, and elipses= specifies whether to show correlation elipses.\n\nlibrary(psych)\npairs.panels(s[], smooth = FALSE, lm = TRUE, method = \"pearson\", hist.col = \"#00AFBB\",\n    density = TRUE, ellipses = TRUE)\n\n\n\n\n\n\n\n# NAs are ignored by default\ndetach(package:psych)\n\nThe {GGally} package lets us do something similar‚Ä¶\n\nlibrary(GGally)\nggpairs(s, columns = 1:ncol(s))  # NAs produce a warning\n\n\n\n\n\n\n\ndetach(package:GGally)\n\nFinally, we can do a nice correlation plot using the {corrplot} package, where we get a matrix of correlations among variables and a graphical representation of the strength and direction of the bivariate correlation coefficients among them.\n\nlibrary(corrplot)\ncor = cor(s, use = \"pairwise.complete.obs\")\ncorrplot.mixed(cor, lower.col = \"black\", number.cex = 0.7)\n\n\n\n\n\n\n\ndetach(package:corrplot)",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "09-module.html#custom-layouts-for-multiple-plots",
    "href": "09-module.html#custom-layouts-for-multiple-plots",
    "title": "9¬† Exploratory Data Analysis",
    "section": "9.7 Custom Layouts for Multiple Plots",
    "text": "9.7 Custom Layouts for Multiple Plots\nAbove, we saw the typical {base} R method for laying out plots, using the par() command with the mfrow= argument to set up a grid of panels in which to plot to the current graphics output device. There are, however, several easier ways for combining plots into custom layouts. Three alternatives are provided in the {patchwork}, {cowplot}, and {gridExtra} packages. Each starts by generating plot objects, typically using {ggplot2}. First, we create 3 plot objects using the KamilarAndCooper dataset:\n\np1 &lt;- ggplot(data = d, aes(x = log(Body_mass_female_mean), y = log(MaxLongevity_m)))\np1 &lt;- p1 + geom_point(na.rm = TRUE)\np1 &lt;- p1 + geom_smooth(method = \"lm\", na.rm = TRUE)\np1 &lt;- p1 + xlab(\"log(Female Body Size)\") + ylab(\"log(Lifespan)\")\n\np2 &lt;- ggplot(data = d, aes(x = log(Body_mass_male_mean), y = log(MaxLongevity_m)))\np2 &lt;- p2 + geom_point(na.rm = TRUE)\np2 &lt;- p2 + geom_smooth(method = \"lm\", na.rm = TRUE)\np2 &lt;- p2 + xlab(\"log(Male Body Size)\") + ylab(\"log(Lifespan)\")\n\np3 &lt;- ggplot(data = d, aes(x = Family, y = log(Body_mass_female_mean)))\np3 &lt;- p3 + geom_boxplot(na.rm = TRUE)\np3 &lt;- p3 + theme(axis.text.x = element_text(angle = 90))\np3 &lt;- p3 + xlab(\"Family\") + ylab(\"log(Female Body Mass)\")\n\nOnce we have these plot objects, we can arrange them in custom ways using these different packages. Some alternative ways of arranging these three plots in 2 rows, with 2 panels in the top row and 1 in the bottom row, are given below‚Ä¶\nThe {patchwork} package is perhaps the easiest to use!\n\nlibrary(patchwork)\n(p1 | p2)/p3 + plot_annotation(tag_levels = \"A\")\n\n\n\n\n\n\n\ndetach(package:patchwork)\n\nWe can also use {cowplot}‚Ä¶\n\nlibrary(cowplot)\nplot_grid(plot_grid(p1, p2, labels = c(\"A\", \"B\"), label_size = 12, nrow = 1), p3,\n    labels = c(\"\", \"C\"), label_size = 12, nrow = 2)\n\n\n\n\n\n\n\ndetach(package:cowplot)\n\n‚Ä¶ or {gridExtra}\n\nlibrary(gridExtra)\ngrid.arrange(grid.arrange(p1, p2, nrow = 1), p3, nrow = 2)\n\n\n\n\n\n\n\ndetach(package:gridExtra)",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "09-module.html#aggregate-statistics",
    "href": "09-module.html#aggregate-statistics",
    "title": "9¬† Exploratory Data Analysis",
    "section": "9.8 Aggregate Statistics",
    "text": "9.8 Aggregate Statistics\nTo calculate summary statistics for groups of observations in a data frame, there are many different approaches. One is to use the aggregate() function from the {stats} package (a {base} R standard package), which provides a quick way to look at summary statistics for sets of observations, though it requires a bit of clunky code. Here, we apply a particular function (FUN = \"mean\") to mean female body mass, grouped by Family.\n\naggregate(d$Body_mass_female_mean ~ d$Family, FUN = \"mean\", na.rm = TRUE)\n\n##           d$Family d$Body_mass_female_mean\n## 1         Atelidae               6616.2000\n## 2          Cebidae                876.3323\n## 3  Cercopithecidae               6327.8247\n## 4    Cheirogalidae                186.0286\n## 5    Daubentonidae               2490.0000\n## 6        Galagidae                371.6143\n## 7        Hominidae              53443.7167\n## 8      Hylobatidae               6682.1200\n## 9        Indriidae               3886.5333\n## 10       Lemuridae               1991.1200\n## 11   Lepilemuridae                813.5000\n## 12       Lorisidae                489.8625\n## 13     Pitheciidae               1768.5000\n## 14       Tarsiidae                120.0000\n\n\nOr, alternatively‚Ä¶\n\naggregate(x = d[\"Body_mass_female_mean\"], by = d[\"Family\"], FUN = \"mean\", na.rm = TRUE)\n\n##             Family Body_mass_female_mean\n## 1         Atelidae             6616.2000\n## 2          Cebidae              876.3323\n## 3  Cercopithecidae             6327.8247\n## 4    Cheirogalidae              186.0286\n## 5    Daubentonidae             2490.0000\n## 6        Galagidae              371.6143\n## 7        Hominidae            53443.7167\n## 8      Hylobatidae             6682.1200\n## 9        Indriidae             3886.5333\n## 10       Lemuridae             1991.1200\n## 11   Lepilemuridae              813.5000\n## 12       Lorisidae              489.8625\n## 13     Pitheciidae             1768.5000\n## 14       Tarsiidae              120.0000\n\n\n\n# | include: false\ndetach(package:curl)\ndetach(package:tidyverse)",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "09-module.html#concept-review",
    "href": "09-module.html#concept-review",
    "title": "9¬† Exploratory Data Analysis",
    "section": "Concept Review",
    "text": "Concept Review\n\nSummary statistics: summary(), skim()\nBasic plotting: boxplot(), barplot(), histogram()\nPlotting with {ggplot2}: ggplot(data = , mapping = aes()) + geom() + theme() + ...\nSummarizing by groups: aggregrate()\nArranging plots with {patchwork}, {cowplot}, and {gridExtra}",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "10-module.html",
    "href": "10-module.html",
    "title": "10¬† Data Wrangling",
    "section": "",
    "text": "10.1 Objectives",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "10-module.html#objectives",
    "href": "10-module.html#objectives",
    "title": "10¬† Data Wrangling",
    "section": "",
    "text": "The objective of this module to introduce you to data wrangling and transformation using the {dplyr} and {tidyr} packages, which are part of the {tidyverse} family.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "10-module.html#preliminaries",
    "href": "10-module.html#preliminaries",
    "title": "10¬† Data Wrangling",
    "section": "10.2 Preliminaries",
    "text": "10.2 Preliminaries\n\nInstall (but do not load yet) this package in R: {tidylog}\nInstall and load this package: {magrittr}\nLoad {tidyverse}\nLoad in the KamilarAndCooper dataset we used in Module 09 as a ‚Äútibble‚Äù named d\n\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/KamilarAndCooperData.csv\"\nd &lt;- read_csv(f, col_names = TRUE)  # creates a 'tibble'\nhead(d)\n\n## # A tibble: 6 √ó 45\n##   Scientific_Name        Superfamily Family Genus Species Brain_Size_Species_M‚Ä¶¬π\n##   &lt;chr&gt;                  &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;                    &lt;dbl&gt;\n## 1 Allenopithecus_nigrov‚Ä¶ Cercopithe‚Ä¶ Cerco‚Ä¶ Alle‚Ä¶ nigrov‚Ä¶                   58.0\n## 2 Allocebus_trichotis    Cercopithe‚Ä¶ Cerco‚Ä¶ Allo‚Ä¶ tricho‚Ä¶                   NA  \n## 3 Alouatta_belzebul      Ceboidea    Ateli‚Ä¶ Alou‚Ä¶ belzeb‚Ä¶                   52.8\n## 4 Alouatta_caraya        Ceboidea    Ateli‚Ä¶ Alou‚Ä¶ caraya                    52.6\n## 5 Alouatta_guariba       Ceboidea    Ateli‚Ä¶ Alou‚Ä¶ guariba                   51.7\n## 6 Alouatta_palliata      Ceboidea    Ateli‚Ä¶ Alou‚Ä¶ pallia‚Ä¶                   49.9\n## # ‚Ñπ abbreviated name: ¬π‚ÄãBrain_Size_Species_Mean\n## # ‚Ñπ 39 more variables: Brain_Size_Female_Mean &lt;dbl&gt;, Brain_size_Ref &lt;chr&gt;,\n## #   Body_mass_male_mean &lt;dbl&gt;, Body_mass_female_mean &lt;dbl&gt;,\n## #   Mass_Dimorphism &lt;dbl&gt;, Mass_Ref &lt;chr&gt;, MeanGroupSize &lt;dbl&gt;,\n## #   AdultMales &lt;dbl&gt;, AdultFemale &lt;dbl&gt;, AdultSexRatio &lt;dbl&gt;,\n## #   Social_Organization_Ref &lt;chr&gt;, InterbirthInterval_d &lt;dbl&gt;, Gestation &lt;dbl&gt;,\n## #   WeaningAge_d &lt;dbl&gt;, MaxLongevity_m &lt;dbl&gt;, LitterSz &lt;dbl&gt;, ‚Ä¶",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "10-module.html#data-wrangling-using-dplyr",
    "href": "10-module.html#data-wrangling-using-dplyr",
    "title": "10¬† Data Wrangling",
    "section": "10.3 Data Wrangling Using {dplyr}",
    "text": "10.3 Data Wrangling Using {dplyr}\nThe {dplyr} package, included in the {tidyverse}, provides ‚Äúa flexible grammar of data manipulation‚Äù that makes many of the manipulations that we explore in Module 07 and Module 09 much easier and much more intuitive!\n\n\n\n\n\n\n\n\n\nAmong other functions, {dplyr} introduces a set of verbs (filter(), select(), arrange(), rename(), mutate(), summarize(), and group_by()) that can be used to perform useful operations on ‚Äútibbles‚Äù and related tabular data structures (e.g., normal data frames and data tables). Before using {dplyr} for summarizing data and producing aggregate statistics, let‚Äôs look in general at what we can do with these verbs‚Ä¶\n\nfilter()\nThe filter() function lets us pull out rows from a data frame that meet a particular criterion or set of criteria:\n\n\n\n\n\n\n\n\n\n\n\nFROM: Baumer et al.¬†(2017). Modern Data Science with R. Chapman and Hall/CRC.\n\n\n\n\n# selecting rows..\ns &lt;- filter(d, Family == \"Hominidae\" & Mass_Dimorphism &gt; 2)\nhead(s)\n\n## # A tibble: 3 √ó 45\n##   Scientific_Name Superfamily Family    Genus   Species  Brain_Size_Species_Mean\n##   &lt;chr&gt;           &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;                      &lt;dbl&gt;\n## 1 Gorilla_gorilla Hominoidea  Hominidae Gorilla gorilla                     490.\n## 2 Pongo_abelii    Hominoidea  Hominidae Pongo   abelii                      390.\n## 3 Pongo_pygmaeus  Hominoidea  Hominidae Pongo   pygmaeus                    377.\n## # ‚Ñπ 39 more variables: Brain_Size_Female_Mean &lt;dbl&gt;, Brain_size_Ref &lt;chr&gt;,\n## #   Body_mass_male_mean &lt;dbl&gt;, Body_mass_female_mean &lt;dbl&gt;,\n## #   Mass_Dimorphism &lt;dbl&gt;, Mass_Ref &lt;chr&gt;, MeanGroupSize &lt;dbl&gt;,\n## #   AdultMales &lt;dbl&gt;, AdultFemale &lt;dbl&gt;, AdultSexRatio &lt;dbl&gt;,\n## #   Social_Organization_Ref &lt;chr&gt;, InterbirthInterval_d &lt;dbl&gt;, Gestation &lt;dbl&gt;,\n## #   WeaningAge_d &lt;dbl&gt;, MaxLongevity_m &lt;dbl&gt;, LitterSz &lt;dbl&gt;,\n## #   Life_History_Ref &lt;chr&gt;, GR_MidRangeLat_dd &lt;dbl&gt;, Precip_Mean_mm &lt;dbl&gt;, ‚Ä¶\n\n\n\nNOTE: The first argument of any of the {dplyr} verbs is the .data argument. That is the line of code above is equivalent to s &lt;- filter(.data = d, Family == \"Hominidae\" & Mass_Dimorphism &gt; 2)\n\n\n\nselect()\nThe select() function lets us pull out only particular columns from a data frame:\n\n\n\n\n\n\n\n\n\n\n\nFROM: Baumer et al.¬†(2017). Modern Data Science with R. Chapman and Hall/CRC.\n\n\n\n\n# selecting specific columns...\ns &lt;- select(d, Family, Genus, Body_mass_male_mean)\nhead(s)\n\n## # A tibble: 6 √ó 3\n##   Family          Genus          Body_mass_male_mean\n##   &lt;chr&gt;           &lt;chr&gt;                        &lt;dbl&gt;\n## 1 Cercopithecidae Allenopithecus                6130\n## 2 Cercopithecidae Allocebus                       92\n## 3 Atelidae        Alouatta                      7270\n## 4 Atelidae        Alouatta                      6525\n## 5 Atelidae        Alouatta                      5800\n## 6 Atelidae        Alouatta                      7150\n\n\n\n\narrange()\nThe arrange() function lets us sort a data frame based on a select variable or set of variables:\n\n\n\n\n\n\n\n\n\n\n\nFROM: Baumer et al.¬†(2017). Modern Data Science with R. Chapman and Hall/CRC.\n\n\n\n\n# reordering a data frame by a set of variables...\ns &lt;- arrange(d, Family, Genus, Body_mass_male_mean)\nhead(s)\n\n## # A tibble: 6 √ó 45\n##   Scientific_Name    Superfamily Family   Genus   Species Brain_Size_Species_M‚Ä¶¬π\n##   &lt;chr&gt;              &lt;chr&gt;       &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;                    &lt;dbl&gt;\n## 1 Alouatta_guariba   Ceboidea    Atelidae Alouat‚Ä¶ guariba                   51.7\n## 2 Alouatta_caraya    Ceboidea    Atelidae Alouat‚Ä¶ caraya                    52.6\n## 3 Alouatta_seniculus Ceboidea    Atelidae Alouat‚Ä¶ senicu‚Ä¶                   55.2\n## 4 Alouatta_palliata  Ceboidea    Atelidae Alouat‚Ä¶ pallia‚Ä¶                   49.9\n## 5 Alouatta_belzebul  Ceboidea    Atelidae Alouat‚Ä¶ belzeb‚Ä¶                   52.8\n## 6 Alouatta_pigra     Ceboidea    Atelidae Alouat‚Ä¶ pigra                     51.1\n## # ‚Ñπ abbreviated name: ¬π‚ÄãBrain_Size_Species_Mean\n## # ‚Ñπ 39 more variables: Brain_Size_Female_Mean &lt;dbl&gt;, Brain_size_Ref &lt;chr&gt;,\n## #   Body_mass_male_mean &lt;dbl&gt;, Body_mass_female_mean &lt;dbl&gt;,\n## #   Mass_Dimorphism &lt;dbl&gt;, Mass_Ref &lt;chr&gt;, MeanGroupSize &lt;dbl&gt;,\n## #   AdultMales &lt;dbl&gt;, AdultFemale &lt;dbl&gt;, AdultSexRatio &lt;dbl&gt;,\n## #   Social_Organization_Ref &lt;chr&gt;, InterbirthInterval_d &lt;dbl&gt;, Gestation &lt;dbl&gt;,\n## #   WeaningAge_d &lt;dbl&gt;, MaxLongevity_m &lt;dbl&gt;, LitterSz &lt;dbl&gt;, ‚Ä¶\n\n\nWe can also specify the direction in which we want the data frame to be sorted:\n\n# `desc()` can be used to reverse the order\ns &lt;- arrange(d, desc(Family), Genus, Species, desc(Body_mass_male_mean))\nhead(s)\n\n## # A tibble: 6 √ó 45\n##   Scientific_Name        Superfamily Family Genus Species Brain_Size_Species_M‚Ä¶¬π\n##   &lt;chr&gt;                  &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;                    &lt;dbl&gt;\n## 1 Tarsius_bancanus       Tarsioidea  Tarsi‚Ä¶ Tars‚Ä¶ bancan‚Ä¶                   3.16\n## 2 Tarsius_dentatus       Tarsioidea  Tarsi‚Ä¶ Tars‚Ä¶ dentat‚Ä¶                  NA   \n## 3 Tarsius_syrichta       Tarsioidea  Tarsi‚Ä¶ Tars‚Ä¶ syrich‚Ä¶                   3.36\n## 4 Cacajao_calvus         Ceboidea    Pithe‚Ä¶ Caca‚Ä¶ calvus                   76   \n## 5 Cacajao_melanocephalus Ceboidea    Pithe‚Ä¶ Caca‚Ä¶ melano‚Ä¶                  68.8 \n## 6 Callicebus_donacophil‚Ä¶ Ceboidea    Pithe‚Ä¶ Call‚Ä¶ donaco‚Ä¶                  NA   \n## # ‚Ñπ abbreviated name: ¬π‚ÄãBrain_Size_Species_Mean\n## # ‚Ñπ 39 more variables: Brain_Size_Female_Mean &lt;dbl&gt;, Brain_size_Ref &lt;chr&gt;,\n## #   Body_mass_male_mean &lt;dbl&gt;, Body_mass_female_mean &lt;dbl&gt;,\n## #   Mass_Dimorphism &lt;dbl&gt;, Mass_Ref &lt;chr&gt;, MeanGroupSize &lt;dbl&gt;,\n## #   AdultMales &lt;dbl&gt;, AdultFemale &lt;dbl&gt;, AdultSexRatio &lt;dbl&gt;,\n## #   Social_Organization_Ref &lt;chr&gt;, InterbirthInterval_d &lt;dbl&gt;, Gestation &lt;dbl&gt;,\n## #   WeaningAge_d &lt;dbl&gt;, MaxLongevity_m &lt;dbl&gt;, LitterSz &lt;dbl&gt;, ‚Ä¶\n\n\n\n\nrename()\nThe rename() function allows us to change the names of particular columns in a data frame:\n\n# renaming columns...\ns &lt;- rename(d, Female_Mass = Body_mass_female_mean)\nhead(s$Female_Mass)\n\n## [1] 3180   84 5520 4240 4550 5350\n\n\n\n\nmutate()\nThe mutate() function allows us to add new columns to a data frame:\n\n\n\n\n\n\n\n\n\n\n\nFROM: Baumer et al.¬†(2017). Modern Data Science with R. Chapman and Hall/CRC.\n\n\n\n\n# and adding new columns...\ns &lt;- mutate(d, Binomial = paste(Genus, Species, sep = \" \"))\nhead(s$Binomial)  # or head(s[['Binomial']])\n\n## [1] \"Allenopithecus nigroviridis\" \"Allocebus trichotis\"        \n## [3] \"Alouatta belzebul\"           \"Alouatta caraya\"            \n## [5] \"Alouatta guariba\"            \"Alouatta palliata\"\n\n\n\n\nsummarize() and group_by()\nThe {dplyr} package makes it easy to summarize data using more convenient functions than the {base} function aggregate(), which we looked at in Module 09. The summarize() function specifies a list of summary variables that will appear in the output, along with the operations that will be performed on vectors in the data frame to produce those summary variables:\n\n\n\n\n\n\n\n\n\n\n\nFROM: Baumer et al.¬†(2017). Modern Data Science with R. Chapman and Hall/CRC.\n\n\n\n\n# the n() function returns the number of rows in the data frame\ns &lt;- summarize(d, n_cases = n(), avgF = mean(Body_mass_female_mean, na.rm = TRUE),\n    avgM = mean(Body_mass_male_mean, na.rm = TRUE))\ns\n\n## # A tibble: 1 √ó 3\n##   n_cases  avgF  avgM\n##     &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1     213 5396. 8112.\n\n\nAdditionally, the group_by() function allows us to construct these summary variables for sets of observations defined by a particular categorical variable, as we did above with aggregate().\n\nby_family &lt;- group_by(d, Family)\n# here, n() returns the number of rows in the group being considered\ns &lt;- summarise(by_family, n_cases = n(), avgF = mean(Body_mass_female_mean, na.rm = TRUE),\n    avgM = mean(Body_mass_male_mean, na.rm = TRUE))\ns\n\n## # A tibble: 14 √ó 4\n##    Family          n_cases   avgF   avgM\n##    &lt;chr&gt;             &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n##  1 Atelidae             12  6616.  7895.\n##  2 Cebidae              37   876.  1012.\n##  3 Cercopithecidae      79  6328.  9543.\n##  4 Cheirogalidae         7   186.   193.\n##  5 Daubentonidae         1  2490   2620 \n##  6 Galagidae             7   372.   395.\n##  7 Hominidae             6 53444. 98681.\n##  8 Hylobatidae          11  6682.  6926.\n##  9 Indriidae             9  3887.  3638.\n## 10 Lemuridae            17  1991.  2077.\n## 11 Lepilemuridae         6   814.   792 \n## 12 Lorisidae             8   490.   512.\n## 13 Pitheciidae          10  1768.  1955.\n## 14 Tarsiidae             3   120    131",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "10-module.html#other-useful-dplyr-functions",
    "href": "10-module.html#other-useful-dplyr-functions",
    "title": "10¬† Data Wrangling",
    "section": "10.4 Other Useful {dplyr} Functions",
    "text": "10.4 Other Useful {dplyr} Functions\n\nungroup() - clears group metadata from a table put in place group_by()\nbind_rows() and bind_cols() - adds rows and columns, respectively, to a dataframe or tibble; when binding rows, if the column names do not match, the column will still be added and missing values filled with NA; when binding columns, the number of rows in each dataframe needs to be the same\npull() - pulls a single variable out of a dataframe as a vector\nsample_n() - randomly samples a set of ‚Äúsize=‚Äù rows from a dataframe with (‚Äúreplace=TRUE‚Äù) or without (‚Äúreplace=FALSE‚Äù) replacement; this function is being superceded in favor of slice_sample(), where an additional argument (n= or prop=) allows you to specify the number or proportion of rows, respectively, to sample randomly\ndrop_na() - drops rows from a dataframe that have NA values for any variable names passed as arguments to the function\nrowwise() - allows you to explicitly perform functions on a data frame on a row-at-a-time basis, which is useful if a vectorized function does not exit\n\nA full list of {dplyr} functions and their descriptions is available here.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "10-module.html#joining-tables",
    "href": "10-module.html#joining-tables",
    "title": "10¬† Data Wrangling",
    "section": "10.5 Joining Tables",
    "text": "10.5 Joining Tables\nOne of the other major forms of data wrangling that we often need to do is to combine variables from different tabular data structures into a new table. This process is often referred to as performing a ‚Äúmutating join‚Äù or simply a ‚Äújoin‚Äù.\n\nNOTE: For those with experience with other database systems, it is related to the ‚ÄúJOIN‚Äù commands in SQL.\n\nThe process works by matching observations in two different tables by a common key variable and then selecting additional variables of interest to pull from each of the tables. A simple example is the following‚Ä¶ suppose we have two tables, one that contains average brain sizes for particular species of primates and one that contains individual body sizes for some of the same species, plus others. In the latter table, too, we may have data from multiple individuals of the same species represented.\n\ntable1 &lt;- tibble(Taxon = c(\"Gorilla\", \"Human\", \"Chimpanzee\", \"Orangutan\", \"Baboon\"),\n    Avg_Brain_Size = c(470, 1100, 350, 340, 140))\ntable1\n\n## # A tibble: 5 √ó 2\n##   Taxon      Avg_Brain_Size\n##   &lt;chr&gt;               &lt;dbl&gt;\n## 1 Gorilla               470\n## 2 Human                1100\n## 3 Chimpanzee            350\n## 4 Orangutan             340\n## 5 Baboon                140\n\ntable2 &lt;- tibble(Taxon = c(\"Gorilla\", \"Gorilla\", \"Gorilla\", \"Human\", \"Human\", \"Chimpanzee\",\n    \"Orangutan\", \"Orangutan\", \"Macaque\", \"Macaque\", \"Macaque\"), Body_Weight = c(80,\n    81, 77, 48, 49, 38, 37, 36, 6, 7, 6))\ntable2\n\n## # A tibble: 11 √ó 2\n##    Taxon      Body_Weight\n##    &lt;chr&gt;            &lt;dbl&gt;\n##  1 Gorilla             80\n##  2 Gorilla             81\n##  3 Gorilla             77\n##  4 Human               48\n##  5 Human               49\n##  6 Chimpanzee          38\n##  7 Orangutan           37\n##  8 Orangutan           36\n##  9 Macaque              6\n## 10 Macaque              7\n## 11 Macaque              6\n\n\n\nInner Joins\nAn inner join or equijoin matches up sets of observations between two tables whenever their keys are equal. The output of an inner join is a new data frame that contains all rows from the left-hand (x) and right-hand (y) tables where there are matching values in the key column, plus all columns from x and y. If there are multiple matches between the tables, all combination of the matches are returned. This is represented schematically below:\n\n\n\n\n\n\n\n\n\n\n\nFROM: Wickham & Grolemund (2017). R for Data Science. O‚ÄôReilly Media, Inc.\n\n\n\n\ninner_join(table1, table2, by = \"Taxon\")\n\n## # A tibble: 8 √ó 3\n##   Taxon      Avg_Brain_Size Body_Weight\n##   &lt;chr&gt;               &lt;dbl&gt;       &lt;dbl&gt;\n## 1 Gorilla               470          80\n## 2 Gorilla               470          81\n## 3 Gorilla               470          77\n## 4 Human                1100          48\n## 5 Human                1100          49\n## 6 Chimpanzee            350          38\n## 7 Orangutan             340          37\n## 8 Orangutan             340          36\n\n\n\n\nOuter Joins\nWhile an inner join keeps only observations that appear in both tables, different flavors of outer joins keep observations that appear in at least one of the tables. There are three types of outer joins:\n\nA left join returns all rows from the left-hand table, x, and all columns from x and y. Rows in x with no match in y will have NA values in the new columns from the y table. If there are multiple matches between x and *y, all combinations of the matches are returned.\nA right join returns all rows from the right-hand table, y, and all columns from x and y. Rows in y with no match in x will have NA values in the new columns from the x table. If there are multiple matches between x and y, all combinations of the matches are returned.\nA full join returns all rows and all columns in both the left-hand (x) and right-hand (y) tables, joining them where there are matches. Where there are not matching values, the join returns NA for the columns from table where they are missing.\n\nThe following figure shows a schematic representation of these various types of outer joins:\n\n\n\n\n\n\n\n\n\n\n\nFROM: Wickham & Grolemund (2017). R for Data Science. O‚ÄôReilly Media, Inc.\n\n\n\n\nleft_join(table1, table2, by = \"Taxon\")\n\n## # A tibble: 9 √ó 3\n##   Taxon      Avg_Brain_Size Body_Weight\n##   &lt;chr&gt;               &lt;dbl&gt;       &lt;dbl&gt;\n## 1 Gorilla               470          80\n## 2 Gorilla               470          81\n## 3 Gorilla               470          77\n## 4 Human                1100          48\n## 5 Human                1100          49\n## 6 Chimpanzee            350          38\n## 7 Orangutan             340          37\n## 8 Orangutan             340          36\n## 9 Baboon                140          NA\n\nright_join(table1, table2, by = \"Taxon\")\n\n## # A tibble: 11 √ó 3\n##    Taxon      Avg_Brain_Size Body_Weight\n##    &lt;chr&gt;               &lt;dbl&gt;       &lt;dbl&gt;\n##  1 Gorilla               470          80\n##  2 Gorilla               470          81\n##  3 Gorilla               470          77\n##  4 Human                1100          48\n##  5 Human                1100          49\n##  6 Chimpanzee            350          38\n##  7 Orangutan             340          37\n##  8 Orangutan             340          36\n##  9 Macaque                NA           6\n## 10 Macaque                NA           7\n## 11 Macaque                NA           6\n\nfull_join(table1, table2, by = \"Taxon\")\n\n## # A tibble: 12 √ó 3\n##    Taxon      Avg_Brain_Size Body_Weight\n##    &lt;chr&gt;               &lt;dbl&gt;       &lt;dbl&gt;\n##  1 Gorilla               470          80\n##  2 Gorilla               470          81\n##  3 Gorilla               470          77\n##  4 Human                1100          48\n##  5 Human                1100          49\n##  6 Chimpanzee            350          38\n##  7 Orangutan             340          37\n##  8 Orangutan             340          36\n##  9 Baboon                140          NA\n## 10 Macaque                NA           6\n## 11 Macaque                NA           7\n## 12 Macaque                NA           6\n\n\n\n\nOther Joins\nThere are also two additional join types that may be sometimes be useful‚Ä¶ note that these joins only return columns from the left-hand table, x.\n\nA semi_join returns rows from the left-hand table, x, where there are matching values in y, but keeping just the columns from x. A semi_join differs from an inner_join because an inner_join will return a row of x for every matching row of y (so some x rows can be duplicated), whereas a semi_join will never duplicate rows of x.\nAn anti_join returns all rows from the left-hand table,x where there are not matching values in y, keeping just the columns from x.\n\n\nsemi_join(table1, table2, by = \"Taxon\")\n\n## # A tibble: 4 √ó 2\n##   Taxon      Avg_Brain_Size\n##   &lt;chr&gt;               &lt;dbl&gt;\n## 1 Gorilla               470\n## 2 Human                1100\n## 3 Chimpanzee            350\n## 4 Orangutan             340\n\nanti_join(table1, table2, by = \"Taxon\")\n\n## # A tibble: 1 √ó 2\n##   Taxon  Avg_Brain_Size\n##   &lt;chr&gt;           &lt;dbl&gt;\n## 1 Baboon            140\n\n\nThe cheatsheet on Data Transformation with {dplyr} provides a nice overview of these and additional data wrangling functions included the {dplyr} package.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "10-module.html#chaining-and-piping",
    "href": "10-module.html#chaining-and-piping",
    "title": "10¬† Data Wrangling",
    "section": "10.6 Chaining and Piping",
    "text": "10.6 Chaining and Piping\nOne other cool thing about the {dplyr} package is that it provides a convenient way to chain together operations on a data frame using the ‚Äúforward pipe‚Äù operator (%&gt;%). The %&gt;% operator basically takes what is on the left-hand side (LHS) of the operator and directly applies the function call on the right-hand side (RHS) of the operator to it. That is, it ‚Äúpipes‚Äù what is on the LHS of the operator directly to the first argument of the function on the right. This process allows us to build of chains of successive operations, each one being applied to the outcome of the previous operation in the chain.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEXAMPLE:\n\n# this...\nd %&gt;%\n    select(Scientific_Name, Body_mass_female_mean) %&gt;%\n    head()\n\n## # A tibble: 6 √ó 2\n##   Scientific_Name             Body_mass_female_mean\n##   &lt;chr&gt;                                       &lt;dbl&gt;\n## 1 Allenopithecus_nigroviridis                  3180\n## 2 Allocebus_trichotis                            84\n## 3 Alouatta_belzebul                            5520\n## 4 Alouatta_caraya                              4240\n## 5 Alouatta_guariba                             4550\n## 6 Alouatta_palliata                            5350\n\n# is equivalent to...\nhead(select(d, Scientific_Name, Body_mass_female_mean))\n\n## # A tibble: 6 √ó 2\n##   Scientific_Name             Body_mass_female_mean\n##   &lt;chr&gt;                                       &lt;dbl&gt;\n## 1 Allenopithecus_nigroviridis                  3180\n## 2 Allocebus_trichotis                            84\n## 3 Alouatta_belzebul                            5520\n## 4 Alouatta_caraya                              4240\n## 5 Alouatta_guariba                             4550\n## 6 Alouatta_palliata                            5350\n\n\nThe forward pipe is useful because it allows us to write and follow code from left to right (as when writing in English), instead of right to left with many nested parentheses.\n\nCHALLENGE\n\nIn one line of code, do the following:\n\nAdd a variable, Binomial to our data frame d, which is a concatenation of the Genus and Species‚Ä¶\nTrim the data frame to only include the variables Binomial, Family, Body_mass_female_mean, Body_mass_male_mean and Mass_Dimorphism‚Ä¶\nGroup these variables by Family‚Ä¶\nCalculate the average value for female body mass, male body mass, and mass dimorphism (remember, you will need to specify na.rm = TRUE‚Ä¶)\nAnd arrange by decreasing average mass dimorphism.\n\n\n\n\nShow Code\ns &lt;- mutate(d, Binomial = paste(Genus, Species, sep = \" \")) %&gt;%\n    select(Binomial, Family, Body_mass_female_mean, Body_mass_male_mean, Mass_Dimorphism) %&gt;%\n    group_by(Family) %&gt;%\n    summarise(avgF = mean(Body_mass_female_mean, na.rm = TRUE), avgM = mean(Body_mass_male_mean,\n        na.rm = TRUE), avgBMD = mean(Mass_Dimorphism, na.rm = TRUE)) %&gt;%\n    arrange(desc(avgBMD))\ns\n\n\nShow Output\n## # A tibble: 14 √ó 4\n##    Family            avgF   avgM avgBMD\n##    &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n##  1 Hominidae       53444. 98681.  1.81 \n##  2 Cercopithecidae  6328.  9543.  1.49 \n##  3 Atelidae         6616.  7895.  1.23 \n##  4 Tarsiidae         120    131   1.09 \n##  5 Pitheciidae      1768.  1955.  1.09 \n##  6 Cebidae           876.  1012.  1.07 \n##  7 Lemuridae        1991.  2077.  1.06 \n##  8 Daubentonidae    2490   2620   1.05 \n##  9 Lorisidae         490.   512.  1.05 \n## 10 Galagidae         372.   395.  1.05 \n## 11 Hylobatidae      6682.  6926.  1.03 \n## 12 Cheirogalidae     186.   193.  1.02 \n## 13 Lepilemuridae     814.   792   0.980\n## 14 Indriidae        3887.  3638.  0.950\n\n\n\nThere are several other, very cool, ‚Äúspecial case‚Äù pipe operators that are useful in particular situations. These are available from the {magrittr} package. [Actually, the functionality of the forward pipe operator also comes from the {magrittr} package, but it is replicated in {dplyr}.]\n\nThe ‚Äútee‚Äù pipe (%T&gt;%) allows you to pipe the outcome of a process into a new expression (just like the forward pipe operator does) and to simultaneously return the original value instead of the forward-piped result to an intermediate expression. This is useful, for example, for printing out or plotting intermediate results. In the example below, we filter our data frame for just observations of the genus Alouatta, print those to the screen as an intermediate side effect, and pass the filtered data to the summarise() function.\n\n\ns &lt;- filter(d, Genus == \"Alouatta\") %T&gt;%\n    print() %&gt;%\n    summarise(avgF = mean(Body_mass_female_mean, na.rm = TRUE))\n\n## # A tibble: 6 √ó 45\n##   Scientific_Name    Superfamily Family   Genus   Species Brain_Size_Species_M‚Ä¶¬π\n##   &lt;chr&gt;              &lt;chr&gt;       &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;                    &lt;dbl&gt;\n## 1 Alouatta_belzebul  Ceboidea    Atelidae Alouat‚Ä¶ belzeb‚Ä¶                   52.8\n## 2 Alouatta_caraya    Ceboidea    Atelidae Alouat‚Ä¶ caraya                    52.6\n## 3 Alouatta_guariba   Ceboidea    Atelidae Alouat‚Ä¶ guariba                   51.7\n## 4 Alouatta_palliata  Ceboidea    Atelidae Alouat‚Ä¶ pallia‚Ä¶                   49.9\n## 5 Alouatta_pigra     Ceboidea    Atelidae Alouat‚Ä¶ pigra                     51.1\n## 6 Alouatta_seniculus Ceboidea    Atelidae Alouat‚Ä¶ senicu‚Ä¶                   55.2\n## # ‚Ñπ abbreviated name: ¬π‚ÄãBrain_Size_Species_Mean\n## # ‚Ñπ 39 more variables: Brain_Size_Female_Mean &lt;dbl&gt;, Brain_size_Ref &lt;chr&gt;,\n## #   Body_mass_male_mean &lt;dbl&gt;, Body_mass_female_mean &lt;dbl&gt;,\n## #   Mass_Dimorphism &lt;dbl&gt;, Mass_Ref &lt;chr&gt;, MeanGroupSize &lt;dbl&gt;,\n## #   AdultMales &lt;dbl&gt;, AdultFemale &lt;dbl&gt;, AdultSexRatio &lt;dbl&gt;,\n## #   Social_Organization_Ref &lt;chr&gt;, InterbirthInterval_d &lt;dbl&gt;, Gestation &lt;dbl&gt;,\n## #   WeaningAge_d &lt;dbl&gt;, MaxLongevity_m &lt;dbl&gt;, LitterSz &lt;dbl&gt;, ‚Ä¶\n\ns\n\n## # A tibble: 1 √ó 1\n##    avgF\n##   &lt;dbl&gt;\n## 1 5217.\n\n\n\nThe ‚Äúassignment‚Äù pipe (%&lt;&gt;%) evaluates the expression on the right-hand side of the pipe operator and reassigns the resultant value to the left-hand side.\n\n\ns &lt;- filter(d, Genus == \"Alouatta\")\ns %&lt;&gt;%\n    select(Genus, Species)\ns\n\n## # A tibble: 6 √ó 2\n##   Genus    Species  \n##   &lt;chr&gt;    &lt;chr&gt;    \n## 1 Alouatta belzebul \n## 2 Alouatta caraya   \n## 3 Alouatta guariba  \n## 4 Alouatta palliata \n## 5 Alouatta pigra    \n## 6 Alouatta seniculus\n\n\n\nFinally, the ‚Äúexposition‚Äù pipe (%$%) exposes the names within the object on the left-hand side of the pipe to the right-hand side expression.\n\n\ns &lt;- filter(d, Genus == \"Alouatta\") %$%\n    paste0(Genus, \" \", Species)\ns\n\n## [1] \"Alouatta belzebul\"  \"Alouatta caraya\"    \"Alouatta guariba\"  \n## [4] \"Alouatta palliata\"  \"Alouatta pigra\"     \"Alouatta seniculus\"",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "10-module.html#dot-syntax-and-the-pipe",
    "href": "10-module.html#dot-syntax-and-the-pipe",
    "title": "10¬† Data Wrangling",
    "section": "10.7 Dot Syntax and the Pipe",
    "text": "10.7 Dot Syntax and the Pipe\nNormally, when we use the forward pipe operator, the LHS of the operator is passed to the first argument of the function on the RHS. Thus, the following are all equivalent:\n\ns &lt;- filter(d, Genus == \"Alouatta\")\ns &lt;- d %&gt;%\n    filter(Genus == \"Alouatta\")\nd %&gt;%\n    filter(Genus == \"Alouatta\") -&gt; s\n\nThe behavior of the forward pipe operator means we can use it do something like the following, where d is implicitly piped into the data argument for ggplot()\n\nd %&gt;%\n    ggplot(aes(x = log(Body_mass_female_mean), y = log(Brain_Size_Species_Mean))) +\n    geom_point()\n\n\n\n\n\n\n\n\nWe can also use dot (.) syntax with the {magrittr} forward pipe operator to pass the LHS of a statement to somewhere other than the first argument of the function on the RHS. Thus‚Ä¶\ny %&gt;% function(x, .) is equivalent to function(x, y)\n‚Ä¶ which means we can do something like this to pipe d into a function such as lm() (‚Äúlinear model‚Äù), where the data frame that the function is run on is not the first argument:\n\nd %&gt;%\n    lm(log(Body_mass_female_mean) ~ log(Brain_Size_Species_Mean), data = .)\n\n## \n## Call:\n## lm(formula = log(Body_mass_female_mean) ~ log(Brain_Size_Species_Mean), \n##     data = .)\n## \n## Coefficients:\n##                  (Intercept)  log(Brain_Size_Species_Mean)  \n##                        3.713                         1.135\n\n\nWe can also use the {magrittr} pipe‚Äôs curly brace ({}) syntax to wrap the RHS of a statement and pass the LHS into several places:\n\nd %&gt;%\n    {\n        plot(log(.$Brain_Size_Species_Mean), log(.$Body_mass_female_mean))\n    }\n\n\n\n\n\n\n\n\n\nNOTE: R recently introduced a ‚Äúnative‚Äù pipe operator |&gt; into its {base} syntax. It behaves very similarly to the {magrittr} forward pipe, but does not support dot syntax the same way. It also requires an explicit functions call on the RHS, which means appending () to the end of the function name, rather than just using the name. The first version of the line below using %&gt;% could be used to take the log of all Brain_Size_Female_Mean values in d, while the second version, using |&gt;, needs to have () appended to the functions calls to work properly.\n\n\nd %&gt;%\n    select(Brain_Size_Female_Mean) %&gt;%\n    log %&gt;%\n    head\n\n## # A tibble: 6 √ó 1\n##   Brain_Size_Female_Mean\n##                    &lt;dbl&gt;\n## 1                   3.98\n## 2                  NA   \n## 3                   3.94\n## 4                   3.87\n## 5                   3.89\n## 6                   3.87\n\nd |&gt;\n    select(Brain_Size_Female_Mean) |&gt;\n    log() |&gt;\n    head()\n\n## # A tibble: 6 √ó 1\n##   Brain_Size_Female_Mean\n##                    &lt;dbl&gt;\n## 1                   3.98\n## 2                  NA   \n## 3                   3.94\n## 4                   3.87\n## 5                   3.89\n## 6                   3.87",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "10-module.html#useful-related-packages",
    "href": "10-module.html#useful-related-packages",
    "title": "10¬† Data Wrangling",
    "section": "10.8 Useful Related Packages",
    "text": "10.8 Useful Related Packages\nThe {tidylog} package provides wrappers around many {dplyr} and {tidyr} package functions that provide logged feedback on the outcome of those functions, which can be useful for understanding the effects of whatever data wrangling processes we run. For example, running filter() will provide feedback on the number of runs removed and kept as part of a filtering operation‚Ä¶\n\nlibrary(tidylog)\n\n## \n## Attaching package: 'tidylog'\n\n\n## The following objects are masked from 'package:dplyr':\n## \n##     add_count, add_tally, anti_join, count, distinct, distinct_all,\n##     distinct_at, distinct_if, filter, filter_all, filter_at, filter_if,\n##     full_join, group_by, group_by_all, group_by_at, group_by_if,\n##     inner_join, left_join, mutate, mutate_all, mutate_at, mutate_if,\n##     relocate, rename, rename_all, rename_at, rename_if, rename_with,\n##     right_join, sample_frac, sample_n, select, select_all, select_at,\n##     select_if, semi_join, slice, slice_head, slice_max, slice_min,\n##     slice_sample, slice_tail, summarise, summarise_all, summarise_at,\n##     summarise_if, summarize, summarize_all, summarize_at, summarize_if,\n##     tally, top_frac, top_n, transmute, transmute_all, transmute_at,\n##     transmute_if, ungroup\n\n\n## The following objects are masked from 'package:tidyr':\n## \n##     drop_na, fill, gather, pivot_longer, pivot_wider, replace_na,\n##     separate_wider_delim, separate_wider_position,\n##     separate_wider_regex, spread, uncount\n\n\n## The following object is masked from 'package:stats':\n## \n##     filter\n\n# compare...\ns &lt;- dplyr::filter(d, Family == \"Hominidae\" & Mass_Dimorphism &gt; 2)\n# to...\ns &lt;- filter(d, Family == \"Hominidae\" & Mass_Dimorphism &gt; 2)\n\n## filter: removed 210 rows (99%), 3 rows remaining\n\n\nSimilarly, running sample_n() will give us logged output on that random record sampling process‚Ä¶\n\n# compare...\ns &lt;- dplyr::sample_n(d, size = 100, replace = FALSE)\n# to...\ns &lt;- sample_n(d, size = 100, replace = FALSE)\n\n## sample_n: removed 113 rows (53%), 100 rows remaining\n\ndetach(package:tidylog)\n\n\nNOTE: Loading in {tidylog} function will conflict with or ‚Äúmask‚Äù corresponding function names from {dplyr} and {tidyr}. In the examples above, then, to run the {tidyverse} versions of filter() and select_n(), it was necessary to use the :: notation to specifically call the {dplyr} version of the functions directly. The {tidylog} versions of these functions run a bit more slowly, so if speed is important, you may not want to use {tidylog}, you may want to call the {dplyr} or {tidyr} functions explicitly after loading {tidylog}, or you may want to simply call the {tidylog} version of a function explicitly.\n\n\ns &lt;- tidylog::filter(d, Family == \"Hominidae\" & Mass_Dimorphism &gt; 2)\n\n## filter: removed 210 rows (99%), 3 rows remaining\n\n\n\n# | include: false\ndetach(package:magrittr)\ndetach(package:tidyverse)",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "10-module.html#concept-review",
    "href": "10-module.html#concept-review",
    "title": "10¬† Data Wrangling",
    "section": "Concept Review",
    "text": "Concept Review\n\nUsing {dplyr}: select(), filter(), arrange(), rename(), mutate(), summarise(), group_by()\nChaining and piping with %&gt;% and other pipe operators\nJoining tables: inner_join(), left_join(), right_join(), full_join()\nUsing the {tidylog} package",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "11-module.html",
    "href": "11-module.html",
    "title": "11¬† Functions and Flow Control",
    "section": "",
    "text": "11.1 Objectives",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Functions and Flow Control</span>"
    ]
  },
  {
    "objectID": "11-module.html#objectives",
    "href": "11-module.html#objectives",
    "title": "11¬† Functions and Flow Control",
    "section": "",
    "text": "The objective of this module to become familiar with how some additional basic programming concepts are implemented in R.",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Functions and Flow Control</span>"
    ]
  },
  {
    "objectID": "11-module.html#preliminaries",
    "href": "11-module.html#preliminaries",
    "title": "11¬† Functions and Flow Control",
    "section": "11.2 Preliminaries",
    "text": "11.2 Preliminaries\n\nInstall this package in R: {sjmisc}\nLoad {tidyverse}",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Functions and Flow Control</span>"
    ]
  },
  {
    "objectID": "11-module.html#functions",
    "href": "11-module.html#functions",
    "title": "11¬† Functions and Flow Control",
    "section": "11.3 Functions",
    "text": "11.3 Functions\nOne of the strengths of using a programming language like R for data manipulation and analysis is that we can write our own functions to things we need to do in a particular way, e.g., to create a custom analysis or visualization. In Module 04 we practiced writing our own simple function, and we will revisit that here.\nRecall that the general format for a function is as follows:\nfunction_name &lt;- function(&lt;argument list&gt;) {\n  &lt;function body&gt;\n}\nFunctions that we define ourselves can have multiple arguments, and each argument can have a default value. Arguments are separated by commas and default values are specified in the list of arguments. For example, suppose we wanted to make a function that added a user-specified prefix to every entry in a particular named variable in a data frame, we could write the following:\n\nadd_prefix &lt;- function(df, prefix = \"\", variable) {\n    df[[variable]] &lt;- paste0(prefix, df[[variable]])\n    return(df)\n}\n\nmy_data &lt;- data.frame(name = c(\"Ned\", \"Sansa\", \"Cersei\", \"Tyrion\", \"Jon\", \"Daenerys\",\n    \"Aria\", \"Brienne\", \"Rickon\", \"Edmure\", \"Petyr\", \"Jamie\", \"Robert\", \"Stannis\",\n    \"Theon\"), house = c(\"Stark\", \"Stark\", \"Lannister\", \"Lannister\", \"Stark\", \"Targaryen\",\n    \"Stark\", \"Tarth\", \"Stark\", \"Tully\", \"Baelish\", \"Lannister\", \"Baratheon\", \"Baratheon\",\n    \"Greyjoy\"), code = sample(1e+05:999999, 15, replace = FALSE))\n\ndf &lt;- add_prefix(my_data, variable = \"house\")  # uses default prefix\nhead(df)\n\n##       name     house   code\n## 1      Ned     Stark 364806\n## 2    Sansa     Stark 833932\n## 3   Cersei Lannister 326416\n## 4   Tyrion Lannister 172650\n## 5      Jon     Stark 670343\n## 6 Daenerys Targaryen 261323\n\ndf &lt;- add_prefix(my_data, prefix = \"House \", variable = \"house\")\nhead(df)\n\n##       name           house   code\n## 1      Ned     House Stark 364806\n## 2    Sansa     House Stark 833932\n## 3   Cersei House Lannister 326416\n## 4   Tyrion House Lannister 172650\n## 5      Jon     House Stark 670343\n## 6 Daenerys House Targaryen 261323\n\ndf &lt;- add_prefix(my_data, prefix = \"00001-\", variable = \"code\")\nhead(df)\n\n##       name     house         code\n## 1      Ned     Stark 00001-364806\n## 2    Sansa     Stark 00001-833932\n## 3   Cersei Lannister 00001-326416\n## 4   Tyrion Lannister 00001-172650\n## 5      Jon     Stark 00001-670343\n## 6 Daenerys Targaryen 00001-261323\n\n\n\nNOTE: Arguments can be passed to a function in any order, as long as the argument name is included. For example, for the add_prefix() function above, the following are equivalent:\n\n\nhead(add_prefix(df = my_data, variable = \"house\"))\n\n##       name     house   code\n## 1      Ned     Stark 364806\n## 2    Sansa     Stark 833932\n## 3   Cersei Lannister 326416\n## 4   Tyrion Lannister 172650\n## 5      Jon     Stark 670343\n## 6 Daenerys Targaryen 261323\n\n# versus...\nhead(add_prefix(variable = \"house\", df = my_data))\n\n##       name     house   code\n## 1      Ned     Stark 364806\n## 2    Sansa     Stark 833932\n## 3   Cersei Lannister 326416\n## 4   Tyrion Lannister 172650\n## 5      Jon     Stark 670343\n## 6 Daenerys Targaryen 261323\n\n\nNote that in the example above, because the argument name prefix was excluded, the default value was used.\nR also uses positional matching to assign values to arguments when argument names are excluded. Note the difference in the results of these lines:\n\nhead(add_prefix(my_data, \"00001-\", \"code\"))\n\n##       name     house         code\n## 1      Ned     Stark 00001-364806\n## 2    Sansa     Stark 00001-833932\n## 3   Cersei Lannister 00001-326416\n## 4   Tyrion Lannister 00001-172650\n## 5      Jon     Stark 00001-670343\n## 6 Daenerys Targaryen 00001-261323\n\n# versus...\nhead(add_prefix(my_data, \"House \", \"house\"))\n\n##       name           house   code\n## 1      Ned     House Stark 364806\n## 2    Sansa     House Stark 833932\n## 3   Cersei House Lannister 326416\n## 4   Tyrion House Lannister 172650\n## 5      Jon     House Stark 670343\n## 6 Daenerys House Targaryen 261323\n\n# versus...\nhead(add_prefix(my_data, \"\", \"house\"))\n\n##       name     house   code\n## 1      Ned     Stark 364806\n## 2    Sansa     Stark 833932\n## 3   Cersei Lannister 326416\n## 4   Tyrion Lannister 172650\n## 5      Jon     Stark 670343\n## 6 Daenerys Targaryen 261323\n\n\nIf we try to run the line below, however, it throws an error because too few (unnamed) arguments are passed to the function for it to be able to disambiguate them:\n\nhead(add_prefix(my_data, \"00001-\"))\n\nUnless otherwise specified, functions return the result of the last expression evaluated in the function body. However, is good programming practice to explicitly specify the object or value you want returned from the function with return(&lt;value&gt;) or return(&lt;object&gt;).",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Functions and Flow Control</span>"
    ]
  },
  {
    "objectID": "11-module.html#conditional-expressions",
    "href": "11-module.html#conditional-expressions",
    "title": "11¬† Functions and Flow Control",
    "section": "11.4 Conditional Expressions",
    "text": "11.4 Conditional Expressions\nConditional expressions are a basic feature of any programming language. They are used for ‚Äúflow control‚Äù, i.e., to structure what your program does when. The most common conditional expression is the ‚Äúif‚Ä¶ else‚Ä¶‚Äù statement, which is used to direct flow of a program between two paths. The general form is:\nif (&lt;test&gt;) {\n  &lt;action 1&gt;\n} else {\n  &lt;action 2&gt;\n}\nAs an example‚Ä¶\n\ni &lt;- TRUE\n\nif (i == TRUE) {\n    print(\"Yes\")\n} else {\n    print(\"No\")\n}\n\n## [1] \"Yes\"\n\ni &lt;- FALSE\n\nif (i == TRUE) {\n    print(\"Yes\")\n} else {\n    print(\"No\")\n}\n\n## [1] \"No\"\n\n\nA related form is the ifelse() function, which has three arguments: the test condition, the value to be returned or expression to be run if the test condition is true, and the value to be returned or expression to be run if the test condition is false. Unlike the ‚Äúif‚Ä¶ else‚Ä¶‚Äù formulation, the ifelse() function can work on a vector, too, and returns a vector.\n\ni &lt;- 9\nifelse(i &lt;= 10, \"Yes\", \"No\")\n\n## [1] \"Yes\"\n\ni &lt;- 11\nifelse(i &lt;= 10, \"Yes\", \"No\")\n\n## [1] \"No\"\n\ni &lt;- c(9, 10, 11)\nifelse(i &lt;= 10, \"Yes\", \"No\")\n\n## [1] \"Yes\" \"Yes\" \"No\"\n\n\n\nNOTE: There is also a {dplyr} version of the ‚Äúif‚Ä¶ else‚Ä¶‚Äù conditional: if_else(). I tend to use this one much more than the {base} R version.\n\nThe function case_when() is the equivalent of mixing several ‚Äúif‚Ä¶ else‚Ä¶‚Äù statements.\n\ni &lt;- 1:10\noutput &lt;- case_when(i &lt;= 3 ~ \"small\", i &lt;= 7 ~ \"medium\", i &lt;= 10 ~ \"large\")\noutput\n\n##  [1] \"small\"  \"small\"  \"small\"  \"medium\" \"medium\" \"medium\" \"medium\" \"large\" \n##  [9] \"large\"  \"large\"\n\n\nFor conditional statements, there are two additional functions that are often useful: any() and all(). The any() function takes a vector of logical values and returns TRUE if any of the elements is TRUE, while the all() function takes a vector of logical values and returns TRUE if all elements are TRUE.\n\ni &lt;- c(9, 10, 11)\nany(i &lt;= 10)\n\n## [1] TRUE\n\nall(i &lt;= 10)\n\n## [1] FALSE",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Functions and Flow Control</span>"
    ]
  },
  {
    "objectID": "11-module.html#relational-operators",
    "href": "11-module.html#relational-operators",
    "title": "11¬† Functions and Flow Control",
    "section": "11.5 Relational Operators",
    "text": "11.5 Relational Operators\nThe following relational operators are often used in conditional expressions:\n\nless than, greater than: &lt;, &gt;\nless than or equal to, greater than or equal to: &lt;=, &gt;=\nequal to: == NOTE: This uses a double equal sign!\nnot equal to: !=",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Functions and Flow Control</span>"
    ]
  },
  {
    "objectID": "11-module.html#logical-and-other-operators",
    "href": "11-module.html#logical-and-other-operators",
    "title": "11¬† Functions and Flow Control",
    "section": "11.6 Logical and Other Operators",
    "text": "11.6 Logical and Other Operators\nThe following additional operators are also useful and important:\n\n!: logical NOT Note that this operator can be applied to values and to functions\n&: element-wise logical AND (applies to element in a vector)\n&&: logical AND (applies to single conditions or first element in vector)\n|: element-wise logical OR (applies to element in a vector)\n||: logical OR (applies to single conditions or first element in vector)\n%in%: tests for membership in a vector\nthe {sjmisc} package adds a ‚Äúnot in‚Äù operator to test for membership in a vector: %nin%\n\nExamples:\n\ni &lt;- c(9, 10, 11)\n!any(i &lt;= 10)  # logical NOT\n\n## [1] FALSE\n\n!all(i &lt;= 10)  # logical NOT\n\n## [1] TRUE\n\n\n\ni &lt;- 1:20\ni &lt; 12 & (i%%3) == 0  # element-wise logical AND\n\n##  [1] FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE\n## [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\n\ni[1] &lt; 12 && (i[1]%%3) == 0  # logical AND\n\n## [1] FALSE\n\n\n\ni &gt; 10 | (i%%2) == 0  # element-wise logical OR\n\n##  [1] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE\n## [13]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n\n\n\ni[1] &gt; 10 || (i[1]%%2) == 0  # logical OR\n\n## [1] FALSE\n\n\n\na &lt;- c(\"There\", \"is\", \"grandeur\", \"in\", \"this\", \"view\", \"of\", \"life\")\nb &lt;- \"grandeur\"\nb %in% a  # membership\n\n## [1] TRUE\n\nb &lt;- c(\"selection\", \"life\")\nb %in% a\n\n## [1] FALSE  TRUE\n\n\n\nlibrary(sjmisc)\nb %nin% a  # not membership\n\n## [1]  TRUE FALSE\n\ndetach(package:sjmisc)",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Functions and Flow Control</span>"
    ]
  },
  {
    "objectID": "11-module.html#iterating-with-loops",
    "href": "11-module.html#iterating-with-loops",
    "title": "11¬† Functions and Flow Control",
    "section": "11.7 Iterating with Loops",
    "text": "11.7 Iterating with Loops\n\nfor() Loops\nWhen we want to execute a particular piece of code multiple times, for example to iterate over a set of values or to apply the same function to a set of elements in a vector, one way (but not the only way!) to do this is with a loop. There are several different constructions we can use for looping, one of the most common of which is a for() loop. The basic contruction for a for() loop is:\nfor (&lt;index&gt; in &lt;range&gt;){\n  &lt;code to execute&gt;\n}\nThe following examples print out each element in a vector, v:\n\nv &lt;- seq(from = 100, to = 120, by = 2)\nfor (i in 1:length(v)) {\n    # here, we are looping over the indices of v\n    print(v[i])\n}\n\n## [1] 100\n## [1] 102\n## [1] 104\n## [1] 106\n## [1] 108\n## [1] 110\n## [1] 112\n## [1] 114\n## [1] 116\n## [1] 118\n## [1] 120\n\nfor (i in seq_along(v)) {\n    # seq_along() also loops over the indices of v\n    print(v[i])\n}\n\n## [1] 100\n## [1] 102\n## [1] 104\n## [1] 106\n## [1] 108\n## [1] 110\n## [1] 112\n## [1] 114\n## [1] 116\n## [1] 118\n## [1] 120\n\nfor (i in v) {\n    # here we loop over the elements of v\n    print(i)\n}\n\n## [1] 100\n## [1] 102\n## [1] 104\n## [1] 106\n## [1] 108\n## [1] 110\n## [1] 112\n## [1] 114\n## [1] 116\n## [1] 118\n## [1] 120\n\n\nIt is good form and improves efficiency if we allocate memory to whatever output we may want to generate inside of a loop beforehand. For example, if we want to use a loop to calculate the median() across all Platyrrhine primate genera for the female brain size, female body size, and canine dimorphism variables in the Kamilar and Cooper dataset we have used previously, we could do the following:\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/KamilarAndCooperData.csv\"\nd &lt;- read_csv(f, col_names = TRUE)  # creates a 'tibble'\ns &lt;- filter(d, Family %in% c(\"Atelidae\", \"Pitheciidae\", \"Cebidae\")) %&gt;%\n    select(Brain_Size_Female_Mean, Body_mass_female_mean, Canine_Dimorphism)\n# good practice\noutput &lt;- vector(\"double\", ncol(s))\nfor (i in seq_along(s)) {\n    output[[i]] &lt;- median(s[[i]], na.rm = TRUE)\n}\noutput\n\n## [1]   29.0500 1030.0000    1.2955\n\n\nThe following is another very common way to do this, but is less efficient as we are rewriting the object output (and making it one element longer) in every iteration of the loop.\n\n# not so good practice\noutput &lt;- vector()\nfor (i in seq_along(s)) {\n    output &lt;- c(output, median(s[[i]], na.rm = TRUE))\n}\noutput\n\n## [1]   29.0500 1030.0000    1.2955\n\n\n\n\nCHALLENGE\nWrite a for() loop to print out each row in the data frame my_data.\n\n\nShow Code\nfor (i in 1:nrow(my_data)) {\n    print(my_data[i, ])\n}\n\n\nShow Output\n##   name house   code\n## 1  Ned Stark 364806\n##    name house   code\n## 2 Sansa Stark 833932\n##     name     house   code\n## 3 Cersei Lannister 326416\n##     name     house   code\n## 4 Tyrion Lannister 172650\n##   name house   code\n## 5  Jon Stark 670343\n##       name     house   code\n## 6 Daenerys Targaryen 261323\n##   name house   code\n## 7 Aria Stark 618373\n##      name house   code\n## 8 Brienne Tarth 347418\n##     name house   code\n## 9 Rickon Stark 915720\n##      name house   code\n## 10 Edmure Tully 814126\n##     name   house   code\n## 11 Petyr Baelish 777117\n##     name     house   code\n## 12 Jamie Lannister 557903\n##      name     house   code\n## 13 Robert Baratheon 499862\n##       name     house   code\n## 14 Stannis Baratheon 697960\n##     name   house   code\n## 15 Theon Greyjoy 476711\n\n\n\nWrite a for() loop to print out the reverse of each element in the code vector in the data frame my_data.\n\nHINT: Check out the function stri_reverse() from the {stringi} package.\n\n\n\nShow Code\nfor (i in my_data$code) {\n    print(stringi::stri_reverse(i))\n}\n\n\nShow Output\n## [1] \"608463\"\n## [1] \"239338\"\n## [1] \"614623\"\n## [1] \"056271\"\n## [1] \"343076\"\n## [1] \"323162\"\n## [1] \"373816\"\n## [1] \"814743\"\n## [1] \"027519\"\n## [1] \"621418\"\n## [1] \"711777\"\n## [1] \"309755\"\n## [1] \"268994\"\n## [1] \"069796\"\n## [1] \"117674\"\n\n\n\n\n\nwhile() Loops\nAn alternative to using a for() loop for repeating a particular block of code is to use a while() loop. The general construction for a while() loop is:\nwhile (&lt;test expression&gt;) {\n  &lt;code to execute&gt;\n}\nHere, the test expression is evaluated at the start of the loop, and the body of the loop is only entered if the result is TRUE. Once the statements inside the loop are executed, flow returns to the top of the loop to evaluate the test expression again. That process is repeated until the test expression is FALSE, and then the loop is exited and control moves on to subsequent parts of the program.\nAs above, the following example prints out each element in the vector, v:\n\nv &lt;- seq(from = 100, to = 120, by = 2)\ni &lt;- 1\nwhile (i &lt;= length(v)) {\n    print(v[i])\n    i &lt;- i + 1\n}\n\n## [1] 100\n## [1] 102\n## [1] 104\n## [1] 106\n## [1] 108\n## [1] 110\n## [1] 112\n## [1] 114\n## [1] 116\n## [1] 118\n## [1] 120",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Functions and Flow Control</span>"
    ]
  },
  {
    "objectID": "11-module.html#vectorization-and-functionals",
    "href": "11-module.html#vectorization-and-functionals",
    "title": "11¬† Functions and Flow Control",
    "section": "11.8 Vectorization and Functionals",
    "text": "11.8 Vectorization and Functionals\nIn many cases what we want to do in a loop is apply the same function or operation to each of the elements in a vector, matrix, data frame, list or part thereof‚Ä¶ and often we do not actually need a loop to do that. Rather, we can often use what is called a vectorized function, or functional. The function sapply(), and related functions (e.g., apply(), lapply(), mapply(), vapply()) are examples of functionals: they allow us to perform element-wise operations on the entries in a data object.\nThe function sapply() takes two arguments, a data object (a vector, list, or data frame) and a function (FUN=) to apply to its elements. Each element of the data object is passed on to the function, and the result is returned and concatenated into a vector of the same length as the original data object. lapply() is similar except that the output is a list rather than a vector.\nThe following examples replicate what we did with for() loops above. Here, s is a data frame, and the function median() is being applied to each element, i.e., each variable, in that data frame:\n\noutput &lt;- sapply(s, FUN = median, na.rm = TRUE)\n# Here we are passing on an extra argument to the `median()`function, i.e.,\n# `na.rm=TRUE`. This is an example of 'dot-dot-dot' (`...`) being an extra\n# argument of the `sapply()` function where those arguments are 'passed\n# through' as arguments of the `FUN=` function. Basically, this means that we\n# can pass on an arbitrary set and number of arguments into `sapply()` which,\n# in this case, are then being used in the `median()` function.\noutput\n\n## Brain_Size_Female_Mean  Body_mass_female_mean      Canine_Dimorphism \n##                29.0500              1030.0000                 1.2955\n\nclass(output)\n\n## [1] \"numeric\"\n\noutput &lt;- lapply(s, FUN = median, na.rm = TRUE)\noutput\n\n## $Brain_Size_Female_Mean\n## [1] 29.05\n## \n## $Body_mass_female_mean\n## [1] 1030\n## \n## $Canine_Dimorphism\n## [1] 1.2955\n\nclass(output)\n\n## [1] \"list\"\n\n\nThe map() family of functions from the {purrr} package (which is part of the {tidyverse} set of packages) works very similarly to the apply() functions:\n\noutput &lt;- map_dbl(s, .f = median, na.rm = TRUE)\n# note the argument `.f=` instead of `FUN=` `map_dbl()` returns an atomic\n# vector of type 'double'\noutput\n\n## Brain_Size_Female_Mean  Body_mass_female_mean      Canine_Dimorphism \n##                29.0500              1030.0000                 1.2955\n\nclass(output)  # returns a vector, like `sapply()`\n\n## [1] \"numeric\"\n\noutput &lt;- map(s, .f = median, na.rm = TRUE)\n# `map()` returns a list\noutput\n\n## $Brain_Size_Female_Mean\n## [1] 29.05\n## \n## $Body_mass_female_mean\n## [1] 1030\n## \n## $Canine_Dimorphism\n## [1] 1.2955\n\nclass(output)  # returns a list, like `lapply()`\n\n## [1] \"list\"\n\n# `map_dfr()` returns a data frame\noutput &lt;- map_dfr(s, .f = median, na.rm = TRUE)\nclass(output)  # returns a data frame, unlike any of the `apply()` functions\n\n## [1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\noutput\n\n## # A tibble: 1 √ó 3\n##   Brain_Size_Female_Mean Body_mass_female_mean Canine_Dimorphism\n##                    &lt;dbl&gt;                 &lt;dbl&gt;             &lt;dbl&gt;\n## 1                   29.0                  1030              1.30\n\n\n\n# | include: false\ndetach(package:tidyverse)",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Functions and Flow Control</span>"
    ]
  },
  {
    "objectID": "11-module.html#concept-review",
    "href": "11-module.html#concept-review",
    "title": "11¬† Functions and Flow Control",
    "section": "Concept Review",
    "text": "Concept Review\n\nFunctions: arguments, default values, and ‚Äúdot-dot-dot‚Äù (...)\nConditional expressions: if... else..., ifelse(), if_else(), case_when()\nIterating with loops: for() loops, while() loops\nFunctionals for vectorizing data manipulations: apply() and map() families of function",
    "crumbs": [
      "Part I - Using ***R*** and ***RStudio***",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Functions and Flow Control</span>"
    ]
  },
  {
    "objectID": "12-module.html",
    "href": "12-module.html",
    "title": "12¬† Descriptive Statisics and Sampling",
    "section": "",
    "text": "12.1 Objectives",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Descriptive Statisics and Sampling</span>"
    ]
  },
  {
    "objectID": "12-module.html#objectives",
    "href": "12-module.html#objectives",
    "title": "12¬† Descriptive Statisics and Sampling",
    "section": "",
    "text": "The objective of this module is to review some key terms and ideas that form the foundation of statistics and statistical inference. In particular, this module considers ways for describing distributions of data, particularly measures of central tendency, spread (i.e., variation), and shape in our observations of a variable, which play an important role in both descriptive statistics and in various approaches to statistical hypothesis testing.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Descriptive Statisics and Sampling</span>"
    ]
  },
  {
    "objectID": "12-module.html#preliminaries",
    "href": "12-module.html#preliminaries",
    "title": "12¬† Descriptive Statisics and Sampling",
    "section": "12.2 Preliminaries",
    "text": "12.2 Preliminaries\n\nInstall these packages in R: {mosiac}, {radiant}, {moments}, {sciplot}, {infer}\nLoad {tidyverse}",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Descriptive Statisics and Sampling</span>"
    ]
  },
  {
    "objectID": "12-module.html#important-terms",
    "href": "12-module.html#important-terms",
    "title": "12¬† Descriptive Statisics and Sampling",
    "section": "12.3 Important Terms",
    "text": "12.3 Important Terms\n\nPopulation - all of the elements from a set of data (e.g., all of the gorillas in the world) = N\nSample - one or more observations drawn from a population by some kind of sampling process (e.g., the set of gorillas living in Rwanda, the set of gorilla skeletons found in a museum) = n\n\n\nNOTE: We often assume that a sampling process is random, but there are lots of ways in which sampling might be biased, thus the samples we work with may not be (and often are not) random samples!\n\n\nParameter - a measurable characteristic of a population that summarizes data (e.g., the mean value of the femur length of all gorillas)\n\n\nNOTE: Population means for a given variable, x, are often indicated as \\(\\mu_x\\)\n\n\nStatistic - a measurable characteristic about a sample that summarizes data (e.g., the mean femur length of gorilla femurs found at the American Museum of Natural History)\n\n\nNOTE: Sample means for a given variable, x, are often indicated as \\(\\bar{x}\\)\n\nWhen we use statistical methods or attempt statistical inference - whether using a null hypothesis significance testing (NHST) framework or using Bayesian approaches - we are basically trying to estimate and draw conclusions about population-level parameters and processes and their distributions based on observations or measurements we take from a sample. Sometimes, we are simply trying evaluate whether it is reasonable to assume that our sample is drawn from a population with particular characteristics. Other times, we may be trying to understand what variables explain variation we see in a response measure of interest or be trying to evaluate which among a set of alternative models best predicts a given response.\nRegardless, we should always keep in mind that the process of trying to draw conclusions about a population based on a sample can be complicated by the fact that‚Ä¶\n\nour sample may be biased, non-random, or non-representative in some way\nthere may be unknown or unobserved variables that impact how the sample is related to the population\nthe assumptions we make about the population that our sample is drawn from might not be correct",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Descriptive Statisics and Sampling</span>"
    ]
  },
  {
    "objectID": "12-module.html#describing-sets-of-observations",
    "href": "12-module.html#describing-sets-of-observations",
    "title": "12¬† Descriptive Statisics and Sampling",
    "section": "12.4 Describing Sets of Observations",
    "text": "12.4 Describing Sets of Observations\nIt is important for us to be able to describe the general characteristics of the distribution of a set of observations or measurements about a population or a sample, and we often do this by calculating some measure(s) of central tendency, some measure(s) of spread around that statistic, and some measure of the shape of a distribution. The ‚Äúfive-number summary‚Äù that we have talked about previously provides some such statistics.\n\nMeasures of Central Tendency\n\nMedian - the middle value in a rank ordered series of values\nMean - the sum of measured values divided by \\(n\\), a.k.a., the average or the arithimetic mean\nMode - the most common measurement of values observed\nHarmonic mean - the reciprocal of the average of the reciprocals of a set of values\n\nThe measures above are relevant to summarizing observations about processes that are additive.\n\nGeometric mean - a measure of central tendency for processes that are exponential (e.g., some phases of population growth in natural populations) or multiplicative (e.g., increases in area or volume that accompany increases in linear dimension) in nature, rather than additive = the \\(n^{th}\\) root of the product of the values, taken across a set of \\(n\\) values; for the mathematically inclined, it also equals the antilog of the averaged log values\n\n\n\nCHALLENGE\n\nGiven a vector, x &lt;- c(1,2,3,4,5,6,7,8,9,10,25,50,100,200,1000), write your own function to determine the geometric mean of the values in a vector. Remember the general form for functions is: &lt;function name&gt; &lt;- function(&lt;arguments&gt;) {&lt;code&gt;}\n\n\nHINT: Taking the \\(n^{th}\\) root of a number is equivalent to raising the number to the power of \\(\\frac{1}{n}\\)\n\n\n\nShow Code\nx &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 25, 50, 100, 200, 1000)\ngeometric_mean &lt;- function(x) {\n    prod(x)^(1/length(x))\n}\ngeometric_mean(x)\n\n\nShow Output\n## [1] 13.50559\n\n\n\nShow Code\ngeometric_mean &lt;- function(x) {\n    exp(mean(log(x)))\n}\ngeometric_mean(x)\n\n\nShow Output\n## [1] 13.50559\n\n\n\nWhat happens if you have NAs or zeros or negative numbers in your vector?\n\nHINT: Including an na.rm=TRUE argument and the function na.omit() in your function code to ignore data points with  values may help you write more generic functions!\n\n\n\nMeasures of Spread\nIn addition to measures of central tendency, measures of spread or variability in the distribution of variables of interest are some of the most important summary statistics to calculate. The total range (min to max) is one measure of spread, as is the interquartile range (25th to 75th quartile), which as we have seen are both part of the ‚Äúfive-number summary‚Äù.\nWe more commonly characterize spread, however, in terms of some measure of the deviation of a set of values from the mean of those values. One such measure is the sum of squares‚Ä¶\n\nSum of Squares = the sum of the squared deviations of a set of values from the mean of that set\n\n\nNOTE: Why do we use the sum of the squared deviations of values from the mean rather than just the sum of deviations? Because the latter would simply be ZERO!\n\n\n\nCHALLENGE\n\nWrite a function to calculate the sum of squares for a vector.\n\n\n\nShow Code\nsum_of_squares &lt;- function(x) {\n    sum((x - mean(x))^2)\n}\nsum_of_squares(x)\n\n\nShow Output\n## [1] 917183.3\n\n\n\nShow Code\n# This is equivalent to...\n\nsum_of_squares &lt;- function(x) {\n    sum(x^2) - length(x) * mean(x)^2\n}\nsum_of_squares(x)\n\n\nShow Output\n## [1] 917183.3\n\n\n\nA shortcut to calculate the sum of squares for a vector, x, that does not actually require calculating mean(x) is the (sum of the squared values in the dataset) minus the (square of the summed values) / n, or‚Ä¶\n\\[\\sum\\limits_{i=1}^{k}(x^2) - \\frac{(\\sum\\limits_{i=1}^{k} x)^2}{n}\\]\nThus, another formula for the sum of squares is the following:\n\nsum_of_squares &lt;- function(x) {\n    sum(x^2) - (sum(x))^2/length(x)\n}\nsum_of_squares(x)\n\n## [1] 917183.3\n\n\nNote that the sum of squares always increases with sample size‚Ä¶ you can see this by adding more data points to your vector. Thus, to be able to compare across data sets of different size, we are often more interested in the average deviation of values from the mean rather than the straight sum of squares, i.e., a mean squared deviation.\nThis is the definition of the variability or variance in a dataset. If we are simply interested in describing the mean squared deviation in a population, where we have a value or measurement for every case (e.g., the femur length of all of the gorillas in a museum population), we could then just divide the sum of squares by the number of cases.\n\nPopulation Variance (\\(\\sigma^2\\)) = \\(\\frac{SS}{N}\\)\n\nIn R parlance, we can write this as:\n\npop_var &lt;- function(x) {\n    sum((x - mean(x))^2)/(length(x))\n}\npop_var(x)\n\n## [1] 61145.56\n\n\nIf, however, we have not measured all of the individual cases in population - i.e., if we are, instead, dealing with a sample from the population and are trying to use that sample to say something about the population from which it is drawn (e.g., to say something about gorilla femur lengths in general based on those that appear in a museum sample) - then we need to use a slightly different formula to get an unbiased estimate of the population variance. Such an estimate for a population parameter, based on data from a sample, is calculated as:\n\nSample Variance (an estimator of the population variance) = \\(\\frac{SS}{n-1}\\)\n\nIn this formula, \\(n - 1\\) is the number of degrees of freedom implied by the sample. The degrees of freedom is the number of values used to calculate a sample statistic that are ‚Äúfree to vary‚Äù. For example, we used n observations to calculate the mean of our sample, which implies \\(n - 1\\) degrees of freedom (i.e., if we know the mean and \\(n - 1\\) values, then we also know the last value‚Ä¶ it is not ‚Äúfree to vary‚Äù). We then use that statistic about our sample (i.e., the sample mean) as an estimate of the population mean, which is then used to derive an estimate of the population variance based on the sample variance.\n\n\nCHALLENGE\n\nWrite a function to calculate the variance for a vector of values representing a sample of measurements. Remember this means dividing the sample sum of squares by \\(n-1\\).\n\n\n\nShow Code\nsample_var &lt;- function(x) {\n    sum((x - mean(x))^2)/(length(x) - 1)\n}\nsample_var(x)\n\n\nShow Output\n## [1] 65513.1\n\n\n\nCompare the results of your function to the built-in R function, var(), which calculates sample variance.\n\n\nShow Code\nvar(x)\n\n\nShow Output\n## [1] 65513.1\n\n\n\n\n\nMeasures of Shape\nTwo common measures of the shape of a distribution include its skewness and kurtosis.\nSkewness measures the asymmetry of a distribution. Symmetrical distributions have zero skewness. Those with a longer or fatter tail on the right-hand side are called ‚Äúright-skewed‚Äù (and have positive skewness), while those with a longer or fatter tail on the left-hand side are called ‚Äúleft-skewed‚Äù (and have negative skewness). One measure of skewness is\n\\[\\frac{\\sum\\limits_{i=1}^{N}(x_i-\\bar{x})^3}{(N-1)\\sigma^3}\\] ‚Ä¶ where \\(N\\) is the number of observations, \\(\\bar{x}\\) is the mean, and \\(\\sigma\\) is the standard deviation. That is, skewness is based on the cube of deviations of each observation from the mean, whereas variance is based on squared deviations from the mean. Generally, a distribution with a skewness value of between -0.5 and +0.5 is not considered to be far from symmetrical.\nEXAMPLES\n\nx &lt;- rnorm(1e+05, 0, 1)  # draw a random sample from a normal distribution with mean = 0 and stdev = 1\nhist(x, main = \"Normal: Symmetrical\", freq = FALSE)  # plot as a histogram\n\n\n\n\n\n\n\n(skewness &lt;- sum((x - mean(x))^3)/((length(x) - 1) * sd(x)^3))\n\n## [1] 0.002477533\n\nx &lt;- rbeta(1e+05, 9, 3)  # draw a random sample from a beta distribution with alpha = 9 and beta = 2\nhist(x, main = \"Beta: Left-Skewed\", freq = FALSE)  # plot as a histogram\n\n\n\n\n\n\n\n(skewness &lt;- sum((x - mean(x))^3)/((length(x) - 1) * sd(x)^3))\n\n## [1] -0.6086246\n\n\nKurtosis measures the peakedness or flatness of a distribution compared to a normal distribution. Distributions with high kurtosis have a sharper peak and fatter tails, while those with low kurtosis have a flatter, wider peak and thinner tails. The formula for kurtosis is‚Ä¶\n\\[\\frac{\\sum\\limits_{i=1}^{N}(x_i-\\bar{x})^4}{(N-1)\\sigma^4}\\]\nThat is, kurtosis is based on deviations of each observation raised to the fourth power. Data drawn from a normal distribution have an expected kurtosis of three, while those with ‚Äúnegative‚Äù kurtosis have a value of greater than three and those with ‚Äúpostive‚Äù kurtosis have values of less than three.\n\nx &lt;- rnorm(1e+05, 0, 1)  # draw a random sample from a normal distribution with mean = 0 and stdev = 1\n(kurtosis &lt;- sum((x - mean(x))^4)/((length(x) - 1) * sd(x)^4))\n\n## [1] 2.993786\n\n\nThe {moments} package has functions for skewness() and kurtosis().\n\nlibrary(moments)\nx &lt;- rnorm(1e+05, 0, 1)\nskewness(x)\n\n## [1] -0.002196731\n\nkurtosis(x)\n\n## [1] 2.979952\n\ndetach(package:moments)\n\n\n\nQuestions to Explore\n\nFor a random variable, how is variance related to sample size?\n\nWe will explore this, and at the same time practice a bit about loops in R programming, via simulation, where we repeatedly draw samples of random variables from a specific distribution. As an example, we will we draw from a normal distribution with a mean of 10 and a standard deviation of 2.\nTo visualize this distribution, we can use the plotDist() function from the {mosaic} package‚Ä¶\n\nlibrary(mosaic)\nmu &lt;- 10\nsigma &lt;- 2\nplotDist(\"norm\", mean = mu, sd = sigma, xlab = \"x\", ylab = \"Frequency\")\n\n\n\n\n\n\n\n# `plotDist()` comes from the {mosaic} package and makes it easy to plot a\n# distribution\n\nIn the following code block, we first set up a plot window to hold the results of our simulations and plot a line for the known population variance (i.e., the square of \\(\\sigma\\), the standard deviation). We then use nested for loops to iterate the process of drawing samples of a specific size, \\(n\\), from the specified distribution. We do this for samples of size \\(n\\) = 5, 10, 15‚Ä¶ up to 100, and we draw out 50 replicates of each sample size. Recall from Module 11 that the structure for for loops is:\nfor (&lt;index&gt; in &lt;range&gt;){\n  &lt;code to execute&gt;\n}\n\n# set up plot window\nplot(c(0, 100), c(0, 15), type = \"n\", xlab = \"Sample Size\", ylab = \"Variance\")\n\n# add the population variance (= square of population standard deviation) to\n# the plot\nabline(h = 4, col = \"red\", lwd = 2, lty = 2)\n\n# run simulations and add results to plot\nmu &lt;- 10\nsigma &lt;- 2\n# samples of 5, 10, 15...\nfor (n in seq(from = 5, to = 100, by = 5)) {\n    # set up a variable, reps, to hold the set of variances calculated for each\n    # replicate\n    reps &lt;- vector(\"double\", 50)\n    # 50 replicates\n    for (i in 1:50) {\n        x &lt;- rnorm(n, mean = mu, sd = sigma)\n        points(n, var(x))\n        reps[[i]] = var(x)\n        # this is a common programming motif in R and is more memory and time\n        # efficient than another common motif, `reps &lt;- c(reps, var(x))`\n    }\n    points(n, mean(reps), bg = \"red\", pch = 23, cex = 2)  # plots average\n}\n\n\n\n\n\n\n\n\n\nHow does sample variance compare to population variance? What happens to the sample variances as sample size increases?\n\nAnother measure of spread around a mean that we often see reported is the standard deviation. The standard deviation is simply the square root of the variance (\\(\\sqrt{\\sigma^2} = \\sigma\\)). The advantage of using the standard deviation as a statistic or parameter is that the units of standard deviation are the same as the units of our original measurement (rather than being units squared, which are our units for variance).\nIn R we can write‚Ä¶\n\nx &lt;- rnorm(1000, mean = 10, sd = 2)\npop_sd &lt;- function(x) {\n    sqrt(pop_var(x))\n}\npop_sd(x)\n\n## [1] 2.065599\n\nsample_sd &lt;- function(x) {\n    sqrt(sample_var(x))\n}\nsample_sd(x)\n\n## [1] 2.066633\n\n\nThe sdpop() function from the {radiant} package can be used to calculate the standard deviation for a completely sampled population.\n\nlibrary(radiant)\nsdpop(x)\n\n## [1] 2.065599\n\ndetach(package:radiant)\n\nThe built-in R function sd() can be used to calculate the standard deviation of a sample.\n\nsd(x)\n\n## [1] 2.066633",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Descriptive Statisics and Sampling</span>"
    ]
  },
  {
    "objectID": "12-module.html#using-measures-of-spread",
    "href": "12-module.html#using-measures-of-spread",
    "title": "12¬† Descriptive Statisics and Sampling",
    "section": "12.5 Using Measures of Spread",
    "text": "12.5 Using Measures of Spread\n\nSampling Distributions\nSince one of the goals of statistics is to estimate and make inferences about population-level parameters based on characteristics of a sample, it is important that we be able to judge and report just how reliable or unreliable our statistical estimates those population-level parameters are.\nTo explore how we do this, let‚Äôs start with a population where we KNOW every data point. We will use that first to describe our population empirically. We will then draw samples out of that population and see how well that the samples we draw can be used to describe the population.\nLoad the ‚ÄúIMDB-movies.csv‚Äù dataset from the ada-datasets repository on GitHub as a ‚Äútibble‚Äù, d. This dataset contains data on close to 29,000 movies scraped in early 2020 from the online Internet Movie Database, including their year of title, director, year of production, running time, average viewer rating on a 10 point scale, and the number of votes that rating is based on. [Note that in collating this data, I excluded movies with fewer than 1000 votes.] Once we load the data, we will filter the dataset to keep only movies with a startYear from 1999 to 2019, which should leave us with 17,628 movies. We are going to use this dataset as our POPULATION.\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/IMDB-movies.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\nd &lt;- filter(d, startYear %in% 1999:2019)  # the %in% operator is VERY useful!\nhead(d)\n\n## # A tibble: 6 √ó 10\n##   tconst    titleType primaryTitle startYear runtimeMinutes genres averageRating\n##   &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;            &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;\n## 1 tt0035423 movie     Kate & Leop‚Ä¶      2001            118 Comed‚Ä¶           6.4\n## 2 tt0069049 movie     The Other S‚Ä¶      2018            122 Drama            6.8\n## 3 tt0111068 movie     Sangharsh         1999            127 Actio‚Ä¶           6.8\n## 4 tt0112444 movie     My Teacher'‚Ä¶      1999             89 Comed‚Ä¶           5.5\n## 5 tt0113026 movie     The Fantast‚Ä¶      2000             86 Music‚Ä¶           5.6\n## 6 tt0118589 movie     Glitter           2001            104 Drama‚Ä¶           2.2\n## # ‚Ñπ 3 more variables: numVotes &lt;dbl&gt;, nconst &lt;chr&gt;, director &lt;chr&gt;\n\n\nFirst, we will do some exploratory data analysis on this dataset using, in part, some functions from the {mosaic} package. {mosaic} makes some kinds of quick exploratory data analysis very fast and easy to do (though we already know how to do lots of what we can do with {mosaic} in other ways)!\nFunctions in the {mosaic} package have a data= argument as well as a ~ argument that specifies which variable of interest from the data= argument the function is to be applied to.\n\nboxplot(averageRating ~ startYear, data = d, xlab = \"Year\", ylab = \"Average Rating\")\n\n\n\n\n\n\n\n# the `histogram()` function from {mosaic} plots neat 'augmented' histograms\nhistogram(~averageRating, data = d, xlab = \"Average Rating\")\n\n\n\n\n\n\n\n# the `favstats()` function from {mosiac} calculates a variant of the 5-number\n# summary\n(pop_stats &lt;- favstats(~averageRating, data = d))\n\n##  min  Q1 median  Q3 max     mean       sd     n missing\n##    1 5.6    6.4 7.1 9.9 6.238972 1.160221 17628       0\n\n\nNow, let‚Äôs draw a single SAMPLE of 100 movies randomly from this population and visualize its average viewer rating. After setting the random number seed (analogous to setting what position we start at in a traditional table of random numbers), run the subsequent lines of code several times and look at how the results differ‚Ä¶\n\nNOTE: We use set.seed() here so that each time this function is run, it returns the same sequence of random numbers until the seed is reset.\n\n\nset.seed(1)\n\n\nn &lt;- 100\ns &lt;- sample_n(d, size = n, replace = FALSE)\n# `sample_n()` from {dplyr} selects rows at random from a data frame it's\n# another SUPER useful function\ntable(s$startYear)\n\n## \n## 1999 2000 2001 2002 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 \n##    2    4    2    2    4    3    1    5    6    4   10    7    4    6    6    7 \n## 2016 2017 2018 2019 \n##   10    7    8    2\n\nboxplot(averageRating ~ startYear, data = s, xlab = \"Year\", ylab = \"Average Rating\")\n\n\n\n\n\n\n\nhistogram(~averageRating, data = s, xlab = \"Average Rating\")\n\n\n\n\n\n\n\n(samp_stats &lt;- favstats(~averageRating, data = s))\n\n##  min  Q1 median Q3 max  mean       sd   n missing\n##  3.3 5.5   6.45  7 8.5 6.325 1.004673 100       0\n\n\nNote that each time we select a sample and calculate summary statistics, such as the mean and standard deviation of a sample, we get slightly different results. If we repeat this sampling process multiple times, we can use the results to generate a distribution for a particular summary statistic of interest, e.g., for the mean or the median. This process generates what is called a sampling distribution for the statistic.\nThe code below allows us to generate a sampling distribution virtually. The do() * construction from {mosaic}, in combination with one of the {mosaic} package‚Äôs aggregating functions (e.g., mean(), median(), etc.), can be used to repeat sampling from the population a user-specified number of times, calculate a summary statistic, and then bundle the results into a vector all in the same line. [Of course, we could also write a loop to do the same thing‚Ä¶ and there are other ways we could do this as well (see examples below).]\nThis process of simulating samples drawn from a population and then generating statistics on the basis of each of our virtual samples is a very powerful tool that we will apply over and over again as we talk about statistical inference. This process is the basis for bootstrapping confidence intervals (see Module 14) and for conducting randomization/permutation tests (see Module 16).\n\nUsing the {mosaic} Package\n\n# using `do(reps) *` from {mosaic} to generate a sampling distribution\nreps &lt;- 1000\nsamp_dist_mean &lt;- do(reps) * mean(~averageRating, data = sample_n(d, size = n, replace = FALSE))\n# generates a sampling distribution\nmean_plot &lt;- histogram(~mean, data = samp_dist_mean, xlab = \"Sampling Distribution for the\\nMean of Average Rating\")\nsamp_dist_median &lt;- do(reps) * median(~averageRating, data = sample_n(d, size = n,\n    replace = FALSE))\n# generates a sampling distribution\nmedian_plot &lt;- histogram(~median, data = samp_dist_median, xlab = \"Sampling Distribution for the\\nMedian of Average Rating\")\n\n\n\nUsing the {purrr} Package\n\nlibrary(purrr)\n# using `map()` from {purrr} to generate a sampling distribution\nsamp_dist_mean_alt1 &lt;- map(1:reps, ~mean(~averageRating, data = sample_n(d, size = n,\n    replace = FALSE))) |&gt;\n    unlist()\nmean_plot_alt1 &lt;- histogram(samp_dist_mean_alt1, xlab = \"Sampling Distribution for the\\nMean of Average Rating\")\ndetach(package:purrr)\n\n\n\nUsing the {infer} Package\n\n# using `rep_sample_n()` from {infer} to generate a sampling distribution\nlibrary(infer)\nsamp_dist_mean_alt2 &lt;- d |&gt;\n    rep_sample_n(size = n, reps = reps, replace = FALSE) |&gt;\n    group_by(replicate) |&gt;\n    summarize(mean = mean(averageRating)) |&gt;\n    pull(mean)\ndetach(package:infer)\n\nmean_plot_alt2 &lt;- histogram(samp_dist_mean_alt2, xlab = \"Sampling Distribution for the\\nMean of Average Rating\")\n\n\nlibrary(cowplot)\nplot_grid(mean_plot, median_plot, ncol = 2)\n\n\n\n\n\n\n\ndetach(package:cowplot)\n\n\nIMPORTANT NOTE: The histograms we plot in the code above are for sampling distributions of the statistics in question. They do not represent the distribution of values in any particular sample! This is SUPER IMPORTANT to recognize and keep in mind.\n\nThe mean of the sampling distribution (i.e., the mean of \\(\\bar{x}\\)) for a particular statistic should be a really good point estimate of the population value for that statistic (i.e., \\(\\mu\\)). Compare the following to confirm this:\n\nmean(~mean, data = samp_dist_mean)\n\n## [1] 6.239391\n\n# this is the estimated population mean calculated as the mean of the sampling\n# distribution of sample means\npop_stats$mean  # true population mean\n\n## [1] 6.238972\n\nmean(~median, data = samp_dist_median)\n\n## [1] 6.38125\n\n# this is the estimated population median calculated as the mean of the\n# sampling distribution of sample medians\npop_stats$median  # true population median\n\n## [1] 6.4\n\n\n\n\n\nStandard Errors\nSo, just how reliable or unreliable are these estimates of a population parameter based on the mean of the sampling distribution for a statistic of interest? That is, how far off is a statistic that we calculate based on a sampling distribution likely to be from the true POPULATION value of the parameter of interest?\nOne way to quantify the uncertainty is by calculating the variability of the summary statistic of interest across replicate samples drawn from the population. For example, we could calculate the variance of the sampling distribution. More commonly, because variance is expressed in units squared, we take the square root of this variance to calculate the standard deviation of the sampling distribution and thus express our uncertainly in units of the original measurement. Formally, this value is referred to as the standard error (SE) of measurement for any given summary statistic of interest. [Typically, that statistic is the mean, and we thus are often calculating the standard error of the mean.]\nThe SE is, in effect, the average deviation between statistic values calculated from different and incomplete sets of samples drawn from a population and the average statistic value calculated across that set of samples (which should converge on the true population value for that statistic). The SE of the mean is thus a measure of how dispersed sample means (\\(\\bar{x}\\)) are expected to be around the estimated population mean (\\(\\mu\\)) (i.e., how far off from the true population mean an estimate based on a sample of size \\(n\\) is likely to be). Similarly, the SE of the median would be a measure of how dispersed sample medians are expected to be, on average, around the estimated population median, and so forth.\n\nEstimating a SE from a Sampling Distribution\nTo estimate the SE from a sampling distribution, we simply take the standard standard deviation of the set of values comprising that distribution. Above, we used the do() * &lt;function&gt; construction to generate sampling distributions for the mean and median of average viewer ratings in the ‚Äúmovies.csv‚Äù dataset, so all we need to do is pull out the standard deviation of those distributions to estimate the standard error.\n\nse_mean &lt;- favstats(~mean, data = samp_dist_mean)$sd\n# or, se_mean &lt;- sd(samp_dist_mean$mean)\n(paste0(\"Estimated population mean = \", round(favstats(~mean, data = samp_dist_mean)$mean,\n    3), \" ¬± \", round(se_mean, 3), \" SE based on \", reps, \" samples of size \", n))\n\n## [1] \"Estimated population mean = 6.239 ¬± 0.121 SE based on 1000 samples of size 100\"\n\nse_median &lt;- favstats(~median, data = samp_dist_median)$sd\n# or, se_median &lt;- sd(samp_dist_median$median)\n\n(paste0(\"Estimated population median = \", round(favstats(~median, data = samp_dist_median)$mean,\n    3), \" ¬± \", round(se_median, 3), \" SE based on \", reps, \" samples of size \",\n    n))\n\n## [1] \"Estimated population median = 6.381 ¬± 0.134 SE based on 1000 samples of size 100\"\n\n\nAs we might expect for any measure of uncertainty or error, the SE [1] increases with the variability in a sample (i.e., estimates based on high-variability samples should be more uncertain) and [2] decreases with the size of the sample (i.e., estimates based on larger samples should be less uncertain). The SE thus reflects a ratio of variance to sample size.\n\n\n\nQuestions to Explore\n\nHow does changing the size of our samples (n) impact the mean and SE of our estimate of the population parameter?\nHow does changing the number of replicate samples (reps) impact the mean and SE of our estimate of the population parameter?\n\nWe can explore these two questions with the following code‚Ä¶ note that we just take one set of 2000 replicates and then resample from that set randomly to explore the impact of different, smaller numbers of replicates:\n\nreps &lt;- 2000\ns &lt;- tibble(n = numeric(), mean = numeric(), .index = numeric())\n\nfor (n in seq(from = 20, to = 200, by = 20)) {\n    samp_dist &lt;- {\n        do(reps) * sample_n(d, size = n, replace = FALSE)\n    } |&gt;\n        group_by(.index) |&gt;\n        dplyr::summarise(mean = mean(averageRating)) |&gt;\n        mutate(n = n)\n    s &lt;- bind_rows(s, samp_dist)\n}\n\noutput &lt;- tibble(n = numeric(), reps = numeric(), samp_dist_mean = numeric(), samp_dist_se = numeric())\n\nfor (reps in c(10, 25, 50, 100, 250, 500, 1000, 2000)) {\n    subsample &lt;- s |&gt;\n        group_by(n) |&gt;\n        sample_n(reps, replace = FALSE) |&gt;\n        dplyr::summarise(samp_dist_mean = mean(mean), samp_dist_se = sd(mean)) |&gt;\n        mutate(reps = reps)\n    output &lt;- bind_rows(output, subsample)\n}\n\n\np1 &lt;- ggplot(data = output, aes(x = n, y = samp_dist_mean)) + geom_line() + facet_grid(~reps) +\n    xlab(\"Sample Size\") + ylab(\"Mean\") + ylim(6, 6.5)\np2 &lt;- ggplot(data = output, aes(x = n, y = samp_dist_se)) + geom_line() + facet_grid(~reps) +\n    xlab(\"Sample Size\") + ylab(\"SE\") + ylim(0, 0.5)\np3 &lt;- ggplot(data = output, aes(x = n)) + geom_line(aes(y = samp_dist_mean + samp_dist_se),\n    color = \"blue\") + geom_line(aes(y = samp_dist_mean)) + geom_line(aes(y = samp_dist_mean -\n    samp_dist_se), color = \"blue\") + facet_grid(~reps) + xlab(\"Sample Size\") + ylab(\"Mean ¬± SE\") +\n    ylim(5.8, 6.7)\n\n\nlibrary(cowplot)\nplot_grid(p1, p2, p3, nrow = 3)\n\n\n\n\n\n\n\ndetach(package:cowplot)\n\nNotice that as the number of replicates increases, our estimates of both the mean and SE become less variable, while as the sample size increases, our SE decreases (and our estimate of the mean thus becomes less uncertain), regardless of number of replicates.\n\nCalculating SEs from the Population Variance\nNote that above, we estimated the SE by taking the standard deviation of a sampling distribution, where we derived that distribution by taking multiple samples from a perfectly known population. However, if we know the actual population variance (\\(\\sigma^2\\)) or population standard deviation (\\(\\sigma\\)), which we do in this case, we can actually calculate the expected SE for samples of a given size directly, without basing that on an empirical sampling distribution that we have derived from repeated sampling. This is because, mathematically, the variance of a fully realized sampling distribution (i.e., of taking all possible samples of size \\(n\\) from a population) is equal to the variance of the population divided by the sample size. The square root of the variance of the sampling distribution is the standard deviation, i.e., the standard error.\n\\[SE = \\sqrt{\\frac{\\sigma^2}{n}} = \\frac{\\sigma}{\\sqrt{n}}\\]\nwhere \\(\\sigma^2\\) is the population variance (and \\(\\sigma\\) is thus the population standard deviation) and \\(n\\) is the sample size.\nWe can compare the SEs for different sample sizes estimated above from our empirical sampling distributions with the SEs calculated directly from the population variance using the code below.\n\n# select estimated SEs for different sample sizes with 1000 reps from our\n# output tibble\nsampling_output &lt;- filter(output, reps == 1000) |&gt;\n    select(n, samp_dist_se)\n# create a tibble of SEs based on the known population variance\npop_output &lt;- tibble(n = numeric(), pop_se = numeric())\nfor (n in seq(from = 10, to = 200, by = 10)) {\n    pop_se &lt;- sqrt(pop_var(d$averageRating)/n)\n    pop_output &lt;- bind_rows(pop_output, c(n = n, pop_se = pop_se))\n}\n(compare &lt;- inner_join(sampling_output, pop_output, by = \"n\"))\n\n## # A tibble: 10 √ó 3\n##        n samp_dist_se pop_se\n##    &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n##  1    20       0.257  0.259 \n##  2    40       0.183  0.183 \n##  3    60       0.150  0.150 \n##  4    80       0.129  0.130 \n##  5   100       0.114  0.116 \n##  6   120       0.102  0.106 \n##  7   140       0.0981 0.0981\n##  8   160       0.0871 0.0917\n##  9   180       0.0859 0.0865\n## 10   200       0.0775 0.0820\n\n# the sample_dist_se and pop_se columns should be very close in value!\n\n\n\nEstimating SEs from a Single Sample\nOf course, in practice we often do not know the true population variance or standard deviation, nor do we have the opportunity to generate a sampling distribution empirically and then use this to estimate the SE. Instead, we collect typically just collect a single sample from a population about which we know very little.\nIn these cases, we use some statistic about our single sample as a point estimate for the parameter value in our population (e.g., \\(\\bar{x}\\) for \\(\\mu\\)), and we use the variance or standard deviation and size of that single sample to estimate the SE around that point estimate for our statistic of interest: i.e., square root of (sample variance / sample size) or sample standard deviation / (square root of sample size)\n\\[SE = \\sqrt{\\frac{s^2}{n}} = \\frac{s}{\\sqrt{n}}\\]\nwhere \\(s^2\\) is the sample variance and \\(s\\) is the sample standard deviation.\n\n\n\nCHALLENGE\n\nWrite your own function to calculate the standard error of the mean for a vector of values representing a single sample of observations from a population. You can use either your own function for the sample variance that you created above or the built-in var() function. There are, of course, several ways you could do this. Then, use your new function with summarize() and sample_n() to extract a single sample of size ‚Äún=100‚Äù from d and calculate an estimate of the population SE for that sample size.\n\n\n\nShow Code\nn &lt;- 100\nset.seed(100)\nmy_se1 &lt;- function(x) {\n    sqrt(sample_var(x)/length(x))\n}\nmy_se2 &lt;- function(x) {\n    sqrt(var(x)/length(x))\n}\nmy_se3 &lt;- function(x) {\n    sd(x)/sqrt(length(x))\n}\n\nnew_sample &lt;- sample_n(d, size = n)\nsummarize(new_sample, se1 = my_se1(averageRating), se2 = my_se2(averageRating), se3 = my_se3(averageRating))\n\n\nShow Output\n## # A tibble: 1 √ó 3\n##     se1   se2   se3\n##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 0.118 0.118 0.118\n\n\n\nThe package {sciplot} includes the function, se(), for calculating standard errors (as do others).\n\nlibrary(sciplot)\nsummarize(new_sample, se4 = sciplot::se(averageRating))\n\n## # A tibble: 1 √ó 1\n##     se4\n##   &lt;dbl&gt;\n## 1 0.118\n\ndetach(package:sciplot)",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Descriptive Statisics and Sampling</span>"
    ]
  },
  {
    "objectID": "12-module.html#concept-review",
    "href": "12-module.html#concept-review",
    "title": "12¬† Descriptive Statisics and Sampling",
    "section": "Concept Review",
    "text": "Concept Review\n\n\\(\\mu, \\bar{x}\\) = population mean, sample mean\n\n\\(\\bar{x}\\) is an estimator for \\(\\mu\\)\n\n\\(\\sigma^2, s^2\\) = population variance, sample variance\n\\(\\sigma, s\\) = population standard deviation, sample standard deviation\n\n\\(s\\) is an estimator for \\(\\sigma\\)\n\nSample variance and standard deviation: var(), sd()\n\nThese are measures of the variation/spread in a sample\n\nSampling distributions and standard errors\n\nMean of the sampling distribution of \\(\\bar{x}\\) is an estimate of \\(\\mu\\)\nStandard deviation of the sampling distribution of \\(\\bar{x}\\) = \\(\\frac{\\sigma}{\\sqrt{n}}\\) = standard error\n\nThis is a measure of the variation/spread in a sample statistic\n\nStandard errors: sciplot::se()\n\nUsing the sample_n() function from {dplyr} and do() * &lt;function&gt; construction from {mosaic} to generate sampling distributions",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Descriptive Statisics and Sampling</span>"
    ]
  },
  {
    "objectID": "13-module.html",
    "href": "13-module.html",
    "title": "13¬† Probability and Distributions",
    "section": "",
    "text": "13.1 Objectives",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Probability and Distributions</span>"
    ]
  },
  {
    "objectID": "13-module.html#objectives",
    "href": "13-module.html#objectives",
    "title": "13¬† Probability and Distributions",
    "section": "",
    "text": "The objective of this module is to gently begin our discussion of statistical inference and statistical modeling. Doing so requires that we first cover some basics of probability and statistical distributions.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Probability and Distributions</span>"
    ]
  },
  {
    "objectID": "13-module.html#preliminaries",
    "href": "13-module.html#preliminaries",
    "title": "13¬† Probability and Distributions",
    "section": "13.2 Preliminaries",
    "text": "13.2 Preliminaries\n\nInstall and load this package in R: {manipulate}\nLoad {tidyverse}, {mosaic}, and {cowplot}",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Probability and Distributions</span>"
    ]
  },
  {
    "objectID": "13-module.html#probability",
    "href": "13-module.html#probability",
    "title": "13¬† Probability and Distributions",
    "section": "13.3 Probability",
    "text": "13.3 Probability\nThe term probability is applied to population-level variables to describe the magnitude of chance associated with particular observations or events. Probabilities summarize the relative frequencies of different possible outcomes and are properties of distributions of variables. Every variable has a distribution that can be described empirically and visualized, as we have done in some of our previous modules. And, sometimes, these empirical distributions are nicely approximated by particular theoretical distributions with well-known mathematical properties, a fact which forms the basis for traditional frequentist (or classical) statistical inference.\nProbabilities for events or collections of events necessarily vary between zero and one. Outcomes or combinations of outcomes that are impossible have \\(Pr = 0\\), those that are certain have \\(Pr = 1\\).\n\nEXAMPLE: If we roll a (fair, unbiased) die, there are 6 possible outcomes, and each has a probability of occurring of 1 in 6. We can estimate these probabilities using data on the outcome of lots of observations of independent die rolls. This is referred to as a frequentist or classical way of thinking about the probability of these different outcomes‚Ä¶ the relative frequency with which a particular event occurs over numerous identical, independent, objective trials.\n\nWe will use the {manipulate} package and the sample() function to explore the effects of sample size on estimates of the probability of different outcomes of the a process of rolling a (fair, unbiased) die. The {manipulate} package allows us to create an interactive plot that lets us dynamically change something about the values being plotted. We will set up a simulation where the probability of each possible outcome of the process of rolling a die one time (‚Äú1‚Äù, ‚Äú2‚Äù,‚Ä¶, ‚Äú6‚Äù) is 1 in 6, but our estimate of the probability of each possible outcome will change with sample size. In the code below, we use the powerful sample() function, which takes several arguments - a set of elements to sample from (x=), the number of elements to draw (size=), and whether or not to draw with replacement (replace=). After typing in and running the code below, play with the slider to change the number of die rolls being simulated.\n\noutcomes &lt;- c(1, 2, 3, 4, 5, 6)\nmanipulate(histogram(sample(x = outcomes, size = n, replace = TRUE), breaks = c(0.5,\n    1.5, 2.5, 3.5, 4.5, 5.5, 6.5), type = \"density\", main = paste(\"Histogram of Outcomes of \",\n    n, \" Die Rolls\", sep = \"\"), xlab = \"Roll\", ylab = \"Probability\"), n = slider(0,\n    10000, initial = 100, step = 100))\n\n\nCHALLENGE\nWrite your own function, roll(), to simulate rolling a die where you pass the number of rolls as an argument (nrolls=) with a default value of 1. Then, use your function to simulate rolling two dice a total 1000 times and take the sum of the rolls. Plot a histogram of those results. What happens if you roll each die 100 times? 10,000 times?\n\n\nShow Code\nroll &lt;- function(nrolls = 1) {\n    sample(1:6, nrolls, replace = TRUE)\n}  # function with default of 1 roll\nnrolls &lt;- 1000\ntwo_dice &lt;- roll(nrolls) + roll(nrolls)\nhistogram(two_dice, breaks = c(1.5:12.5), type = \"density\", main = \"Rolling Two Dice\",\n    xlab = \"Sum of Rolls\", ylab = \"Probability\")",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Probability and Distributions</span>"
    ]
  },
  {
    "objectID": "13-module.html#rules-of-probability",
    "href": "13-module.html#rules-of-probability",
    "title": "13¬† Probability and Distributions",
    "section": "13.4 Rules of Probability",
    "text": "13.4 Rules of Probability\nThe following are a set of standard rule of probability that are worth reviewing:\n\n\\(Pr (+)\\) = Probability that something occurs = 1\n\\(Pr (\\emptyset)\\) = Probability that nothing occurs = 0\n\\(Pr (A)\\) = Probability that a particular event, \\(A\\), occurs\n\n\\[0 \\leq Pr (A) \\leq 1\\]\n\n\\(Pr (A \\cup B)\\) = Probability that a particular event \\(A\\) or a particular event \\(B\\) occurs = UNION\n\n\\[Pr (A \\cup B) = Pr (A) + Pr (B) - Pr (A \\cap B)\\]\n\nIf events \\(A\\) and \\(B\\) are mutually exclusive, then this simplifies to‚Ä¶ \\[Pr (A) + Pr (B)\\]\n\n\n\\(Pr (A \\cap B)\\) = Probability that both \\(A\\) and \\(B\\) occur simultaneously = INTERSECTION\n\n\\[Pr (A \\cap B) = Pr (A \\vert B) \\times Pr (B) = Pr (B \\vert A) \\times Pr (A)\\]\n\nThe pipe operator ( \\(\\vert\\) ) can be read as ‚Äúgiven‚Äù and indicates conditional probability (see below)\nIf \\(Pr (A \\cap B) = 0\\), then we say the events are mutually exclusive (e.g., you cannot have a die roll be 1 and 2)\nIf the two events are independent (i.e., if the probability of one does not depend on the probability of the other), then \\(Pr (A \\cap B)\\) simplifies to‚Ä¶ \\[Pr (A) \\times Pr (B)\\]\n\n\nProbability of the COMPLEMENT of \\(A\\) (i.e., not \\(A\\)) = \\(Pr (ƒÄ) = 1 - Pr (A)\\)\nCONDITIONAL PROBABILITY is the probability of an event occuring after taking into account the occurrence of another event, i.e., one event is conditioned on the occurrence of a different event. For example, the probability of a die coming up as a ‚Äú1‚Äù given that we know the die came up as an odd number (‚Äú1‚Äù, ‚Äú3‚Äù, or ‚Äú5‚Äù) is a conditional probability.\n\n\\[Pr (A \\vert B) = Pr (A \\cap B) \\div Pr (B)\\]\n\nIf event \\(A\\) and event \\(B\\) are independent, then \\[Pr (A \\vert B) = [Pr (A) \\times Pr (B) ] \\div Pr (B) = Pr (A)\\]\nIf event \\(A\\) and \\(B\\) are not independent, then \\[Pr (A \\vert B) ‚â† Pr (A)\\]\n\n\nCHALLENGE\nYou have a deck of 52 cards‚Ä¶ Ace to 10 plus 3 face cards in each suit. You draw a card at random.\n\nWhat is the probability that you draw a face card?\n\n\n\\(Pr (face\\ card)\\)\n\n12 of 52 cards = 0.2307692\n\n\n\nWhat is the probability that you draw a King?\n\n\n\\(Pr(King)\\)\n\n3 of 52 cards = 0.05769231\n\n\n\nWhat is the probability that you draw a spade?\n\n\n\\(Pr(spade)\\)\n\n13 of 52 cards = 0.25\n\n\n\nWhat is the probability that you draw a spade given that you draw a face card? (CONDITIONAL, INDEPENDENT EVENTS)\n\n\nIntuitively‚Ä¶\n\n3 of 12 cards = 0.25\n\nFormally‚Ä¶\n\\(Pr(spade \\vert face\\ card) = Pr(spade) \\times Pr(face\\ card) \\div Pr(face\\ card)\\)\n\n(13 of 52) \\(\\times\\) (12 of 52) \\(\\div\\) (12 of 52) = 0.25\n\n\n\nWhat is the probability that you draw a King given that you draw a face card? (CONDITIONAL, NOT INDEPENDENT EVENTS)\n\n\nIntuitively‚Ä¶\n\n4 of 12 cards = 0.3333333\n\nFormally‚Ä¶\n\\(Pr(King \\vert face\\ card) = Pr(King \\cap face\\ card) \\div Pr(face\\ card) =\\)\n\\(Pr(face\\ card \\vert King) \\times Pr(King) \\div Pr(face\\ card)\\)\n\n1 \\(\\times\\) (4 of 52) \\(\\div\\) (12 of 52) = 0.3333333\n\n\n\nWhat is the probability that you draw a card that is both from a red suit (hearts or diamonds) and a face card? (INTERSECTION, INDEPENDENT EVENTS)\n\n\nIntuitively‚Ä¶\n\n6 of 52 cards = 0.1153846\n\nFormally‚Ä¶\n\\(Pr (red \\cap face\\ card) = Pr (red \\vert face\\ card) \\times Pr (face\\ card) =\\)\n\\([Pr(red) \\times Pr(face\\ card)] \\div Pr(face\\ card) \\times Pr(face\\ card)\\)\nwhere‚Ä¶\n\\(Pr (red)\\)\n\n26 of 52 cards = 0.5\n\n\\(Pr (face\\ card)\\)\n\n12 of 52 cards = 0.2307692\n\nso‚Ä¶\n\n[(26 of 52) \\(\\times\\) (12 of 52)] \\(\\div\\) (12 of 52) \\(\\times\\) (12 of 52) = 0.1153846\n\n\n\nWhat is the probability that you draw a card that is either a club or not a face card? (UNION, NOT INDEPENDENT EVENTS)\n\n\nIntuitively‚Ä¶\n\n(13 club cards \\(+\\) 40 not face cards \\(-\\) 10 club cards that are not face cards) of 52 cards = 43 of 52 cards = 0.8269231\n\nFormally‚Ä¶\n\\(Pr (club \\cup not\\ a\\ face\\ card) =\\)\n\\(Pr (club) + Pr (not\\ a\\ face\\ card) - Pr (club \\cap not\\ a\\ face\\ card) =\\)\n\\(Pr (club) + Pr (not\\ a\\ face\\ card) - Pr (club \\vert not\\ a\\ face\\ card) \\times Pr (not\\ a\\ face\\ card)\\)\nwhere‚Ä¶\n\\(Pr (club)\\)\n\n13 of 52 cards = 0.25\n\n\\(Pr (not\\ a\\ face\\ card)\\)\n\n40 of 52 cards = 0.7692308\n\n\\(Pr (club \\vert not\\ a\\ face\\ card)\\)\n\n10 of 40 = 0.25\n\nso‚Ä¶\n\n(13 of 52) \\(+\\) (40 of 52) \\(-\\) [(10 of 40) \\(\\times\\) (40 of 52)] = 0.8269231",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Probability and Distributions</span>"
    ]
  },
  {
    "objectID": "13-module.html#random-variables",
    "href": "13-module.html#random-variables",
    "title": "13¬† Probability and Distributions",
    "section": "13.5 Random Variables",
    "text": "13.5 Random Variables\nA random variable is a variable whose outcomes are assumed to arise by chance or according to some random or stochastic process. The chances of observing a specific outcome, or an outcome value within a specific interval, has associated with it a probability.\nRandom variables come in two varieties:\n\nDiscrete Random Variables are random variables that can assume only a countable number of discrete possibilities (e.g., counts of outcomes in a particular category, e.g., rolls of a die). We can assign a probability to each possible outcome.\nContinuous Random Variables are random variables that can assume any real number value within a given range (e.g., measurements of body weight, number of offspring, linear dimensions, etc.). We cannot assign a specific probability to each possible outcome value as the set of possible outcomes is (theoretically) infinite, but we can assign probabilites to intervals of outcome values.\n\nWith these basics in mind, we can define a few more terms:\nA probability function is a mathematical function that describes the chance associated with a random variable either having particular outcomes (for discrete variables) or falling within a given range of outcome values (for continuous variables). Familiar statistical distributions (e.g., the normal distribution, the beta distribution, the Poisson distribution) are all examples of probability functions.\nWe can distinguish two types of probability functions, associated with these different kinds of random variables.\n\nProbability Mass Functions\nProbability Mass Functions (PMFs) are associated with discrete random variables. These functions describe the probability that a random variable takes a particular discrete value.\nTo be a valid PMF, a function \\(f(x)\\) must satisfy the following conditions:\n\nThere are \\(k\\) distinct outcomes \\(x_1, x_2, ... ,x_k\\)\n\\(0 \\leq Pr (X=x_i) \\leq 1\\) for all \\(x_i\\)\n\\(\\sum\\limits_{i=1}^{k} Pr (X=x_i) = 1\\)\n\n\nEXAMPLE: Flipping a Fair Coin\n\noutcomes &lt;- c(\"heads\", \"tails\")\nprob &lt;- c(1/2, 1/2)\nbarplot(prob, ylim = c(0, 0.6), names.arg = outcomes, space = 0.1, xlab = \"outcome\",\n    ylab = \"Pr(X = outcome)\", main = \"Probability Mass Function\")\n\n\n\n\n\n\n\ncumprob &lt;- cumsum(prob)\ncumoutcomes &lt;- c(\"heads\", \"heads + tails\")\nbarplot(cumprob, names.arg = cumoutcomes, space = 0.1, xlab = \"outcome\", ylab = \"Cumulative Pr(X)\",\n    main = \"Cumulative Probability\")\n\n\n\n\n\n\n\n\n\n\nEXAMPLE: Rolling a Fair Die\n\noutcomes &lt;- c(1, 2, 3, 4, 5, 6)\nprob &lt;- c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6)\nbarplot(prob, ylim = c(0, 0.5), names.arg = outcomes, space = 0.1, xlab = \"outcome\",\n    ylab = \"Pr(X = outcome)\", main = \"Probability Mass Function\")\n\n\n\n\n\n\n\ncumprob &lt;- cumsum(prob)\ncumoutcomes &lt;- c(\"1\", \"1 to 2\", \"1 to 3\", \"1 to 4\", \"1 to 5\", \"1 to 6\")\nbarplot(cumprob, names.arg = cumoutcomes, space = 0.1, xlab = \"outcome\", ylab = \"Cumulative Pr(X)\",\n    main = \"Cumulative Probability\")\n\n\n\n\n\n\n\n\n\n\n\nProbability Density Functions\nProbability Density Functions (PDFs) are associated with continuous random variables. These functions describe the probability that a random variable falls within a given range of outcome values. The probability associated with that range equals the area under the density function for that range.\nTo be a valid PDF, a function \\(f(x)\\) must satisfy the following:\n\n\\(f(x)\\geq 0\\) for all \\(-\\infty \\leq x \\leq +\\infty\\). That is, the function \\(f(x)\\) is non-negative everywhere.\n\\(\\int_\\limits{-\\infty}^{+\\infty} f(x) dx = 1\\). That is, the total area under the function \\(f(x)\\) = 1\n\n\nExample: Exploring the Beta Distribution\nThe Beta Distribution refers to a family of continuous probability distributions defined over the interval [0, 1] and parameterized by two positive shape coefficients, denoted by \\(\\alpha\\) and \\(\\beta\\), that appear as exponents of the random variable \\(x\\) and control the shape of the distribution. The beta distribution function is‚Ä¶\n\\[f(x) = x^{\\alpha-1}(1-x)^{\\beta-1}\\]\n\nNOTE: There is nothing special about the Beta Distribution for this example, we are just using it to show how probabilities are equivalent to areas under a function.\n\nWe can explore the beta distribution using the {manipulate} package by entering the code below. With the domain of \\(x\\) restricted to [0, 1], and with \\(\\alpha\\) initially set to 2 and \\(\\beta\\) initially set to 1, we see the PDF for the beta distribution is triangular. Try playing with different values for \\(\\alpha\\) and \\(\\beta\\). The dbeta() function provides the value of the function at the specified values of x.\n\nmanipulate(ggplot(data = data.frame(x = c(0, 1)), aes(x)) + stat_function(fun = dbeta,\n    args = list(shape1 = alpha, shape2 = beta), n = 1000) + xlab(\"x\") + ylab(\"f(x)\") +\n    labs(title = \"Exploring the Beta Distribution\", subtitle = paste0(\"Cumulative Probability = \",\n        round(pbeta(x, alpha, beta), 2))) + stat_function(fun = dbeta, xlim = c(0,\n    x), args = list(shape1 = alpha, shape2 = beta), n = 1000, geom = \"area\"), alpha = slider(0,\n    10, initial = 2, step = 0.1), beta = slider(0, 10, initial = 1, step = 0.1),\n    x = slider(0, 1, initial = 0, step = 0.01))\n\nIs this a PDF? Why or why not? Yes‚Ä¶ it satisfies both criteria for a PDF.\n\n\\(f(x) \\geq 0\\) for all \\(-\\infty \\leq x \\leq +\\infty\\)\nThe total area under \\(f(x)\\) = 1\n\nWe can show this interactively by playing with the slider for \\(x\\) and looking at the shaded area, which represents the cumulative probability integrated across \\(f(x)\\) from \\(-\\infty\\) to \\(x\\).\n\n\n\nCumulative Distribution Functions\nThe cumulative distribution function, or CDF, of a random variable is defined as the probability of observing a random variable \\(X\\) taking the value of \\(x\\) or less, i.e., \\(F(x) = Pr (X \\leq x)\\).\nThis definition actually applies regardless of whether \\(X\\) is discrete or continuous. Note here we are using the notation \\(F(x)\\) for the cumulative distribution function rather than \\(f(x)\\), which we use for the probability density or mass function. For a continuous variable, the PDF is simply the first derivative of the CDF, i.e., \\(f(x) = dF(x)\\).\nThe built in R ‚Äúprobability‚Äù function for the Beta Distribution, pbeta(), gives us this cumulative probability directly, if we specify the values of \\(\\alpha\\) and \\(\\beta\\). E.g., for \\(\\alpha\\) = 2 and \\(\\beta\\) = 1‚Ä¶\n\npbeta(0.75, 2, 1)  # cumulative probability for x ‚â§ 0.75\n\n## [1] 0.5625\n\npbeta(0.5, 2, 1)  # cumulative probability for x ‚â§ 0.50\n\n## [1] 0.25\n\n\nIn general, we find the cumulative probability for a continuous random variable by calculating the area under the probability density function of interest from \\(-\\infty\\) to \\(x\\). This is what is is being returned from pbeta().\n\nThe other related R functions for the Beta Distribution, i.e., rbeta(), dbeta(), and qbeta(), are also useful. rbeta() draws random observations from a specfied beta distribution. dbeta() gives the point estimate of the beta density function at the value of the argument \\(x\\), and qbeta() is essentially the converse of pbeta(), i.e., it tells you the value of \\(x\\) that is associated with a particular cumulative probability, or quantile, of the cumulative distribution function.\n\n\ndbeta(0.75, 2, 1)\n\n## [1] 1.5\n\nqbeta(0.5625, 2, 1)\n\n## [1] 0.75\n\n\nWe can also define the survival function for a random variable \\(X\\) as:\n\\[S(x) = Pr (X \\gt x) = 1 - Pr (X \\leq x) = 1 - F(x)\\]\nPMFs and PDFs for many other standard or well studied distributions have comparable r, d, p, and q functions to those for the Beta Distribution.\nTL/DR: R has four built-in functions that can be applied to a variety of standard and well understood statistical distributions:\n\nr - the random generation function, which draws a random variable from the given distribution.\nd - the density function, PMF or PDF, which describes the distribution of values for the function across the range of \\(x\\) values: \\(f(x)\\).\np - the cumulative distribution function, CDF, which gives the cumulative probability for all values from \\(-\\infty\\) to \\(x\\) for a given distribution: \\(F(x)\\) = Pr \\((X\\) \\(\\leq\\) \\(x)\\).\nq - the quantile function, which is the converse of p: the value of \\(x\\) at which the CDF has the value \\(q\\), i.e., \\(F(x_q) = q\\).\n\nNote the relationship between the p and q functions:\n\npbeta(0.7, 2, 1)  # yields 0.49 - x values ‚â§ 0.7 comprise 49% of the CDF\n\n## [1] 0.49\n\nqbeta(0.49, 2, 1)  # yields 0.7 - 49% of the CDF falls in the range x ‚â§ 0.7\n\n## [1] 0.7\n\n\n\n\nExpected Mean and Variance\nThe mean value (or expectation) of a discrete random variable with a given probability mass function is equivalent to a ‚Äúlong-term average‚Äù, i.e., what you would expect the average value of the variable to be if you sampled from the PMF many, many times. This expectation can be expressed generally as follows:\n\\[\\mu_X = Expectation\\ for\\ X = \\sum x_i \\times Pr (X=x_i)\\ for\\ all\\ x\\ from\\ x_i\\ to\\ x_k\\]\nLikewise, the expected variance of a discrete random variable, \\(X\\), across a large sample from the PMF is:\n\\[\\sigma_X^2 = Variance\\ of\\ X = \\sum (x_i - \\mu_X)^2 \\times Pr (X=x_i)\\ for\\ all\\ x\\ from\\ x_i\\ to\\ x_k\\]\nApplying these formulae to die rolls, we could calculate the expectation for \\(X\\) for a large set of die rolls as follows‚Ä¶\n\\[(1 \\times 1/6) + (2 \\times 1/6)\\  +\\ ...\\ +\\ (6 \\times 1/6) = 3.5\\]\n\nm &lt;- sum(seq(1:6) * 1/6)\nm\n\n## [1] 3.5\n\n\nAnd the expected variance would be‚Ä¶\n\\[[(1 - 3.5)^2 \\times (1/6)]\\ +\\  [(2 - 3.5)^2 \\times (1/6)]\\ +\\ ...\\ +\\ [(6 - 3.5)^2 \\times (1/6)] = 2.916667\\]\n\nvar &lt;- sum((seq(1:6) - mean(seq(1:6)))^2 * (1/6))\nvar\n\n## [1] 2.916667\n\n\nLikewise, we can calculate the expectation and variance for a continuous random variable, \\(X\\), with a given probability density function generally as follows:\n\\[\\mu_X = Expectation\\ for\\ X = \\int\\limits_{-\\infty}^{+\\infty} x f(x) dx\\]\n\\[\\sigma_X^2 = Variance\\ of\\ X = \\int\\limits_{-\\infty}^{+\\infty} (x - \\mu_X)^2 f(x) dx\\]\n\nNOTE: To demonstrate these numerically would require a bit of calculus, i.e., integration, which we will not go through here.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Probability and Distributions</span>"
    ]
  },
  {
    "objectID": "13-module.html#useful-probability-distributions",
    "href": "13-module.html#useful-probability-distributions",
    "title": "13¬† Probability and Distributions",
    "section": "13.6 Useful Probability Distributions",
    "text": "13.6 Useful Probability Distributions\n\nProbability Mass Functions\n\nThe Bernoulli Distribution\nThe Bernoulli Distribution is the probability distribution of a binary random variable, i.e., a variable that has only two possible outcomes, such as success or failure, heads or tails, true or false. If \\(p\\) is the probability of one outcome, then \\(1-p\\) has to be the probabilty of the alternative. For flipping a fair coin, for example, \\(p\\) = 0.5 and \\(1-p\\) also = 0.5.\nFor the Bernoulli Distribution, the probability mass function is:\n\\[f(x) = p^x(1-p)^{1-x}\\]\nwhere x = {0 or 1}\nFor this distribution, the expectation and variance across a large set of trials would be: \\(\\mu_X = p\\) and \\(\\sigma_X^2 = p(1-p)\\)\n\n\n\nCHALLENGE\nUsing the Bernoulli distribution, calculate the expectation for drawing a spade from a deck of cards? What is the variance in this expectation across a large number of draws?\n\\[Pr (spade) = (13/52)^1 \\times (39/52)^0 = 0.25\\]\n\\[Var (spade) = (13/52) \\times (1-13/52) = (0.25) \\times (0.75) = 0.1875\\]\nIn code, we can simulate this‚Ä¶\n\nreps &lt;- 1e+05\ncard &lt;- sample(1:52, reps, replace = TRUE)\nspade &lt;- ifelse(card &lt;= 13, TRUE, FALSE)\n(exp_spade &lt;- sum(spade)/reps)\n\n## [1] 0.24968\n\n(exp_var_spade &lt;- var(spade))\n\n## [1] 0.1873418\n\n\n\nThe Binomial Distribution\nThe Bernoulli distribution is a special case of the Binomial Distribution. The binomial distribution is typically used to model the probability of a number of ‚Äúsuccesses‚Äù, k, out of a set of ‚Äútrials‚Äù, n, i.e., for counts of a particular outcome.\nAgain, the probability of success on each trial = \\(p\\) and the probability of not success = \\(1-p\\).\nFor the Binomial Distribution, the probability mass function is:\n\\[f(x)=\\binom{n}{k} p^k (1-p)^{n-k}\\]\nwhere \\(x\\) = {0, 1, 2, ‚Ä¶ , n} and where\n\\[\\binom{n}{k}=\\frac{n!}{k!(n-k)!}\\]\nThis is read as ‚Äú\\(n\\) choose \\(k\\)‚Äù, i.e., the probability of \\(k\\) successes out of \\(n\\) trials. This is also called the ‚Äúbinomial coefficient‚Äù.\nFor this distribution, the expectation (i.e., expected number of successes out of \\(k\\) trials) and the expected variance are as follows:\n\\[\\mu_X = np\\] and\n\\[\\sigma_X^2 = np(1-p)\\]\nRecall, \\(\\mu_X\\) = expected number of successes in \\(n\\) trials. Where \\(n\\) = 1, the binomial distribution simplifies to the Bernoulli distribution.\n\n\n\nCHALLENGE\n\nWhat is the chance of getting a ‚Äú1‚Äù on each of six consecutive rolls of a die? Recall that rolling a ‚Äú1‚Äù = a success, while rolling something other than a ‚Äú1‚Äù is not a success.\nWhat about of getting exactly three ‚Äú1‚Äùs (i.e., 3 successes).\nWhat is the expected number of ‚Äú1‚Äùs to occur in six consecutive rolls (i.e., what is the expected number of successes) and the variance around that expectation?\n\n\nn &lt;- 6  # number of trials\nk &lt;- 6  # exact number of successes\np &lt;- 1/6\n(all_ones &lt;- (factorial(n)/(factorial(k) * factorial(n - k))) * (p^k) * (1 - p)^(n -\n    k))\n\n## [1] 2.143347e-05\n\nk &lt;- 3  # exact number of successes\n(three_ones &lt;- (factorial(n)/(factorial(k) * factorial(n - k))) * (p^k) * (1 - p)^(n -\n    k))\n\n## [1] 0.05358368\n\n# expected number of successes\n(expected_ones &lt;- n * p)\n\n## [1] 1\n\n(expected_variance &lt;- n * p * (1 - p))\n\n## [1] 0.8333333\n\n\nAs for other distributions, R has a built in d (density) function, the dbinom() function, that you can use to solve for the probability of a given outcome directly, i.e., Pr \\((X = x)\\).\n\ndbinom(x = k, size = n, prob = p)\n\n## [1] 0.05358368\n\n\nWe can also use the built in function pbinom() to return the value of the cumulative distribution function for the binomial distribution, i.e., the probability of observing up to and including a given number of successes in \\(n\\) trials.\nSo, for example, the chances of observing exactly 0, 1, 2, 3, ‚Ä¶ 6 rolls of ‚Äú1‚Äù on 6 rolls of a die are‚Ä¶\n\nprobset &lt;- dbinom(x = 0:6, size = 6, prob = 1/6)\n# x is number of successes, size is number of trials\nbarplot(probset, names.arg = 0:6, space = 0, xlab = \"outcome = # of 'ones' seen in 6 rolls\",\n    ylab = \"Pr(X = outcome)\", main = \"Probability Mass Function\")\n\n\n\n\n\n\n\ncumprob = cumsum(probset)\nbarplot(cumprob, names.arg = 0:6, space = 0.1, xlab = \"outcome\", ylab = \"Cumulative Pr(X)\",\n    main = \"Cumulative Probability\")\n\n\n\n\n\n\n\nsum(probset)  # equals 1, as it should\n\n## [1] 1\n\n\nThe chance of observing exactly 3 rolls of ‚Äú1‚Äù is‚Ä¶\n\ndbinom(x = 3, size = 6, prob = 1/6)\n\n## [1] 0.05358368\n\n\nAnd the chance of observing up to and including 3 rolls of ‚Äú1‚Äù in 6 rolls is‚Ä¶\n\npbinom(q = 3, size = 6, prob = 1/6)\n\n## [1] 0.991298\n\n# note the name of the argument is `q=` not `x=`\n\n‚Ä¶ which can also be calculated by summing the relevant individual outcome probabilities‚Ä¶\n\n# this sums the probabilities of 0, 1, 2, and 3 successes\nsum(dbinom(x = 0:3, size = 6, prob = 1/6))\n\n## [1] 0.991298\n\n\nThe probability of observing more than 3 rolls of ‚Äú1‚Äù is given as‚Ä¶\n\n1 - pbinom(q = 3, size = 6, prob = 1/6)\n\n## [1] 0.008701989\n\n\nor, alternatively‚Ä¶\n\npbinom(q = 3, size = 6, prob = 1/6, lower.tail = FALSE)\n\n## [1] 0.008701989\n\n\nThe probability of observing three or more rolls of ‚Äú1‚Äù is‚Ä¶\n\n1 - pbinom(q = 2, size = 6, prob = 1/6)\n\n## [1] 0.06228567\n\n# note here that the `q=` argument is '2'\n\nor, alternatively‚Ä¶\n\npbinom(q = 2, size = 6, prob = 1/6, lower.tail = FALSE)\n\n## [1] 0.06228567\n\n\nAbove, we were using the theoretical binomial distribution to answer our questions about expectations‚Ä¶ Let‚Äôs do this same process by simulation!\n\n# simulate num_rolls rolls reps times\nreps &lt;- 1e+06\nnum_rolls &lt;- 6\n# set up vectors to hold simulation results\nall_ones &lt;- vector(mode = \"logical\", length = reps)\nthree_ones &lt;- vector(mode = \"logical\", length = reps)\ncount_ones &lt;- vector(mode = \"logical\", length = reps)\nfor (i in 1:reps) {\n    rolls &lt;- sample(c(1, 2, 3, 4, 5, 6), size = num_rolls, replace = TRUE)\n    all_ones[[i]] &lt;- all(rolls == 1)\n    if (sum(rolls == 1) == 3) {\n        three_ones[[i]] &lt;- TRUE\n    } else {\n        three_ones[[i]] &lt;- FALSE\n    }\n    count_ones[[i]] &lt;- sum(rolls == 1)\n}\n# all 'ones' in 6 rolls\nsum(all_ones)/reps\n\n## [1] 2e-05\n\n# three 'ones' in 6 rolls\nsum(three_ones)/reps\n\n## [1] 0.053175\n\n# expected # 'ones' in 6 rolls\nmean(count_ones)\n\n## [1] 0.997626\n\n# variance in expected # 'ones' across rolls\nsum((count_ones - mean(count_ones))^2)/(length(count_ones))\n\n## [1] 0.8320604\n\n\n\nThe Poisson Distribution\nThe Poisson Distribution is often used to model open ended counts of independently occuring events, for example the number of cars that pass a traffic intersection over a given interval of time or the number of times a monkey scratches itself during a given observation interval. The probability mass function for the Poisson distribution is described by a single parameter, \\(\\lambda\\), where \\(\\lambda\\) can be interpreted as the mean number of occurrences of the event in the given interval.\nThe probability mass function for the Poisson Distribution is:\n\\[f(x)=\\frac{\\lambda^x\\exp(-\\lambda)}{x!}\\]\nwhere \\(x\\) = {0, 1, 2, ‚Ä¶}\nFor this distribution, \\(\\mu_X = \\lambda\\) and \\(\\sigma_X^2 = \\lambda\\)\nNote that for the Poisson Distribution the mean and the variance are the same!\nLet‚Äôs use R and the {moasic} package to look at the probability mass functions for different values of \\(\\lambda\\):\n\nl &lt;- 3.5\np1 &lt;- plotDist(\"pois\", lambda = l, main = paste0(\"Poisson Distribution\\nwith lambda=\",\n    l), xlab = \"x\", ylab = \"Pr(X=x)\")\nl &lt;- 10\np2 &lt;- plotDist(\"pois\", lambda = l, main = paste0(\"Poisson Distribution\\nwith lambda=\",\n    l), xlab = \"x\", ylab = \"Pr(X=x)\")\nl &lt;- 20\np3 &lt;- plotDist(\"pois\", lambda = l, main = paste0(\"Poisson Distribution\\nwith lambda=\",\n    l), xlab = \"x\", ylab = \"Pr(X=x)\")\nplot_grid(p1, p2, p3, nrow = 1)\n\n\n\n\n\n\n\n\nAs we did for other distributions, we can also use the built in p function for the Poisson distribution, ppois(), to return the value of the cumulative distribution function, i.e., the probability of observing up to and including a specific number of events in the given interval.\n\nl &lt;- 3.5\np1 &lt;- plotDist(\"pois\", lambda = l, kind = \"cdf\", main = paste0(\"Cumulative Probability\\nwith lambda=\",\n    l), xlab = \"x\", ylab = \"Pr(X‚â§x)\", type = \"l\")\nl &lt;- 10\np2 &lt;- plotDist(\"pois\", lambda = l, kind = \"cdf\", main = paste0(\"Cumulative Probability\\nwith lambda=\",\n    l), xlab = \"x\", ylab = \"Pr(X‚â§x)\", type = \"l\")\nl &lt;- 20\np3 &lt;- plotDist(\"pois\", lambda = l, kind = \"cdf\", main = paste0(\"Cumulative Probability\\nwith lambda=\",\n    l), xlab = \"x\", ylab = \"Pr(X‚â§x)\", type = \"l\")\nplot_grid(p1, p2, p3, nrow = 1)\n\n\n\n\n\n\n\n\n\n\n\nProbability Density Functions\n\nThe Uniform Distribution\nThe Uniform Distribution is the simplest probability density function describing a continuous random variable. The probability is uniform and does not fluctuate across the range of \\(x\\) values in a given interval.\nThe probability density function for the Uniform Distribution is:\n\\[f(x)=\\frac{1}{b-a}\\]\nwhere \\(a \\leq x \\leq b\\) and the function is 0 for \\(x &lt; a\\) and \\(x &gt; b\\)\nWhat would you predict the expectation (mean) should be for a uniform distribution? Not surprisingly, for this distribution:\n\\[\\mu_x = \\frac{a+b}{2}\\]\nand\n\\[\\sigma_x^2 = \\frac{(b-a)^2}{12}\\]\nLet‚Äôs plot a uniform distribution across a given range, from \\(a\\) = 4 to \\(b\\) = 8‚Ä¶\n\na &lt;- 4\nb &lt;- 8\nx &lt;- seq(from = a - 1, to = b + 1, by = 0.01)\nfx &lt;- dunif(x, min = a, max = b)  # dunif() evaluates the density at each x\nplot(x, fx, ylim = c(0, max(fx) + 0.1), type = \"l\", xlab = \"x\", ylab = \"f(x)\", main = \"Probability Density Function\")\n\n\n\n\n\n\n\n\nNote that for the uniform distribution, the cumulative density function increases linearly over the given interval.\n\n# punif() is the cumulative probability density up to a given x\nplot(x, punif(q = x, min = a, max = b), ylim = c(0, 1.1), type = \"l\", xlab = \"x\",\n    ylab = \"Pr(X ‚â§ x)\", main = \"Cumulative Probability\")\n\n\n\n\n\n\n\n\n\n\n\nCHALLENGE\nSimulate a sample of 10,000 random numbers from a uniform distribution in the interval between \\(a\\) = 6 and \\(b\\) = 8. Calculate the mean and variance of this simulated sample and compare it to the expectation for these parameters.\n\n\nShow Code\na &lt;- 6\nb &lt;- 8\nnums &lt;- runif(n = 10000, min = a, max = b)\n(m &lt;- mean(nums))\n\n\nShow Output\n## [1] 7.004909\n\n\n\nShow Code\n(v &lt;- var(nums))\n\n\nShow Output\n## [1] 0.3381391\n\n\n\nShow Code\n(expected_mean &lt;- (a + b)/2)\n\n\nShow Output\n## [1] 7\n\n\n\nShow Code\n(expected_variance &lt;- ((b - a)^2)/12)\n\n\nShow Output\n## [1] 0.3333333\n\n\n\n\nThe Normal Distribution\nThe Normal or Gaussian Distribution is perhaps the most familiar and most commonly applied probability density functions for modeling continuous random variables. Why is the normal distribution so important? Well, many traits are normally distributed, and the additive combination of many random factors is also commonly normally distributed. Even more importantly, as we will see in Module 14, the sampling distribution for many summary statistics (e.g., sample means) tends to be normally distributed when sample size is sufficiently large, and this fact is central to statistical inference.\nFor the normal distribution, two parameters, \\(\\mu\\) and \\(\\sigma\\), are used to describe the shape of the distribution.\nThe code below allows us to visualize and play around with a normal distribution. First, try maniupulating \\(\\mu\\) and \\(\\sigma\\).\n\nmanipulate(ggplot(data = data.frame(x = c(mu - 6 * sigma, mu + 6 * sigma)), aes(x)) +\n    stat_function(fun = dnorm, args = list(mean = mu, sd = sigma), n = 1000) + xlab(\"x\") +\n    ylab(\"f(x)\") + labs(title = \"Exploring the Normal Distribution\", subtitle = paste0(\"Probability ¬± \",\n    n_sigma, \" SD around Mean = \", round(pnorm(mu + n_sigma * sigma, mu, sigma) -\n        pnorm(mu - n_sigma * sigma, mu, sigma), 4))) + stat_function(fun = dnorm,\n    xlim = c(mu - n_sigma * sigma, mu + n_sigma * sigma), args = list(mean = mu,\n        sd = sigma), n = 1000, geom = \"area\", fill = \"red\", alpha = 0.5, color = \"red\"),\n    mu = slider(-100, 100, initial = 0, step = 10), sigma = slider(0, 30, initial = 5,\n        step = 1), n_sigma = slider(0, 4, initial = 0, step = 0.25))\n\nThe function, dnorm() gives the value of the normal probabilty density function at a given value of \\(x\\). \\(x\\) can range from -\\(\\infty\\) to +\\(\\infty\\). [Recall, it does not really make sense to talk about the ‚Äúprobability‚Äù associated with a given specific value of a continuous variable as this is a density not a mass function‚Ä¶ but we can talk about the probability of \\(x\\) falling within a given interval.]\nThe pnorm() function, as with the p function for other distributions, returns the cumulative probability of observing a value less than or equal to a given value of \\(x\\), i.e., Pr \\((X\\) \\(\\leq\\) \\(x)\\). We can use the pnorm() function to calculate the probability of an observation drawn from the population falling within a particular, arbitrary interval.\nFor example, for a normally distributed population variable with \\(\\mu\\) = 6 and \\(\\sigma\\) = 2, the probability of a random observation falling between 7 and 8 is‚Ä¶\n\np &lt;- pnorm(8, mean = 6, sd = 2) - pnorm(7, mean = 6, sd = 2)\np\n\n## [1] 0.1498823\n\n\nIn the code above, this is what we are doing when we play interactively with \\(n\\_sigma\\)‚Ä¶ we are using the difference between two pnorm() calculations to determine the probability of an observation falling within \\(n\\_sigma\\) standard deviations of the mean of a particular normal distribution.\n\nNOTE: Regardless of the specific values of \\(\\mu\\) and \\(\\sigma\\), about 95% of the normal distribution falls within 2 standard deviations of the mean, and about 68% of the distribution falls within 1 standard deviation. Check this out by playing with the \\(n\\_sigma\\) slider.\n\n\n\n\n\n\n\n\n\n\nAnother one of the family of normal distribution functions in R - the qnorm() or quantile function - will tell us the value of \\(x\\) below which a given proportion of the cumulative probability function falls. For example, if we wanted to see the value of \\(x\\) below which 2.5% of a normal distribution with mean of 5 and standard deviation of 6 falls, we could run the following:\n\nqnorm(0.025, mean = 5, sd = 3, lower.tail = TRUE)\n\n## [1] -0.879892\n\n# lower.tail = TRUE by default...if we used lower.tail = FALSE this would give\n# us the value of x for the upper.tail of the distribution\n\n\n\n\nCHALLENGE\n\nCreate a vector, v, containing n random numbers selected from a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). Use 1000 for n, 3.5 for \\(\\mu\\), and 4 for \\(\\sigma\\).\n\n\nHINT: Such a function exists! rnorm(). We also call set.seed() before rnorm() so that each time this function is run, it returns the same sequence of random numbers until the seed is reset.\n\n\nCalculate the mean, variance, and standard deviation for your sample of random numbers.\nPlot a histogram of your random numbers.\n\n\n\nShow Code\nn &lt;- 1000\nmu &lt;- 3.5\nsigma &lt;- 4\nset.seed(1)\nv &lt;- rnorm(n, mu, sigma)\nmean(v)\n\n\nShow Output\n## [1] 3.453407\n\n\n\nShow Code\nvar(v)\n\n\nShow Output\n## [1] 17.13681\n\n\n\nShow Code\nsd(v)\n\n\nShow Output\n## [1] 4.139663\n\n\n\nShow Code\nhistogram(v, main = paste0(\"Random Draws from a Normal Distribution\\nwith Mean = \",\n    mu, \" and SD = \", sigma), type = \"density\", center = mu)\n\n\n\n\n\n\n\n\n\nWe can easily plot the density and cumulative probability functions for this normal distribution using the plotDist() function from the {mosaic} package:\n\npdf &lt;- plotDist(\"norm\", mean = mu, sd = sigma, xlab = \"X\", ylab = \"Density\")\ncdf &lt;- plotDist(\"norm\", mean = mu, sd = sigma, kind = \"cdf\", xlab = \"X\", ylab = \"Cumulative Probability\")\nplot_grid(pdf, cdf)\n\n\n\n\n\n\n\n\n\nQ-Q Plots\nA quantile-quantile or Q-Q plot can be used to look at whether a set of data seem to follow a normal distribution. A Q‚ÄìQ plot is a general graphical method for comparing two probability distributions. To examine a set of data for normality graphically, you plot the quantiles for your actual data (as the y values) versus theoretical quantiles (as the x values) pulled from a normal distribution. If the two distributions being compared are similar, the points in the plot should lie approximately on the line y = x.\nIf we do this for the random variables we just generated, this should be apparent since we have simulated our initial vector of data from a normal distribution.\nTo quickly do a Q-Q plot, call the two R functions qqnorm() and qqline() using the vector of data you want to examine as an argument.\n\nqqnorm(v, main = \"QQ Plot - Random Normal Variable\")\nqqline(v, col = \"gray\")\n\n\n\n\n\n\n\n\nThis is the same as doing the following:\n\nStep 1: Generate a sequence of probability points in the interval from 0 to 1 equivalent in length to vector v:\n\n\np &lt;- ppoints(length(v))\n# the `ppoints()` function generates evenly distributed points between 0 and 1\nhead(p)\n\n## [1] 0.0005 0.0015 0.0025 0.0035 0.0045 0.0055\n\ntail(p)\n\n## [1] 0.9945 0.9955 0.9965 0.9975 0.9985 0.9995\n\n\n\nStep 2: Calculate the theoretical quantiles for this set of probabilities based on the distribution you want to compare to (in this case, the normal distribution):\n\n\ntheoretical_q &lt;- qnorm(p)\n# generates quantiles for each p based on a normal distribution (in this case,\n# the standard normal)\n\n\nStep 3: Calculate the quantiles for your set of observed data for the same number of points:\n\n\nobserved_q &lt;- quantile(v, ppoints(v))  # finds quantiles in our actual data\n\n\nStep 4: Plot these quantiles against one another:\n\n\nplot(theoretical_q, observed_q, main = \"QQ Plot - Random Normal Variable\", xlab = \"Theoretical Quantiles\",\n    ylab = \"Sample Quantiles\")\n\n\n\n\n\n\n\n\n\n\n\nCHALLENGE\nWhat happens if you simulate fewer observations in your vectors? Try with n = 100.\n\n\nShow Code\nn &lt;- 100\nmu &lt;- 3.5\nsigma &lt;- 4\nv &lt;- rnorm(n, mu, sigma)\nqqnorm(v, main = \"QQ Plot - Random Normal Variable\")\nqqline(v, col = \"gray\")\n\n\n\n\n\n\n\n\n\nShow Code\n# with fewer simulated points, line isn't quite as good a fit\n\n\nWhat happens if you simulate observations from a different distribution?\n\n\nShow Code\nn &lt;- 1000\nv &lt;- rbeta(n, shape1 = 1.3, shape2 = 2)\nqqnorm(v, main = \"QQ Plot - Random Beta Variable\")\nqqline(v, col = \"gray\")\n\n\n\n\n\n\n\n\n\nShow Code\n# with a different distribution, Q-Q plot isn't linear!\n\n\n\nThe ‚ÄúStandard Normal‚Äù Distribution\nAny normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) can be converted into what is called the standard normal distribution, where the mean is 0 and the standard deviation is 1. This is done by subtracting the mean from all observations and dividing these differences by the standard deviation. The resultant values are referred to a Z scores, and they reflect the number of standard deviations an observation is from the mean.\n\nmu &lt;- 5\nsigma &lt;- 8\n# simulate from a normal distribution with mean 5 and SD 8\nx &lt;- rnorm(10000, mean = mu, sd = sigma)\nx_plot &lt;- histogram(x, center = mu, main = paste0(\"Mean = \", round(mean(x), 3), \"\\nSD = \",\n    round(sd(x), 3)))\n# standardize the scores\nz &lt;- (x - mean(x))/sd(x)\nz_plot &lt;- histogram(z, center = 0, main = paste0(\"Mean = \", round(mean(z), 3), \"\\nSD = \",\n    round(sd(z), 3)))\nplot_grid(x_plot, z_plot)",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Probability and Distributions</span>"
    ]
  },
  {
    "objectID": "13-module.html#sampling-distributions-redux",
    "href": "13-module.html#sampling-distributions-redux",
    "title": "13¬† Probability and Distributions",
    "section": "13.7 Sampling Distributions Redux",
    "text": "13.7 Sampling Distributions Redux\nIt is important to recognize that, above, we were dealing with probability distributions of discrete and continuous random variables as they relate to populations. But, as we have talked about before, we almost never measure entire populations‚Ä¶ instead, we measure samples from populations and we characterize our samples using various statistics. The theoretical probability distributions described above (and others) are models for how we connect observed sample data to populations, taking into account various assumptions, and this is what allows us to do many types of inferential statistics. The most fundamental assumption is that the observations we make are independent from one another and are identically distributed, an assumption often abbreviated as iid. Obvious cases of violation of this assumption are rife in the scientific literature, and we should always be cautious about making this assumption!\nHowever, the important thing for us to know is that we can get unbiased estimates of population level parameters based on sample statistics.\n\n\nCHALLENGE\nLet‚Äôs imagine a population of 1 million zombies whose age at zombification is characterized by a normal distribution with a mean of 25 years and a standard deviation of 5 years.\nCreate a variable x that describes this population and calculate the mean and standard deviation of the age at zombification.\n\n\nShow Code\nset.seed(1)\nx &lt;- rnorm(1e+06, 25, 5)\nhistogram(x, type = \"density\")\n\n\n\n\n\n\n\n\n\nShow Code\n(mu &lt;- mean(x))  # population mean\n\n\nShow Output\n## [1] 25.00023\n\n\n\nShow Code\n(sigma &lt;- sqrt(sum((x - mean(x))^2)/length(x)))  # population SD\n\n\nShow Output\n## [1] 5.000924\n\n\n\n\nNOTE: We do not use the sd() function as this would divide by length(x)-1. Check that out using sd(x)\n\n\nsd(x)\n\n## [1] 5.000926\n\n\nNow, suppose we now sample from the zombie population by trapping sets of zombies and determining the mean age at zombification in each set. We sample without replacement from the original population for each set‚Ä¶\n\n\nCHALLENGE\nCreate a loop to sample the zombie population 5000 times with samples of size 10 and store these results in a list, s.\n\n\nShow Code\nk &lt;- 5000  # number of samples\nn &lt;- 10  # size of each sample\ns &lt;- list()  # create a dummy variable to hold each sample\n# dummy variable needs to be a list, not a vector, since each element will hold\n# a vector of numbers\nfor (i in 1:k) {\n    s[[i]] &lt;- sample(x, size = n, replace = FALSE)\n}\nhead(s, 1)  # an example of one sample of 10 ages\n\n\nShow Output\n## [[1]]\n##  [1] 32.48970 25.31725 25.52090 33.68150 22.90267 14.67928 26.26720 23.67120\n##  [9] 17.89460 31.55967\n\n\n\nFor each of these samples, we can then calculate a mean age, which is a statistic describing each sample. That statistic itself is a random variable with a mean and distribution! As we saw in Module 12, this is known as a sampling distribution.\n\n\nCHALLENGE\nCreate a loop to calculate the mean of each of your 5000 samples and store the result in a vector, m.\n\n\nShow Code\nm &lt;- vector(length = k)  # create a dummy variable to hold the mean of each sample\n# here the dummy variable can be a vector and we can preallocate its length\nfor (i in 1:k) {\n    m[[i]] &lt;- mean(s[[i]])\n}\nhead(m)\n\n\nShow Output\n## [1] 25.39840 22.48537 26.99638 26.14543 24.84191 25.72639\n\n\n\nSo, how does the sampling distribution of mean ages compare to the population distribution of ages? The mean of the two is pretty close to the same! The sampling distribution mean, which is an average of the set of sample averages, is an unbiased estimator for the population mean.\n\nmean(m)  # this is the mean of our set of sample means\n\n## [1] 25.0161\n\n# i.e., the mean of the sampling distribution is almost equal to...\nmu  # the true population mean\n\n## [1] 25.00023\n\n\nAgain, what we have just calculated is the mean of the sampling distribution, which is simply the average of the means of each sample we have made. This value should be really close to the population mean, and the sampling distribution - the distribution of sample means - should be about normally distributed around the true population mean.\n\n(p &lt;- ggplot(data = as.data.frame(m), aes(x = m)) + geom_histogram(binwidth = function(x) (max(m) -\n    min(m))/20, alpha = 0.75) + labs(title = paste0(\"Sampling Distribution\"), subtitle = paste0(\"Means of \",\n    k, \" samples of size \", n)) + xlab(\"Sample Mean\") + ylab(\"Frequency\") + geom_vline(xintercept = mu,\n    color = \"blue\", linetype = 2, size = 1) + annotate(geom = \"text\", x = mu, y = k *\n    0.06, label = \"True Population Mean\\n\", color = \"blue\", angle = 90, size = 6) +\n    geom_vline(xintercept = mean(m), color = \"red\", linetype = 2, size = 1) + annotate(geom = \"text\",\n    x = mean(m) + 1, y = k * 0.06, label = \"Mean of Sample Means\\n\", color = \"red\",\n    angle = 90, size = 6))\n\n## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n## ‚Ñπ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nNow, let‚Äôs take a closer look at our samples of zombies that we extracted from the population of zombies‚Ä¶\n\n\nCHALLENGE\nCalculate a vector of standard error estimates based on the 5000 samples of zombies you have drawn from your population and then plot a histogram of the distribution of these standard errors. That is, for each sample, estimate the standard error of the sample (\\(\\frac{s}{\\sqrt{n}}\\)). How does the mean of this set of standard errors compare to the standard error estimated from the population variance? How does it compare to the standard error calculated from the standard deviation of the sampling distribution?\n\n\nShow Code\nsample_sd &lt;- vector(length = k)  # create a dummy variable to hold the SD of each sample\nfor (i in 1:k) {\n    sample_sd[[i]] &lt;- sd(s[[i]])  # a vector of SDs for each sample\n}\nsample_se &lt;- sample_sd/sqrt(n)  # a vector of SEs estimated from each sample\npop_se &lt;- sigma/(sqrt(n))  # a single value estimated from the population SD\nsampling_dist_se &lt;- sd(m)  # a single value calculated from our sampling distibution SD\n\n(p &lt;- ggplot(data = as.data.frame(sample_se), aes(x = sample_se)) + geom_histogram(binwidth = function(x) (max(sample_se) -\n    min(sample_se))/20, alpha = 0.75) + labs(title = paste0(\"Distribution of SEs estimated from\\n\",\n    k, \" samples of size \", n)) + xlab(\"Estimated SE\") + ylab(\"Frequency\") + geom_vline(xintercept = mean(sample_se),\n    color = \"red\", linetype = 2, size = 1) + annotate(geom = \"text\", x = mean(sample_se),\n    y = k * 0.075, label = \"Mean Estimated SE\\n\", color = \"red\", angle = 90, size = 4) +\n    geom_vline(xintercept = pop_se, color = \"blue\", linetype = 2, size = 1) + annotate(geom = \"text\",\n    x = pop_se, y = k * 0.075, label = \"\\n\\nSE calculated from known population\\nstandard deviation\",\n    color = \"blue\", angle = 90, size = 4) + geom_vline(xintercept = sampling_dist_se,\n    color = \"green\", linetype = 2, size = 1) + annotate(geom = \"text\", x = sampling_dist_se,\n    y = k * 0.075, label = \"\\nSE estimated as SD of sampling distribution\", color = \"green\",\n    angle = 90, size = 4))\n\n\n\n\n\n\n\n\n\nShow Code\nmean(sample_se)  # which is almost equal to...\n\n\nShow Output\n## [1] 1.532473\n\n\n\nShow Code\npop_se  # which is almost equal to...\n\n\nShow Output\n## [1] 1.581431\n\n\n\nShow Code\nsampling_dist_se\n\n\nShow Output\n## [1] 1.604008\n\n\n\nThus, the standard error calculated from an individual sample can be used as an estimator for the standard deviation of the sampling distribution. This is useful, since it means that, if our sample is large enough, we do not have to repeatedly sample from the population to get an estimate of the sampling distribution - we can instead estimate it directly using our data!\nNote that as our sample size increases, the standard error calculated from the population variance should decrease, as should the standard deviation in the sampling distribution, i.e., in estimates of the population mean drawn from successive samples. This should be apparent intuitively‚Ä¶ as each sample drawn from a population gets larger, the estimate of the mean value of those samples should vary less and less and thus have lower ‚Äúerror‚Äù.\nDespite their similarities, the standard error of the mean estimated for a given sample and the standard deviation of that sample tell us different things. The standard error is an estimate of how far a given sample mean is likely to be from the population mean - it is a measure of uncertainty. The standard deviation of a sample is a measure of the degree to which individual values within a sample differ from the mean for that sample.\n\n# | include: false\ndetach(package:cowplot)\ndetach(package:manipulate)\ndetach(package:mosaic)\ndetach(package:tidyverse)",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Probability and Distributions</span>"
    ]
  },
  {
    "objectID": "13-module.html#concept-review",
    "href": "13-module.html#concept-review",
    "title": "13¬† Probability and Distributions",
    "section": "Concept Review",
    "text": "Concept Review\n\nUseful R functions for distributions: r (random value generation), d (point density), p (cumulative probability distribution), q (quantile)\nUseful distributions:\n\nPMFs for discrete random variables - Bernoulli, Binomial, Poisson\nPDFs for continuous random variables - Beta, Uniform, Normal and Standard Normal\n\nStandard error of the mean for a sample of size \\(n\\):\n\n(population standard deviation, or \\(\\sigma\\)) \\(\\div\\) \\(\\sqrt{n}\\)\n‚âà standard deviation of the sampling distribution for a large set of samples of size \\(n\\)\n‚âà (standard deviation of a single sample of size \\(n\\), or \\(s\\)) \\(\\div\\) \\(\\sqrt{n}\\)",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Probability and Distributions</span>"
    ]
  },
  {
    "objectID": "14-module.html",
    "href": "14-module.html",
    "title": "14¬† Confidence Intervals",
    "section": "",
    "text": "14.1 Objectives",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "14-module.html#objectives",
    "href": "14-module.html#objectives",
    "title": "14¬† Confidence Intervals",
    "section": "",
    "text": "The objective of this module is to provide an introduction to statistical inference and hypothesis testing by introducing the idea of confidence intervals around statistics.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "14-module.html#preliminaries",
    "href": "14-module.html#preliminaries",
    "title": "14¬† Confidence Intervals",
    "section": "14.2 Preliminaries",
    "text": "14.2 Preliminaries\n\nInstall and load this package in R: {boot}\nLoad {tidyverse}, {manipulate}, and {mosaic}",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "14-module.html#confidence-intervals",
    "href": "14-module.html#confidence-intervals",
    "title": "14¬† Confidence Intervals",
    "section": "14.3 Confidence Intervals",
    "text": "14.3 Confidence Intervals\nThe standard error for a statistic that we calculate given its sampling distribution or that we estimate based on a particular sample can be used to derive another measure of uncertainty in the statistic value - the confidence interval, or CI. The CI is another way of describing a statistic‚Äôs sampling distribution, and it plays a central role in basic inferential statistics.\nConceptually, the confidence interval is an interval around our estimate of mean of the sampling distribution for a particular statistic (typically a mean), and it gives us a range of values into which subsequent estimates of a statistic would be expected to fall some critical proportion of the time, if the sampling exercise were to be repeated. We can talk thus about different confidence intervals (e.g., 50%, 95%, 99%), where, intuitively, higher confidence is associated with a wider interval. The ‚Äú95% CI‚Äù around a statistic, for example, describes the range of values into which a new estimate of the statistic, derived from a subsequent sample, would be expected to fall 95% of the time. The ‚Äú99% CI‚Äù around the initial statistic would be slightly wider, while the ‚Äú50% CI‚Äù would be narrower.\n\nCalculating Theoretical CIs\nTheoretical confidence intervals typically represent the range of values corresponding to the central X% of a given sampling distribution with a mean of \\(m\\) and a standard deviation of \\(se\\). We can calculate theoretical CIs in several ways.\nThe typical way to calculate a confidence interval is as the value of the statistic being considered \\(¬±\\) some critical value \\(\\times\\) the standard error of the statistic, where the critical value is determined with respect to the quantiles bracketing the central proportion of interest for a particular zero-centered, standardized theoretical distribution (e.g., the standard normal or the Student‚Äôs \\(t\\)). The lower and upper limits of this CI thus should bracket the central proportion of interest of the sampling distribution of the statistic.\nAlternatively, we can calculate the lower and upper limits directly from the appropriate quantile values of the relevant non-standardized version of the particular theoretical distribution (e.g., the normal, with mean \\(m\\) and standard deviation \\(se\\)).\nFor large sample sizes (\\(n\\) ‚â• ~30), the theoretical distribution used to define the critical values and the relevant quantiles is typically the normal distribution, while for smaller sample sizes (\\(n\\) &lt; ~30), a Student‚Äôs t distribution is used. This is because, according to the Central Limit Theorem (see below), the sampling distribution for many summary statistics (such as the mean) tends to be normal when sample size is reasonably large.\nAs an example CI calculation, we will generate a vector, v, of 10,000 random numbers selected from a normal distribution with a mean of 25 and standard deviation of 10. We then will estimate the mean, standard deviation, and standard error based on a sample of 40 observations drawn from that vector and then use these to calculate the theoretical 95% CI.\nFirst, we generate our vector of random numbers and calculate the relevant summary statistics‚Ä¶\n\nn &lt;- 10000\nmu &lt;- 25\nsigma &lt;- 10\nset.seed(1)\nv &lt;- rnorm(n, mu, sigma)\ns &lt;- sample(v, size = 40, replace = FALSE)\n(m &lt;- mean(s))\n\n## [1] 24.50788\n\n(se &lt;- sigma/sqrt(length(s)))  # if population standard deviation is known\n\n## [1] 1.581139\n\n(sd &lt;- sd(s))\n\n## [1] 10.35182\n\n(se &lt;- sd(s)/sqrt(length(s)))  # if population standard deviation is unknown\n\n## [1] 1.636766\n\n# alternatively, we could use `sciplot::se(s)`\n(se &lt;- sciplot::se(s))\n\n## [1] 1.636766\n\n\nNow, to calculate the 95% CI around our estimate of the mean, we need to calculate the critical values that delimit the central 95% of the standard normal distribution and then use these to define the upper and lower bounds of the 95% CI around our sample mean. The central 95% of a distribution is that falling between the 2.5% and 97.5% quantiles (i.e., all but 2.5% of the distribution in the lower and upper tails). If we define \\(\\alpha\\) as \\(1-CI/100\\), then what we want to find is the \\(\\alpha/2\\) and \\(1 - (\\alpha/2)\\) quantiles, i.e., the \\(0.025\\) and \\(0.975\\) quantiles. [As we will see below, mathematically, this is equivalent to ¬± the \\(0.975\\) quantile.]\nWe use the ‚Äúquantile‚Äù function for the standard normal function (qnorm() with mean=0 and sd=1, which are the default argument values) to generate these critical values.\nRecall that when we give the qnorm() function a particular percentile value as an argument, it returns a number, \\(X\\), below which that proportion of the cumulative probability distribution falls, and when we are considering a standard normal distribution, \\(X\\) is in units of standard deviations. Thus, qnorm(0.025, mean=0, sd=1) tells us the number of standard deviations (roughly -1.96) away from a mean (of 0) that corresponds to up to 2.5% of the cumulative probability distribution for a standard normal curve. Similarly, qnorm(0.975, mean=0, sd=1) tells us the number of standard deviations (roughly +1.96) away from the mean up to which 97.5% of the standard normal distribution falls. 95% of the standard normal distribution falls between these two values. Thus, to calculate the CI for our sampling distribution, we can do the following:\n\npercent_ci &lt;- 95\nalpha &lt;- 1 - percent_ci/100  # alpha = 0.05\nlower &lt;- m + qnorm(alpha/2) * se\n# where qnorm(alpha /2) is the 2.5% quantile of the standard normal\n# distribution\nupper &lt;- m + qnorm(1 - alpha/2) * se\n# where qnorm(1 - alpha / 2) is the 97.5% quantile of the standard normal\n# distribution\n(ci &lt;- c(lower, upper))\n\n## [1] 21.29988 27.71589\n\n\nAn easier way to do this is simply as follows:\n\n(ci &lt;- m + c(-1, 1) * qnorm(1 - alpha/2) * se)\n\n## [1] 21.29988 27.71589\n\n\nbecause these ways of calculating the lower CI bound are equivalent:\n\n(lower &lt;- m + 1 * qnorm(alpha/2) * se)\n\n## [1] 21.29988\n\n(lower &lt;- m + -1 * qnorm(1 - alpha/2) * se)\n\n## [1] 21.29988\n\n\nWe could also do this‚Ä¶\n\n# get the value associated with the alpha / 2 lower tail\n(lower &lt;- m + qnorm(alpha/2) * se)\n\n## [1] 21.29988\n\n# get the value associated with the alpha / 2 upper tail\n(upper &lt;- m + qnorm(alpha/2, lower.tail = FALSE) * se)\n\n## [1] 27.71589\n\n\nAnd we can also make our own generic CI() function based on assuming that our the sampling distribution for our estimate of the mean is normal:\n\nCI &lt;- function(x, level = 0.95) {\n    alpha &lt;- 1 - level\n    ci &lt;- mean(x) + c(-1, 1) * qnorm(1 - (alpha/2)) * sqrt(var(x)/length(x))\n    return(ci)\n}\n\nCI(s)\n\n## [1] 21.29988 27.71589\n\n\nFinally, we could also characterize the central CI% of the sampling distribution directly by calling the qnorm() function on a non-standardized normal distribution. This is less common, though, because it is less easily applied to distributions other than the normal family (e.g., to the \\(t\\) distribution).\n\n(lower &lt;- qnorm(alpha/2, m, se))\n\n## [1] 21.29988\n\n(upper &lt;- qnorm(1 - alpha/2, m, se))\n\n## [1] 27.71589\n\n\nThe following code will let us interactively visualize CIs around an estimate of a mean with a normal sampling distribution.\n\nmanipulate(ggplot(data = data.frame(x = c(sampling_dist_mean - 4 * sampling_dist_sd,\n    sampling_dist_mean + 4 * sampling_dist_sd)), aes(x)) + stat_function(fun = dnorm,\n    args = list(mean = sampling_dist_mean, sd = sampling_dist_sd), n = 1000) + xlab(\"Sampling Distribution Mean\") +\n    ylab(\"\") + labs(title = \"Exploring Confidence Intervals\", subtitle = paste0(\"Sampling Distribution SD (= SE): \",\n    sampling_dist_sd, \"\\n\", round(percent_CI, 2), \"% CI: \", round(sampling_dist_mean -\n        qnorm((1 - percent_CI/100)/2) * sampling_dist_sd, 2), \" to \", round(sampling_dist_mean +\n        qnorm((1 - percent_CI/100)/2) * sampling_dist_sd, 2))) + geom_vline(xintercept = sampling_dist_mean -\n    qnorm((1 - percent_CI/100)/2) * sampling_dist_sd, color = \"blue\", linetype = \"dashed\") +\n    geom_vline(xintercept = sampling_dist_mean + qnorm((1 - percent_CI/100)/2) *\n        sampling_dist_sd, color = \"blue\", linetype = \"dashed\") + geom_vline(xintercept = sampling_dist_mean,\n    color = \"black\", linetype = \"solid\") + stat_function(fun = dnorm, xlim = c(sampling_dist_mean -\n    qnorm((1 - percent_CI/100)/2) * sampling_dist_sd, sampling_dist_mean + qnorm((1 -\n    percent_CI/100)/2) * sampling_dist_sd), args = list(mean = sampling_dist_mean,\n    sd = sampling_dist_sd), n = 1000, geom = \"area\", fill = \"red\", alpha = 0.5, color = \"red\"),\n    sampling_dist_mean = slider(-100, 100, initial = 0, step = 10), sampling_dist_sd = slider(0,\n        100, initial = 1, step = 1), percent_CI = slider(0, 99, initial = 95, step = 1))\n\n\n\nInterpretation of CIs\nThere are two ways that CIs are generally interpreted:\n\nBased on the given data (with a particular mean, variance, and sample size), we are \\(X\\)% confident that the true mean of the population lies between the lower and upper bounds.\nThe mean of a repeated sample of the same size drawn from this same underlying distribution is expected to fall into the given interval \\(X\\)% of the time.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "14-module.html#the-central-limit-theorem",
    "href": "14-module.html#the-central-limit-theorem",
    "title": "14¬† Confidence Intervals",
    "section": "14.4 The Central Limit Theorem",
    "text": "14.4 The Central Limit Theorem\nOur construction of CIs thus far has implicitly taken advantage of one of the most important theorems in statistics, the Central Limit Theorem (CLT). The key importance of the CLT for us is that it states that the sampling distribution of averages (or sums or other summary statistics‚Ä¶) of ‚Äúindependent and identically distributed‚Äù (or \\(iid\\)) random variables approaches a normal distribution as the sample size increases. It is this fact that allows us to have a good sense of the mean and distribution of average events in a population even though we only observe one or a small number of samples of those events and even though we do not know what the actual population distribution is! In fact, the CLT says nothing about the probability distribution for events in the original population, and that is exactly where its usefulness lies‚Ä¶ that original probability distribution can be normal, skewed, exponential, or even all kinds of odd!\n\nTL/DR: The CLT means can we typically assume normality for the sampling distribution, i.e., for the distribution of the sample mean (or of the sample sum or the sample mode, etc‚Ä¶) no matter what kind of probability distribution characterizes the initial population, as long as our sample size is large enough and our samples are independent and identically distributed. It is thus the CLT that allows us to make inferences about a population based on a sample.\n\nJust to explore this a bit, let‚Äôs do some simulations. We are going to take lots of averages of samples from a particular non-normal distribution and then look at the distribution of those averages.\nImagine we have some event that occurs in a population according to some probability mass function like the Poisson where we know \\(\\lambda=14\\). Recall that the expectations of \\(\\mu\\) and \\(\\sigma^2\\) for the Poisson distribution are both equal to \\(\\lambda\\).\nNow let‚Äôs imagine taking a bunch of random samples of size 10 from this population. We will take 1000 random samples of this size, calculate the average of each sample, and plot a histogram of those averages‚Ä¶ it will be close to normal, and the standard deviation of those averages - i.e., the SD of the sampling distribution - should be roughly equal to the estimated standard error, i.e., the square root of (expected variance / sample size). Recall that for a Poisson distribution that \\(\\lambda\\) is the expected variance, so the estimated standard error is simply the \\(\\sqrt{\\lambda/n}\\)\n\nlambda &lt;- 14\nn &lt;- 10\n(se &lt;- sqrt(lambda/n))\n\n## [1] 1.183216\n\n# the estimated SE, here, based on an estimate of the population variance if\n# available\npar(mfrow = c(1, 3))\nhist(rpois(10000, lambda = lambda), probability = TRUE, xlab = \"X\", main = \"Original Distribution\")\nx &lt;- vector(length = 1000)  # dummy variable to hold sample means\nfor (i in 1:1000) {\n    x[[i]] &lt;- mean(rpois(n = n, lambda = lambda))\n}\nhist(x, breaks = seq(from = min(x), to = max(x), length.out = 20), probability = TRUE,\n    xlab = \"Mean x\", main = \"Sampling Distribution\")\n(se &lt;- sd(x))\n\n## [1] 1.160762\n\n# the estimated SE, here based on the standard deviation of the actual sampling\n# distribution\nqqnorm(x)\nqqline(x)\n\n\n\n\n\n\n\n\nLet‚Äôs now do the same for samples of size 1000. We see that the mean of the sampling distribution stays essentially the same, the distribution is still normal, but the standard deviation - the spread - of the sampling distribution is lower.\n\nlambda &lt;- 14\nn &lt;- 1000\n(se &lt;- sqrt(lambda/n))\n\n## [1] 0.1183216\n\n# the estimated SE, here, based on an estimate of the population variance if\n# available\npar(mfrow = c(1, 3))\nhist(rpois(10000, lambda = lambda), probability = TRUE, xlab = \"x\", main = \"Original Distribution\")\nx &lt;- vector(length = 1000)\nfor (i in 1:1000) {\n    x[[i]] &lt;- mean(rpois(n = n, lambda = lambda))\n}\nhist(x, breaks = seq(from = min(x), to = max(x), length.out = 20), probability = TRUE,\n    xlab = \"Mean X\", main = \"Sampling Distribution\")\n(se &lt;- sd(x))\n\n## [1] 0.1179025\n\n# the estimated SE, here based on the standard deviation of the actual sampling\n# distribution\nqqnorm(x)\nqqline(x)\n\n\n\n\n\n\n\n\nNote that the sampling distribution gets narrower as the sample size increases.\nWe can convert the sampling distribution to a standard normal by subtracting off the mean of the distribution and dividing by the standard deviation and then plotting a histogram of those values along with a normal curve.\n\npar(mfrow = c(1, 2))\nhist(x, breaks = seq(from = floor(min(x)), to = ceiling(max(x)), length.out = 20),\n    probability = TRUE)\nz &lt;- (x - mean(x))/sd(x)  # converts the vector of means to z scores\nhist(z, breaks = seq(from = min(z), to = max(z), length.out = 20), probability = TRUE,\n    ylim = c(0, 0.5))  # plots the histogram of z scores\ncurve(dnorm(x, 0, 1), -4, 4, ylim = c(0, 0.5), xlab = \"Z\", ylab = \"Density\", add = TRUE)  # plots a standard normal curve\n\n\n\n\n\n\n\n\nPretty normal looking, right?\nHere‚Äôs an example of the CLT in action using the sum() of our random variables instead of the mean() as the statistic of interest ‚Ä¶\n\nlambda &lt;- 14\nn &lt;- 1000\npar(mfrow = c(1, 3))\nhist(rpois(10000, lambda = lambda), probability = TRUE, xlab = \"X\", main = \"Original Distribution\")\nx &lt;- vector(length = 1000)\nfor (i in 1:1000) {\n    x[[i]] &lt;- sum(rpois(n = n, lambda = lambda))\n}\nhist(x, breaks = seq(from = min(x), to = max(x), length.out = 20), probability = TRUE,\n    xlab = \"Sum X\", main = \"Sampling Distribution\")\n# convert the sampling distribution to standard normal\nz &lt;- (x - mean(x))/sd(x)\nhist(z, breaks = seq(from = -4, to = 4, length.out = 20), probability = TRUE)\n# plots a standard normal curve\ncurve(dnorm(x, 0, 1), -4, 4, ylim = c(0, 0.5), xlab = \"Z\", ylab = \"Density\", add = TRUE)\n\n\n\n\n\n\n\n\nAgain, pretty normal looking!\nTrying this for an intial beta rather than Poisson distribution‚Ä¶\n\nalpha &lt;- 1.1\nbeta &lt;- 2.9\nn &lt;- 1000\npar(mfrow = c(1, 3))\nhist(rbeta(10000, shape1 = alpha, shape2 = beta), probability = TRUE, xlab = \"X\",\n    main = \"Original Distribution\")\nx &lt;- vector(length = 1000)\nfor (i in 1:1000) {\n    x[[i]] &lt;- mean(rbeta(n = n, shape1 = alpha, shape2 = beta))\n}\nhist(x, breaks = seq(min(x), max(x), length.out = 20), probability = TRUE, xlab = \"Mean X\",\n    main = \"Sampling Distribution\")\n# convert the sampling distribution to standard normal\nz &lt;- (x - mean(x))/sd(x)\nhist(z, breaks = seq(from = -4, to = 4, length.out = 20), probability = TRUE)\n# plots a standard normal curve\ncurve(dnorm(x, 0, 1), -4, 4, ylim = c(0, 0.5), xlab = \"Z\", ylab = \"Density\", add = TRUE)\n\n\n\n\n\n\n\n\nOnce again, the sampling distribution is normal, even though the original distribution is far from it!\nTake Home Points:\n[1] The CLT states that, regardless of the underlying probability distribution of a population of independent, identically distributed (\\(iid\\)) random variables, the distribution of a statistic (e.g., means, or sums or standard deviations, etc‚Ä¶) for samples drawn from that underlying distribution:\n\nwill be approximately normal,\nwill be centered at the population mean for the statistic, and - will have a standard deviation roughly equal to the standard error of the mean for the statistic.\n\nAdditionally, it suggests that variables that are expected to be the sum of multiple independent processes (e.g., measurement errors) will also have distributions that are nearly normal.\n[2] Calculating a statistic based on this distribution (e.g., the mean) and adding/subtracting the relevant standard quantile \\(\\times\\) the standard error yields a confidence interval for the relevant statistic, which gets wider as the coverage increases and gets narrower with less variability or larger sample sizes.\n[3] As sample size increases, the standard error of the statistic decreases and the sampling distribution becomes more and more normal (i.e., has less skew and kurtosis, which are higher order moments of the distribution).\nFor a nice interactive simulation demonstrating the Central Limit Theorem, check out this cool website.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "14-module.html#cis-for-sample-proportions",
    "href": "14-module.html#cis-for-sample-proportions",
    "title": "14¬† Confidence Intervals",
    "section": "14.5 CIs for Sample Proportions",
    "text": "14.5 CIs for Sample Proportions\nSo far, we‚Äôve talked about CIs for sample means of a continuous random variable, but what about for other statistics, e.g., sample proportions for discrete binary variables. For example, if you have a sample of n trials where you record the success or failure of a binary event, you can obtain an estimate of the proportion of successes, \\(x/n\\). If you perform another n trials, the new estimate will vary from sample to sample in the same way that averages of a continuous random variable (e.g., zombie age) will vary.\nTaking a similar approach as we did above for normally distributed continuous random variables, we can generate confidence intervals for the proportion of successes across trials, i.e., for a discrete binary variable, by taking the expectation and adding/subtracting a critical value \\(\\times\\) a standard error.\nRecall from our discussion of discrete random binary variables that the expectation for proportion of successes, which we will denote as \\(\\pi\\) (where \\(\\pi\\), for ‚Äúproportion‚Äù, is analogous to \\(\\mu\\), for ‚Äúmean‚Äù) is simply the average number of successes across multiple trials, and the expectation for the variance in this proportion is \\(\\pi \\times (1-\\pi)\\).\nAs for the mean of a continuous random variable, the expected sampling distribution for the proportion of successes across many trials of sample size \\(n\\) is approximately normal and centered at \\(\\pi\\), and its standard deviation is estimated by \\(\\sqrt{\\pi(1-\\pi)/n}\\), which is, essentially, the standard error of the mean: it is the square root of (the expected variance / sample size). As above for \\(\\mu\\), if we do not already have a population estimate for \\(\\pi\\), we can estimate this from a sample as \\(\\hat{p}=x/n\\)\n\nNOTE: The approximation of normality for the sampling distribution of proportion of successes holds true only if both \\(n\\times\\pi\\) and \\(n\\times(1-\\pi)\\) are greater than roughly 5, so we should always check this when working with proportion data!\n\n\nCHALLENGE\nSuppose a polling group in the United States is interested in the proportion of voting-age citizens in their state that already know they will vote for Bernie Sanders in the upcoming presidential election. The group obtains a ‚Äúyes‚Äù‚Äù or ‚Äúno‚Äù answer from 1000 randomly selected individuals. Of these individuals, 856 say they know they‚Äôll vote for Sanders How would we characterize the mean and variability associated with this proportion?\n\nn &lt;- 1000\nx &lt;- 856\np_hat &lt;- x/n  # our estimate of pi\np_hat\n\n## [1] 0.856\n\n\nAre \\(n\\times\\pi\\) and \\(n\\times(1-\\pi)\\) both &gt; 5? Yes!\n\nn * p_hat\n\n## [1] 856\n\nn * (1 - p_hat)\n\n## [1] 144\n\nse &lt;- sqrt((p_hat) * (1 - p_hat)/n)  # estimated SE for proportion data\nse\n\n## [1] 0.01110243\n\n\nSo, what is the 95% CI around our estimate of the proportion of people who already know how they will vote?\n\npar(mfrow = c(1, 1))\ncurve(dnorm(x, mean = p_hat, sd = se), p_hat - 4 * se, p_hat + 4 * se, xlab = \"\",\n    ylab = \"Density\", main = \"95% Confidence Interval around\\nExpected Proportion of Decided Voters \")\nupper &lt;- p_hat + qnorm(0.975) * se\nlower &lt;- p_hat - qnorm(0.975) * se\n(ci &lt;- c(lower, upper))\n\n## [1] 0.8342396 0.8777604\n\n# or\n(ci &lt;- p_hat + c(-1, 1) * qnorm(0.975) * se)\n\n## [1] 0.8342396 0.8777604\n\npolygon(cbind(c(ci[1], seq(from = ci[1], to = ci[2], length.out = 1000), ci[2]),\n    c(0, dnorm(seq(from = ci[1], to = ci[2], length.out = 1000), mean = p_hat, sd = se),\n        0)), border = \"black\", col = \"gray\")\nabline(v = ci)\nabline(v = p_hat)\nabline(h = 0)",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "14-module.html#small-sample-cis",
    "href": "14-module.html#small-sample-cis",
    "title": "14¬† Confidence Intervals",
    "section": "14.6 Small Sample CIs",
    "text": "14.6 Small Sample CIs\nThus far, we have discussed creating confidence intervals for sample statistics based on the CLT and the normal distribution, and our intervals took the form:\nmean \\(¬±\\) \\(Z\\) (the critical value, based on a quantile from the standard normal curve) \\(\\times\\) standard error\nBut, when the size of our sample is small (\\(n\\) &lt; 30), instead of using the normal distribution to calculate our CIs, statisticians typically use a different distribution to generate the relevant quantiles to multiply the standard error by‚Ä¶ the t distribution (a.k.a., Gosset‚Äôs \\(t\\) or Student‚Äôs \\(t\\) distribution).\n\nNOTE: This is a typical case that we will encounter, as we often do not have information about the population that a sample is drawn from!\n\nThe \\(t\\) distribution is a continuous probability distribution very similar in shape to the normal, and is generally used when dealing with statistics (such as means and standard deviations) that are estimated from a sample rather than being known parameters about a population. Any particular \\(t\\) distribution looks a lot like a normal distribution in that it is bell-shaped, symmetric, unimodal, and (if standardized) zero-centered.\nThe choice of the appropriate \\(t\\) distribution to use in any particular statistical test is based on the number of degrees of freedom (df), i.e., the number of individual components in the calculation of a given statistic (such as the mean or standard deviation) that are ‚Äúfree to change‚Äù.\nWe can thus think of the \\(t\\) distribution as representing a family of curves that, as the number of degrees of freedom increases, approaches the normal curve. At low numbers of degrees of freedom, the tails of the distribution get fatter and the peak of the distribution gets lower.\nConfidence intervals based on the \\(t\\) distribution are of the form:\nmean \\(¬±\\) \\(T\\) (the critical value, based on a quantile from the \\(t\\) distribution) \\(\\times\\) standard error\nThe only change from CIs based on the normal distribution is that we have replaced the \\(Z\\) quantile of the standard normal with a \\(T\\) quantile.\nLet‚Äôs explore this a bit‚Ä¶\nRecall that a standard normal distribution is generated by normalizing a set of data (subtracting the mean from each observation and then dividing all of these differences by the standard deviation of the distribution)‚Ä¶\n\\[(\\bar{x}-\\mu)/\\sigma\\]\nFor a sample, if we subtract the population mean from each observation and then divide each difference, instead, by the standard error of the mean, i.e., \\((\\bar{x}-\\mu)/SE\\), the result is not a normal distribution, but rather a \\(t\\) distribution! We are taking into account sample size by dividing by the standard error of the mean rather than by the population standard deviation.\nThe code below plots a standard normal distribution in red and then superimposes several \\(t\\) distributions with varying degrees of freedom, specified using the df= argument.\n\nmu &lt;- 0\nsigma &lt;- 1\ncurve(dnorm(x, mu, 1), mu - 4 * sigma, mu + 4 * sigma, main = \"Normal Distribution (red) and\\nStudent's t Distributions (blue)\",\n    xlab = \"x\", ylab = \"f(x)\", col = \"red\", lwd = 3)\nfor (i in c(1, 2, 3, 4, 5, 10, 20, 100)) {\n    curve(dt(x, df = i), mu - 4 * sigma, mu + 4 * sigma, main = \"T Curve\", xlab = \"x\",\n        ylab = \"f(x)\", add = TRUE, col = \"blue\", lty = 5)\n}\n\n\n\n\n\n\n\n\nAs for other distributions, R implements d (density), p (cumulative probability), q (quantile), and r (random draw) functions for the t distribution.\nThe fatter tails of the \\(t\\) distibution at small sample sizes lead naturally to more extreme quantile values given a specific probability than we would see for the normal distribution. If we define a CI based on quantiles of the \\(t\\) distribution, then, they will be correspondingly slightly wider than those based on the normal distribution for small values of \\(df\\).\nWe can see this in an example, where we draw samples from a population of values defined by a normal distribution with a particular mean and standard deviation. Recall that above we estimated the 95% CI for such a sample drawn from a normal distribution as follows:\n\nn &lt;- 1e+05\nmu &lt;- 3.5\nsigma &lt;- 4\nalpha &lt;- 0.05\nx &lt;- rnorm(n, mu, sigma)  # x is our large population\nhist(x)\n\n\n\n\n\n\n\nsample_size &lt;- 30\ns &lt;- sample(x, size = sample_size, replace = FALSE)  # s is a sample from that population\n(m &lt;- mean(s))\n\n## [1] 2.921573\n\n(sd &lt;- sd(s))\n\n## [1] 4.198638\n\n(se &lt;- sd(s)/sqrt(length(s)))\n\n## [1] 0.7665629\n\nci_normal &lt;- m + c(-1, 1) * qnorm(1 - 0.05/2) * se\n# (1-alpha)/2 each in the upper and lower tails of the distribution\nci_normal\n\n## [1] 1.419138 4.424009\n\n\nNow, let‚Äôs look at the CIs calculated based using the \\(t\\) distribution for the same sample size. For sample size 30, the difference in the CIs is negligible.\n\nci_t &lt;- m + c(-1, 1) * qt(1 - 0.05/2, df = sample_size - 1) * se\n# (1-alpha)/2 each in the upper and lower tails of the distribution\nci_t\n\n## [1] 1.353776 4.489371\n\n\nHowever, if we use a sample size of 5, the equivalent CI based on the \\(t\\) distribution is much wider.\n\nsample_size &lt;- 5\nsmall_s &lt;- sample(x, size = sample_size, replace = FALSE)\n(m &lt;- mean(small_s))\n\n## [1] 5.307108\n\n(sd &lt;- sd(small_s))\n\n## [1] 2.599329\n\n(se &lt;- sd(small_s)/sqrt(length(small_s)))\n\n## [1] 1.162455\n\nci_normal &lt;- m + c(-1, 1) * qnorm(1 - 0.05/2) * se\n# (1-alpha)/2 each in the upper and lower tails of the distribution\nci_normal\n\n## [1] 3.028738 7.585479\n\nci_t &lt;- m + c(-1, 1) * qt(1 - 0.05/2, df = sample_size - 1) * se\n# (1-alpha)/2 each in the upper and lower tails of the distribution\nci_t\n\n## [1] 2.079615 8.534602",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "14-module.html#calculating-cis-by-bootstrapping",
    "href": "14-module.html#calculating-cis-by-bootstrapping",
    "title": "14¬† Confidence Intervals",
    "section": "14.7 Calculating CIs by Bootstrapping",
    "text": "14.7 Calculating CIs by Bootstrapping\nAn alternative (and arguably better) way to calculate a confidence interval for a given statistic is by ‚Äúbootstrapping‚Äù from the data in a single sample using a Monte Carlo simulation process. Bootstrapping allows us to approximate a sampling distribution even without access to the population from which samples are drawn and without making any assumptions about the theoretical shape of the sampling distribution. The distribution we generate is sometimes referred to as a bootstrap distribution.\n\nWriting our Own Bootstrap\nBelow, we use the sample() function to sample, with replacement, bootstrap replicates of equivalent sample size from our original vector of sample data, s, a total of 10000 times. This is just one approach to how we might perform bootstrapping, using {base} R functions.\n\nNOTE: If we wanted to sample rows from a data frame instead of items from a vector, we could use the {dplyr} functions sample_n() or slice_sample().\n\n\nn_boot &lt;- 10000\nboot &lt;- vector(length = n_boot)  # set up a dummy variable to hold our simulations\nn &lt;- length(s)\n# the size of each bootstrap sample should equivalent to the size our original\n# sample\nfor (i in 1:n_boot) {\n    boot[[i]] &lt;- mean(sample(s, n, replace = TRUE))\n}\n\nThe following code visualizes the bootstrap sampling distribution we have just generated and also compares the various methods we have explored for calculating CIs.\n\nhist(boot, breaks = 25, ylim = c(0, 1600), xlab = \"Mean\", main = \"Bootstrap Sampling Distribution\")\nabline(v = mean(boot), col = \"blue\", lwd = 3)  # mean of our simulated samples\ntext(x = mean(boot) + 0.2, y = 700, \"mean of bootstrap distribution \", col = \"blue\",\n    srt = 90, pos = 3)\nabline(v = mean(s), col = \"black\", lwd = 3, lty = 2)\n# mean of our original vector of samples, s\ntext(x = mean(s) - 0.125, y = 700, \"mean of original sample\", col = \"black\", srt = 90,\n    pos = 3)\n\nabline(v = quantile(boot, 0.025), col = \"blue\")\n# lower ci bound inferred by bootstrapping\nabline(v = quantile(boot, 0.975), col = \"blue\")\n# upper ci bound inferred by bootstrapping\ntext(x = quantile(boot, 0.025) + 0.25, y = 700, \"lower CI - bootstrap\", col = \"blue\",\n    srt = 90, pos = 3)\ntext(x = quantile(boot, 0.975) - 0.125, y = 700, \"upper CI - bootstrap\", col = \"blue\",\n    srt = 90, pos = 3)\n\nci_normal &lt;- mean(s) + c(-1, 1) * qnorm(1 - 0.05/2) * sciplot::se(s)\nci_t &lt;- mean(s) + c(-1, 1) * qt(1 - 0.05/2, df = length(s) - 1) * sciplot::se(s)\n\nabline(v = ci_normal[1], col = \"red\")\n# calculated lower ci bound based on the se of our vector and assuming that our\n# observations are drawn from a normal distribution\nabline(v = ci_normal[2], col = \"red\")\n# calculated upper ci bound based on the se of our vector and assuming that our\n# observations are drawn from a normal distribution\ntext(x = ci_normal[1] - 0.12, y = 700, \"lower CI - normal\", col = \"red\", srt = 90,\n    pos = 3)\ntext(x = ci_normal[2] + 0.245, y = 700, \"upper CI - normal\", col = \"red\", srt = 90,\n    pos = 3)\n\nabline(v = ci_t[1], col = \"green\")\n# calculated lower ci bound based on the se of our vector and assuming that our\n# observations are drawn from a t distribution\nabline(v = ci_t[2], col = \"green\")\n# calculated upper ci bound based on the se of our vector and assuming that our\n# observations are drawn from a t distribution\ntext(x = ci_t[1] - 0.15, y = 700, \"lower CI - t\", col = \"green\", srt = 90, pos = 3)\ntext(x = ci_t[2] + 0.275, y = 700, \"upper CI - t\", col = \"green\", srt = 90, pos = 3)\n\n\n\n\n\n\n\n\nIn this example, we used the quantile() function to return the observations satisfying the \\(n^{th}\\) quantile of our particular distribution of bootstrapped sample means. We use this function to define the lower and upper bounds for the bootstrap confidence interval around the mean of our variable, x.\nNote that this compares very favorably to the CIs estimated by assuming that the sampling distribution follows either a normal or \\(t\\) distribution\n\n\nOther Bootstrapping Options\nAs is typically the case with R, we have many choices for perform we might perform boostrapping! All of the options below are alternatives to the process we used above and generate similar bootstrap sampling distributions and corresponding CIs.\n\nUsing the {mosaic} Package\nWe are already familiar with the do(reps) * formulation from the {mosaic} package‚Ä¶\n\nboot &lt;- do(n_boot) * mean(~sample(s, length(s), replace = TRUE))\nhist(boot$mean, breaks = 25, ylim = c(0, 1600), xlab = \"Mean\", main = \"Bootstrap Sampling Distribution\\nfrom do(reps) *\")\n\n\n\n\n\n\n\n\n\n\nUsing the {infer} Package\nAlternatively, can use the rep_sample_n() function from the {infer} package to perform boostrapping. This function takes a data frame as an argument and randomly samples from that data frame a specified number of times (defined by the ‚Äúsize=‚Äù argument) to produce a large data object consisting of samples for each replicate and a column data that indicates which replicate each sample datum comes from. To use rep_sample_n(), we need to first convert our original sample vector s into a data frame. We can then use group_by() and summarize() from {dplyr} to generate a bootstrap sampling distribution, as we did above.\n\nlibrary(infer)\nboot &lt;- as.data.frame(s) |&gt;\n    rep_sample_n(size = length(s), replace = TRUE, reps = n_boot) |&gt;\n    group_by(replicate) |&gt;\n    summarize(mean = mean(s))\nhist(boot$mean, breaks = 25, ylim = c(0, 1500), xlab = \"Mean\", main = \"Bootstrap Sampling Distribution\\nfrom rep_sample_n()\")\n\n\n\n\n\n\n\n\n\n\nUsing the {boot} Package\nWe can also use the fast and versatile {boot} package to do bootstrapping for any statistic.\nFor this approach, we define a custom function that we will use to generate the statistic we are interested in that we will then calculate for each bootstrapped sample:\n\nlibrary(boot)\n# here, the `stat()` function we define calculates the mean value of a\n# bootstrap sample taken with replacement from a vector of interest passed into\n# `data` the function we write for `statistic=` has two arguments, a data set\n# and a set of indices, generated at random by the `boot()` function, to sample\n# from the dataset this corresponds to the default argument `stype='i'` in the\n# `boot()` call\nstat &lt;- function(data, indices) {\n    return(mean(data[indices]))\n}\n\nThen, we run the boot() function passing in the data to be resampled from (‚Äúdata=‚Äù), the statistic we want to calculate (‚Äústatistic=‚Äù), and the number of bootstrap replicates (or ‚Äúresamples‚Äù) we want using the argument ‚ÄúR=‚Äù.\n\nboot &lt;- boot(data = s, statistic = stat, R = n_boot)  # stype='i' by default\n# the object returned includes a table, `t`, of `stat`s results from each\n# bootstrap\n\nAgain, it is interesting to plot the bootstrap sampling distibution and compare methods for calculating CIs.\n\n# Visualizing the results and comparing methods for calculating CIs\nhist(boot$t, breaks = 25, ylim = c(0, 1500), xlab = \"Mean\", main = \"Bootstrap Sampling Distribution\\nfrom boot()\")\n\nabline(v = mean(boot$t), col = \"blue\", lwd = 3)  # mean of our simulated samples\ntext(x = mean(boot$t) + 0.2, y = 700, \"mean of bootstrap distribution \", col = \"blue\",\n    srt = 90, pos = 3)\nabline(v = mean(s), col = \"black\", lwd = 3, lty = 2)\n# mean of our original vector of samples, s\ntext(x = mean(s) - 0.125, y = 700, \"mean of original sample\", col = \"black\", srt = 90,\n    pos = 3)\n\n# calculate and plot bootstrap CIs\nlower_boot &lt;- quantile(boot$t, 0.025)\nabline(v = lower_boot, col = \"blue\")\nupper_boot &lt;- quantile(boot$t, 0.975)\nabline(v = upper_boot, col = \"blue\")\ntext(x = lower_boot + 0.25, y = 700, \"lower CI - bootstrap\", col = \"blue\", srt = 90,\n    pos = 3)\ntext(x = upper_boot - 0.125, y = 700, \"upper CI - bootstrap\", col = \"blue\", srt = 90,\n    pos = 3)\n\n\n\n\n\n\n\n\nWe can also use the boot.ci() function from the {boot} package to calculate CIs for using various methods, although the available methods do not include exactly the one we most want, i.e., the quantile method. Instead, all the options for boot.ci() generate equi-tailed CIs based on presumed theoretical shapes for the sampling distribution. The ‚Äúbasic‚Äù and ‚Äúpercent‚Äù intervals are the closest to the empirical quantile method.\n\nhist(boot$t, breaks = 25, ylim = c(0, 1500), xlab = \"Mean\", main = \"Bootstrap Sampling Distribution\\nfrom boot()\")\nci &lt;- boot.ci(boot)\nabline(v = ci$basic[4], col = \"green\", lwd = 2)\nabline(v = ci$basic[5], col = \"green\", lwd = 2)\nabline(v = ci$percent[4], col = \"red\", lwd = 2)\nabline(v = ci$percent[5], col = \"red\", lwd = 2)\nabline(v = ci$bca[4], col = \"purple\", lwd = 2)\nabline(v = ci$bca[5], col = \"purple\", lwd = 2)\nabline(v = ci$normal[2], col = \"black\", lwd = 2)\nabline(v = ci$normal[3], col = \"black\", lwd = 2)\n\n\n\n\n\n\n\n\nThese alternative CIs are all very close to one another and are close to what we calculated using the quantile() approach!\n\n\n\nCHALLENGE\n\nHow does the CI calculated by bootstrap simulation compare to that calculated based on assuming a normal or \\(t\\) distribution?\nHow does the width of the CI change with decreasing or increasing \\(n\\) (the number of observations drawn from your sample with replacement)? For example, if we set \\(n\\) at 5? At 50? At 500?",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "14-module.html#concept-review",
    "href": "14-module.html#concept-review",
    "title": "14¬† Confidence Intervals",
    "section": "Concept Review",
    "text": "Concept Review\n\nWe can generate confidence intervals around a statistic (e.g., a mean) estimated from a sample as:\n\nmean \\(¬±\\) critical value (i.e., the relevant quantile for the standardized version of a particular theoretical sampling distribution such as the normal or Student‚Äôs \\(t\\)) \\(\\times\\) standard error in the statistic, which is typically estimated as the standard deviation of a sample divided by the square root of the sample size, or \\(\\frac{s}{\\sqrt{n}}\\)\n\nBootstrapping allows us to estimate confidence intervals by simulation from a sample, without assuming a particular shape for the sampling distribution",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "15-module.html",
    "href": "15-module.html",
    "title": "15¬† Classical Hypothesis Testing",
    "section": "",
    "text": "15.1 Objectives",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Classical Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "15-module.html#objectives",
    "href": "15-module.html#objectives",
    "title": "15¬† Classical Hypothesis Testing",
    "section": "",
    "text": "The objective of this module is to introduce basic hypothesis testing from a classical or ‚Äúfrequentist‚Äù statistics approach.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Classical Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "15-module.html#preliminaries",
    "href": "15-module.html#preliminaries",
    "title": "15¬† Classical Hypothesis Testing",
    "section": "15.2 Preliminaries",
    "text": "15.2 Preliminaries\n\nLoad {tidyverse} and {mosaic}",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Classical Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "15-module.html#null-and-alternative-hypotheses",
    "href": "15-module.html#null-and-alternative-hypotheses",
    "title": "15¬† Classical Hypothesis Testing",
    "section": "15.3 Null and Alternative Hypotheses",
    "text": "15.3 Null and Alternative Hypotheses\n\n\n\n\n\n\n\n\n\n\n\nClassical hypothesis testing typically involves formally stating a claim - the null hypothesis - which is then followed up by statistical evaluation of the null versus an alternative hypothesis. The null hypothesis is interpreted as a baseline hypothesis and is the claim that is presumed to be true. That claim is typically that a particular value of a population parameter estimated by a sample statistic we have calculated is consistent with a particular null expectation. The alternative hypothesis is the conjecture that we are testing, usually that the sample statistic is inconsistent with a null expectation.\nTypically, our null and alternative hypotheses are expressed something like as follows:\n\n\\(H_0\\) = null hypothesis = a sample statistic shows no deviation from what is expected or neutral based on the parameter space of possible outcomes under the presumed random sampling process.\n\n\nNOTE: This parameter space is defined both by how we sample and how we intend to sample, including our stopping rules (e.g., number of observations, amount of time we plan to sample, and so on).\n\n\n\\(H_A\\) = alternative hypothesis = a sample statistic deviates more than expected by chance from what is expected or neutral.\n\nWe can test several different comparisons between \\(H_0\\) and \\(H_A\\).\n\n\\(H_A &gt; H_0\\), which constitutes an ‚Äúupper one-tailed test‚Äù (i.e., our sample statistic is greater than that expected under the null)\n\\(H_A &lt; H_0\\), which constitutes a ‚Äúlower one-tailed test‚Äù (i.e., our sample statistic is less than that expected under the null)\n\\(H_A ‚â† H_0\\), which constitutes a ‚Äútwo-tailed test‚Äù (i.e., our sample statistic is different, maybe greater maybe less, than that expected under the null)\n\nTo formally do any statistical test under such a ‚Äúnull hypothesis significance testing‚Äù (or NHST) framework, we need some kind of statistical evidence - typically based on probabilities and confidence intervals - to reject the null hypothesis in favor of an alternative hypothesis. This evidence involves considering some measure of how unexpected it would be for a sample we have collected to have been drawn, by chance, from a particular null distribution.\nTo effect a hypothesis test, we then need to‚Ä¶\n\nCalculate a test statistic based on our data.\nCalculate the p value associated with that test statistic, which is the probability of obtaining, by chance, a test statistic that is as high or higher than our calculated one, assuming the null hypothesis is true. Classically, this is done by comparing the value to some appropriate standardized sampling distribution with well-known mathematical properties (e.g., normal or \\(t\\)) to yield the p value.\nEvaluate whether the p value is less than or greater than the significance level, or \\(\\alpha\\), that we set for our test. That is, \\(\\alpha\\) can be thought of as the cutoff level for p values below which we feel comfortable rejecting a null hypothesis (i.e., we reject \\(H_0\\) when \\(p &lt; \\alpha\\)).\n\nUnder the null hypothesis significance testing framework, there are then four possible outcomes to our statistical decision, each with an associated probability:\n\n\n\n\n\n\n\n\nWhat is True\nWhat We Decide\nResult\n\n\n\n\n\\(H_0\\)\n\\(H_0\\)\nCorrectly ‚Äúaccept‚Äù the null (\\(1 - \\alpha\\))\n\n\n\\(H_0\\)\n\\(H_A\\)\nFalsely reject the null (Type I error) \\(\\alpha\\)\n\n\n\\(H_A\\)\n\\(H_0\\)\nFalsely ‚Äúaccept‚Äù the null (Type II error) \\(\\beta\\)\n\n\n\\(H_A\\)\n\\(H_A\\)\nCorrectly reject the null (\\(1 - \\beta\\)), or ‚Äúpower‚Äù\n\n\n\nWe typically approach hypothesis testing by trying to minimize our probability of committing a Type I error (\\(\\alpha\\))‚Ä¶ i.e., we aim for having a high bar for falsely rejecting the null hypothesis (e.g., for incorrectly finding an innocent person guilty) and thus set \\(\\alpha\\) very low (0.05 or 0.01) to reflect what we decide is an acceptable level of Type I error.\nWhen we set a high bar for falsely rejecting the null, we necessarily lower the bar for incorrectly accepting (i.e., for failing to reject) the null (e.g., for concluding that a guilty person is innocent). The probability of incorrectly ‚Äúaccepting‚Äù the null is referred to as Type II error (\\(\\beta\\)). The ‚Äúpower‚Äù of a statistical test is (\\(1 - \\beta\\)), the probability with which we correctly reject the null.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Classical Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "15-module.html#parametric-hypothesis-testing",
    "href": "15-module.html#parametric-hypothesis-testing",
    "title": "15¬† Classical Hypothesis Testing",
    "section": "15.4 Parametric Hypothesis Testing",
    "text": "15.4 Parametric Hypothesis Testing\nIn traditional parametric statistics, we make the assumption that the sampling distribution of our statistic of interest (e.g., a mean) takes the form of a particular well-understood mathematical distribution (e.g., the normal), and we calculate a test statistic that basically summarizes the ‚Äúlocation‚Äù of a summary statistic about our data relative to that implied, theoretical sampling distribution.\nThe particular value of our test statistic is determined by both the difference between the original sample statistic and the expected null value (e.g., the difference between the mean of our sample and the expected population mean) and the standard error of the sample statistic. The value of our test statistic (i.e., the ‚Äúdistance‚Äù of that test statistic from what is expected) and the shape of the presumed sampling distribution for that statistic are the sole drivers of the smallness of the p value. The p value effectively represents the area under the sampling distribution associated with test statistic values as or more extreme than the one we observed.\nHow do we calculate the p value for a parametric test?\n\nSpecify the sample statistic we want to evaluate (e.g., the mean).\nSpecify the test statistic of interest and the form of the sampling distribution for that statistic (e.g., \\(Z\\) or \\(T\\) and a standard normal or T distribution).\nCalculate the tail probability, i.e., the probability of obtaining a statistic (e.g., a mean) as or more extreme than was observed assuming that null distribution.\n\n\n15.4.1 Important Comments on P Values\n\nThe p value is simply the probability of seeing some descriptive statistic (mean, median, regression coefficient, whatever) given the data you have and given particular assumptions you make about the model generating the data! The p value says nothing about effect size, biological meaning, or the accuracy of the model.\nStatistical ‚Äúsignificance‚Äù, i.e., if some statistic you calculate has a low p value, does NOT mean the effect is strong is or biologically important!\nThere is no problem with reporting p values‚Ä¶ but using p values as line for dichotomously classifying results into ‚Äúsignificant‚Äù versus ‚Äúnot significant‚Äù (or for interpreting results in this way) is problematic and not good statistical practice!\nFinally, it is valuable, I believe, to contrast p values as estimated via the classical/Fisherian frequentist approaches where the ‚Äúmodel‚Äù generating the data is one that is reasonably approximated by some well-characterized mathematical distribution (Gaussian, binomial, etc.) versus p values estimated based on permutation/resampling from the particular data we have collected (see Module 16. That latter approach is much more relevant for the situation that many of us in this class face, where we often do not have a good idea about what the data-generating model or the resultant distribution of the data is likely to be.\n\n\n\nWorking with Means\n\nOne Sample \\(Z\\) and \\(T\\) Test\nLet‚Äôs do an example where we try to evaluate whether the mean of a single set of observations is significantly different than that expected under a null hypothesis‚Ä¶ i.e., this is a ONE-SAMPLE test.\nSuppose we have a vector describing the adult weights of vervet monkeys trapped in South Africa during the 2015 trapping season. We have the sense they are heavier than vervets we trapped in previous years, which averaged 5.0 kilograms. We calculate the mean of our sample of trapped vervets and then need to decide whether that mean is significantly greater than our expectation based on prior information.\n\n\n\n\n\n\n\n\n\nFirst, we read in our data:\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/vervet-weights.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\nhead(d)\n\n## # A tibble: 6 √ó 2\n##      id weight\n##   &lt;dbl&gt;  &lt;dbl&gt;\n## 1     1   5.17\n## 2     2   7.13\n## 3     3   4.7 \n## 4     4   6.1 \n## 5     5   6.36\n## 6     6   4.93\n\nmean(d$weight)\n\n## [1] 5.323922\n\n\n\nWhat is our \\(H_0\\)?\nWhat is our \\(H_A\\)?\nWhat is the hypothesis we want to test? Is it two-tailed? Upper-tailed? Lower-tailed?\nCalculate the mean, standard deviation, and SE of the sample\n\n\n\nShow Code\nmu &lt;- 5\nx &lt;- d$weight  # current weights\nn &lt;- length(x)\n(m &lt;- mean(x))\n\n\n## [1] 5.323922\n\n\nShow Code\n(s &lt;- sd(x))\n\n\n## [1] 0.9754016\n\n\nShow Code\n(se &lt;- s/sqrt(n))\n\n\n## [1] 0.1365835\n\n\n\nPlot a histogram of the sample\n\n\n\nShow Code\nhistogram(x, breaks = seq(from = m - 4 * s, to = m + 4 * s, length.out = 20), main = \"Histogram of Vervet\\nWeights\",\n    xlab = \"X\", ylab = \"Proportion of Total\", type = \"density\", ylim = c(0, 3), col = rgb(0,\n        0, 1, 0.5))\nladd(panel.abline(v = mu, col = \"black\", lty = 1, lwd = 2))  # expected mean\nladd(panel.abline(v = m, col = \"black\", lty = 3, lwd = 2))  # observed mean\n\n\n\n\n\n\n\n\n\n\nUsing the plotDist() function from {mosaic}, plot the location of your sample mean, the sampling distribution suggested by your sample‚Äôs standard error, and the location of the expected mean from the 2015 data.\n\n\n\nShow Code\nplotDist(\"norm\", mean = m, sd = se, xlim = c(m - 4 * se, m + 4 * se), add = TRUE,\n    lwd = 1, col = \"black\", lty = 1)\n\n\n\n\n\n\n\n\n\nThe following code demonstrates that more than 95% of the sampling distribution suggested by your sample is above the expected mean of 5.0‚Ä¶\n\nz &lt;- qnorm(0.05)  # define lower bound of upper 95% of distribution\nladd(panel.polygon(cbind(c(m + z * se, seq(from = m + z * se, to = max(x), length.out = 1000),\n    max(x)), c(0, dnorm(seq(from = m + z * se, to = max(x), length.out = 1000), m,\n    se), 0)), border = \"black\", col = rgb(1, 0, 1, 0.5)))\n\n\n\n\n\n\n\n\nFor a formal statistical test of whether our sample mean is greater than the expected mean, we first calculate the test statistic. It takes a familiar form‚Ä¶ it is effectively the position of our sample mean relative to the expected population mean and the expected population standard deviation (which we have estimated from our sample).\n\\[Z = \\frac{\\bar{x}-\\mu}{s/\\sqrt{n}}\\]\nwhere:\n\n\\(\\bar{x}\\) = mean of sample observations\n\\(\\mu\\) = expected mean\n\\(s\\) = sample standard deviation (as an estimate of the population SD)\n\\(n\\) = number of sample observations\n\nOr, to use our variables from above‚Ä¶\n\nz &lt;- (m - mu)/se\nz\n\n## [1] 2.3716\n\n\nIn this case, our test statistic, \\(Z\\), is a quantile‚Ä¶ the estimated number of standard errors away from the expected population mean that our sample mean falls based on the implied sampling distribution. If our sample mean is greater than expected, then the test statistic is positive; if our sample mean is less than expected, then the test statistic is negative.\nTo evaluate whether \\(Z\\) is ‚Äúsignificant‚Äù under a NHST framework, we need to calculate the total probability of our seeing a deviation from the expected mean as great or greater than this by chance. For a two-tailed test, this deviation can be in either the positive or negative direction, while for a one-tailed test, we are only interested in the probability associated with our specific alternative hypothesis (i.e., \\(\\mu_{H_A} \\gt \\mu_{H_0}\\) or \\(\\mu_{H_A} \\lt \\mu_{H_0}\\)).\nTo calculate these probabilities, we can use the pnorm() function. Because in calculating a \\(Z\\) score we have converted our sample mean to the standard normal scale, the mean= and sd= arguments of pnorm() are the defaults of 0 and 1, respectively.\nFor this specific example, where we want to test the idea that our sample mean is greater than expected, we want the probability of seeing a \\(Z\\) as large or larger by chance. This corresponds to the area under the normal curve to the right of \\(Z\\) (the ‚Äúupper.tail‚Äù), which we can calculate as:\n\np &lt;- 1 - pnorm(z)\np\n\n## [1] 0.008855621\n\n# or\np &lt;- pnorm(z, lower.tail = FALSE)\np\n\n## [1] 0.008855621\n\n\nTo visualize this on a standard normal curve‚Ä¶\n\nplotDist(\"norm\", main = paste0(\"Standard Normal Distribution\\nblue area = \", round(p,\n    4) * 100, \"%\"), ylab = \"\", xlab = \"SD\")\nladd(panel.abline(v = z, col = \"blue\", lty = 1, lwd = 2))\nladd(panel.polygon(cbind(c(z, seq(from = z, to = 4, length.out = 100), 4), c(0, dnorm(seq(from = z,\n    to = 4, length.out = 100), 0, 1), 0)), border = \"black\", col = rgb(0, 0, 1, 0.5)))\n\n\n\n\n\n\n\n\nWe can also consider the ‚Äúsignificance‚Äù in a different way, by asking does our value for \\(Z\\) fall in the upper 5% of the normal distribution for \\(Z\\) scores. For this, we can use qnorm() to return the critical value demarking the boundary of the upper 5% of the distribution and see of \\(Z\\) falls to the right of that, which indeed it does.\n\nplotDist(\"norm\", main = paste0(\"Standard Normal Distribution\\nred area = upper 5%\"),\n    ylab = \"\", xlab = \"SD\")\nladd(panel.abline(v = z, col = \"blue\", lty = 1, lwd = 2))\ncritical_val &lt;- qnorm(0.95)\nladd(panel.abline(v = critical_val, col = \"red\", lty = 2, lwd = 2))\nladd(panel.polygon(cbind(c(critical_val, seq(from = critical_val, to = 4, length.out = 100),\n    4), c(0, dnorm(seq(from = critical_val, to = 4, length.out = 100), 0, 1), 0)),\n    border = \"black\", col = rgb(1, 0, 0, 0.5)))\nladd(panel.polygon(cbind(c(z, seq(from = z, to = 4, length.out = 100), 4), c(0, dnorm(seq(from = z,\n    to = 4, length.out = 100), 0, 1), 0)), border = \"black\", col = rgb(0, 0, 1, 0.5)))\n\n\n\n\n\n\n\n\nAnother way to think about whether there is a ‚Äúsignificant‚Äù difference between our observed and expected mean weight is to look at the 95% CI around our estimate of mean weight. We can calculate this by hand for the one-tailed test‚Ä¶\n\nalpha &lt;- 0.05\nci &lt;- m - qnorm(1 - alpha, mean = 0, sd = 1) * se  # by hand... lower limit for 95% of sampling distribution\nhistogram(x, breaks = seq(from = m - 4 * s, to = m + 4 * s, length.out = 20), main = \"Histogram of Vervet\\nWeights\",\n    xlab = \"X\", ylab = \"Proportion of Total\", type = \"density\", ylim = c(0, 3), col = rgb(0,\n        0, 1, 0.5))\nladd(panel.abline(v = mu, col = \"black\", lty = 1, lwd = 2))  # expected mean\nladd(panel.abline(v = m, col = \"black\", lty = 3, lwd = 2))  # observed mean\nladd(panel.abline(v = ci, col = \"red\", lty = 3, lwd = 2))  # lower bound for 95% CI\nz &lt;- qnorm(0.05)  # define lower bound of upper 95% of distribution\nladd(panel.polygon(cbind(c(m + z * se, seq(from = m + z * se, to = max(x), length.out = 1000),\n    max(x)), c(0, dnorm(seq(from = m + z * se, to = max(x), length.out = 1000), m,\n    se), 0)), border = \"black\", col = rgb(1, 0, 1, 0.5)))\n\n\n\n\n\n\n\n\n‚Ä¶ and for a two-tailed CI‚Ä¶\n\nci &lt;- m + c(-1, 1) * qnorm(1 - alpha/2, mean = 0, sd = 1) * se  # by hand...\nladd(panel.abline(v = ci, col = \"red\", lty = 3, lwd = 2))  # upper and lower bounds for 95% CI\n\n\n\n\n\n\n\n\nAs noted above, our sample size from a population is typically limited. So, instead of using the normal distribution (as we did here) to determine the p value of our statistic, we actually should use the \\(t\\) distribution, which, as we have seen, has slightly fatter tails. The statistic and process is exactly the same, though, as for the normal distribution.\n\\[T = \\frac{\\bar{x}-\\mu}{s/\\sqrt{n}}\\]\nor, equivalently‚Ä¶\n\\[T = \\frac{\\bar{x}-\\mu}{\\sqrt{s^2/n}}\\]\nwith\n\\[df = n - 1\\]\n\nz &lt;- (m - mu)/se\n(p &lt;- 1 - pt(z, df = n - 1))\n\n## [1] 0.01080157\n\n(p &lt;- pt(z, df = n - 1, lower.tail = FALSE))\n\n## [1] 0.01080157\n\n(critical_val &lt;- qt(0.95, df = n - 1))\n\n## [1] 1.675905\n\n\n\nplotDist(\"t\", df = n - 1, main = paste0(\"t Distribution with DF = \", n - 1, \"\\nblue area = \",\n    round(p, 4) * 100, \"%\"), ylab = \"\", xlab = \"SD\")\nladd(panel.abline(v = z, col = \"blue\", lty = 1, lwd = 2))\nladd(panel.polygon(cbind(c(z, seq(from = z, to = 4, length.out = 100), 4), c(0, dt(seq(from = z,\n    to = 4, length.out = 100), df = n - 1), 0)), border = \"black\", col = rgb(0, 0,\n    1, 0.5)))\n\n\n\n\n\n\n\n\nR has built into it a single function, t.test(), that lets us do all this in one line. We give it our data and the expected population mean, \\(\\mu\\), along with the kind of test we want to do.\n\nt_stat &lt;- t.test(x = x, mu = mu, alternative = \"greater\")\nt_stat  # the value of the t statistic is a Z score\n\n## \n##  One Sample t-test\n## \n## data:  x\n## t = 2.3716, df = 50, p-value = 0.0108\n## alternative hypothesis: true mean is greater than 5\n## 95 percent confidence interval:\n##  5.095021      Inf\n## sample estimates:\n## mean of x \n##  5.323922\n\n\nNote that the t.test() function to also calculates \\(t\\)- distribution based CIs for us easily. For the above test, the 95% CI is open-ended on the upper end because we are doing a one-tailed test. Doing the same thing by hand‚Ä¶\n\nalpha &lt;- 0.05\nci &lt;- c(m - qt(1 - alpha, df = n - 1) * se, Inf)  # lower bound calculated by hand... note we do not divide alpha by 2\nci\n\n## [1] 5.095021      Inf\n\n\nFor an equivalent two-tailed test with the same data‚Ä¶\n\nt_stat &lt;- t.test(x = x, mu = mu, alternative = \"two.sided\")\nci &lt;- t_stat$conf.int  # using t test\nci\n\n## [1] 5.049585 5.598258\n## attr(,\"conf.level\")\n## [1] 0.95\n\nci &lt;- m + c(-1, 1) * qt(1 - alpha/2, df = n - 1) * se  # upper and lower bounds calculated by hand\nci\n\n## [1] 5.049585 5.598258\n\n\n\n\n\nCHALLENGE\nAdult lowland woolly monkeys are reported to have an average body weight of 7.2 kilograms. You are working with an isolated population of woolly monkeys from the Colombian Andes that you think may be a different species from lowland form, and you collect a sample of 15 weights from adult individuals at that site. From your sample, you calculate a mean of 6.43 kilograms and a standard deviation of 0.89 kilograms. Perform a hypothesis test to evaluate whether body weights in your population are different from the reported average for lowland woolly monkeys by setting up a ‚Äútwo-tailed‚Äù hypothesis, carrying out the analysis, and interpreting the p value (assume the significance level is \\(\\alpha\\) = 0.05).\n\n\n\n\n\n\n\n\n\n\nHINT: Your sample size is &lt; 30, so you should use the \\(t\\) distribution and do a t test. Do your calculations both by hand and using the t.test() function and confirm that they match.\n\nFirst, read in the data:\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/woolly-weights.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\nhead(d)\n\n## # A tibble: 6 √ó 2\n##      id weight\n##   &lt;dbl&gt;  &lt;dbl&gt;\n## 1     1   6.14\n## 2     2   6.19\n## 3     3   7.08\n## 4     4   5.67\n## 5     5   4.83\n## 6     6   6.83\n\n\nThen, calculate the mean, standard deviation, and SE of your sample:\n\n\nShow Code\nx &lt;- d$weight\nn &lt;- length(x)\n(m &lt;- mean(x))\n\n\nShow Output\n## [1] 6.427333\n\n\n\nShow Code\n(s &lt;- sd(x))\n\n\nShow Output\n## [1] 0.8968235\n\n\n\nShow Code\n(se &lt;- s/sqrt(n))\n\n\nShow Output\n## [1] 0.2315588\n\n\n\nFinally, calculate the \\(T\\) statistic and then determine the p value associated with that statistic.\n\nNOTE: Because this is a two-tailed test, we need to calculate the probability of seeing a \\(T\\) statistic as far from the mean in either the positive or negative direction. This is why we sum the probabilities associated with the upper and lower tails‚Ä¶\n\n\n\nShow Code\nmu &lt;- 7.2\nt_stat &lt;- (m - mu)/se\nt_stat\n\n\nShow Output\n## [1] -3.336805\n\n\n\nShow Code\np_upper &lt;- 1 - pt(abs(t_stat), df = n - 1)\n# or 1 - pt(t_stat, df=n-1, lower.tail = FALSE)\np_lower &lt;- pt(-1 * abs(t_stat), df = n - 1)\n# or pt(t_stat, df=n-1, lower.tail = TRUE)\np &lt;- p_upper + p_lower\np\n\n\nShow Output\n## [1] 0.004890693\n\n\n\nTo visualize this‚Ä¶\n\nplotDist(\"t\", df = n - 1, main = paste0(\"t Distribution with DF = \", n - 1, \"\\nred area = 2.5% in each tail\",\n    \"\\nblue = \", round(p, 4) * 100, \"%\"), ylab = \"\", xlab = \"SD\", xlim = c(-4, 4))\nladd(panel.abline(v = abs(t_stat), col = \"blue\", lty = 1, lwd = 2))\nladd(panel.abline(v = -1 * abs(t_stat), col = \"blue\", lty = 1, lwd = 2))\n\n# plot upper tail\nladd(panel.polygon(cbind(c(abs(t_stat), seq(from = abs(t_stat), to = 4, length.out = 100),\n    4), c(0, dt(seq(from = abs(t_stat), to = 4, length.out = 100), df = n - 1), 0)),\n    border = \"black\", col = rgb(0, 0, 1, 0.5)))\n\n# plot lower tail\nladd(panel.polygon(cbind(c(-4, seq(from = -4, to = -1 * abs(t_stat), length.out = 100),\n    -1 * abs(t_stat)), c(0, dt(seq(from = -4, to = -1 * abs(t_stat), length.out = 100),\n    df = n - 1), 0)), border = \"black\", col = rgb(0, 0, 1, 0.5)))\n\nalpha &lt;- 0.05\ncritical_val &lt;- qt(1 - alpha/2, df = n - 1)  # identify critical values - boundaries for 95% of the t distribution\nladd(panel.abline(v = abs(critical_val), col = \"red\", lty = 2, lwd = 2))\nladd(panel.abline(v = -1 * abs(critical_val), col = \"red\", lty = 2, lwd = 2))\n\nladd(panel.polygon(cbind(c(critical_val, seq(from = critical_val, to = 4, length.out = 100),\n    4), c(0, dt(seq(from = critical_val, to = 4, length.out = 100), df = n - 1),\n    0)), border = \"black\", col = rgb(1, 0, 0, 0.5)))\n\nladd(panel.polygon(cbind(c(-4, seq(from = -4, to = -1 * abs(critical_val), length.out = 100),\n    -1 * abs(critical_val)), c(0, dt(seq(from = -4, to = -1 * abs(critical_val),\n    length.out = 100), df = n - 1), 0)), border = \"black\", col = rgb(1, 0, 0, 0.5)))\n\n\n\n\n\n\n\n\nWe can implement a simple test to see if the value of our \\(T\\) statistic is farther away from zero than the critical value‚Ä¶\n\ntest &lt;- (abs(t_stat) &gt; critical_val)\n# boolean test as to whether t is larger than the critical value at either tail\ntest\n\n## [1] TRUE\n\n\nAnother way to think about whether there is a significant difference between our observed and expected mean weight is to look at the 95% CI around our estimate of mean weight. We can calculate this by hand or use the output of the t test‚Ä¶\n\n(ci &lt;- m + c(-1, 1) * critical_val * se)  # by hand\n\n## [1] 5.930689 6.923978\n\nt.test(x = x, mu = mu, alternative = \"two.sided\")  # `t.test()` function\n\n## \n##  One Sample t-test\n## \n## data:  x\n## t = -3.3368, df = 14, p-value = 0.004891\n## alternative hypothesis: true mean is not equal to 7.2\n## 95 percent confidence interval:\n##  5.930689 6.923978\n## sample estimates:\n## mean of x \n##  6.427333\n\n\nThe 95% CI does not include the expected mean weight.\n\nTwo Sample \\(Z\\) and \\(T\\) Test\nSometimes we want to compare two groups of measurements to one another, which boils down to a hypothesis test for the difference between two means, \\(\\mu1\\) and \\(\\mu2\\). The null hypothesis is that the difference between these means is zero.\nBefore getting to the appropriate test, there are a couple of things that we need to consider:\n[1] How, if at all, are the two samples related to one another? Sometimes we may have PAIRED samples (e.g., the same individuals before and after some treatment) and sometimes the samples are UNPAIRED or INDEPENDENT (e.g., weights for different samples of black-and-white colobus monkeys collected in the rainy versus dry seasons).\n[2] Are the variances in the two samples roughly equal or not? E.g., if we are comparing male and female heights, are the variances comparable?\n\nSamples with Unequal Variances\nFor the most generic case, where the two samples are independent and we cannot assume the variances of the two samples are equal, we can do what is called Welch‚Äôs t test where our test statistic is:\n\\[T = \\frac{\\bar{x_2} - \\bar{x_1} - \\mu}{\\sqrt{s_1^2/n_1 + s_2^2/n_2}}\\]\nwhere:\n\n\\(\\bar{x_1}\\) and \\(\\bar{x_2}\\) = means of observations in each sample group\n\\(\\mu\\) = expected difference in means between sample groups under the null hypothesis, which is usually zero\n\\(s_1\\) and \\(s_2\\) = standard deviations of each sample group\n\\(n_1\\) and \\(n_2\\) = numbers of observations in each sample group\n\n\n\n\n\nCHALLENGE\nLet‚Äôs do an example. Load in a file of black-and-white colobus weights and examine the str() of the file.\n\n\n\n\n\n\n\n\n\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/colobus-weights.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\nhead(d)\n\n## # A tibble: 6 √ó 3\n##      id weight sex  \n##   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;\n## 1     1   7.24 male \n## 2     2   6.09 male \n## 3     3   6.97 male \n## 4     4   6.98 male \n## 5     5   6.08 male \n## 6     6   6.22 male\n\n\nThen, create two vectors, x and y, for male and female weights. Plot these in boxplots side by side and then calculate the mean, sd, and sample size for both males and females. Do the variances look similar for the two sexes?\n\n\nShow Code\nx &lt;- d$weight[d$sex == \"male\"]\ny &lt;- d$weight[d$sex == \"female\"]\npar(mfrow = c(1, 2))\nminval &lt;- min(c(x, y)) - 0.1\nmaxval &lt;- max(c(x, y)) + 0.1\nboxplot(x, ylim = c(minval, maxval), main = \"Weight (kg)\", xlab = \"Males\")\n# `ylim=` argument uses x and y ranges to set range for y axis\nboxplot(y, ylim = c(minval, maxval), main = \"Weight (kg)\", xlab = \"Females\")\n\n\n\n\n\n\n\n\n\nShow Code\nm1 &lt;- mean(x)\nm2 &lt;- mean(y)\nmu &lt;- 0  # you could leave this out... the default argument value is 0\ns1 &lt;- sd(x)\ns2 &lt;- sd(y)\nn1 &lt;- length(x)\nn2 &lt;- length(y)\n\n\nNow calculate the \\(T\\) statistic and test the two-tailed hypothesis that the sample means differ. Note that for the Welch‚Äôs t test, the number of degrees of freedom is calculated as:\n\\[df = \\frac{(s_1^2/n_1 + s_2^2/n_2)^2}{(s_1^2/n_1)^2/(n_1-1)+(s_2^2/n_2)^2/(n_2-1)}\\]\n\ndf &lt;- (s2^2/n2 + s1^2/n1)^2/((s2^2/n2)^2/(n2 - 1) + (s1^2/n1)^2/(n1 - 1))\ndf\n\n## [1] 31.21733\n\n\n\n\nShow Code\nalpha &lt;- 0.05\nt_stat &lt;- (m2 - m1 - mu)/sqrt(s2^2/n2 + s1^2/n1)\nt_stat\n\n\nShow Output\n## [1] -11.45952\n\n\n\nShow Code\n# note that because our hypothesis is 2-tailed, it does not matter which group\n# (males or females) is m1 and which is m2, so we take the absolute values of t\n# below when testing whether it is greater than the critical value\ncritical_val &lt;- qt(1 - alpha/2, df = df)\n# identify the critical value, i.e., how far apart the means of the two samples\n# need to be to be more extreme than expected by chance at the given alpha\n# level\ncritical_val\n\n\nShow Output\n## [1] 2.038938\n\n\n\nShow Code\ntest &lt;- abs(t_stat) &gt; critical_val  # boolean test\ntest  # if true, the two means are significantly different\n\n\nShow Output\n## [1] TRUE\n\n\n\nWe can visualize this as below by plotting histograms of body weights for males and females and adding the sampling distribution curves and 95% CIs around the estimates of the means for the two sexes atop of the histogram.\n\npar(mfrow = c(1, 1))\nhist(x, breaks = seq(from = minval, to = maxval, length.out = 15), xlim = c(minval,\n    maxval), ylim = c(0, 9), main = \"Histogram and Sampling Distributions\\nfor Body Weights\\n(red = females, blue = males)\",\n    col = rgb(0, 0, 1, 0.5))\n# `xlim=` argument uses x and y ranges to set range for y axis\nhist(y, breaks = seq(from = minval, to = maxval, length.out = 15), xlim = c(minval,\n    maxval), col = rgb(1, 0, 0, 0.5), add = TRUE)\ncurve(dnorm(x, m1, s1/sqrt(n1)), n = 1000, add = TRUE)\ncurve(dnorm(x, m2, s2/sqrt(n2)), n = 1000, add = TRUE)\n\nabline(v = m1 - qt(1 - alpha/2, df = df) * s1/sqrt(n1))\nabline(v = m1 + qt(1 - alpha/2, df = df) * s1/sqrt(n1))\nabline(v = m2 - qt(1 - alpha/2, df = df) * s2/sqrt(n2))\nabline(v = m2 + qt(1 - alpha/2, df = df) * s2/sqrt(n2))\n\n\n\n\n\n\n\n\n\nNOTE: The 95% CIs around each sample mean do not overlap, so the means are significantly different.\n\nWe can do the same thing using the t.test() function.\n\n(abs(m1 - m2))  # difference between means\n\n## [1] 1.449\n\nt_stat &lt;- t.test(x = x, y = y, mu = 0, alternative = \"two.sided\", var.equal = FALSE)\n# var.equal = FALSE is the DEFAULT for `t.test()`\nt_stat\n\n## \n##  Welch Two Sample t-test\n## \n## data:  x and y\n## t = 11.46, df = 31.217, p-value = 1.023e-12\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  1.191186 1.706814\n## sample estimates:\n## mean of x mean of y \n##     6.689     5.240\n\n\n\nNOTE: The 95% CI returned by the t test is the interval around the DIFFERENCE between the means.\n\n\nSamples with Equal Variances\nThere‚Äôs a simpler \\(T\\) statistic we can use if the variances of the two samples are more or less equal.\n\\[T=\\frac{\\bar{x_2}-\\bar{x_1}-\\mu}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\]\nwhere:\n\n\\(s_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2}\\)\n\\(\\bar{x_1}\\) and \\(\\bar{x_2}\\) = means of observations in each sample group\n\\(\\mu\\) = expected difference in means between sample groups (usually set to zero)\n\\(s_p\\) = pooled sample standard deviation\n\\(n_1\\) and \\(n_2\\) = numbers of observations in each sample group\n\nThe degrees of freedom for this test is calculated as follows:\n\\[df=n_1+n_2-2\\]\nCalculating the \\(t\\) statistic and degrees of freedom by hand‚Ä¶\n\ns &lt;- sqrt((((n1 - 1) * s1^2) + ((n2 - 1) * s2^2))/(n1 + n2 - 2))\nt_stat &lt;- (m2 - m1 - mu)/(sqrt(s^2 * (1/n1 + 1/n2)))\nt_stat\n\n## [1] -11.45952\n\ndf &lt;- n1 + n2 - 2\ndf\n\n## [1] 38\n\n\n‚Ä¶ matches what we see when we use t.test().\n\nt_stat &lt;- t.test(x = x, y = y, mu = 0, var.equal = TRUE, alternative = \"two.sided\")\nt_stat\n\n## \n##  Two Sample t-test\n## \n## data:  x and y\n## t = 11.46, df = 38, p-value = 6.787e-14\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  1.193025 1.704975\n## sample estimates:\n## mean of x mean of y \n##     6.689     5.240\n\n\n\nNOTE: A crude test for equality of variances is to divide the larger by the smaller and if the result is &lt; 2, you can go ahead and used the pooled variance version of the test (which has many fewer degrees of freedom).\n\nIn our case, we should not really use the equal variances version of the t test, since the ratio of variances exceeds 2‚Ä¶\n\nvar(x)/var(y)\n\n## [1] 2.746196\n\n\nWe can use the var.test() function to conduct an actual statistical test on the ratio of variances, which compares the ratio test statistic we just calculated to an \\(F\\) distribution. The \\(F\\) distribution is often used to model ratios of random variables and thus is useful in regression applications and, as here, for testing whether variances from two samples are different. It is dependent upon the specification of a pair of degrees of freedom values supplied as the arguments df1= and df2= (or inferred from the number of observations in each sample).\nBelow, the results of var.test() are saved to a variable. Calling the variable provides a brief descriptive summary.\n\nvt &lt;- var.test(x, y)\nvt\n\n## \n##  F test to compare two variances\n## \n## data:  x and y\n## F = 2.7462, num df = 19, denom df = 19, p-value = 0.03319\n## alternative hypothesis: true ratio of variances is not equal to 1\n## 95 percent confidence interval:\n##  1.086978 6.938128\n## sample estimates:\n## ratio of variances \n##           2.746196\n\n\n\n\nPaired Samples\nFor a paired samples test, the null hypothesis is that the mean of individual paired differences between the two samples (e.g., before and after) is zero.\nOur test statistic is:\n\\[T = \\frac{d-\\mu}{\\sqrt{s_d^2/n}}\\]\nwhere:\n\n\\(\\bar{d}\\) = mean of difference between paired samples\n\\(\\mu\\) = expected mean difference between paired samples (usually set to zero)\n\\(s_d\\) = standard deviation in the set of differences between paired samples\n\\(n\\) = number of sample pairs\n\nAgain, note that \\(\\mu\\) here is the expected difference between the means under the null hypothesis, which is zero, and we are dividing by the standard error of the mean for the set of differences between pairs.\n\n\n\nCHALLENGE\nLet‚Äôs play with a sample‚Ä¶ test scores of individuals taking a certain statistics course pre and post a lecture on null hypothesis significance testing. Load in the test_scores.csv data file, look at it, plot a barchart of values before and after and construct a paired t test to evaluate the means before and after.\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/test_scores.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\nhead(d)\n\n## # A tibble: 6 √ó 3\n##      id `Score before` `Score after`\n##   &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;\n## 1     1           89.0          95.4\n## 2     2           90.0          94.2\n## 3     3           90.7          92.2\n## 4     4           82.1          92.4\n## 5     5           87.4          94.8\n## 6     6           88.3          90.6\n\nx &lt;- d$`Score after` - d$`Score before`\nm &lt;- mean(x)\nmu &lt;- 0  # can leave this out\ns &lt;- sd(x)\nn &lt;- length(x)\nse &lt;- s/sqrt(n)\npar(mfrow = c(1, 2))\nboxplot(d$`Score before`, ylim = c(80, 100), main = \"Score\", xlab = \"Before\")\nboxplot(d$`Score after`, ylim = c(80, 100), main = \"Score\", xlab = \"After\")\n\n\n\n\n\n\n\nt_stat &lt;- (m - mu)/se\nt_stat\n\n## [1] 1.789636\n\nalpha &lt;- 0.05\ncritical_val &lt;- qt(1 - alpha/2, df = n - 1)  # identify critical values\ncritical_val\n\n## [1] 2.093024\n\ntest &lt;- abs(t_stat) &gt; critical_val  # boolean test\ntest\n\n## [1] FALSE\n\nt.test(d$`Score before`, d$`Score after`, df = n - 1, alternative = \"two.sided\",\n    paired = TRUE)\n\n## \n##  Paired t-test\n## \n## data:  d$`Score before` and d$`Score after`\n## t = -1.7896, df = 19, p-value = 0.08946\n## alternative hypothesis: true mean difference is not equal to 0\n## 95 percent confidence interval:\n##  -4.830292  0.377435\n## sample estimates:\n## mean difference \n##       -2.226429\n\n\n\n\nWorking with Proportions\n\nOne Sample \\(Z\\) Test\nAs we have seen, the theoretical sampling distribution of sample means for independent and identically distributed random continuous variables is roughly normal (and, as shown by the CLT, this distribution increasingly approaches normal as sample size increases). Similarly, the sampling distribution for another kind of sample statistic, the number of ‚Äúsuccesses‚Äù \\(x\\) out of a series of \\(k\\) trials is also roughly normally distributed. If the true population proportion of ‚Äúsuccesses‚Äù is \\(\\pi\\), then the sampling distribution for the proportion of successes in a sample of size \\(n\\) is expected to be roughly normally distributed with mean = \\(\\pi\\) and standard error = \\(\\sqrt{\\pi(1-\\pi)/n}\\).\nLet‚Äôs set up a simulation to show this‚Ä¶\nFirst we create a population of 500 ‚Äú1‚Äùs and 500 ‚Äú0‚Äùs, i.e., where \\(\\pi\\) = 0.5‚Ä¶\n\npop &lt;- c(rep(0, 500), rep(1, 500))\n\nNow, we will take 1000 random samples of size \\(n\\)=10 from that population and calculate the proportion of ‚Äú1‚Äùs in each sample‚Ä¶\n\npi &lt;- 0.5\nx &lt;- NULL\nn &lt;- 10\nfor (i in 1:1000) {\n    x[i] &lt;- mean(sample(pop, size = n, replace = FALSE))\n    # taking the mean of a bunch of 0s and 1s yields the proportion of 1s!\n}\nm &lt;- mean(x)\nm\n\n## [1] 0.5056\n\ns &lt;- sd(x)\ns\n\n## [1] 0.1543128\n\nse &lt;- sqrt(pi * (1 - pi)/n)\nse  # the SE is an estimate of the SD of the sampling distribution\n\n## [1] 0.1581139\n\n\nThe same is true if we create a population of 800 ‚Äú1‚Äùs and 200 ‚Äú0‚Äùs, i.e., where \\(\\pi\\) = 0.8‚Ä¶\n\npop &lt;- c(rep(0, 800), rep(1, 200))\npi &lt;- 0.8\nx &lt;- NULL\nn &lt;- 10\nfor (i in 1:1000) {\n    x[i] &lt;- mean(sample(pop, size = n, replace = FALSE))\n    # taking the mean of a bunch of 0s and 1s yields the proportion of 1s!\n}\nm &lt;- mean(x)\nm\n\n## [1] 0.2041\n\ns &lt;- sd(x)\ns\n\n## [1] 0.1261313\n\nse &lt;- sqrt(pi * (1 - pi)/n)\nse  # the SE is an estimate of the SD of the sampling distribution\n\n## [1] 0.1264911\n\n\nThis normal approximation is true as long as \\(n\\) is fairly large and \\(\\pi\\) is not close to 0 or 1. One rule of thumb is to check that both \\(n\\times\\pi\\) and \\(n\\times(1-\\pi)\\) are greater than 5.\nWith all this in mind, we can construct \\(Z\\) statistics for proportions just like we constructed \\(Z\\) and \\(T\\) statistics for means and test those proportions for differences from an expected value or for differences between two sample proportions. The \\(Z\\) statistic for proportions takes the same general form as that for means‚Ä¶\n\\(Z =\\) (observed statistic - expected statistic) / expected standard error\nor,\n\\[Z = \\frac{\\hat{p}-\\pi}{\\sqrt{\\pi(1-\\pi)/n}}\\]\nwhere:\n\n\\(\\hat{p}\\) = proportion in sample\n\\(\\pi\\) = expected proportion\n\\(n\\) = number of observations in sample\n\n\n\n\nCHALLENGE\nA neotropical ornithological working in the western Amazon deploys 30 mist nets in a 100 ha grid.\n\n\n\n\n\n\n\n\n\nShe monitors the nets on one morning and records whether or not she captures any birds in the net (i.e., a ‚Äúsuccess‚Äù or ‚Äúfailure‚Äù for every net during a netting session). The following vector summarizes her netting results:\n\nv &lt;- c(0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n    0, 1, 0, 1, 1)\n\nHer netting success over the previous three seasons suggests that she should catch birds in 80% of her nets. This season, she feels, her success rate is lower than in previous years (so‚Ä¶ this implies that we want to do a lower one-tailed test). Does her trapping data support this hypothesis?\n\nWhat is \\(H_0\\)?\nWhat is \\(H_A\\)?\nAre both \\(n\\times\\pi\\) and \\(n\\times(1-\\pi)\\) &gt; 5?\nCalculate \\(Z\\) statistic and the p value associated with \\(Z\\). We use the lower.tail=TRUE argument to pnorm() because we‚Äôre testing a lower-tailed one-tailed hypothesis.\n\n\n\nShow Code\nphat &lt;- mean(v)  # the mean of binomial 0,1 variables yields the proportion!\nphat\n\n\nShow Output\n## [1] 0.6\n\n\n\nShow Code\npi &lt;- 0.8\nn &lt;- 30\nz &lt;- (phat - pi)/sqrt(pi * (1 - pi)/30)\n# we use the population expected proportion in the denominator\nz\n\n\nShow Output\n## [1] -2.738613\n\n\n\nShow Code\np &lt;- pnorm(z, lower.tail = TRUE)\np\n\n\nShow Output\n## [1] 0.00308495\n\n\n\nGraphically, our \\(Z\\) value (in blue) is farther from the center of the standard normal curve than the critical value marking the lower 5% of the total distribution (in red).\n\npar(mfrow = c(1, 1))\ncurve(dnorm(x, 0, 1), xlim = c(-4, 4), ylab = \"\", yaxt = \"n\", xlab = \"SD\", main = \"Sampling Distribution\")\n\nabline(v = z, col = \"blue\", lwd = 2)\nabline(v = qnorm(0.05), col = \"red\")\n\n\n\n\n\n\n\n\nThe 95% confidence interval around the sample proportion can be estimated, based on the normal distribution, as follows:\n\nlower &lt;- phat - qnorm(0.975) * sqrt(phat * (1 - phat)/30)\nupper &lt;- phat + qnorm(0.975) * sqrt(phat * (1 - phat)/30)\nci &lt;- c(lower, upper)\nci\n\n## [1] 0.4246955 0.7753045\n\n\nThis approach using quantiles of the standard normal distribution is but one method of calculating CIs for proportion data, and it generates a CI referred to as a Wald confidence interval. Note that this CI does not include the value of \\(\\pi\\)‚Ä¶ rather, \\(\\pi\\) is greater than the upper bound of the CI, suggesting that the observed success rate is indeed lower than in previous years.\nWe can do the same test with a one-liner in R‚Ä¶\n\npt &lt;- prop.test(x = sum(v), n = length(v), p = 0.8, conf.level = 0.95, alternative = \"less\",\n    correct = FALSE)  # use correct=FALSE if we satisfy that n*pi and n*(1-pi) are both &gt;5\npt\n\n## \n##  1-sample proportions test without continuity correction\n## \n## data:  sum out of lengthv out of v\n## X-squared = 7.5, df = 1, p-value = 0.003085\n## alternative hypothesis: true p is less than 0.8\n## 95 percent confidence interval:\n##  0.0000000 0.7328738\n## sample estimates:\n##   p \n## 0.6\n\n\n\nNOTE: The CI returned here is different than we calculated based on the normal distribution, although the p value is the same‚Ä¶ prop.test() implements a slightly different procedure for estimating the CI rather than basing this on the normal distribution and the CLT.\n\n\nTwo Sample \\(Z\\) Test\nThe \\(Z\\) statistic for the two-sample test comparing proportions is also very similar to that for comparing means.\n\\[Z=\\frac{\\hat{p_2}-\\hat{p_1}-\\pi}{\\sqrt{p^*(1-p^*)(1/n_1+1/n_2)}}\\]\nwhere:\n\n\\(p^* = \\frac{x_1 + x_2}{n_1+n_2}\\) = pooled proportion\n\\(\\hat{p_1}\\) and \\(\\hat{p_2}\\) = proportions of ‚Äúsuccesses‚Äù in each sample group\n\\(\\pi\\) = expected difference in proportions between sample groups (usually set to zero)\n\\(n_1\\) and \\(n_2\\) = numbers of observations in each sample group\n\n\n\n\nCHALLENGE\nA biologist studying two species of tropical bats captures females of both species in a mist net over the course of a week of nightly netting. For each species, the researcher records whether females is lactating or not.\n\n\n\n\n\n\n\n\n\nThe two vectors below summarize the data for each species.\n\nspecies1 &lt;- c(1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,\n    1, 0)\nspecies2 &lt;- c(1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,\n    0, 1, 1, 0, 1, 1, 1)\n\nBased on your mist netting data, do the species differ significantly in the proportion of lactating females? What are \\(H_0\\) and \\(H_A\\)?\n\npstar &lt;- (sum(species1) + sum(species2))/(length(species1) + length(species2))\npstar\n\n## [1] 0.6363636\n\nphat1 &lt;- mean(species1)\nphat1\n\n## [1] 0.56\n\nphat2 &lt;- mean(species2)\nphat2\n\n## [1] 0.7\n\npi &lt;- 0\nz &lt;- (phat2 - phat1 - pi)/sqrt((pstar * (1 - pstar)) * (1/length(species1) + 1/length(species2)))\nz\n\n## [1] 1.074709\n\np_upper &lt;- 1 - pnorm(z, lower.tail = TRUE)\np_lower &lt;- pnorm(z, lower.tail = FALSE)\n# two-tailed probability, so we add the upper and lower tails\np &lt;- p_upper + p_lower\np\n\n## [1] 0.2825049\n\ncritical_val &lt;- qnorm(1 - alpha/2)  # identify critical values\ncritical_val\n\n## [1] 1.959964\n\ntest &lt;- abs(z) &gt; critical_val  # boolean test\ntest\n\n## [1] FALSE\n\n\nWe can use the prop.test() function to do this in one line.\n\nprop.test(x = c(sum(species2), sum(species1)), n = c(length(species2), length(species1)),\n    alternative = \"two.sided\", correct = FALSE)\n\n## \n##  2-sample test for equality of proportions without continuity correction\n## \n## data:  c out of csum(species2) out of length(species2)sum(species1) out of length(species1)\n## X-squared = 1.155, df = 1, p-value = 0.2825\n## alternative hypothesis: two.sided\n## 95 percent confidence interval:\n##  -0.1144634  0.3944634\n## sample estimates:\n## prop 1 prop 2 \n##   0.70   0.56\n\n# use correct=FALSE if we satisfy that n*pi and n*(1-pi) are both &gt;5",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Classical Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "15-module.html#concept-review",
    "href": "15-module.html#concept-review",
    "title": "15¬† Classical Hypothesis Testing",
    "section": "Concept Review",
    "text": "Concept Review\n\n\\(Z\\) and \\(T\\) tests are used to evaluate whether a given sample statistic (e.g., a mean or proportion) deviates significantly from what is expected under a null model or whether two samples statistics deviate significantly from one another\nThey are used for dealing with normally distributed, continuous variables or those that can be approximated closely by the normal distribution (e.g., proportion data)\n\nWe REJECT a \\(H_0\\) if the p value obtained for a given \\(Z\\) or \\(T\\) test statistic is &lt; \\(\\alpha\\)\nCIs for our sample statistic are calculated as \\(mean ¬± [T_{(1-\\alpha/2)}\\) or \\(Z_{(1-\\alpha/2)}] \\times SE\\), and we can REJECT an \\(H_0\\) if the \\((1-\\alpha)\\) CI around does not include the expected value of the statistic\nWhen we are dealing with data for a population or for sample sizes &gt; 30, or when we are dealing with proportions, we use \\(Z\\) distribution quantiles for calculating CIs and p values, but for sample sizes &lt; 30, we use \\(t\\) distribution quantiles\n\n\n\nFormula Summary\n\nThe \\(Z\\) or \\(T\\) Statistic for Testing a Single Mean\n\\[Z\\ {\\rm or}\\ T = \\frac{\\bar{x}-\\mu}{s/\\sqrt{n}}\\]\n\n\nThe \\(T\\) Statistic for Comparing Means\n\nUnequal Variance\n\\[Z\\ {\\rm or}\\ T = \\frac{\\bar{x_2} - \\bar{x_1} - \\mu}{\\sqrt{s_1^2/n_1 + s_2^2/n_2}}\\]\n\n\nEqual Variance\n\\[Z\\ {\\rm or}\\ T =\\frac{\\bar{x_2}-\\bar{x_1}-\\mu}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\]\n\n\nPaired Samples\n\\[Z\\ {\\rm or}\\ T = \\frac{d-\\mu}{\\sqrt{s_d^2/n}}\\]\n\n\n\nThe \\(Z\\) Statistic for Testing a Single Proportion\n\\[Z = \\frac{\\hat{p}-\\pi}{\\sqrt{\\pi(1-\\pi)/n}}\\]\n\n\nThe \\(Z\\) Statistic for Comparing Proportions\n\\[Z = \\frac{\\hat{p_2}-\\hat{p_1}-\\pi}{\\sqrt{p^*(1-p^*)(1/n_1+1/n_2)}}\\]",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Classical Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "16-module.html",
    "href": "16-module.html",
    "title": "16¬† Using Permutation Tests",
    "section": "",
    "text": "16.1 Objectives",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Using Permutation Tests</span>"
    ]
  },
  {
    "objectID": "16-module.html#objectives",
    "href": "16-module.html#objectives",
    "title": "16¬† Using Permutation Tests",
    "section": "",
    "text": "The objective of this module is to extend our hypothesis testing framework to include permutation/randomization tests.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Using Permutation Tests</span>"
    ]
  },
  {
    "objectID": "16-module.html#preliminaries",
    "href": "16-module.html#preliminaries",
    "title": "16¬† Using Permutation Tests",
    "section": "16.2 Preliminaries",
    "text": "16.2 Preliminaries\n\nInstall the following packages in R: {coin}, {jmuOutlier}, and {infer}\nLoad {tidyverse} and {mosaic}",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Using Permutation Tests</span>"
    ]
  },
  {
    "objectID": "16-module.html#permutation-methods",
    "href": "16-module.html#permutation-methods",
    "title": "16¬† Using Permutation Tests",
    "section": "16.3 Permutation Methods",
    "text": "16.3 Permutation Methods\nThe inferential framework described in Module 15 is grounded in the idea that the sampling distributions for summary statistics of interest are appropriately modeled using well-known theoretical distributions‚Ä¶ but that may not always be the case. An alternative approach is for us to build up (rather than assume) sampling distributions for statistics of interest using so-called ‚Äúpermutation‚Äù or ‚Äúrandomization‚Äù methods, which are a type of simulation. These approaches are conceptually related to the bootstrapping that we employed in Module 14 as an alternative method of generating sampling distributions and CIs.\nLike bootstrapping, permutation/randomization tests generate a kind of sampling distribution (called a ‚Äúpermutation distribution‚Äù or ‚Äúnull distribution) for the null hypothesis to which we compare a test statistic calculated from our observations. We generate the permutation or null distribution by resampling from a set of actually observed outcomes while permuting (‚Äùshuffling‚Äù) some attribute of our actual observations in accordance with what we would expect if our null hypothesis were true.\nThus, whereas a ‚Äúbootstrap sampling distribution‚Äù for generating CIs is created via simulation by repeatedly resampling observations from a single sample with replacement, a ‚Äúpermutation distribution‚Äù or ‚Äúnull distribution‚Äù is generated by resampling many times from that sample without replacement while shuffling attributes of each observation. Depending on what type of test we are doing (e.g., comparing a mean or proportion from a single sample to a hypothesized expectation, comparing means or proportions between two samples, evaluating whether regression coefficients are significant, etc.), exactly what we are shuffling in each resampling simulation may vary. In general permutation/randomization tests are extremely flexible and can be used for virtually any kind of test we may imagine.\nFor a true ‚Äúpermutation test‚Äù, we would sample all possible permutations of our original data to construct the permutation distribution, which can be computationally prohibitive if the sample size is even moderately large. However, we can conduct ‚Äúapproximate permutation tests‚Äù by simply using a large number of resamples to approximate the permutation distribution. These are generally referred to as randomization tests.\nLet‚Äôs use the same data we used in Module 15 and conduct the same kinds of statistical tests we did previously, but this time we will use randomization to generate approximate permutation distributions for test statistics under the null hypothesis.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Using Permutation Tests</span>"
    ]
  },
  {
    "objectID": "16-module.html#working-with-means",
    "href": "16-module.html#working-with-means",
    "title": "16¬† Using Permutation Tests",
    "section": "16.4 Working with Means",
    "text": "16.4 Working with Means\n\nOne Sample Permutation Test\nRecall that in one of the tests we conducted in Module 15 was to evaluate whether vervet monkeys trapped during the 2015 trapping season in South Africa differed in weight from the expectation based on previous years (i.e., \\(H_0: \\mu_0 = 5.0\\ kg\\)). Here, we start by generating a test statistic that is the actual difference between our observed mean and the expected mean rather than an \\(Z\\) or \\(T\\) score estimating how many SEs our observed mean is away from the expected population mean.\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/vervet-weights.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\nx &lt;- d$weight  # current weights\nn &lt;- length(x)\nm &lt;- mean(x)  # average weight of our sample\nmu &lt;- 5  # expected weight (null hypothesis)\n(actual_diff &lt;- m - mu)  # difference between our sample and expected\n\n## [1] 0.3239216\n\n# we use this as our test statistic!\n\nRecall that permutation/randomization tests work by resampling and shuffling attributes of the observed data many times in order to come up with a distribution of test statistics that we might plausibly get under a particular null hypothesis. They do not rely on assuming that the data come from a particular theoretical distribution.\nWhat is it that we would need to permute about our vervet body size data for it to be consistent with the null hypothesis of no difference between our sample mean of 2015 weights and the expectation based on weights from previous seasons, i.e., that \\(mean\\ observed - expected\\ weights\\) (\\(\\mu_A - \\mu_0\\)) = ZERO? Well, we could simply randomize the sign of the value of \\(individual\\ observed - expected\\ weights\\) in each simulation and then take the mean to generate a permutation distribution of \\(mean\\ observed - expected\\ weights\\) under the null hypothesis.\nThe p value calculated for a permutation/randomization test is the probability of getting a test statistic, by chance, as or more extreme than our observed one. We can calculate this empirically based on the permutation distribution, simply by counting the number of simulated test statistics that exceed our observed test statistic and then dividing by the number of permutations.\n\nNOTE: Sometimes, we add 1 to both the numerator and denominator for this calculation to ensure that we never have a p value of exactly 0.\n\nBelow, we do 10,000 permutations where we randomly shuffle the sign of each individual observation‚Äôs difference from the expected mean under the null hypothesis. We then compare the observed difference between the mean of our sample and the expected mean to this distribution and calculate the p value associated with getting a difference as or more extreme than we observed. Lastly, we plot the approximate permutation distribution we generated and superimpose our observed difference as the test statistic.\n\nnperm &lt;- 10000  # number of permutation simulations\npermuted_diff &lt;- vector(length = n)  # set up a dummy vector to hold results for each permutation\nfor (i in 1:nperm) {\n    # now we scramble the sign of individual observed - expected weights, and\n    # then take mean\n    permuted_diff[[i]] &lt;- mean(sample(c(-1, 1), length(x), replace = TRUE) * abs(x -\n        mu))\n}\n\n# calculate the two.sided p value\n(p &lt;- (sum(permuted_diff &gt;= abs(actual_diff)) + sum(permuted_diff &lt;= -abs(actual_diff)))/nperm)\n\n## [1] 0.0197\n\n# or\n(p &lt;- sum(abs(permuted_diff) &gt;= abs(actual_diff))/nperm)\n\n## [1] 0.0197\n\n# or add 1 to both numerator and denominator to avoid p = 0\n\nhistogram(permuted_diff, type = \"count\", xlab = \"\", ylab = \"# Permutations\", main = \"Histogram of Permutation Distribution\")\nladd(panel.abline(v = actual_diff, lty = 3, lwd = 2))\nladd(panel.text(x = actual_diff, y = nperm * 0.08, \"Test Statistic\", srt = 90, pos = 4,\n    offset = 1))\n\n\n\n\n\n\n\n\nThe function perm.test from the package {jmuOutlier} conducts the same kind of test. By default, the function runs 10,000 simulations, and the test statistic being calculated is the mean, but other functions are possible.\n\nlibrary(jmuOutlier)\n# first show plot\nperm.test(x, alternative = \"two.sided\", mu = mu, plot = TRUE, num.sim = nperm)\nabline(v = actual_diff, lty = 3, lwd = 2)\ntext(x = actual_diff, y = nperm * 0.065, \"Test Statistic\", srt = 90, pos = 4, offset = 1)\n\n\n\n\n\n\n\n# then 2 tailed p value\nperm.test(x, alternative = \"two.sided\", mu = mu, plot = FALSE, num.sim = nperm)\n\n## [[1]]\n## [1] \"One-sample permutation test was performed.\"\n## \n## [[2]]\n## [1] \"p-value was estimated based on 10000 simulations.\"\n## \n## $alternative\n## [1] \"two.sided\"\n## \n## $mu\n## [1] 5\n## \n## $p.value\n## [1] 0.0233\n\ndetach(package:jmuOutlier)\n\nWe can also write our own custom function to perform the same kind of 1-sample permutation test!\n\nperm1samp &lt;- function(x, stat = \"mean\", mu = 0, nperm = 10000, alternative = c(\"two.sided\",\n    \"less\", \"greater\")) {\n    # can modify function by adding alternative `stat=`\n    test_data &lt;- x - mu\n    n &lt;- length(test_data)\n    if (stat == \"mean\") {\n        myfun &lt;- function(x) {\n            mean(x)\n        }\n    }\n    test_stat &lt;- myfun(test_data)\n    perm_stat &lt;- vector(length = nperm)\n    for (i in 1:nperm) {\n        perm_stat[[i]] &lt;- myfun(sample(c(-1, 1), n, replace = TRUE) * abs(test_data))\n    }\n    # or, as a 1-liner that does not require a loop perm_stat &lt;-\n    # replicate(nperm, myfun(sample(c(-1,1), n, replace=TRUE) *\n    # abs(test_data)))\n\n    if (alternative[1] == \"less\") {\n        p_perm &lt;- sum(perm_stat &lt;= test_stat)/nperm\n    } else if (alternative[1] == \"greater\") {\n        p_perm &lt;- sum(perm_stat &gt;= test_stat)/nperm\n    } else {\n        p_perm &lt;- sum(abs(perm_stat) &gt;= abs(test_stat))/nperm\n    }\n    list(test_stat = test_stat, perm_stat = perm_stat, p_perm = p_perm, type = alternative[1])\n}\n\nnperm &lt;- 10000\noutput &lt;- perm1samp(x, stat = \"mean\", mu = mu, nperm = nperm)\n\nhistogram(output$perm_stat, type = \"count\", ylab = \"# Permutations\", main = \"Histogram of Permutation Distribution\",\n    xlab = ifelse(output$type == \"less\" | output$type == \"greater\", paste0(\"One-Sided Test\\nProportion of distribution \",\n        output$type, \" than test stat = \", output$p_perm), paste0(\"Two-Sided Test\\nProportion of distribution more extreme than test stat = \",\n        output$p_perm)))\nladd(panel.abline(v = output$test_stat, lty = 3, lwd = 2))\nif (output$type == \"two.sided\") {\n    ladd(panel.abline(v = -output$test_stat, lty = 3, lwd = 2, col = \"red\"))\n}\nladd(panel.text(x = output$test_stat, y = nperm * 0.08, \"Test Statistic\", srt = 90,\n    pos = 4, offset = 1))\n\n\n\n\n\n\n\n\n\n\nTwo Sample Permutation Test\nAnother of the tests we conducted in Module 15 was to evaluate whether male and female black-and-white colobus differ in body weight. Let‚Äôs load in that data again and then calculate, as a test statistic, the difference in average weights for females versus males.\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/colobus-weights.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\nhead(d)\n\n## # A tibble: 6 √ó 3\n##      id weight sex  \n##   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;\n## 1     1   7.24 male \n## 2     2   6.09 male \n## 3     3   6.97 male \n## 4     4   6.98 male \n## 5     5   6.08 male \n## 6     6   6.22 male\n\nx &lt;- d[d$sex == \"female\", ]$weight\ny &lt;- d[d$sex == \"male\", ]$weight\n(actual_diff &lt;- mean(x) - mean(y))\n\n## [1] -1.449\n\n\nTo test whether this difference is significant by permutation, we would generate a permutation distribution consistent with a null hypothesis of no difference in weight between the sexes by simulation, where, for each simulation, we permute the sex assigned to each observation and then calculate the difference (female - male) between the sexes in each permuted set. We then calculate the p value associated with getting a difference between male and female mean weights as or more extreme than the one we observed. [We again calculate this as a ‚Äútwo-sided‚Äù probability, i.e., we are testing the \\(H_A\\) that‚Ä¶\n\\(mean(female) - mean(male) ‚â† 0\\)\nrather than‚Ä¶\n\\(mean(female) - mean(male) &gt; 0\\)\nor‚Ä¶\n\\(mean(female) - mean(male) &lt; 0\\)\nboth of which would be ‚Äúone-sided‚Äù tests.]\nLastly, we visualize the approximate permutation distribution and superimpose our observed difference in weights between males and females.\n\nnperm &lt;- 10000  # number of permutation simulations\n# create a dummy vector to hold results for each permutation\npermuted_diff &lt;- vector(length = n)\ntest_data &lt;- d\nfor (i in 1:nperm) {\n    # scramble the sex vector `sample()` with a vector as an argument yields a\n    # random permutation of the vector\n    test_data$sex &lt;- sample(test_data$sex)\n    x &lt;- test_data[test_data$sex == \"female\", ]$weight\n    y &lt;- test_data[test_data$sex == \"male\", ]$weight\n    permuted_diff[[i]] &lt;- mean(x) - mean(y)\n}\n\n(p &lt;- (sum(permuted_diff &gt;= abs(actual_diff)) + sum(permuted_diff &lt;= -abs(actual_diff)))/nperm)\n\n## [1] 0\n\nhistogram(permuted_diff, type = \"count\", xlab = \"\", ylab = \"# Permutations\", main = \"Histogram of Permutation Distribution\",\n    xlim = c(-1.75, 1.75))\n\n\n\n\n\n\n\nladd(panel.abline(v = actual_diff, lty = 3, lwd = 2))\n\n\n\n\n\n\n\nladd(panel.text(x = actual_diff, y = nperm * 0.08, \"Test Statistic\", srt = 90, pos = 4,\n    offset = 1))\n\n\n\n\n\n\n\n\nThe same perm.test() function from {jmuOutlier} that we used above for a 1-sample permutation test also allows for 2-sample tests.\n\n# library(jmuOutlier) x &lt;- d[d$sex=='female',]$weight y &lt;-\n# d[d$sex=='male',]$weight mu &lt;- 0 # expected difference between means under\n# null perm.test(x, y, alternative = 'two.sided', mu = mu, plot = TRUE, num.sim\n# = nperm) abline(v=actual_diff) perm.test(x, y, alternative = 'two.sided', mu\n# = mu, plot = FALSE, num.sim = nperm) detach(package:jmuOutlier)\n\n\nNOTE: In this example, the vertical line showing our actual difference in means is off-scale on the left-hand side of the plot!\n\nThe function independence_test() from the {coin} package conducts the same kind of 2-sample test with a simpler construction that does not require us specifying two vectors to compare‚Ä¶\n\nlibrary(coin)\n\n## Loading required package: survival\n\nindependence_test(weight ~ as.factor(sex), alternative = \"two.sided\", distribution = \"approximate\",\n    data = d)\n\n## \n##  Approximative General Independence Test\n## \n## data:  weight by as.factor(sex) (female, male)\n## Z = -5.4998, p-value &lt; 1e-04\n## alternative hypothesis: two.sided\n\ndetach(package:coin)\n\nFinally, we can also write our own generic 2-sample permutation function that allows us to compare custom test statistics. Here, we first define a function for our test statistic, mean_diff(), i.e., a difference of means, and then define a function for the test.\n\nperm2samp &lt;- function(x, y, stat = \"mean_diff\", nperm = 10000, alternative = c(\"two.sided\",\n    \"less\", \"greater\")) {\n    # can modify function by adding alternative `stat=`\n    if (stat == \"mean_diff\") {\n        myfun &lt;- function(x, y) {\n            mean(x) - mean(y)\n        }\n    }\n    test_stat &lt;- myfun(x, y)\n    test_data &lt;- c(x, y)  # puts two vectors of samples together\n    perm_stat &lt;- vector(length = nperm)\n    for (i in 1:nperm) {\n        indexes &lt;- sample(length(test_data), length(x), replace = FALSE)\n        # the row above generates a randomized vector of indices that is the\n        # length of the x sample... this is equivalent to permuting which group\n        # the x sample belongs to\n        s1 &lt;- test_data[indexes]\n        s2 &lt;- test_data[-indexes]\n        perm_stat[[i]] &lt;- myfun(s1, s2)\n    }\n    if (alternative[1] == \"less\") {\n        p_perm &lt;- sum(perm_stat &lt;= test_stat)/nperm\n    } else if (alternative[1] == \"greater\") {\n        p_perm &lt;- sum(perm_stat &gt;= test_stat)/nperm\n    } else {\n        p_perm &lt;- sum(abs(perm_stat) &gt;= abs(test_stat))/nperm\n    }\n    list(test_stat = test_stat, perm_stat = perm_stat, p_perm = p_perm, type = alternative[1])\n}\n\nx &lt;- d[d$sex == \"female\", ]$weight\ny &lt;- d[d$sex == \"male\", ]$weight\n\noutput &lt;- perm2samp(x, y, stat = \"mean_diff\", nperm = 10000)\n\nhistogram(output$perm_stat, type = \"count\", ylab = \"# Permutations\", main = \"Histogram of Permutation Distribution\",\n    xlab = ifelse(output$type == \"less\" | output$type == \"greater\", paste0(\"One-Sided Test\\nProportion of distribution \",\n        output$type, \" than test stat = \", output$p_perm), paste0(\"Two-Sided Test\\nProportion of distribution more extreme than test stat = \",\n        output$p_perm)), xlim = c(-1.75, 1.75))\nladd(panel.abline(v = output$test_stat, lty = 3, lwd = 2))\nif (output$type == \"two.sided\") {\n    ladd(panel.abline(v = -output$test_stat, lty = 3, lwd = 2, col = \"red\"))\n}\nladd(panel.text(x = output$test_stat, y = nperm * 0.08, \"Test Statistic\", srt = 90,\n    pos = 4, offset = 1))\n\n\n\n\n\n\n\n\nHere‚Äôs a graphical version of what we are doing‚Ä¶",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Using Permutation Tests</span>"
    ]
  },
  {
    "objectID": "16-module.html#working-with-proportions",
    "href": "16-module.html#working-with-proportions",
    "title": "16¬† Using Permutation Tests",
    "section": "16.5 Working with Proportions",
    "text": "16.5 Working with Proportions\nWe can take the same permutation/randomization approach and use the same 1- and 2-sample tests with proportion data.\nIn Module 15, we looked at a set of data on captures of birds in 30 nets on a morning of mist netting (i.e., a ‚Äúsuccess‚Äù or ‚Äúfailure‚Äù for every net during a netting session) and tested whether the proportion of ‚Äúsuccesses‚Äù was lower than the expectation of 80% was lower than in previous years. The code below lets us do this using a 1-sample randomization test.\n\nv &lt;- c(0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n    0, 1, 0, 1, 1)\nnperm &lt;- 10000\n\n# using {jmuOutlier} library(jmuOutlier) perm.test(v, alternative='less',\n# mu=0.8, plot=FALSE, num.sim = nperm) detach(package:jmuOutlier)\n\n# using our custom function\noutput &lt;- perm1samp(v, stat = \"mean\", mu = 0.8, alternative = \"less\", nperm = nperm)\n\nhistogram(output$perm_stat, type = \"count\", ylab = \"# Permutations\", main = \"Histogram of Permutation Distribution\",\n    xlab = ifelse(output$type == \"less\" | output$type == \"greater\", paste0(\"One-Sided Test\\nProportion of distribution \",\n        output$type, \" than test stat = \", output$p_perm), paste0(\"Two-Sided Test\\nProportion of distribution more extreme than test stat = \",\n        output$p_perm)))\nladd(panel.abline(v = output$test_stat, lty = 3, lwd = 2))\nif (output$type == \"two.sided\") {\n    ladd(panel.abline(v = -output$test_stat, lty = 3, lwd = 2, col = \"red\"))\n}\nladd(panel.text(x = output$test_stat, y = nperm * 0.08, \"Test Statistic\", srt = 90,\n    pos = 4, offset = 1))\n\n\n\n\n\n\n\n\nIn Module 15, we also compared data on captures of two species of bats over the course of a week of nightly mist-netting. For each species, the record indicates whether a captured female was lactating (1) or not (0), and we wanted to test whether this proportion differed between species. We can do this with a 2-sample randomization test.\n\nspecies1 &lt;- c(1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,\n    1, 0)\nspecies2 &lt;- c(1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,\n    0, 1, 1, 0, 1, 1, 1)\n(actual_diff &lt;- mean(species1) - mean(species2))\n\n## [1] -0.14\n\n# this calculates a difference in proportions\nnperm &lt;- 10000\n\n# using {jmuOutlier} library(jmuOutlier) perm.test(species1, species2,\n# alternative = 'two.sided', plot=FALSE, num.sim = nperm)\n# detach(package:jmuOutlier)\n\n# using our custom function\noutput &lt;- perm2samp(species1, species2, stat = \"mean_diff\", nperm = nperm)\n\nhistogram(output$perm_stat, type = \"count\", ylab = \"# Permutations\", main = \"Histogram of Permutation Distribution\",\n    xlab = ifelse(output$type == \"less\" | output$type == \"greater\", paste0(\"One-Sided Test\\nProportion of distribution \",\n        output$type, \" than test stat = \", output$p_perm), paste0(\"Two-Sided Test\\nProportion of distribution more extreme than test stat = \",\n        output$p_perm)))\nladd(panel.abline(v = output$test_stat, lty = 3, lwd = 2))\nif (output$type == \"two.sided\") {\n    ladd(panel.abline(v = -output$test_stat, lty = 3, lwd = 2, col = \"red\"))\n}\nladd(panel.text(x = output$test_stat, y = nperm * 0.08, \"Test Statistic\", srt = 90,\n    pos = 4, offset = 1))\n\n\n\n\n\n\n\n\n\nNOTE: The p values associated with these permutation/randomization based tests on proportions are not so close to those using the prop.test() function for the same data that we ran in Module 15. This is because prop.test() is calculating the p value associated with our test statistic presuming that the distribution of sampling proportions is roughly normal, whereas this test is calculating p values by counting the proportion of simulations!",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Using Permutation Tests</span>"
    ]
  },
  {
    "objectID": "16-module.html#using-the-infer-package",
    "href": "16-module.html#using-the-infer-package",
    "title": "16¬† Using Permutation Tests",
    "section": "16.6 Using the {infer} Package",
    "text": "16.6 Using the {infer} Package\nThe {infer} package offers a convenient set of functions and a standard workflow for using permutation methods for hypothesis testing, whether we are dealing with means, differences between means, proportions, or differences in proportions.\n\n\n\n\n\n\n\n\n\nThe {infer} package includes a set of functions that allow us to easily generate shuffled or permuted datasets and then conduct hypothesis testing with these datasets just as we have done above. We will explore how we do this with the {infer} package using the black-and-white colobus weight data we used above.\nWe first read in our data and load the {infer} package:\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/colobus-weights.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\nlibrary(infer)\n\nThen, we use the function specify() to indicate the variables we are interested in. The specify() function takes an argument of formula= response~explanatory. If we are working with proportion data, we also need to provide an argument for what value of the explanatory variable counts as a success, using success=, but that is not the case for our colobus weight data, where our response variable is numeric, not binary.\n\nd &lt;- d |&gt;\n    specify(formula = weight ~ sex)\n# this does not change the data frame but adds some meta data\nhead(d)\n\n## Response: weight (numeric)\n## Explanatory: sex (factor)\n## # A tibble: 6 √ó 2\n##   weight sex  \n##    &lt;dbl&gt; &lt;fct&gt;\n## 1   7.24 male \n## 2   6.09 male \n## 3   6.97 male \n## 4   6.98 male \n## 5   6.08 male \n## 6   6.22 male\n\n\nThe function hypothesize() is then used to declare the null hypothesis we wish to test. We use the argument null=\"point\" if we are doing a test of a single mean (mu=) or proportion (p=) or null=\"independence\" if we are doing a test of the independence of two variables (i.e., a two-sample test).\n\nd &lt;- d |&gt;\n    hypothesize(null = \"independence\")\n# again, this does not change the data frame but adds some meta data\nhead(d)\n\n## Response: weight (numeric)\n## Explanatory: sex (factor)\n## Null Hypothesis: independence\n## # A tibble: 6 √ó 2\n##   weight sex  \n##    &lt;dbl&gt; &lt;fct&gt;\n## 1   7.24 male \n## 2   6.09 male \n## 3   6.97 male \n## 4   6.98 male \n## 5   6.08 male \n## 6   6.22 male\n\n\nWe then use the generate() function to generate replicates of ‚Äúshuffled‚Äù or ‚Äúpermuted‚Äù data under the assumption that the null hypothesis is true. That is, we generating datasets from which we then derived a null distribution for our statistic of interest.\n\nd_permutations &lt;- d |&gt;\n    generate(reps = 1000, type = \"permute\")\nnrow(d)\n\n## [1] 40\n\nnrow(d_permutations)\n\n## [1] 40000\n\n\nNext, we use calculate() to calculate summary statistics of interest for each replicate. We can calculate a variety of different statistics, including the mean (‚Äúmean‚Äù), proportion (‚Äúprop‚Äù), difference in means (‚Äúdiff in means‚Äù), difference in proportions (‚Äúdiff in props‚Äù), and others. This step generates a null distribution for our summary statistic. For differences in means or proportions, we need to also include an argument specifying the order of the levels of the explanatory variable for subtraction.\n\nnull_distribution &lt;- d_permutations |&gt;\n    calculate(stat = \"diff in means\", order = c(\"male\", \"female\"))\n# to subtract female weight from male weight\n\nWe can then use the function visualize() to examine the null distribution (alternatively, we can use histogram(), in which case we then need to specify the variable name=\"stat\")\n\nvisualize(null_distribution, bins = 20)\n\n\n\n\n\n\n\nhistogram(null_distribution$stat, nint = 20)\n\n\n\n\n\n\n\n\nSo, now we have a null or permutation distribution for the test statistic‚Ä¶ then, we can also use {infer} package functions to easily calculate hte observed summary statistic that we want to compare to this null distribution by simply leaving out the hypothesize() and generate() steps.\n\nobserved_stat &lt;- read_csv(f, col_names = TRUE) |&gt;\n    specify(weight ~ sex) |&gt;\n    calculate(stat = \"diff in means\", order = c(\"male\", \"female\"))\n\n## Rows: 40 Columns: 3\n## ‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n## Delimiter: \",\"\n## chr (1): sex\n## dbl (2): id, weight\n## \n## ‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n## ‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe can plot this observed test statistic atop the null distribution using the shade_p_value() function, which has two arguments: a value for obs_stat= and a direction= (‚Äúleft,‚Äùright‚Äù, or ‚Äúboth‚Äù) corresponding to two different one-tailed or a two-tailed alternative hypothesis.\n\nvisualize(null_distribution, bins = 20) + shade_p_value(obs_stat = observed_stat,\n    direction = \"both\")\n\n\n\n\n\n\n\n\n\nNOTE: This is not very interesting because the observed statistic is way outside of the null distribution!\n\nThe function get_p_value() with arguments of the null distribution, the observed statistic, and the direction will give us the p value generated by permutation.\n\nget_p_value(null_distribution, observed_stat, direction = \"both\")\n\n## Warning: Please be cautious in reporting a p-value of 0. This result is an approximation\n## based on the number of `reps` chosen in the `generate()` step.\n## ‚Ñπ See `get_p_value()` (`?infer::get_p_value()`) for more information.\n\n\n## # A tibble: 1 √ó 1\n##   p_value\n##     &lt;dbl&gt;\n## 1       0\n\n\nAs an additional example of this approach, let‚Äôs consider the vervet trapping data from above once again. Recall that we want to evaluate whether vervet monkeys trapped during the 2015 trapping season differed in weight from the expectation based on previous years, which was 5.0 kg. How do we do this using the {infer} workflow?\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/vervet-weights.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\nnull_distribution &lt;- d |&gt;\n    specify(response = weight) |&gt;\n    # here we specify just a response, not a formula\nhypothesize(null = \"point\", mu = 5) |&gt;\n    # here we are comparing a sample to an expectation\ngenerate(reps = 1000, type = \"bootstrap\") |&gt;\n    calculate(stat = \"mean\")\n# Note that here type='bootstrap'! This is actually doing something a little\n# different than we did above... it is sampling *with replacement* from our set\n# of weights rather than randomly permuting the sign of the difference of each\n# value from the mean! (Here, the number of observations sampled is equal to\n# the original number of observations)\n\nobserved_stat &lt;- d |&gt;\n    specify(response = weight) |&gt;\n    calculate(stat = \"mean\")\n\nvisualize(null_distribution, bins = 20) + shade_p_value(obs_stat = observed_stat,\n    direction = \"both\")\n\n\n\n\n\n\n\nget_p_value(null_distribution, observed_stat, direction = \"both\")\n\n## # A tibble: 1 √ó 1\n##   p_value\n##     &lt;dbl&gt;\n## 1   0.022\n\n\nNote that this p value is pretty similar to that returned by jmuOutlier::perm.test() and by our custom perm1samp() function, though as noted above, those use slightly different permutation processes.\n\nCHALLENGE\nAs a final example of this approach, we will again examine data on captures of two species of bats over the course of a week of nightly mist-netting. For each species, remember that the record indicates whether a captured female was lactating (1) or not (0).\n\nspecies1 &lt;- c(1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,\n    1, 0)\nspecies2 &lt;- c(1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,\n    0, 1, 1, 0, 1, 1, 1)\n\nFirst, we need to massage this data into the correct shape:\n\ns1 &lt;- tibble(species = \"species1\", lactating = species1)\ns2 &lt;- tibble(species = \"species2\", lactating = species2)\nd &lt;- bind_rows(s1, s2)\nd$lactating &lt;- factor(d$lactating)\n\nNow, use the {infer} workflow to test whether the proportion of lactating females among the captured bats differs between species.\n\n\nShow Code\nnull_distribution &lt;- d |&gt;\n    specify(lactating ~ species, success = \"1\") |&gt;\n    hypothesize(null = \"independence\") |&gt;\n    generate(reps = 1000, type = \"permute\") |&gt;\n    calculate(stat = \"diff in props\", order = c(\"species1\", \"species2\"))\n\n# NOTE: This runs a LOT slower than our custom permutation function for the\n# same number of reps!\nvisualize(null_distribution, bins = 20)\n\n\n\n\n\n\n\n\n\nShow Code\nobserved_stat &lt;- d |&gt;\n    specify(lactating ~ species, success = \"1\") |&gt;\n    calculate(stat = \"diff in props\", order = c(\"species1\", \"species2\"))\n\nvisualize(null_distribution, bins = 10) + shade_p_value(observed_stat, direction = \"both\")\n\n\n\n\n\n\n\n\n\nShow Code\nget_p_value(null_distribution, observed_stat, direction = \"both\")\n\n\nShow Output\n## # A tibble: 1 √ó 1\n##   p_value\n##     &lt;dbl&gt;\n## 1   0.416\n\n\n\nShow Code\ndetach(package:infer)\n\n\nThis p value is similar to that returned by our custom permutation test!",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Using Permutation Tests</span>"
    ]
  },
  {
    "objectID": "16-module.html#advantages-of-permutation-tests",
    "href": "16-module.html#advantages-of-permutation-tests",
    "title": "16¬† Using Permutation Tests",
    "section": "16.7 Advantages of Permutation Tests",
    "text": "16.7 Advantages of Permutation Tests\nSome of the key advantages of permutation tests include the following:\n\nThey are ‚Äúdistribution-free‚Äù - i.e., they do not make any assumptions about the distribution of the underlying data\nThere is no presumption of random sampling from some imaginary hypothetical population\nUnlike some other nonparametric tests, they do not depend on large sample sizes for their validity\nMany common nonparametric tests are permutation tests, just carried out on RANKS\nThey can be used with many kinds of data (nominal, ordinal, interval/ratio)\nThey are relatively straightforward to conduct and interpret",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Using Permutation Tests</span>"
    ]
  },
  {
    "objectID": "16-module.html#concept-review",
    "href": "16-module.html#concept-review",
    "title": "16¬† Using Permutation Tests",
    "section": "Concept Review",
    "text": "Concept Review\n\nPermutation/randomization tests involve these steps:\n\nCompute a sample statistic of your choice (mean, median, proportion, etc.) using the set of original observations\nRearrange attributes of the original observations in all or a very large number of random possible permutations, computing the test statistic each time, to yield a permutation or null distribution for the test statistic under the null hypothesis\nLook at the value of the sample statistic relative to the permutation distribution\nBased on the permutation distribution, calculate the probability of seeing a sample statistic as high or higher than that observed, i.e., the portion of the permutation distribution that equals or exceed the value of the sample statistic‚Ä¶ this is the permutation test p value\n\njmuOutlier::perm.test(), coin::indepedence_test(), or write your own!\n{infer} workflow\n\n\nNOTE: This permutation-based approach to statistical inference is consistent with the philosophy promoted by statistician Allen Downey that, ‚ÄúThere is really only test‚Äù, and that at all statistical tests are based on the same framework. Note that this is still a form of null hypothesis significance testing, or NHST!",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Using Permutation Tests</span>"
    ]
  },
  {
    "objectID": "17-module.html",
    "href": "17-module.html",
    "title": "17¬† Error, Power, and Effect Size",
    "section": "",
    "text": "17.1 Objectives",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Error, Power, and Effect Size</span>"
    ]
  },
  {
    "objectID": "17-module.html#objectives",
    "href": "17-module.html#objectives",
    "title": "17¬† Error, Power, and Effect Size",
    "section": "",
    "text": "The objective of this module is to discuss the concepts of Type I and Type II error, the multiple testing problem, statistical power, and effect size and outline how we can use R to investigate these via simulation and built-in functions.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Error, Power, and Effect Size</span>"
    ]
  },
  {
    "objectID": "17-module.html#preliminaries",
    "href": "17-module.html#preliminaries",
    "title": "17¬† Error, Power, and Effect Size",
    "section": "17.2 Preliminaries",
    "text": "17.2 Preliminaries\n\nLoad {tidyverse} and {manipulate}",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Error, Power, and Effect Size</span>"
    ]
  },
  {
    "objectID": "17-module.html#overview",
    "href": "17-module.html#overview",
    "title": "17¬† Error, Power, and Effect Size",
    "section": "17.3 Overview",
    "text": "17.3 Overview\nLet‚Äôs return to the concepts of error and power. Recall that Type I error occurs when you incorrectly reject a true \\(H_0\\). In any given hypothesis test, the probability of a Type I error is equivalent to the significance level, \\(\\alpha\\), and it is this type of error we are often trying to minimize when we are doing classical statistical inference. Type II error occurs when you incorrectly fail to reject a false \\(H_0\\) (in other words, fail to find evidence in support of a true \\(H_A\\)). Since we do not know what the true \\(H_A\\) actually is, the probability of committing such an error, labeled \\(\\beta\\), is not usually known in practice.\n\n\n\n\n\n\n\n\nWhat is True\nWhat We Decide\nResult\n\n\n\n\n\\(H_0\\)\n\\(H_0\\)\nCorrectly ‚Äòaccept‚Äô the null\n\n\n\\(H_0\\)\n\\(H_A\\)\nFalsely reject the null (Type I error)\n\n\n\\(H_A\\)\n\\(H_0\\)\nFalsely ‚Äòaccept‚Äô the null (Type II error)\n\n\n\\(H_A\\)\n\\(H_A\\)\nCorrectly reject the null",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Error, Power, and Effect Size</span>"
    ]
  },
  {
    "objectID": "17-module.html#type-i-error-and-multiple-testing",
    "href": "17-module.html#type-i-error-and-multiple-testing",
    "title": "17¬† Error, Power, and Effect Size",
    "section": "17.4 Type I Error and Multiple Testing",
    "text": "17.4 Type I Error and Multiple Testing\nBecause of how we define \\(\\alpha\\), the chance probability of falsely rejecting \\(H_0\\) when \\(H_0\\) is actually true, we would expect to find some ‚Äúsignificant‚Äù results if we run enough independent hypothesis tests. For example, if we set \\(\\alpha\\) at 0.05, we would expect to find one ‚Äúsignificant‚Äù result in roughly every 20 tests we run, just by chance. The relation of \\(\\alpha\\) to the distribution of a variable under a null hypothesis (\\(\\mu\\) = \\(\\mu_0\\)) versus an alternative hypothesis (e.g., \\(\\mu\\) &gt; \\(\\mu_0\\)) is shown in the figure below (this example is for an upper one-tailed test). It should be clear that we can reduce the chance of Type I error by decreasing \\(\\alpha\\) (shifting the critical value to the right in the \\(H_0\\) distribution). Type I error will also be reduced as the means get further apart or as the standard deviation of the distributions shrinks.\n\n\n\n\n\n\n\n\n\nLet‚Äôs explore this via simulation.\nWe will write some code to simulate a bunch of random datasets from a normal distribution where we set the expected population mean (\\(\\mu_0\\)) and standard deviation (\\(\\sigma\\)) and then calculate a \\(Z\\) (or \\(T\\)) statistic and p value for each one. We will then look at the ‚ÄúType I‚Äù error rate‚Ä¶ the proportion of times that, based on our sample, we would conclude that it was not drawn from the distribution we know to be true.\nFirst, let‚Äôs set up a skeleton function we will call typeI() to evaluate the Type I error rate. It should take, as arguments, the parameters of the normal distribution for the null hypothesis we want to simulate from (\\(\\mu_0\\) and \\(\\sigma\\)), our sample size, our \\(\\alpha\\) level, what ‚Äúalternative‚Äù type of \\(Z\\) (or \\(T\\)) test we want to do (‚Äúgreater‚Äù, ‚Äúless‚Äù, or ‚Äútwo.tailed‚Äù), and the number of simulated datasets we want to generate. Copy and paste in the code below (and note that we set default values for \\(\\alpha\\) and the number of simulations). Note that we can use the ‚Äút‚Äù family of functions instead of the ‚Äúnorm‚Äù family.\n\n# function skeleton\ntypeI &lt;- function(mu0, sigma, n, alternative = \"two.tailed\", alpha = 0.05, k = 10000) {\n}\n\nNow, we will add the body of the function.\n\ntypeI &lt;- function(mu0, sigma, n, alternative = \"two.tailed\", alpha = 0.05, k = 10000) {\n    p &lt;- rep(NA, k)\n    # set a vector of k empty p values, one for each simulation\n    for (i in 1:k) {\n        # sets up a loop to run k simulations\n        x &lt;- rnorm(n = n, mean = mu0, sd = sigma)\n        # draws a sample of size n from our distribution\n        m &lt;- mean(x)  # calculates the mean\n        s &lt;- sd(x)  # calculates the standard deviation\n        z &lt;- (m - mu0)/(s/sqrt(n))\n        # calculates the Z statistic for the sample drawn from the null\n        # distribution relative to the null distribution; alternatively use t\n        # &lt;- (m-mu0)/(s/sqrt(n))\n        if (alternative == \"less\") {\n            p[[i]] &lt;- pnorm(z, lower.tail = TRUE)\n            # calculates the associated p value; alternatively, use p[[i]] &lt;-\n            # pt(t, df = n-1, lower.tail = TRUE)\n        }\n        if (alternative == \"greater\") {\n            p[[i]] &lt;- pnorm(z, lower.tail = FALSE)\n            # calculates the associated p value; alternatively, use p[[i]] &lt;-\n            # pt(t, df = n-1, lower.tail=FALSE)\n        }\n        if (alternative == \"two.tailed\") {\n            if (z &gt; 0) {\n                p[[i]] &lt;- 2 * pnorm(z, lower.tail = FALSE)\n            }\n            # alternatively, use if (t &gt; 0) {p[[i]] &lt;- pt(t, df = n-1,\n            # lower.tail = FALSE)}\n            if (z &lt; 0) {\n                p[[i]] &lt;- 2 * pnorm(z, lower.tail = TRUE)\n            }\n            # alternatively, use if (t &lt; 0) {p[[i]] &lt;- pt(t, df = n-1,\n            # lower.tail = TRUE)}\n        }\n    }\n\n    curve(dnorm(x, mu0, sigma/sqrt(n)), mu0 - 4 * sigma/sqrt(n), mu0 + 4 * sigma/sqrt(n),\n        main = paste(\"Sampling Distribution Under Null Hypothesis\\n\n      Type I error rate from simulation = \",\n            length(p[p &lt; alpha])/k, sep = \"\"), xlab = \"x\", ylab = \"Pr(x)\", col = \"red\",\n        xlim = c(mu0 - 4 * sigma/sqrt(n), mu0 + 4 * sigma/sqrt(n)), ylim = c(0, dnorm(mu0,\n            mu0, sigma/sqrt(n))))\n    abline(h = 0)\n\n    if (alternative == \"less\") {\n        polygon(cbind(c(mu0 - 4 * sigma/sqrt(n), seq(from = mu0 - 4 * sigma/sqrt(n),\n            to = mu0 - qnorm(1 - alpha) * sigma/sqrt(n), length.out = 100), mu0 -\n            qnorm(1 - alpha) * sigma/sqrt(n))), c(0, dnorm(seq(from = mu0 - 4 * sigma/sqrt(n),\n            to = mu0 - qnorm(1 - alpha) * sigma/sqrt(n), length.out = 100), mean = mu0,\n            sd = sigma/sqrt(n)), 0), border = \"black\", col = \"grey\")\n        q &lt;- pnorm(mu0 - qnorm(1 - alpha) * sigma/sqrt(n), mean = mu0, sd = sigma/sqrt(n)) -\n            pnorm(mu0 - 4 * sigma/sqrt(n), mean = mu0, sd = sigma/sqrt(n))\n    }\n\n    if (alternative == \"greater\") {\n        polygon(cbind(c(mu0 + qnorm(1 - alpha) * sigma/sqrt(n), seq(from = mu0 +\n            qnorm(1 - alpha) * sigma/sqrt(n), to = mu0 + 4 * sigma/sqrt(n), length.out = 100),\n            mu0 + 4 * sigma/sqrt(n))), c(0, dnorm(seq(from = mu0 + qnorm(1 - alpha) *\n            sigma/sqrt(n), to = mu0 + 4 * sigma/sqrt(n), length.out = 100), mean = mu0,\n            sd = sigma/sqrt(n)), 0), border = \"black\", col = \"grey\")\n        q &lt;- pnorm(mu0 + 4 * sigma/sqrt(n), mean = mu0, sd = sigma/sqrt(n)) - pnorm(mu0 +\n            qnorm(1 - alpha) * sigma/sqrt(n), mean = mu0, sd = sigma/sqrt(n))\n    }\n    if (alternative == \"two.tailed\") {\n        polygon(cbind(c(mu0 - 4 * sigma/sqrt(n), seq(from = mu0 - 4 * sigma/sqrt(n),\n            to = mu0 - qnorm(1 - alpha/2) * sigma/sqrt(n), length.out = 100), mu0 -\n            qnorm(1 - alpha/2) * sigma/sqrt(n))), c(0, dnorm(seq(from = mu0 - 4 *\n            sigma/sqrt(n), to = mu0 - qnorm(1 - alpha/2) * sigma/sqrt(n), length.out = 100),\n            mean = mu0, sd = sigma/sqrt(n)), 0), border = \"black\", col = \"grey\")\n        polygon(cbind(c(mu0 + qnorm(1 - alpha/2) * sigma/sqrt(n), seq(from = mu0 +\n            qnorm(1 - alpha/2) * sigma/sqrt(n), to = mu0 + 4 * sigma/sqrt(n), length.out = 100),\n            mu0 + 4 * sigma/sqrt(n))), c(0, dnorm(seq(from = mu0 + qnorm(1 - alpha/2) *\n            sigma/sqrt(n), to = mu0 + 4 * sigma/sqrt(n), length.out = 100), mean = mu0,\n            sd = sigma/sqrt(n)), 0), border = \"black\", col = \"grey\")\n        q &lt;- pnorm(mu0 - qnorm(1 - alpha/2) * sigma/sqrt(n), mean = mu0, sd = sigma/sqrt(n)) -\n            pnorm(mu0 - 4 * sigma/sqrt(n), mean = mu0, sd = sigma/sqrt(n)) + pnorm(mu0 +\n            4 * sigma/sqrt(n), mean = mu0, sd = sigma/sqrt(n)) - pnorm(mu0 + qnorm(1 -\n            alpha/2) * sigma/sqrt(n), mean = mu0, sd = sigma/sqrt(n))\n    }\n    # print(round(q,digits=3)); this prints area in the shaded portion(s) of\n    # the curve\n    return(length(p[p &lt; alpha])/k)\n    # returns the proportion of simulations where p &lt; alpha; alternatively, use\n    # `return(mean(p&lt;alpha))`\n}\n\nHere, \\(\\mu_0\\) is our sample mean and \\(\\sigma\\) is our sample standard deviation.\nCan you explain what each step of this code is doing?\nNow, run our Type I error test function with a couple of different values of \\(\\mu_0\\), \\(\\sigma\\), and \\(\\alpha\\). What error rates are returned? They should be always be close to \\(\\alpha\\)!\n\neI &lt;- typeI(mu0 = -3, sigma = 2, n = 1000, alternative = \"greater\", alpha = 0.05)\n\n\n\n\n\n\n\neI &lt;- typeI(mu0 = 5, sigma = 2, n = 1000, alternative = \"less\", alpha = 0.01)\n\n\n\n\n\n\n\neI &lt;- typeI(mu0 = 10, sigma = 4, n = 1000, alternative = \"two.tailed\", alpha = 0.05)\n\n\n\n\n\n\n\n\n\nCHALLENGE\nHow does the Type I error rate change with \\(n\\)? With \\(\\sigma\\)? With \\(\\alpha\\)?\n\nHINT: It shouldn‚Äôt change much‚Ä¶ the Type I error rate is defined by \\(\\alpha\\)!",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Error, Power, and Effect Size</span>"
    ]
  },
  {
    "objectID": "17-module.html#multiple-comparison-corrections",
    "href": "17-module.html#multiple-comparison-corrections",
    "title": "17¬† Error, Power, and Effect Size",
    "section": "17.5 Multiple Comparison Corrections",
    "text": "17.5 Multiple Comparison Corrections\nOne way we can address the multiple testing problem mentioned above is by using what is called the Bonferroni correction, which suggests that when doing a total of \\(k\\) independent hypothesis tests, each with a significance level of \\(\\alpha\\), we should adjust the \\(\\alpha\\) level we use to interpret statistical significance as follow: \\(\\alpha_B = \\alpha/k\\). For example, if we run 10 independent hypothesis tests, then we should set our adjusted \\(\\alpha\\) level for each test as 0.05/10 = 0.005.\nWith the Bonferroni correction, we are essentially saying that we want to control the rate at which we have even one incorrect rejection of \\(H_0\\) given the entire family of tests we do. This is also referred to as limiting the ‚Äúfamily-wise error rate‚Äù to level \\(\\alpha\\).\n\n# example Bonferroni correction\nalpha &lt;- 0.05\npvals &lt;- c(1e-04, 0.003, 0.005, 0.01, 0.02, 0.04, 0.045, 0.11, 0.18, 0.23)\n# vector of p values associated with a set of tests\nsig &lt;- pvals &lt;= alpha/length(pvals)\n# returns a boolean vector of pvals less than or equal to the adjusted alpha\nsig  # first 3 values are less than the adjusted alpha\n\n##  [1]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nMany statisticians consider the Bonferroni correction to be an overly conservative one, and there are other corrections we might use to account for multiple testing.\nOne common alternative is the Benjamini & Hochberg correction, which is less conservative. It attempts to control for the ‚Äúfalse discovery rate‚Äù, which is different than the ‚Äúfamily-wise error rate‚Äù. Here, we aim to limit the number of false ‚Äúdiscoveries‚Äù (i.e., incorrect rejections of the null hypothesis) out of a set of discoveries (i.e., out of the set of results where we would reject the null hypothesis) to \\(\\alpha\\).\n\nCalculate \\(p\\) values for all tests\nOrder \\(p\\) values from smallest to largest (from \\(p_1\\) to \\(p_m\\))\nCall any \\(p\\) value where \\(p_i ‚â§ \\alpha \\times i/m\\) significant\n\n\nalpha &lt;- 0.05\npsig &lt;- NULL\npvals &lt;- c(1e-04, 0.003, 0.005, 0.01, 0.02, 0.04, 0.045, 0.11, 0.18, 0.27)\nfor (i in 1:length(pvals)) {\n    psig[i] &lt;- alpha * i/length(pvals)\n}\nd &lt;- tibble(rank = c(1:10), pvals = pvals, psig = psig)\np &lt;- ggplot(data = d, aes(x = rank, y = pvals)) + geom_point() + geom_line(aes(x = rank,\n    y = psig))\np\n\n\n\n\n\n\n\nsig &lt;- pvals &lt;= psig  # vector of significant pvalues\nsig  # first 5 values are less than the adjusted alpha, the remaining are not\n\n##  [1]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n\n\nAn alternative way of thinking about this is to adjust p values themselves rather than the \\(\\alpha\\) levels. We can do this with a built-in R function, p.adjust(), which makes the calculation easy. Using this function, we can specify the kind of correction method we want to apply.\n\nsig &lt;- p.adjust(pvals, method = \"bonferroni\") &lt;= 0.05\nsig  # first 3 adjusted p values are less alpha\n\n##  [1]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\nsig &lt;- p.adjust(pvals, method = \"BH\") &lt;= 0.05\nsig  # first 5 adjusted p values are less alpha\n\n##  [1]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Error, Power, and Effect Size</span>"
    ]
  },
  {
    "objectID": "17-module.html#type-ii-error",
    "href": "17-module.html#type-ii-error",
    "title": "17¬† Error, Power, and Effect Size",
    "section": "17.6 Type II Error",
    "text": "17.6 Type II Error\nBy reducing the \\(\\alpha\\) level we use as our criterion for statistical significance, we can reduce the chance of committing a Type I error (i.e., of incorrectly rejecting a true null hypothesis)‚Ä¶ but doing so directly increases our chance of committing a Type II error (i.e., of incorrectly failing to reject a false null). The shaded area in the figure below, \\(\\beta\\), is the probability of incorrectly failing to reject the null‚Ä¶\n\n\n\n\n\n\n\n\n\nIt should be clear from this figure that if the critical value (which, again, is defined by \\(\\alpha\\)) is shifted to the right (i.e., if we reduce \\(\\alpha\\)), or if \\(\\mu\\) under the alternative hypothesis shifts left, then \\(\\beta\\), the area under the alternative hypothesis distribution curve to the left of the critical value, increases! Intuitively, this makes sense: the lower the difference between the true \\(\\mu_A\\) value and \\(\\mu_0\\) and/or the higher the \\(\\alpha\\) level, the harder it will be to reject the null hypothesis that \\(\\mu\\) = \\(\\mu_0\\).\nIn practice, we cannot usually calculate \\(\\beta\\) because of the need to know where the true distribution is really centered (i.e., we need to know the value of \\(\\mu_A\\), which is often unknown). However, we can explore via simulation what \\(\\beta\\) is expected to look like under different alternative hypotheses (e.g., under different \\(\\mu_A\\)) and under different sample sizes and \\(\\alpha\\) levels.\nLet‚Äôs do this using the simulation approach we developed above. Again, we will write some code to simulate a bunch of random datasets, this time drawn from a normal distribution associated with a particular alternative hypothesis, \\(H_A\\), that we define‚Ä¶ i.e., where we specify \\(\\mu_A\\) and \\(\\sigma\\), i.e., a mean and a standard deviation. We then calculate a \\(Z\\) (or \\(T\\)) statistic based on each sample dataset relative to \\(\\mu_0\\), the expected mean under \\(H_0\\), and determine the associated p value for each one. Based on this, we can calculate the Type II error rate‚Ä¶ the proportion of times that, based on our sample, we would conclude that it was drawn from the \\(H_0\\) distribution rather than the \\(H_A\\) distribution that we set to be true. Note that, as above, we can use the ‚Äút‚Äù family of functions in lieu of the ‚Äúnorm‚Äù family.\n\ntypeII &lt;- function(mu0, muA, sigma, n, alternative = \"two.tailed\", alpha = 0.05,\n    k = 10000) {\n    p &lt;- rep(NA, k)  # sets up a vector of empty p values\n    for (i in 1:k) {\n        x &lt;- rnorm(n = n, mean = muA, sd = sigma)\n        # draw a sample of size n from Ha\n        m &lt;- mean(x)\n        s &lt;- sd(x)\n        z &lt;- (m - mu0)/(s/sqrt(n))\n        # calculates the Z statistic for the sample drawn from Ha relative to\n        # the null distribution or... t &lt;- (m-mu0)/(s/sqrt(n)) # calculates the\n        # t statistic\n        if (alternative == \"greater\") {\n            p[[i]] &lt;- pnorm(z, lower.tail = FALSE)\n            # calculates the associated p value for the Z statistic or...\n            # p[[i]] &lt;- pt(t,lower.tail=FALSE,df=n-1) calculates the associated\n            # p value for the t statistic\n            hyp &lt;- \"muA &gt; mu0\"\n        }\n        if (alternative == \"less\") {\n            p[[i]] &lt;- pnorm(z, lower.tail = TRUE)\n            # or... p[[i]] &lt;- pt(t, lower.tail = TRUE, df = n - 1)\n            hyp &lt;- \"muA &lt; mu0\"\n        }\n        if (alternative == \"two.tailed\") {\n            if (z &gt; 0) {\n                p[[i]] &lt;- 2 * pnorm(z, lower.tail = FALSE)\n            }\n            if (z &lt; 0) {\n                p[[i]] &lt;- 2 * pnorm(z, lower.tail = TRUE)\n            }\n            # if (t &gt; 0) {p[[i]] &lt;- 2 * pt(t, lower.tail = FALSE, df = n-1)} if\n            # (t &lt; 0) {p[[i]] &lt;- 2 * pt(t, lower.tail = TRUE, df = n-1)}\n            hyp &lt;- \"muA ‚â† mu0\"\n        }\n    }\n\n    curve(dnorm(x, mu0, sigma/sqrt(n)), mu0 - 4 * sigma/sqrt(n), mu0 + 4 * sigma/sqrt(n),\n        main = paste(\"Sampling Distributions Under Null (red) and Alternative (blue) Hypotheses\\n\n      Type II error rate from simulation = \",\n            length(p[p &gt;= alpha])/k, sep = \"\"), xlab = \"x\", ylab = \"Pr(x)\", col = \"red\",\n        xlim = c(min(c(mu0 - 4 * sigma/sqrt(n), muA - 4 * sigma/sqrt(n))), max(c(mu0 +\n            4 * sigma/sqrt(n), muA + 4 * sigma/sqrt(n)))), ylim = c(0, max(c(dnorm(mu0,\n            mu0, sigma/sqrt(n))), dnorm(muA, muA, sigma/sqrt(n)))))\n\n    curve(dnorm(x, muA, sigma/sqrt(n)), muA - 4 * sigma/sqrt(n), muA + 4 * sigma/sqrt(n),\n        col = \"blue\", add = TRUE)\n\n    abline(h = 0)\n\n    if (alternative == \"less\") {\n        polygon(cbind(c(mu0 - qnorm(1 - alpha) * sigma/sqrt(n), seq(from = mu0 -\n            qnorm(1 - alpha) * sigma/sqrt(n), to = muA + 4 * sigma/sqrt(n), length.out = 100),\n            muA + 4 * sigma/sqrt(n))), c(0, dnorm(seq(from = mu0 - qnorm(1 - alpha) *\n            sigma/sqrt(n), to = muA + 4 * sigma/sqrt(n), length.out = 100), mean = muA,\n            sd = sigma/sqrt(n)), 0), border = \"black\", col = \"grey\")\n        abline(v = mu0 - qnorm(1 - alpha) * sigma/sqrt(n), col = \"black\", lty = 3,\n            lwd = 2)\n    }\n\n    if (alternative == \"greater\") {\n        polygon(cbind(c(muA - 4 * sigma/sqrt(n), seq(from = muA - 4 * sigma/sqrt(n),\n            to = mu0 + qnorm(1 - alpha) * sigma/sqrt(n), length.out = 100), mu0 +\n            qnorm(1 - alpha) * sigma/sqrt(n))), c(0, dnorm(seq(from = muA - 4 * sigma/sqrt(n),\n            to = mu0 + qnorm(1 - alpha) * sigma/sqrt(n), length.out = 100), mean = muA,\n            sd = sigma/sqrt(n)), 0), border = \"black\", col = \"grey\")\n        abline(v = mu0 + qnorm(1 - alpha) * sigma/sqrt(n), col = \"black\", lty = 3,\n            lwd = 2)\n    }\n\n    if (alternative == \"two.tailed\") {\n        abline(v = mu0 - qnorm(1 - alpha/2) * sigma/sqrt(n), col = \"black\", lty = 3,\n            lwd = 2)\n        abline(v = mu0 + qnorm(1 - alpha/2) * sigma/sqrt(n), col = \"black\", lty = 3,\n            lwd = 2)\n        if (z &gt; 0) {\n            # greater\n            polygon(cbind(c(muA - 4 * sigma/sqrt(n), seq(from = muA - 4 * sigma/sqrt(n),\n                to = mu0 + qnorm(1 - alpha/2) * sigma/sqrt(n), length.out = 100),\n                mu0 + qnorm(1 - alpha/2) * sigma/sqrt(n))), c(0, dnorm(seq(from = muA -\n                4 * sigma/sqrt(n), to = mu0 + qnorm(1 - alpha/2) * sigma/sqrt(n),\n                length.out = 100), mean = muA, sd = sigma/sqrt(n)), 0), border = \"black\",\n                col = \"grey\")\n        }\n\n        if (z &lt; 0) {\n            # less\n            polygon(cbind(c(mu0 - qnorm(1 - alpha/2) * sigma/sqrt(n), seq(from = mu0 -\n                qnorm(1 - alpha/2) * sigma/sqrt(n), to = muA + 4 * sigma/sqrt(n),\n                length.out = 100), muA + 4 * sigma/sqrt(n))), c(0, dnorm(seq(from = mu0 -\n                qnorm(1 - alpha/2) * sigma/sqrt(n), to = muA + 4 * sigma/sqrt(n),\n                length.out = 100), mean = muA, sd = sigma/sqrt(n)), 0), border = \"black\",\n                col = \"grey\")\n        }\n    }\n\n    return(length(p[p &gt;= alpha])/k)\n}\n\n\nCHALLENGE\nExplore this function using different values of \\(\\mu_0\\), \\(\\sigma\\), \\(n\\), and different types of one- and two-tailed tests.\n\n# Ha &lt; H0\neII &lt;- typeII(mu0 = 5, muA = 2, sigma = 4, n = 18, alternative = \"less\")\n\n\n\n\n\n\n\n# Ha ‚â† H0\neII &lt;- typeII(mu0 = 5, muA = 7, sigma = 2, n = 15, alternative = \"two.tailed\")\n\n\n\n\n\n\n\n# Ha &gt; H0\neII &lt;- typeII(mu0 = 0, muA = 0.5, sigma = 1, n = 30, alternative = \"greater\", alpha = 0.01)\n\n\n\n\n\n\n\n\n\nWhat happens if you increase \\(\\sigma\\) keeping other values constant?\nWhat happens if you reduce \\(n\\) or increase \\(n\\)?\nWhat happens if you increase the difference between \\(\\mu_A\\) and \\(\\mu_0\\)?",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Error, Power, and Effect Size</span>"
    ]
  },
  {
    "objectID": "17-module.html#power-and-effect-size",
    "href": "17-module.html#power-and-effect-size",
    "title": "17¬† Error, Power, and Effect Size",
    "section": "17.7 Power and Effect Size",
    "text": "17.7 Power and Effect Size\nPower is the probability of correctly rejecting a null hypothesis that is untrue. For a test that has a Type II error rate of \\(\\beta\\), the statistical power is defined, simply, as \\(1-\\beta\\). Power values of 0.8 or greater are conventionally considered to be ‚Äúhigh‚Äù. Power for any given test depends on the difference in means between groups or treatments, \\(\\alpha\\), \\(n\\), and \\(\\sigma\\).\nGenerally speaking, effect size is a quantitative measure of the strength of a phenomenon. Here, we are interested in comparing two sample means, and the most common way to describe the effect size is as a standardized difference between the means of the groups being compared. In this case, we divide the absolute values of the difference between the means by the standard deviation: \\(\\frac{\\vert(\\mu_0 - \\mu_A)\\vert}{\\sigma}\\). This results in a scaleless measure. Conventionally, effect sizes of 0.2 or less are considered to be low and of 0.8 or greater are considered to be high.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Error, Power, and Effect Size</span>"
    ]
  },
  {
    "objectID": "17-module.html#visualizing-power-and-effect-size",
    "href": "17-module.html#visualizing-power-and-effect-size",
    "title": "17¬† Error, Power, and Effect Size",
    "section": "17.8 Visualizing Power and Effect Size",
    "text": "17.8 Visualizing Power and Effect Size\nThe code below lets you explore power and effect size interactively. You can use the sliders to set \\(\\mu_0\\) and \\(\\mu_A\\) for a one-sample test (or, alternatively, think about these as \\(\\mu_1\\) and \\(\\mu_2\\) for a two-sample test), \\(\\sigma\\), \\(\\alpha\\), and \\(n\\), and you can choose whether you are testing a one-sided hypothesis of \\(\\mu_A\\) being ‚Äúgreater‚Äù or ‚Äúless‚Äù than \\(\\mu_0\\) or are testing the two-sided hypothesis that \\(\\mu_A\\) ‚â† \\(\\mu_0\\) (‚Äútwo.tailed‚Äù). The figure will output power and effect size. Note that \\(n\\) is the number of observations (or differences) being considered and \\(\\sigma\\) is the standard deviation in the observation (or difference values).\nFirst, we define a function that plots the relationship between these factors and power:\n\npower.plot &lt;- function(mu0, muA, n, sigma, alpha, alternative = \"two.tailed\") {\n    pow &lt;- 0\n    z &lt;- (muA - mu0)/(sigma/sqrt(n))\n    g &lt;- ggplot(data.frame(mu = c(min(mu0 - 4 * sigma/sqrt(n), muA - 4 * sigma/sqrt(n)),\n        max(mu0 + 4 * sigma/sqrt(n), muA + 4 * sigma/sqrt(n)))), aes(x = mu)) + ggtitle(paste0(\"Explore Power and Effect Size for Z Test\n      for a sample of size \",\n        n))\n\n    g &lt;- g + ylim(c(0, max(dnorm(mu0, mu0, sigma/sqrt(n)) + 0.1, dnorm(muA, muA,\n        sigma/sqrt(n)) + 0.1)))\n\n    g &lt;- g + stat_function(fun = dnorm, geom = \"line\", args = list(mean = mu0, sd = sigma/sqrt(n)),\n        size = 1, col = \"red\", show.legend = TRUE)\n    g &lt;- g + stat_function(fun = dnorm, geom = \"line\", args = list(mean = muA, sd = sigma/sqrt(n)),\n        size = 1, col = \"blue\", show.legend = TRUE)\n\n    if (alternative == \"greater\") {\n        if (z &gt; 0) {\n            xcrit = mu0 + qnorm(1 - alpha) * sigma/sqrt(n)\n            g &lt;- g + geom_segment(x = xcrit, y = 0, xend = xcrit, yend = max(dnorm(mu0,\n                mu0, sigma/sqrt(n)) + 0.025, dnorm(muA, muA, sigma/sqrt(n)) + 0.025),\n                size = 0.5, linetype = 3)\n            g &lt;- g + geom_polygon(data = data.frame(cbind(x = c(xcrit, seq(from = xcrit,\n                to = muA + 4 * sigma/sqrt(n), length.out = 1000), muA + 4 * sigma/sqrt(n)),\n                y = c(0, dnorm(seq(from = xcrit, to = muA + 4 * sigma/sqrt(n), length.out = 1000),\n                  mean = muA, sd = sigma/sqrt(n)), 0))), aes(x = x, y = y), fill = \"blue\",\n                alpha = 0.5)\n            pow &lt;- pnorm(muA + 4 * sigma/sqrt(n), muA, sigma/sqrt(n)) - pnorm(xcrit,\n                muA, sigma/sqrt(n))\n        }\n    }\n\n    if (alternative == \"less\") {\n        if (z &lt; 0) {\n            xcrit = mu0 - qnorm(1 - alpha) * sigma/sqrt(n)\n            g &lt;- g + geom_segment(x = xcrit, y = 0, xend = xcrit, yend = max(dnorm(mu0,\n                mu0, sigma/sqrt(n)) + 0.025, dnorm(muA, muA, sigma/sqrt(n)) + 0.025),\n                size = 0.5, linetype = 3)\n            g &lt;- g + geom_polygon(data = data.frame(cbind(x = c(muA - 4 * sigma/sqrt(n),\n                seq(from = muA - 4 * sigma/sqrt(n), to = xcrit, length.out = 1000),\n                xcrit), y = c(0, dnorm(seq(from = muA - 4 * sigma/sqrt(n), to = xcrit,\n                length.out = 1000), mean = muA, sd = sigma/sqrt(n)), 0))), aes(x = x,\n                y = y), fill = \"blue\", alpha = 0.5)\n            pow &lt;- pnorm(xcrit, muA, sigma/sqrt(n)) - pnorm(muA - 4 * sigma/sqrt(n),\n                muA, sigma/sqrt(n))\n        }\n    }\n\n    if (alternative == \"two.tailed\") {\n        if (z &gt; 0) {\n            xcrit = mu0 + qnorm(1 - alpha/2) * sigma/sqrt(n)\n            g &lt;- g + geom_segment(x = xcrit, y = 0, xend = xcrit, yend = max(dnorm(mu0,\n                mu0, sigma/sqrt(n)) + 0.025, dnorm(muA, muA, sigma/sqrt(n)) + 0.025),\n                size = 0.5, linetype = 3)\n            g &lt;- g + geom_polygon(data = data.frame(cbind(x = c(xcrit, seq(from = xcrit,\n                to = muA + 4 * sigma/sqrt(n), length.out = 1000), muA + 4 * sigma/sqrt(n)),\n                y = c(0, dnorm(seq(from = xcrit, to = muA + 4 * sigma/sqrt(n), length.out = 1000),\n                  mean = muA, sd = sigma/sqrt(n)), 0))), aes(x = x, y = y), fill = \"blue\",\n                alpha = 0.5)\n            pow &lt;- pnorm(muA + 4 * sigma/sqrt(n), muA, sigma/sqrt(n)) - pnorm(xcrit,\n                muA, sigma/sqrt(n))\n        }\n\n        if (z &lt; 0) {\n            xcrit = mu0 - qnorm(1 - alpha/2) * sigma/sqrt(n)\n            g &lt;- g + geom_segment(x = xcrit, y = 0, xend = xcrit, yend = max(dnorm(mu0,\n                mu0, sigma/sqrt(n)) + 0.025, dnorm(muA, muA, sigma/sqrt(n)) + 0.025),\n                size = 0.5, linetype = 3)\n            g &lt;- g + geom_polygon(data = data.frame(cbind(x = c(muA - 4 * sigma/sqrt(n),\n                seq(from = muA - 4 * sigma/sqrt(n), to = xcrit, length.out = 1000),\n                xcrit), y = c(0, dnorm(seq(from = muA - 4 * sigma/sqrt(n), to = xcrit,\n                length.out = 1000), mean = muA, sd = sigma/sqrt(n)), 0))), aes(x = x,\n                y = y), fill = \"blue\", alpha = 0.5)\n            pow &lt;- pnorm(xcrit, muA, sigma/sqrt(n)) - pnorm(muA - 4 * sigma/sqrt(n),\n                muA, sigma/sqrt(n))\n        }\n    }\n\n    g &lt;- g + annotate(\"text\", x = max(mu0, muA) + 2 * sigma/sqrt(n), y = max(dnorm(mu0,\n        mu0, sigma/sqrt(n)) + 0.075, dnorm(muA, muA, sigma/sqrt(n)) + 0.075), label = paste(\"Effect Size = \",\n        round(abs((muA - mu0))/sigma, digits = 3), \"\\nPower = \", round(pow, digits = 3),\n        sep = \"\"))\n    g &lt;- g + annotate(\"text\", x = min(mu0, muA) - 2 * sigma/sqrt(n), y = max(dnorm(mu0,\n        mu0, sigma/sqrt(n)) + 0.075, dnorm(muA, muA, sigma/sqrt(n)) + 0.075), label = \"Red = mu0\\nBlue = muA\")\n    g\n}\n\n\nmanipulate(power.plot(mu0, muA, n, sigma, alpha, alternative), mu0 = slider(-10,\n    10, step = 0.1, initial = 0), muA = slider(-10, 10, step = 0.1, initial = 2),\n    n = slider(1, 50, step = 1, initial = 15), sigma = slider(1, 4, step = 0.1, initial = 2),\n    alpha = slider(0.01, 0.1, step = 0.01, initial = 0.05), alternative = picker(\"two.tailed\",\n        \"greater\", \"less\"))\n\nIn most cases, since we are dealing with limited samples from a population, we will want to use the t rather than the normal distribution as the basis for making our power evaluations. The power.t.test() function lets us easily implement power calculations based on the t distribution, and the results of using it should be very similar to those we found above by simulation. The power.t.test() function takes as possible arguments the sample size, n= (\\(n\\)), the difference, delta= (\\(\\delta\\)) between group means, the standard deviation of the differences between means (sd=, \\(\\sigma\\)), the significance level (sig.level=, \\(\\alpha\\)), the test type= (‚Äútwo.sample‚Äù, ‚Äúone.sample‚Äù, or ‚Äúpaired‚Äù), the alternative= test to run (‚Äútwo.sided‚Äù, ‚Äúone.sided‚Äù), and the desired_power= (\\(1-\\beta\\)). Power, \\(n\\), or the difference between means is left as null and the other arguments are specified. The function then calculates the missing argument.\n\nCHALLENGE\nUsing the code below, which graphs the Type II error rate (\\(\\beta\\)) and power (\\(1-\\beta\\)) for T tests (using the power.t.test() function), explore the effects of changing \\(\\alpha\\), within sample variability (\\(\\sigma\\)), and the difference between sample means (i.e., \\(\\mu_0\\) and \\(\\mu_A\\) for a one sample test (or, equivalently, \\(\\mu_1\\) and \\(\\mu_2\\) for a two sample test). The plot shows the effect size, given the difference between the means, (i.e., \\(\\vert(\\mu_0 - \\mu_A)\\vert/\\sigma\\)) and marks the sample size (\\(n\\)) needed to achieve a power of 0.8.\n\npower.test &lt;- function(mu0, muA, sigma, alpha = 0.05, desired_power = 0.8, type,\n    alternative) {\n\n    p &lt;- 0\n    for (i in 2:200) {\n        x &lt;- power.t.test(n = i, delta = abs(muA - mu0), sd = sigma, sig.level = alpha,\n            power = NULL, type = type, alternative = alternative)\n        p &lt;- c(p, x$power)\n    }\n    d &lt;- data.frame(cbind(1:200, p, 1 - p))\n    critn &lt;- 0\n    for (i in 1:199) {\n        if (p[i] &lt; desired_power && p[i + 1] &gt;= desired_power) {\n            critn &lt;- i + 1\n        } else {\n            critn &lt;- critn\n        }\n    }\n    names(d) &lt;- c(\"n\", \"power\", \"beta\")\n    g &lt;- ggplot(data = d) + xlab(\"sample size n\") + ylab(\"Type II Error Rate, Beta  (Red)\\nand\\nPower, 1-Beta (Blue)\") +\n        ggtitle(\"Power Explorer for T Tests\\n\n      (assuming equal n and variance across the two groups)\") +\n        ylim(0, 1) + geom_point(aes(x = n, y = power), color = \"blue\", alpha = 0.5,\n        size = 0.5) + geom_line(aes(x = n, y = power), colour = \"blue\", alpha = 0.5) +\n        geom_line(aes(x = n, y = desired_power), color = \"black\", lty = 3) + geom_point(aes(x = n,\n        y = beta), color = \"red\", alpha = 0.5, size = 0.5) + geom_line(aes(x = n,\n        y = beta), color = \"red\", alpha = 0.5) + geom_linerange(aes(x = critn, ymin = 0,\n        ymax = desired_power), color = \"black\", alpha = 0.25, lwd = 0.1) + annotate(\"text\",\n        x = 150, y = 0.5, label = paste(\"Effect Size = \", round(abs(mu0 - muA)/sigma,\n            digits = 3), \"\\nCritical n = \", critn, sep = \"\"))\n    print(g)\n}\n\n\nmanipulate(power.test(mu0, muA, sigma, alpha, desired_power, type, alternative),\n    mu0 = slider(-10, 10, initial = 3, step = 1), muA = slider(-10, 10, initial = 0,\n        step = 1), sigma = slider(1, 10, initial = 3, step = 1), alpha = slider(0.01,\n        0.1, initial = 0.05, step = 0.01), desired_power = slider(0, 1, initial = 0.8,\n        step = 0.05), alternative = picker(\"two.sided\", \"one.sided\"), type = picker(\"two.sample\",\n        \"one.sample\", \"paired\"))",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Error, Power, and Effect Size</span>"
    ]
  },
  {
    "objectID": "17-module.html#concept-review",
    "href": "17-module.html#concept-review",
    "title": "17¬† Error, Power, and Effect Size",
    "section": "Concept Review",
    "text": "Concept Review\n\nClassical null hypothesis significance testing tries to minimize the rate of type I error (\\(\\alpha\\), i.e., the probability of incorrectly rejecting a null hypothesis when it‚Äôs correct)\nWhen we try to reduce the rate of type I error (e.g., by decreasing \\(\\alpha\\)), all else being equal, we increase the rate of type II error (\\(\\beta\\), i.e., the probability of incorrectly failing to reject the null hypothesis when it‚Äôs wrong)\nThe power of a statistical test, \\(1-\\beta\\), is our ability to correctly reject the null hypothesis when it is false\nPower depends on the difference between sample means, the standard deviation in the population the sample is drawn from, and the sample size",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Error, Power, and Effect Size</span>"
    ]
  },
  {
    "objectID": "18-module.html",
    "href": "18-module.html",
    "title": "18¬† Introduction to Linear Modeling",
    "section": "",
    "text": "18.1 Objectives",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Introduction to Linear Modeling</span>"
    ]
  },
  {
    "objectID": "18-module.html#objectives",
    "href": "18-module.html#objectives",
    "title": "18¬† Introduction to Linear Modeling",
    "section": "",
    "text": "The objective of this module is to discuss the use of simple linear regression to explore the relationship among two continuous variables: a single predictor variable and a single response variable.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Introduction to Linear Modeling</span>"
    ]
  },
  {
    "objectID": "18-module.html#preliminaries",
    "href": "18-module.html#preliminaries",
    "title": "18¬† Introduction to Linear Modeling",
    "section": "18.2 Preliminaries",
    "text": "18.2 Preliminaries\n\nInstall the following package in R: {lmodel2}\nInstall and load the following package in R: {broom}\nLoad {tidyverse}, {manipulate}, {patchwork}, and {infer}",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Introduction to Linear Modeling</span>"
    ]
  },
  {
    "objectID": "18-module.html#covariance-and-correlation",
    "href": "18-module.html#covariance-and-correlation",
    "title": "18¬† Introduction to Linear Modeling",
    "section": "18.3 Covariance and Correlation",
    "text": "18.3 Covariance and Correlation\nSo far, we have looked principally at single variables, but one of the main things we are often interested in is the relationships among two or more variables. Regression modeling is one of the most powerful and important set of tools for looking at relationships among more than one variable. With our zombie apocalypse survivors dataset, we started to do this using simple bivariate scatterplots‚Ä¶ let‚Äôs look at those data again and do a simple bivariate plot of height by weight.\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/zombies.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\nhead(d)\n\n## # A tibble: 6 √ó 10\n##      id first_name last_name gender height weight zombies_killed\n##   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;          &lt;dbl&gt;\n## 1     1 Sarah      Little    Female   62.9   132.              2\n## 2     2 Mark       Duncan    Male     67.8   146.              5\n## 3     3 Brandon    Perez     Male     72.1   153.              1\n## 4     4 Roger      Coleman   Male     66.8   130.              5\n## 5     5 Tammy      Powell    Female   64.7   132.              4\n## 6     6 Anthony    Green     Male     71.2   153.              1\n## # ‚Ñπ 3 more variables: years_of_education &lt;dbl&gt;, major &lt;chr&gt;, age &lt;dbl&gt;\n\nplot(data = d, height ~ weight)\n\n\n\n\n\n\n\n\nThese variables clearly seem to be related to one another, in that as weight increases, height increases. There are a couple of different ways we can quantify the relationship between these variables. One is by calculating the covariance, which expresses how much two numeric variables ‚Äúchange together‚Äù and whether that change is positive or negative.\nRecall that the variance in a variable is simply the sum of the squared deviatiations of each observation from the mean divided by sample size (n for population variance or n-1 for sample variance). Thus, sample variance is:\n\\[var(x)=\\sum\\frac{(x-\\bar{x})^2}{(n-1)}\\]\nSimilarly, the covariance is simply the product of the deviations of each of two variables from their respective means divided by sample size. Thus, for two vectors, \\(x\\) and \\(y\\), each of length \\(n\\), representing two variables describing a sample‚Ä¶\n\\[cov(x,y) = \\sum\\frac{(x-\\bar{x})(y-\\bar{y})}{(n-1)}\\]\n\nCHALLENGE\nWhat is the covariance between zombie apocalypse survivor weight and zombie apocalypse survivor height? What does it mean if the covariance is positive versus negative? Does it matter if you switch the order of the two variables?\n\n\nShow Code\nw &lt;- d$weight\nh &lt;- d$height\nn &lt;- length(w)  # or length(h)\ncov_wh &lt;- sum((w - mean(w)) * (h - mean(h)))/(n - 1)\ncov_wh\n\n\nShow Output\n## [1] 66.03314\n\n\n\nThe built-in R function cov() yields the same.\n\ncov(w, h)\n\n## [1] 66.03314\n\n\nWe often describe the relationship between two variables using the correlation coefficient, which is a standardized form of the covariance that summarizes, on a scale from -1 to +1, both the strength and direction of a relationship. The correlation is simply the covariance divided by the product of the standard deviations of the two variables.\n\\[cor(x,y)=\\frac{cov(x,y)}{sd(x)sd(y)}\\]\n\n\nCHALLENGE\nCalculate the correlation between zombie apocalypse survivor weight and zombie apocalypse survivor height.\n\n\nShow Code\nsd_w &lt;- sd(w)\nsd_h &lt;- sd(h)\ncor_wh &lt;- cov_wh/(sd_w * sd_h)\ncor_wh\n\n\nShow Output\n## [1] 0.8325862\n\n\n\nAgain, there is a built-in R function cor() which yields the same.\n\ncor(w, h)\n\n## [1] 0.8325862\n\ncor(w, h, method = \"pearson\")\n\n## [1] 0.8325862\n\n\nThis formulation of the correlation coefficient is referred to as Pearson‚Äôs product-moment correlation coefficient and is often abbreviated as \\(\\rho\\).\nThere are other, nonparametric forms of the correlation coefficient we might also calculate, which are based on the relationship among rank scores for the two variables:\n\ncor(w, h, method = \"kendall\")\n\n## [1] 0.6331932\n\ncor(w, h, method = \"spearman\")\n\n## [1] 0.82668",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Introduction to Linear Modeling</span>"
    ]
  },
  {
    "objectID": "18-module.html#regression",
    "href": "18-module.html#regression",
    "title": "18¬† Introduction to Linear Modeling",
    "section": "18.4 Regression",
    "text": "18.4 Regression\nRegression refers to a set of tools that lets us explore the relationships between variables further. In regression analysis, we are typically identifying and exploring linear models, or functions, that describe the relationship between variables. There are a couple of main purposes for undertaking regression analyses:\n\nTo use one or more variables to predict the value of another\nTo develop and choose among different models of the relationship between variables\nTo do analyses of covariation among sets of variables to identify/explore their relative explanatory power\n\nThe general purpose of linear regression is to come up with a model or function that estimates the expected value of one variable (a mean, a proportion, etc.), i.e., the response or outcome variable, given the particular value(s) of another variable (or set of variables), i.e., the predictor variable(s).\nWe are going to start off with simple bivariate regression, where we have a single predictor and a single response variable. In our case, we may be interested in coming up with a model that estimates the expected value for zombie apocalypse survivor height (as the response variable) given zombie apocalypse survivor weight (as the predictor variable). That is, we want to explore possible functions that link these two variables and choose the best one.\nIn general, the model for linear regression represents a dependent (or response) variable, \\(Y\\) as a linear function of an independent (or predictor) variable, \\(X\\).\n\\[Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\]\nThe function has two coefficients. The first, \\(\\beta_0\\) is the intercept, i.e., the value of \\(Y\\) when \\(X\\) = 0. The second \\(\\beta_1\\) is the slope of the line describing the relationship between the predictor and response. The error term, \\(\\epsilon_i\\), is a normal random variable, \\(\\epsilon_i \\sim N(0,\\sigma^2)\\), with the standard deviation assumed to be constant across all values of \\(X\\). A regression analysis calls for estimating the values of all three parameters (\\(\\beta_0\\), \\(\\beta_1\\), and the residual or error term). How this is accomplished will depend on what assumptions are employed in the analysis.\nLooking at our scatterplot above, it seems pretty clear that there is indeed some linear relationship among these variables, and so a reasonable function to connect height to weight should simply be some kind of line of best fit. Recall that the general formula for a line is:\n\\[\\hat{y} = slope \\times x + intercept\\]\nwhere \\(\\hat{y}\\) = our predicted (or expected, or mean) value for y given a particular value of x.\nIn regression parlance‚Ä¶\n\\[\\hat{y_i} = \\beta_1x_i + \\beta_0\\]\nHere, \\(\\beta_1\\) and \\(\\beta_0\\) are referred to as the regression coefficients, and it is those coefficients that our regression analysis is trying to estimate, while minimizing, according to some criterion, the error term. This process of estimation is called ‚Äúfitting the model.‚Äù\n\n18.4.1 Ordinary Least Squares (OLS)\nA typical linear regression analysis further assumes that \\(X\\), our ‚Äúindependent‚Äù or predictor variable, is controlled and thus measured with much greater precision than \\(Y\\), our ‚Äúdependent‚Äù or response variable. Thus the error, \\(\\epsilon_i\\) is assumed to be restricted to the \\(Y\\) dimension, with little or no error in measuring \\(X\\), and we employ ‚Äúordinary least squares‚Äù as our criterion for best fit.\nWhat does this mean? Well, we can imagine a family of lines with different slope (\\(\\beta_1\\)) and intercept (\\(\\beta_0\\)) going through any cloud of points, and one ‚Äúbest fit‚Äù criterion we could use is to find the line whose coefficients (\\(\\beta_1\\) and \\(\\beta_0\\), or slope and intercept) minimize the sum of the squared deviations of each observation in the \\(Y\\) direction from that predicted by the line. This is the basis of ordinary least squares or OLS regression. We want to wind up with an equation that tells us how \\(Y\\) varies in response to changes in \\(X\\).\nIn other words, we want to find \\(\\beta_1\\) and \\(\\beta_0\\) that minimizes‚Ä¶\n\\[\\sum(y_i-\\hat{y})^2\\]\nor, equivalently,\n\\[\\sum(y_i-(\\beta_1 x_i + \\beta_0))^2\\]\nIn terms of our variables, this is‚Ä¶\n\\[\\sum(height - (\\beta_1 weight + \\beta_0))^2\\]\nLet‚Äôs first fit such a model by hand‚Ä¶ The first thing to do is estimate the slope, which we can do if we first ‚Äúcenter‚Äù each of our variables by subtracting the mean from each value (essentially, this shifts the distribution to eliminate the intercept term).\n\nd &lt;- mutate(d, centered_height = height - mean(height))\nd &lt;- mutate(d, centered_weight = weight - mean(weight))\n\np1 &lt;- ggplot(data = d, aes(x = weight, y = height)) + geom_point()\np2 &lt;- ggplot(data = d, aes(x = centered_weight, y = centered_height)) + geom_point()\n\np1 + p2\n\n\n\n\n\n\n\n\nOnce we do this, we just need to minimize‚Ä¶\n\\[\\sum(y_{centered} - (\\beta_1 x_{centered}))^2\\]\nWe can explore finding the best slope (\\(\\beta_1\\)) for this line using an interactive approach.\nFirst we define a custom function‚Ä¶\n\nslope.test &lt;- function(beta1, data) {\n    g &lt;- ggplot(data = data, aes(x = centered_weight, y = centered_height))\n    g &lt;- g + geom_point()\n    g &lt;- g + geom_abline(intercept = 0, slope = beta1, size = 1, colour = \"blue\",\n        alpha = 1/2)\n    ols &lt;- sum((data$centered_height - beta1 * data$centered_weight)^2)\n    g &lt;- g + ggtitle(paste(\"Slope = \", beta1, \"\\nSum of Squared Deviations = \", round(ols,\n        3)))\n    g\n}\n\n‚Ä¶ that we can then play with interactively!\n\nNOTE: The following code is not run‚Ä¶ to use it, copy and paste it into RStudio and then run it.\n\n\nmanipulate(slope.test(beta1, data = d), beta1 = slider(-1, 1, initial = 0, step = 0.005))\n\nSimilarly, we can calculate \\(\\beta_1\\) analytically as follows ‚Ä¶\n\\[\\beta_1 = cor(x,y)\\frac{sd(y)}{sd(x)}=\\frac{cov(x,y)}{var(x)}=\\frac{SS_{XY}}{SS_X}\\]\nWe can also calculate estimates for the standard errors in our regression coefficients analytically as follows:\n\\[SE_{\\beta_1} = \\sqrt\\frac{\\sum\\epsilon_i^2}{(n-2)\\sum(x_i -\\bar{x})^2} = \\sqrt\\frac{\\sum(\\hat{y_i}-\\bar{y})^2}{(n-2)\\sum(x_i -\\bar{x})^2}\\] \\[SE_{\\beta_0} = SE_{\\beta_1}\\sqrt\\frac{\\sum{x_i}^2}{n}\\] As for any standard errors, these are estimates of the standard deviation in the sampling distribution of the two regression coefficients.\n\n\nCHALLENGE\nSolve for \\(\\beta_1\\) by hand‚Ä¶\n\n\nShow Code\n(beta1 &lt;- cor(w, h) * (sd(h)/sd(w)))\n\n\nShow Output\n## [1] 0.1950187\n\n\n\nShow Code\n(beta1 &lt;- cov(w, h)/var(w))\n\n\nShow Output\n## [1] 0.1950187\n\n\n\nShow Code\n(beta1 &lt;- sum((h - mean(h)) * (w - mean(w)))/sum((w - mean(w))^2))\n\n\nShow Output\n## [1] 0.1950187\n\n\n\nThen, to find \\(\\beta_0\\), we can simply plug back into our original regression model. The line of best fit has to run through the centroid of our data points, which is the point determined by the mean of the \\(X\\) values and the mean of the \\(Y\\) values, so we can use the following:\n\\[\\bar{y}=\\beta_1\\bar{x}+\\beta_0\\]\nwhich, rearranged to solve for \\(\\beta_0\\) gives‚Ä¶\n\\[\\beta_0=\\bar{y}-\\beta_1\\bar{x}\\]\n\n\nCHALLENGE\nSolve for \\(\\beta_0\\) by hand‚Ä¶\n\n(beta0 &lt;- mean(h) - beta1 * mean(w))\n\n## [1] 39.56545\n\n\nNote that in the example above, we have taken our least squares criterion to mean minimizing the deviation of each of our \\(Y\\) variables from a line of best fit in a dimension perpendicular to the \\(Y\\) axis. In general, this kind of regression - where deviation is measured perpendicular to one of the axes - is known as Model I regression, and is used when the levels of the predictor variable are either measured without error (or, practically speaking, are measured with much less uncertainty than those of the response variable) or are set by the researcher (e.g., for defined treatment variables in an ecological experiment).\n\n\n18.4.2 The lm() Function\nThe function lm() (‚Äúlinear model‚Äù) in R makes all of the calculations we did above for Model I regression very easy! Below, we pass the zombies dataframe (d) and variables directly to lm() and assign the result to an R object called m. We can then look at the various elements that R calculates about this model.\n\nm &lt;- lm(height ~ weight, data = d)\nm\n\n## \n## Call:\n## lm(formula = height ~ weight, data = d)\n## \n## Coefficients:\n## (Intercept)       weight  \n##      39.565        0.195\n\nnames(m)  # components of the object, m\n\n##  [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n##  [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n##  [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"\n\nm$coefficients  # regression coefficients\n\n## (Intercept)      weight \n##  39.5654460   0.1950187\n\nhead(m$model)  # x values and fitted y values\n\n##     height   weight\n## 1 62.88951 132.0872\n## 2 67.80277 146.3753\n## 3 72.12908 152.9370\n## 4 66.78484 129.7418\n## 5 64.71832 132.4265\n## 6 71.24326 152.5246\n\n\nApplying the tidy() function from the {broom} package to our model makes it easy to extract certain output of interest, such as beta coefficients.\n\ntidy(m)\n\n## # A tibble: 2 √ó 5\n##   term        estimate std.error statistic   p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)   39.6     0.596        66.4 0        \n## 2 weight         0.195   0.00411      47.5 2.65e-258\n\n\nThe glance() function also returns other information of interest about the model.\n\nglance(m)\n\n## # A tibble: 1 √ó 12\n##   r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n##       &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1     0.693         0.693  2.39     2255. 2.65e-258     1 -2289. 4583. 4598.\n## # ‚Ñπ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nIn {ggplot}, we can easily create a plot that adds the linear model along with confidence intervals around the estimated value of y, or \\(\\hat{y}\\) at each x by calling geom_smooth() with the argument method= specified as ‚Äúlm‚Äù. Those intervals are important for when we move on to talking about inference in the regression context.\n\ng &lt;- ggplot(data = d, aes(x = weight, y = height))\ng &lt;- g + geom_point()\ng &lt;- g + geom_smooth(method = \"lm\", formula = y ~ x)\ng\n\n\n\n\n\n\n\n\n\n\n18.4.3 Alternatives to OLS\nThe assumption of greater uncertainty in our response variable than in our predictor variable may be reasonable in controlled experiments, but for natural observations, measurement of the \\(X\\) variable also typically involves some error and, in fact, in many cases we may not be concered about PREDICTING \\(Y\\) from \\(X\\) but rather want to treat both \\(X\\) and \\(Y\\) as independent variables and explore the relationship between them or consider that both are dependent on some additional parameter, which may be unknown. That is, both are measured rather than ‚Äúcontrolled‚Äù and both include uncertainty. We thus are not seeking an equation of how \\(Y\\) varies with changes in \\(X\\), but rather we are look for how they both co-vary in response to some other variable or process. Under these conditions Model II regression analysis may be more appropriate. In Model II approaches, a line of best fit is chosen that minimizes in some way the direct distance of each point to the best fit line. There are several different types of Model II regression, and which to use depends upon the specifics of the case. Common approaches are know as major axis, ranged major axis, and reduced major axis (a.k.a. standard major axis) regression.\nThe {lmodel2} package allows us to do Model II regression easily (as well as Model I). In this package, the signficance of the regression coefficients, which we discuss below, is determined based on permutation.\n\nlibrary(lmodel2)  # load the lmodel2 package\n# Run the regression\nmII &lt;- lmodel2(height ~ weight, data = d, range.y = \"relative\", range.x = \"relative\",\n    nperm = 1000)\nmII\n\n## \n## Model II regression\n## \n## Call: lmodel2(formula = height ~ weight, data = d, range.y =\n## \"relative\", range.x = \"relative\", nperm = 1000)\n## \n## n = 1000   r = 0.8325862   r-square = 0.6931998 \n## Parametric P-values:   2-tailed = 2.646279e-258    1-tailed = 1.32314e-258 \n## Angle between the two OLS regression lines = 4.677707 degrees\n## \n## Permutation tests of OLS, MA, RMA slopes: 1-tailed, tail corresponding to sign\n## A permutation test of r is equivalent to a permutation test of the OLS slope\n## P-perm for SMA = NA because the SMA slope cannot be tested\n## \n## Regression results\n##   Method Intercept     Slope Angle (degrees) P-perm (1-tailed)\n## 1    OLS  39.56545 0.1950187        11.03524       0.000999001\n## 2     MA  39.10314 0.1982313        11.21246       0.000999001\n## 3    SMA  33.92229 0.2342325        13.18287                NA\n## 4    RMA  36.80125 0.2142269        12.09153       0.000999001\n## \n## Confidence intervals\n##   Method 2.5%-Intercept 97.5%-Intercept 2.5%-Slope 97.5%-Slope\n## 1    OLS       38.39625        40.73464  0.1869597   0.2030778\n## 2     MA       37.92239        40.28020  0.1900520   0.2064362\n## 3    SMA       32.74259        35.06211  0.2263120   0.2424301\n## 4    RMA       35.51434        38.06296  0.2054593   0.2231695\n## \n## Eigenvalues: 351.6888 5.48735 \n## \n## H statistic used for computing C.I. of MA: 6.212738e-05\n\npar(mfrow = c(2, 2))\nplot(mII, \"OLS\")\nplot(mII, \"MA\")\nplot(mII, \"SMA\")\nplot(mII, \"RMA\")\n\n\n\n\n\n\n\ndetach(package:lmodel2)\n\nNote that, here, running lmodel2() and using OLS to detemine the best coefficients yields equivalent results to our Model I regression done above using lm().\n\nmI &lt;- lm(height ~ weight, data = d)\nsummary(mI)  # show lm() results\n\n## \n## Call:\n## lm(formula = height ~ weight, data = d)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -7.1519 -1.5206 -0.0535  1.5167  9.4439 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 39.565446   0.595815   66.41   &lt;2e-16 ***\n## weight       0.195019   0.004107   47.49   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.389 on 998 degrees of freedom\n## Multiple R-squared:  0.6932, Adjusted R-squared:  0.6929 \n## F-statistic:  2255 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n# show lmodel2() OLS results\nfilter(mII$regression.results, Method == \"OLS\")\n\n##   Method Intercept     Slope Angle (degrees) P-perm (1-tailed)\n## 1    OLS  39.56545 0.1950187        11.03524       0.000999001\n\npar(mfrow = c(1, 2))\nplot(mII, main = \"lmodel2() OLS\", xlab = \"weight\", ylab = \"height\")\nplot(data = d, height ~ weight, main = \"lm()\")\nabline(mI, col = \"red\")\n\n\n\n\n\n\n\n\n\n\nCHALLENGE\nUsing the zombie apocalypse survivors dataset, try the following‚Ä¶\n\nPlot survivor height as a function of age\nDerive by hand the ordinary least squares regression coefficients \\(\\beta_1\\) and \\(\\beta_0\\) for these data\nConfirm that you get the same results using the lm() function\nRepeat the analysis above for males and females separately\nDo your regression coefficients differ by sex? How might you determine this?\n\n\npar(mfrow = c(1, 1))\nplot(data = d, height ~ age)\n\n\n\n\n\n\n\nhead(d)\n\n## # A tibble: 6 √ó 12\n##      id first_name last_name gender height weight zombies_killed\n##   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;          &lt;dbl&gt;\n## 1     1 Sarah      Little    Female   62.9   132.              2\n## 2     2 Mark       Duncan    Male     67.8   146.              5\n## 3     3 Brandon    Perez     Male     72.1   153.              1\n## 4     4 Roger      Coleman   Male     66.8   130.              5\n## 5     5 Tammy      Powell    Female   64.7   132.              4\n## 6     6 Anthony    Green     Male     71.2   153.              1\n## # ‚Ñπ 5 more variables: years_of_education &lt;dbl&gt;, major &lt;chr&gt;, age &lt;dbl&gt;,\n## #   centered_height &lt;dbl&gt;, centered_weight &lt;dbl&gt;\n\nbeta1 &lt;- cor(d$height, d$age) * sd(d$height)/sd(d$age)\nbeta1\n\n## [1] 0.9425086\n\nbeta0 &lt;- mean(d$height) - beta1 * mean(d$age)\nbeta0\n\n## [1] 48.73566\n\n(m &lt;- lm(height ~ age, data = d))\n\n## \n## Call:\n## lm(formula = height ~ age, data = d)\n## \n## Coefficients:\n## (Intercept)          age  \n##     48.7357       0.9425\n\nmales &lt;- filter(d, gender == \"Male\")\n\n(m &lt;- lm(height ~ age, data = males))\n\n## \n## Call:\n## lm(formula = height ~ age, data = males)\n## \n## Coefficients:\n## (Intercept)          age  \n##      49.341        1.011\n\nfemales &lt;- filter(d, gender == \"Female\")\n(m &lt;- lm(height ~ age, data = females))\n\n## \n## Call:\n## lm(formula = height ~ age, data = females)\n## \n## Coefficients:\n## (Intercept)          age  \n##     48.1811       0.8691",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Introduction to Linear Modeling</span>"
    ]
  },
  {
    "objectID": "18-module.html#inference-in-regression",
    "href": "18-module.html#inference-in-regression",
    "title": "18¬† Introduction to Linear Modeling",
    "section": "18.5 Inference in Regression",
    "text": "18.5 Inference in Regression\n\nClassical Theory-Based Inference\nOnce we have our linear model and associated regression coefficients, we want to know a bit more about the model. First, we want to be able to evaluate whether there is statistical evidence that there is indeed a relationship between these variables. If so, then our regression coefficients can indeed allow us to estimate or predict the value of one variable given another. Additionally, we also would like to be able to extend our estimates from our sample out to the population they are drawn from. These next steps involve the process of statistical inference.\nThe output of the lm() function provides a lot of information useful for inference. Run the command summary() on the output of lm(data = d, height ~ weight)\n\nm &lt;- lm(data = d, height ~ weight)\nsummary(m)\n\n## \n## Call:\n## lm(formula = height ~ weight, data = d)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -7.1519 -1.5206 -0.0535  1.5167  9.4439 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 39.565446   0.595815   66.41   &lt;2e-16 ***\n## weight       0.195019   0.004107   47.49   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.389 on 998 degrees of freedom\n## Multiple R-squared:  0.6932, Adjusted R-squared:  0.6929 \n## F-statistic:  2255 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\nOne of the outputs for the model, seen in the 2nd to last line in the output above, is the ‚ÄúR-squared‚Äù value, or the coefficient of determination, which is a summary of the total amount of variation in the y variable that is explained by the x variable. In our regression, ~69% of the variation in zombie height is explained by zombie weight.\nAnother output is the standard error of the estimate of each regression coefficient, along with a corresponding t value and p value. Recall that t statistics are calculated as the difference between an observed and expected value divided by a standard error. The p value comes from evaluating the magnitude of the t statistic against a t distribution with n-2 degrees of freedom. That is, we can calculate p values mathematically for our statistics (the regression slope and intercept) if the sampling distribution of these statistics conform to a t distribution.\nAs we saw above, running the tidy() function from the {broom} package pulls out a clean table of the relevant information.\n\nm.summary &lt;- tidy(m)\n\nWe can confirm the t statistic and p value output by lm() by hand by calculating t and p based on the regression coefficient estimates and the standard errors of those estimates. The t statistic is simply the value of the estimated regression coefficient divided by the estimate of the standard error for that coefficient (i.e., \\(\\beta/SE_\\beta\\))\n\nm.summary$calc.statistic &lt;- (m.summary$estimate - 0)/m.summary$std.error\nm.summary$calc.p.value &lt;- 2 * pt(m.summary$calc.statistic, df = nrow(d) - 2, lower.tail = FALSE)\n# we use 2 * pt to get the 2-tailed p value alternatively, we could do...\n# m.summary$calc.p.value &lt;- pt(-1*abs(m.summary$calc.statistic), df = nrow(d) -\n# 2, lower.tail = TRUE) + pt(abs(m.summary$calc.statistic), df = nrow(d) - 2,\n# lower.tail = FALSE) or m.summary$calc.p.value &lt;-\n# pt(-1*abs(m.summary$calc.statistic), df = nrow(d) - 2, lower.tail = TRUE) +\n# (1-pt(abs(m.summary$calc.statistic), df = nrow(d) - 2, lower.tail = TRUE))\nm.summary\n\n## # A tibble: 2 √ó 7\n##   term        estimate std.error statistic   p.value calc.statistic calc.p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;\n## 1 (Intercept)   39.6     0.596        66.4 0                   66.4    0        \n## 2 weight         0.195   0.00411      47.5 2.65e-258           47.5    2.65e-258\n\n\nNote that the glance() function also returns other information of interest about a regression model.\n\nglance(m)\n\n## # A tibble: 1 √ó 12\n##   r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n##       &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1     0.693         0.693  2.39     2255. 2.65e-258     1 -2289. 4583. 4598.\n## # ‚Ñπ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nWe can get t distribution-based confidence intervals for our estimates easily, too, using either the by-hand approach we‚Äôve used before or by using a built-in function.\n\nalpha &lt;- 0.05\n# extract CIs from the model with using the results of lm()\n(CI &lt;- confint(m, level = 1 - alpha))\n\n##                  2.5 %     97.5 %\n## (Intercept) 38.3962527 40.7346393\n## weight       0.1869597  0.2030778\n\n# using tidy()\n(CI &lt;- tidy(m, conf.int = TRUE, conf.level = 1 - alpha))\n\n## # A tibble: 2 √ó 7\n##   term        estimate std.error statistic   p.value conf.low conf.high\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)   39.6     0.596        66.4 0           38.4      40.7  \n## 2 weight         0.195   0.00411      47.5 2.65e-258    0.187     0.203\n\n# by hand\nlower &lt;- m.summary$estimate - qt(1 - alpha/2, df = nrow(d) - 2) * m.summary$std.error\nupper &lt;- m.summary$estimate + qt(1 - alpha/2, df = nrow(d) - 2) * m.summary$std.error\nCI &lt;- cbind(lower, upper)\nrownames(CI) &lt;- c(\"(Intercept)\", \"weight\")\ncolnames(CI) &lt;- c(paste0(as.character(alpha/2 * 100), \" %\"), paste0(as.character((1 -\n    alpha/2) * 100), \" %\"))\nCI\n\n##                  2.5 %     97.5 %\n## (Intercept) 38.3962527 40.7346393\n## weight       0.1869597  0.2030778\n\n\n\nNOTE: Remember that these mathematical methods of calculating p values and CIs are only valid if the LINE technical conditions for linear regression are met. These conditions are:\n\nLinearity of the relationship between the explanatory and response variables\nIndependence of observations and their residuals\nNormality of the residual values of the response variable around the regression line\nEquality of the variance in residual values across the range of explanatory variable values\n\n\n\n\nSimulation-Based Inference\n\nSignificance of Coefficients by Permutation\nAs we saw in Module 16, the {infer} package offers a convenient set of functions and a standard workflow for using permutation methods for statistical hypothesis testing, rather than relying on mathematical assumptions. In that module, we considered whether means, differences between means, proportions, or differences in proportions differed from what we would expect under a particular null hypothesis‚Ä¶ and we can apply the same workflow to evaluating regression coefficients! The permutation approach lets us relax the normality condition of the LINE assumptions for linear regression. To demonstrate this process, we will run the same model of height ~ weight using the zombie apocalypse survivors dataset that we ran above:\nFirst, we run the model and pull out \\(\\beta_1\\), the regression slope, and its estimated standard error:\n\n# first define alpha, CI boundaries, and critical values\nalpha &lt;- 0.05\nconfidence_level &lt;- 1 - alpha\np_lower &lt;- alpha/2\np_upper &lt;- 1 - (alpha/2)\ndegrees_of_freedom &lt;- nrow(d) - 2\ncritical_value &lt;- qt(p_upper, df = degrees_of_freedom)\n\n# original slope\noriginal.slope &lt;- lm(data = d, height ~ weight) |&gt;\n    # tidy the model and add the CI based on the t distribution\ntidy(conf.int = TRUE, conf.level = confidence_level) |&gt;\n    # or manually calculate the CI based on the t distribution\nmutate(lower = estimate - std.error * critical_value, upper = estimate + std.error *\n    critical_value) |&gt;\n    filter(term == \"weight\")\noriginal.slope  # show model results for slope of weight\n\n## # A tibble: 1 √ó 9\n##   term   estimate std.error statistic   p.value conf.low conf.high lower upper\n##   &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 weight    0.195   0.00411      47.5 2.65e-258    0.187     0.203 0.187 0.203\n\n\nWe can generate a null distribution for our slope estimates via permutation using {infer} package functions‚Ä¶ essentially, each permutation involves randomly shuffling values of the response variable under the null hypothesis such that they are independent of the explanatory variable (and vice versa).\n\npermuted.slope &lt;- d |&gt;\n    # specify model\nspecify(height ~ weight) |&gt;\n    # use a null hypothesis of independence\nhypothesize(null = \"independence\") |&gt;\n    # generate permutation replicates\ngenerate(reps = 1000, type = \"permute\") |&gt;\n    # calculate the slope statistic\ncalculate(stat = \"slope\")\n\nhead(permuted.slope)  # slopes from first few permutation replicates\n\n## Response: height (numeric)\n## Explanatory: weight (numeric)\n## Null Hypothesis: ind...\n## # A tibble: 6 √ó 2\n##   replicate     stat\n##       &lt;int&gt;    &lt;dbl&gt;\n## 1         1 -0.00600\n## 2         2 -0.00613\n## 3         3 -0.00260\n## 4         4  0.00713\n## 5         5 -0.00258\n## 6         6  0.00873\n\n# create confidence intervals\n\npermuted.slope.summary &lt;- permuted.slope |&gt;\n    # summarize the mean (which should be very close to zero), standard error,\n    # CI based on the SE and t distribution, and CI based on the quantile\n    # (percentile) method\nsummarize(estimate = mean(stat), std.error = sd(stat), lower = estimate - std.error *\n    critical_value, upper = estimate + std.error * critical_value, perm.lower = quantile(stat,\n    p_lower), perm.upper = quantile(stat, p_upper))\n\n# show summary of permuted sampling distribution\npermuted.slope.summary\n\n## # A tibble: 1 √ó 6\n##      estimate std.error   lower  upper perm.lower perm.upper\n##         &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n## 1 -0.00000975   0.00768 -0.0151 0.0151    -0.0146     0.0142\n\n\nThe get_ci() function from {infer} can be used to return these different types of confidence intervals, too‚Ä¶ i.e., either based on the standard error of the permutation distribution or actual quantiles from that distribution.\n\nget_ci(permuted.slope, level = 1 - alpha, type = \"percentile\")\n\n## # A tibble: 1 √ó 2\n##   lower_ci upper_ci\n##      &lt;dbl&gt;    &lt;dbl&gt;\n## 1  -0.0146   0.0142\n\nget_ci(permuted.slope, level = 1 - alpha, type = \"se\", point_estimate = pull(permuted.slope.summary,\n    estimate))\n\n## # A tibble: 1 √ó 2\n##   lower_ci upper_ci\n##      &lt;dbl&gt;    &lt;dbl&gt;\n## 1  -0.0151   0.0150\n\n\nWe can also visualize the null distribution of slope coefficients based on permutation and superimpose our actual slope coefficient.\n\n# plot the null distribution based on permutation\nhist(permuted.slope$stat, main = \"Histogram of Permuted\\nSlope Values\", xlab = \"Slope Coefficient\")\n\n\n\n\n\n\n\n# or using `visualize()` from {infer}\nvisualize(permuted.slope) + shade_p_value(obs_stat = original.slope$estimate, direction = \"two_sided\")\n\n\n\n\n\n\n\n\nFinally, to determine the two-sided p value associated with our observed slope coefficient, we determine the proportion of permuted simulations that yielded a slope estimate as great or greater than the the one based on our original data:\n\np.value &lt;- permuted.slope |&gt;\n    # add a column of the absolute value of the slope\nmutate(abs_stat = abs(stat)) |&gt;\n    # calculate a summary statistic as the proportion of cases where the\n    # absolute value of the permuted slope is greater than or equal to the\n    # absolute value of the observed slope\nsummarize(estimate = mean(abs_stat &gt;= abs(pull(original.slope, estimate))))\n\np.value\n\n## # A tibble: 1 √ó 1\n##   estimate\n##      &lt;dbl&gt;\n## 1        0\n\n# the function `get_p_value()` returns this value directly...\n\n(p.value &lt;- permuted.slope |&gt;\n    get_p_value(obs_stat = original.slope$estimate, direction = \"two_sided\"))\n\n## Warning: Please be cautious in reporting a p-value of 0. This result is an approximation\n## based on the number of `reps` chosen in the `generate()` step.\n## ‚Ñπ See `get_p_value()` (`?infer::get_p_value()`) for more information.\n\n\n## # A tibble: 1 √ó 1\n##   p_value\n##     &lt;dbl&gt;\n## 1       0\n\n\n\n\nCIs for Coefficients by Bootstrapping\nWe can also use the {infer} package to generate CIs around our regression coefficient estimates using bootstrapping.\n\nboot.slope &lt;- d |&gt;\n    # specify model\nspecify(height ~ weight) |&gt;\n    # generate bootstrap replicates\ngenerate(reps = 1000, type = \"bootstrap\") |&gt;\n    # calculate the slope statistic\ncalculate(stat = \"slope\")\n\nhead(boot.slope)  # slopes from first few bootstrap replicates\n\n## Response: height (numeric)\n## Explanatory: weight (numeric)\n## # A tibble: 6 √ó 2\n##   replicate  stat\n##       &lt;int&gt; &lt;dbl&gt;\n## 1         1 0.192\n## 2         2 0.198\n## 3         3 0.194\n## 4         4 0.195\n## 5         5 0.189\n## 6         6 0.195\n\n# create confidence intervals for regression coefficients\n\nboot.slope.summary &lt;- boot.slope |&gt;\n    # summarize the mean, standard error, CI based on the SE and t\n    # distribution, and CI based on the quantile (percentile) method\nsummarize(estimate = mean(stat), std.error = sd(stat), lower = estimate - std.error *\n    critical_value, upper = estimate + std.error * critical_value, boot.lower = quantile(stat,\n    p_lower), boot.upper = quantile(stat, p_upper))\n\n# show summary of bootstrap sampling distribution\nboot.slope.summary\n\n## # A tibble: 1 √ó 6\n##   estimate std.error lower upper boot.lower boot.upper\n##      &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n## 1    0.195   0.00411 0.187 0.203      0.187      0.203\n\n\nAgain, the get_ci() function from {infer} can be used to return these different types of confidence intervals‚Ä¶\n\nCI.percentile &lt;- get_ci(boot.slope, level = 1 - alpha, type = \"percentile\")\nCI.theory &lt;- get_ci(boot.slope, level = 1 - alpha, type = \"se\", point_estimate = pull(boot.slope.summary,\n    estimate))\n\nWe can also easily visualize the sampling distribution (and CI) based on bootstrapping.\n\n# plot the sampling distribution for based on bootstrapping plus a CI\nhist(boot.slope$stat, main = \"Histogram of Bootstrapped\\nSlope Values\", xlab = \"Slope Coefficient\")\nabline(v = CI.percentile)\n\n\n\n\n\n\n\n# or...\nvisualize(boot.slope) + shade_ci(endpoints = CI.percentile)",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Introduction to Linear Modeling</span>"
    ]
  },
  {
    "objectID": "18-module.html#interpreting-regression-results",
    "href": "18-module.html#interpreting-regression-results",
    "title": "18¬† Introduction to Linear Modeling",
    "section": "18.6 Interpreting Regression Results",
    "text": "18.6 Interpreting Regression Results\nAs shown above, estimating our regression coefficients is pretty straightforward. But what do they mean? How do we interpret them?\n\nThe intercept, \\(\\beta_0\\), is the PREDICTED value of \\(y\\), our response variable, when the value of \\(x\\), our explanatory variable, is zero.\nThe slope, \\(\\beta_1\\) is EXPECTED CHANGE in units of \\(y\\) for every 1 unit of change in \\(x\\).\nThe overall equation allows us to calculate PREDICTED values of \\(y\\) for new observations of \\(x\\). We can also calculate CONFIDENCE INTERVALS (CIs) around the predicted mean value of \\(y\\) for each value of \\(x\\) (which addresses our uncertainly in the estimate of the mean), and we can get PREDICTION INTERVALS (PIs) around our prediction (which gives the range of actual values of \\(y\\) that we might expect to see at a given value of \\(x\\)). That is, for each value of \\(x\\), we can calculate a CI, which is the the range of values into which \\(\\hat{y}\\) is expected to fall 95% of the time, and a PI, which is the range of y values into which 95% of predicted individual y values are expected to fall.\n\n\nCHALLENGE\n\nIf zombie apocalypse survivor weight is measured in pounds and zombie apocalypse survivor height is measured in inches, what is the expected height of a survivor weighing 150 pounds?\n\n\n\nShow Code\nbeta0 &lt;- m.summary |&gt;\n    filter(term == \"(Intercept)\") |&gt;\n    pull(estimate)\nbeta1 &lt;- m.summary |&gt;\n    filter(term == \"weight\") |&gt;\n    pull(estimate)\n(h.hat &lt;- beta1 * 150 + beta0)\n\n\nShow Output\n## [1] 68.81825\n\n\n\n\nWhat is the predicted difference in height between a survivor weighing 180 and 220 pounds?\n\n\n\nShow Code\n(h.hat.difference &lt;- (beta1 * 220 + beta0) - (beta1 * 180 + beta0))\n\n\nShow Output\n## [1] 7.800749\n\n\n\n\n\nPredicted Values\nThe predict() function allows us to generate predicted (i.e., \\(\\hat{y}\\)) values for a vector of values of x. Note the structure of the 2nd argument in the function, newdata=‚Ä¶ it has to be a data frame and include the variable name(s) we are going to use to predict y on the basis of. Here, we pass it a vector of actual x values.\n\nm &lt;- lm(data = d, height ~ weight)\ny.hat &lt;- predict(m, newdata = data.frame(weight = d$weight))\ndf &lt;- data.frame(cbind(d$weight, d$height, y.hat))\nnames(df) &lt;- c(\"x\", \"y\", \"yhat\")\nhead(df)\n\n##          x        y     yhat\n## 1 132.0872 62.88951 65.32492\n## 2 146.3753 67.80277 68.11137\n## 3 152.9370 72.12908 69.39103\n## 4 129.7418 66.78484 64.86753\n## 5 132.4265 64.71832 65.39109\n## 6 152.5246 71.24326 69.31059\n\n\nThe augment() function from {broom} also generates comparable predicted or ‚Äúfitted‚Äù values for a model. With default arguments, it generates .fitted and .resid values for the original data and can also calculate .se.fit values (standard errors around the fitted values) by setting se_fit=TRUE (it is FALSE by default). With the newdata= argument, it generates .fitted values for novel data. As for predict(), the newdata= argument has to be a data frame and include the predictor variable name(s).\n\ndf &lt;- augment(m, se_fit = TRUE)\nhead(df)\n\n## # A tibble: 6 √ó 9\n##   height weight .fitted .se.fit .resid    .hat .sigma    .cooksd .std.resid\n##    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n## 1   62.9   132.    65.3  0.0898 -2.44  0.00141   2.39 0.000737       -1.02 \n## 2   67.8   146.    68.1  0.0762 -0.309 0.00102   2.39 0.00000851     -0.129\n## 3   72.1   153.    69.4  0.0841  2.74  0.00124   2.39 0.000817        1.15 \n## 4   66.8   130.    64.9  0.0953  1.92  0.00159   2.39 0.000515        0.803\n## 5   64.7   132.    65.4  0.0890 -0.673 0.00139   2.39 0.0000553      -0.282\n## 6   71.2   153.    69.3  0.0834  1.93  0.00122   2.39 0.000400        0.810\n\n\n\ng &lt;- ggplot(data = df, aes(x = weight, y = .fitted))\ng &lt;- g + geom_point(size = 0.25)\ng &lt;- g + geom_point(aes(x = weight, y = height), color = \"red\")\ng &lt;- g + geom_segment(aes(x = weight, y = .fitted, xend = weight, yend = height),\n    alpha = 0.25)\ng\n\n\n\n\n\n\n\n\nEach vertical line in the figure above represents a residual, the difference between the observed and the fitted or predicted value of \\(y\\) (\\(\\hat{y}\\)) at the given \\(x\\) value.\n\n\nConfidence Intervals around Predicted Means\nThe predict() function also allows us to easily generate confidence intervals around our predicted \\(\\hat{y}\\) values.\n\nci &lt;- predict(m, newdata = data.frame(weight = 150), interval = \"confidence\", level = 1 -\n    alpha)  # for a single value\nci\n\n##        fit      lwr     upr\n## 1 68.81825 68.66211 68.9744\n\nci &lt;- predict(m, newdata = data.frame(weight = d$weight), interval = \"confidence\",\n    level = 1 - alpha)  # for a vector of values\nci &lt;- data.frame(ci)\nci &lt;- cbind(df$weight, ci)\nnames(ci) &lt;- c(\"weight\", \"c.fit\", \"c.lwr\", \"c.upr\")\ng &lt;- ggplot(data = df, aes(x = weight, y = height))\ng &lt;- g + geom_point(alpha = 0.5)\ng &lt;- g + geom_line(data = ci, aes(x = weight, y = c.fit), color = \"black\")\ng &lt;- g + geom_line(data = ci, aes(x = weight, y = c.lwr), color = \"blue\")\ng &lt;- g + geom_line(data = ci, aes(x = weight, y = c.upr), color = \"blue\")\ng\n\n\n\n\n\n\n\n\nThis can be also be done by hand using the data from the augment()ed model:\n\ndf &lt;- df |&gt;\n    # calculate a confidence interval for the predicted values\nmutate(c.lwr = .fitted - qt(1 - alpha/2, nrow(df) - 2) * .se.fit, c.upr = .fitted +\n    qt(1 - alpha/2, nrow(df) - 2) * .se.fit)\nhead(df)\n\n## # A tibble: 6 √ó 11\n##   height weight .fitted .se.fit .resid    .hat .sigma   .cooksd .std.resid c.lwr\n##    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n## 1   62.9   132.    65.3  0.0898 -2.44  0.00141   2.39   7.37e-4     -1.02   65.1\n## 2   67.8   146.    68.1  0.0762 -0.309 0.00102   2.39   8.51e-6     -0.129  68.0\n## 3   72.1   153.    69.4  0.0841  2.74  0.00124   2.39   8.17e-4      1.15   69.2\n## 4   66.8   130.    64.9  0.0953  1.92  0.00159   2.39   5.15e-4      0.803  64.7\n## 5   64.7   132.    65.4  0.0890 -0.673 0.00139   2.39   5.53e-5     -0.282  65.2\n## 6   71.2   153.    69.3  0.0834  1.93  0.00122   2.39   4.00e-4      0.810  69.1\n## # ‚Ñπ 1 more variable: c.upr &lt;dbl&gt;\n\ng &lt;- ggplot(data = df, aes(x = weight, y = height))\ng &lt;- g + geom_point(alpha = 0.5)\ng &lt;- g + geom_line(aes(x = weight, y = .fitted), color = \"black\")\ng &lt;- g + geom_line(aes(x = weight, y = c.lwr), color = \"blue\")\ng &lt;- g + geom_line(aes(x = weight, y = c.upr), color = \"blue\")\ng\n\n\n\n\n\n\n\n\n\n\nPrediction Intervals for Individual Responses\nThe predict() function also allows us to easily generate prediction intervals for the range of possible values of individual observations of \\(y\\) (rather than the 95% CI around \\(\\hat{y}\\) at each \\(x\\).\n\npi &lt;- predict(m, newdata = data.frame(weight = 150), interval = \"prediction\", level = 0.95)  # for a single value\npi\n\n##        fit      lwr      upr\n## 1 68.81825 64.12849 73.50802\n\npi &lt;- predict(m, newdata = data.frame(weight = d$weight), interval = \"prediction\",\n    level = 0.95)  # for a vector of values\npi &lt;- data.frame(pi)\npi &lt;- cbind(d$weight, pi)\nnames(pi) &lt;- c(\"weight\", \"p.fit\", \"p.lwr\", \"p.upr\")\ng &lt;- g + geom_line(data = pi, aes(x = weight, y = p.lwr), color = \"red\")\ng &lt;- g + geom_line(data = pi, aes(x = weight, y = p.upr), color = \"red\")\ng\n\n\n\n\n\n\n\n\nAgain, this can be also be done by hand using the data from the augment()ed model:\n\nsd &lt;- glance(m) |&gt;\n    pull(sigma)  # sd deviation of residuals\n\ndf &lt;- df |&gt;\n    # calculate a confidence interval for the predicted values\nmutate(se.prediction = sqrt(sd^2 + .se.fit^2), p.lower = .fitted - qt(1 - alpha/2,\n    nrow(df) - 2) * se.prediction, p.upper = .fitted + qt(1 - alpha/2, nrow(df) -\n    2) * se.prediction)\ng &lt;- ggplot(data = df, aes(x = weight, y = height))\ng &lt;- g + geom_point(alpha = 0.5)\ng &lt;- g + geom_line(aes(x = weight, y = .fitted), color = \"black\")\ng &lt;- g + geom_line(aes(x = weight, y = c.lwr), color = \"blue\")\ng &lt;- g + geom_line(aes(x = weight, y = c.upr), color = \"blue\")\ng &lt;- g + geom_line(aes(x = weight, y = p.lower), color = \"red\")\ng &lt;- g + geom_line(aes(x = weight, y = p.upper), color = \"red\")\ng\n\n\n\n\n\n\n\n\n\n\nCHALLENGE\nConstruct a linear model using lm() for the regression of zombie apocalypse survivor height on age and predict the mean height (\\(\\hat{h}\\)). Then, use the augment() and mutate() or predict() functions to calculate the t distribution-based 95% confidence interval (CI) around the predicted mean height and the 95% prediction interval (PI) for height for a vector of zombie ages, v &lt;- seq(from=10, to=30, by=1). Finally, plot your points, your regression line, and lines for the lower and upper limits of the CI and of the PI.\n\nNOTE: The augment() and predict() functions work similarly, but whereas predict() returns a vector of fitted \\(y\\) values for each \\(x\\) value in newdata, augment() will return a dataframe of \\(x\\)s, fitted \\(y\\)s, and standard errors for the fitted \\(y\\)s. Helpfully, if the newdata argument is left out for augment(), the function will use the original dataset and simply add in the new fitted \\(y\\), standard errors, etc.\n\n\n\nShow Code\nalpha &lt;- 0.05\nv &lt;- seq(from = 10, to = 30, by = 1)\nm &lt;- lm(data = d, height ~ age)\nsd &lt;- glance(m) |&gt;\n    pull(sigma)\ndf &lt;- augment(m, newdata = data.frame(age = v), se_fit = TRUE) |&gt;\n    # add CI\nmutate(c.lower = .fitted - qt(1 - alpha/2, nrow(d) - 2) * .se.fit, c.upper = .fitted +\n    qt(1 - alpha/2, nrow(d) - 2) * .se.fit) |&gt;\n    # add PI\nmutate(se.prediction = sqrt(sd^2 + .se.fit^2), p.lower = .fitted - qt(1 - alpha/2,\n    nrow(d) - 2) * se.prediction, p.upper = .fitted + qt(1 - alpha/2, nrow(d) - 2) *\n    se.prediction)\n\n# alternatively...  ci &lt;- predict(m, newdata = data.frame(age = v), interval =\n# 'confidence', level = 1 - alpha) pi &lt;- predict(m, newdata = data.frame(age =\n# v), interval = 'prediction', 1 - alpha)\nplot(data = d, height ~ age)\nlines(x = df$age, y = df$.fitted, col = \"black\")\nlines(x = df$age, y = df$c.lower, col = \"blue\")\nlines(x = df$age, y = df$c.upper, col = \"blue\")\nlines(x = df$age, y = df$p.lower, col = \"red\")\nlines(x = df$age, y = df$p.upper, col = \"red\")\n\n\n\n\n\n\n\n\n\nShow Code\n# or\ng1 &lt;- ggplot(data = d, aes(x = age, y = height))\ng1 &lt;- g1 + geom_point(alpha = 0.5)\ng1 &lt;- g1 + geom_line(data = df, aes(x = age, y = .fitted), color = \"black\", lwd = 1)\ng1 &lt;- g1 + geom_line(data = df, aes(x = age, y = c.lower), color = \"blue\")\ng1 &lt;- g1 + geom_line(data = df, aes(x = age, y = c.upper), color = \"blue\")\ng1 &lt;- g1 + geom_line(data = df, aes(x = age, y = p.lower), color = \"red\")\ng1 &lt;- g1 + geom_line(data = df, aes(x = age, y = p.upper), color = \"red\")\ng1\n\n\n\n\n\n\n\n\n\nShow Code\n# or\n\ng2 &lt;- ggplot() + geom_point(data = d, aes(x = age, y = height), alpha = 0.5) + geom_line(data = df,\n    aes(x = age, y = .fitted), color = \"black\", lwd = 1) + geom_ribbon(data = df,\n    aes(x = age, ymin = c.lower, ymax = c.upper), alpha = 0.2, fill = \"blue\") + geom_ribbon(data = df,\n    aes(x = age, ymin = p.lower, ymax = p.upper), alpha = 0.2, fill = \"red\")\ng2\n\n\n\n\n\n\n\n\n\nWe can use a one-liner in {ggplot} to plot the CIs, too‚Ä¶\n\ng3 &lt;- ggplot(data = d, aes(x = age, y = height)) + geom_point(alpha = 0.5) + geom_smooth(method = \"lm\",\n    formula = y ~ x, se = TRUE)\ng3\n\n\n\n\n\n\n\n\nAgain, in these plots, the CI band shows where the mean height (\\(\\hat{h}\\)) is expected to fall in 95% of samples, while the PI band shows where the individual points are expected to fall 95% of the time.\n\n\nCHALLENGE\nFor the same regression model in the challenge above, use bootstrapping with 1000 replicates to generate a quantile-based 95% CI around the estimate of the slope of the relationship. Plot the regression lines associated with each of your first 20 replicates atop a bivariate scatterplot of height ~ age.\n\n\nShow Code\nset.seed(1)\nalpha &lt;- 0.05\np_lower &lt;- alpha/2\np_upper &lt;- 1 - (alpha/2)\nboot.slope &lt;- d |&gt;\n    # specify model\nspecify(height ~ age) |&gt;\n    # generate bootstrap replicates\ngenerate(reps = 1000, type = \"bootstrap\") |&gt;\n    # calculate the slope statistic\ncalculate(stat = \"slope\") |&gt;\n    summarize(boot.mean = mean(stat), boot.lower = quantile(stat, p_lower), boot.upper = quantile(stat,\n        p_upper))\n\nset.seed(1)\nboot.slope &lt;- d |&gt;\n    # specify model\nspecify(height ~ age) |&gt;\n    # generate bootstrap replicates\ngenerate(reps = 20, type = \"bootstrap\")\n\ng &lt;- ggplot(data = d, aes(x = age, y = height)) + geom_point() + geom_smooth(data = boot.slope,\n    aes(x = age, y = height, group = replicate), method = \"lm\", se = FALSE, lwd = 0.1)\ng\n\n\n## `geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Introduction to Linear Modeling</span>"
    ]
  },
  {
    "objectID": "18-module.html#residuals",
    "href": "18-module.html#residuals",
    "title": "18¬† Introduction to Linear Modeling",
    "section": "18.7 Residuals",
    "text": "18.7 Residuals\nFrom our various plots above, it‚Äôs clear that our model is not explaining all of the variation we see in our dataset‚Ä¶ our y points do not all fall on the \\(\\hat{y}\\) line but rather are distributed around it. The distance of each of these points from the predicted value for \\(y\\) at that value of \\(x\\) is known as the ‚Äúresidual‚Äù. We can think about the residuals as ‚Äúwhat is left over‚Äù after accounting for the predicted relationship between \\(x\\) and \\(y\\). Residuals are often thought of as estimates of the ‚Äúerror‚Äù term in a regression model, and most regression analyses assume that residuals are random normal variables with uniform variance across the range of \\(x\\) values (more on this later). In ordinary least squares regression, the line of best fit is defined as that which minimizes the sum of the squared residuals, and the expected value for a residual is 0.\nResiduals are also used to create ‚Äúcovariate adjusted‚Äù variables, as they can be thought of as the response variable, \\(y\\), with the linear effect of other predictor variables removed. We‚Äôll return to this idea when we move on to multivariate regression.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Introduction to Linear Modeling</span>"
    ]
  },
  {
    "objectID": "18-module.html#concept-review",
    "href": "18-module.html#concept-review",
    "title": "18¬† Introduction to Linear Modeling",
    "section": "Concept Review",
    "text": "Concept Review\n\nRegression analysis is used to identify and explore models describing the relationship between variables of interest\nWe can use regression analysis‚Ä¶\n\nto predict the value of a response variable given the value of one or more predictor variables\nto evaluate alternative models of the relationship between a response variable and possible predictor variables\nto explore the relative explanatory power of different predictor variables\n\nLinear regression involves finding the ‚Äúline of best fit‚Äù between the predictor and response variable\nThis means finding the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) that define the line that minizes the sum of the squared deviations of observed and predicted values for the response variable, i.e, that minimizes \\(\\sum(y - \\hat{y})\\) where \\(\\hat{y}= \\beta_0 + \\beta_1 x\\)\nThere are LOTS of types of regression analyses, and we will explore more of them in the modules to come [see Table 8.1 in R in Action]",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Introduction to Linear Modeling</span>"
    ]
  },
  {
    "objectID": "19-module.html",
    "href": "19-module.html",
    "title": "19¬† Elements of Regression Analysis",
    "section": "",
    "text": "19.1 Objectives",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Elements of Regression Analysis</span>"
    ]
  },
  {
    "objectID": "19-module.html#objectives",
    "href": "19-module.html#objectives",
    "title": "19¬† Elements of Regression Analysis",
    "section": "",
    "text": "The objective of this module is to continue our discussion of simple linear regression analysis to understand how the goal of regression is to partition variance in the response variable among different sources, i.e., into that explained by the regression model itself (and, we will see later on in our discussion of multivariate regression, among different factors in that model) versus the left-over error or residual variance. We also go through how to calculate the standard errors for our various regression coefficients and for the predicted values of our response variable based on our regression model, which, as we have seen, are returned by the lm() function. Finally, we also briefly discuss ways to transform non-normally distributed data to make them more appropriate for analysis using linear regression.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Elements of Regression Analysis</span>"
    ]
  },
  {
    "objectID": "19-module.html#preliminaries",
    "href": "19-module.html#preliminaries",
    "title": "19¬† Elements of Regression Analysis",
    "section": "19.2 Preliminaries",
    "text": "19.2 Preliminaries\n\nInstall the following package in R: {ggpubr}\nLoad {tidyverse}, {mosiac}, {car}, {broom}",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Elements of Regression Analysis</span>"
    ]
  },
  {
    "objectID": "19-module.html#analysis-of-variance-tables",
    "href": "19-module.html#analysis-of-variance-tables",
    "title": "19¬† Elements of Regression Analysis",
    "section": "19.3 Analysis of Variance Tables",
    "text": "19.3 Analysis of Variance Tables\nIn our linear models, we can separate or ‚Äúpartition‚Äù the total variation in our y variable (the sum of squares of y, or \\(SSY\\)) into that explained by our model (the regression sum of squares, or \\(SSR\\)) and that which is left over as ‚Äúerror‚Äù (the error sum of squares, or \\(SSE\\)):\n\\[SSY = SSR + SSE\\]\nGraphically‚Ä¶\n\n\n\n\n\n\n\n\n\nLet‚Äôs make sure we have our zombie apocalypse survivors dataset loaded‚Ä¶\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/zombies.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\n\nNow, we run a straightforward bivariate regression model and, using the raw data (which are duplicated in the $model data structure within our model object), we will calculate the various sums of squares of our variables and identify the numbers of degrees of freedom associated with each source of variation. This allows us to generate the ANOVA table for our model, which is a summary of how variance is partitioned among different sources.\n\nm &lt;- lm(data = d, height ~ weight)\n# height - mean(height)\nSSY &lt;- sum((m$model$height - mean(m$model$height))^2)\nSSY\n\n## [1] 18558.61\n\n# predicted height - mean height\nSSR &lt;- sum((m$fitted.values - mean(m$model$height))^2)\nSSR\n\n## [1] 12864.82\n\n# height - predicted height\nSSE &lt;- sum((m$model$height - m$fitted.values)^2)\nSSE\n\n## [1] 5693.785\n\n# or\nSSE &lt;- sum(m$residuals^2)\nSSE\n\n## [1] 5693.785\n\n\nFrom here, we can calculate the variance in each of these components, typically referred to as the mean square, by dividing each sum of squares by its corresponding degrees of freedom (recall that a variance can be thought of as an average ‚Äúsum of squares‚Äù).\nThe degrees of freedom for the regression sum of squares (\\(SSR\\)) is equal to the number of predictor variables (\\(p\\)), which in this case is one (given our regression equation, we need to know only one piece of information, the value of our predictor variable, in order to calculate the predicted value of the response variable). The number of degrees of freedom for the error sum of squares (\\(SSE\\)) is equal to \\(n-2\\) (or \\(n - p - 1\\)). This is because we need to estimate two parameters (\\(\\beta_0\\) and \\(\\beta_1\\)) from our data before we can calculate the error sum of squares. Finally, the number of degrees of freedom for the total sum of squares (\\(SSY\\)) is \\(n-1\\)‚Ä¶ we need to estimate one parameter from our data (the mean value of y) before we can calculate \\(SSY\\).\n\n(df_regression &lt;- 1)  # p = 1\n\n## [1] 1\n\n(df_error &lt;- nrow(d) - df_regression - 1)  # n - p - 1\n\n## [1] 998\n\n(df_y &lt;- nrow(d) - df_regression)  # n - p\n\n## [1] 999\n\nMSR &lt;- SSR/df_regression  # mean variance explained by the regression equation\nMSE &lt;- SSE/df_error  # mean remaining variance\nMSY &lt;- SSY/df_y  # mean overall variance\n\nThe last item we need to calculate is the F ratio, the ratio of the variance explained by the regression model to the remaining, unexplained variance: \\(MSR/MSE\\).\n\nfratio &lt;- MSR/MSE\nfratio\n\n## [1] 2254.931\n\n\nTogether, the values we have calculated above form the main entries in the ANOVA Table for our regression.\n\n\n\n\n\n\n\n\n\n\nSource\nSum of Squares\nDegrees of Freedom\nMean Square (SS/df)\nF ratio\n\n\n\n\nRegression\n\\(SSR\\) = 12864.82\n1\n\\(MSR\\) = 12864.82\n2254.139\n\n\nError\n\\(SSE\\) = 5693.79\n\\(n-2\\) = 998\n\\(MSE\\) = 5.7072\n\n\n\nTotal\n\\(SSY\\) = 18558.61\n\\(n-1\\) = 999\n\\(MSY\\) = 18.57719\n\n\n\n\nWe can then test the overall significance of our regression model by evaluating our F ratio test statistic against an \\(F\\) distribution, taking into account the number of degrees of freedom in each. The \\(F\\) distribution is a continuous probability distribution, defined for \\(x‚â•0\\) and governed by two parameters, \\(df1\\) and \\(df2\\). The critical value for the \\(F\\) statistic above which we would reject the idea that the variance in our two sources is comparable is given by qf(p, df1, df2), where p is 1-\\(\\alpha\\) and df1 and df2 are the degrees of freedom in the sources being compared (regression versus error).\n\nplotDist(\"f\", df1 = 1, df2 = 10, col = \"green\", lty = 3, lwd = 2, main = \"Some Example F Distributions\",\n    sub = \"red vertical line shows critical value\\n\n            for df1=1, df2=998\",\n    ylab = \"f(x)\", xlab = \"x\", xlim = c(0, 5), ylim = c(0, 1.5), key = list(space = \"right\",\n        text = list(c(\"df1=1, df2=1\", \"df1=2, df2=2\", \"df1=4, df2=4\", \"df1=8, df2=100\",\n            \"df1=1, df2=998\")), lines = list(col = c(\"green\", \"blue\", \"red\", \"purple\",\n            \"black\"), lty = c(3, 3, 3, 3, 1), lwd = 2, bty = \"n\", cex = 0.75)))\nplotDist(\"f\", df1 = 2, df2 = 2, col = \"blue\", lty = 3, lwd = 2, add = TRUE)\nplotDist(\"f\", df1 = 4, df2 = 4, col = \"red\", lty = 3, lwd = 2, add = TRUE)\nplotDist(\"f\", df1 = 8, df2 = 100, col = \"purple\", lty = 3, lwd = 2, add = TRUE)\nplotDist(\"f\", df1 = 1, df2 = 998, col = \"black\", lty = 1, lwd = 2, add = TRUE)\ncrit &lt;- qf(p = 0.95, df1 = 1, df2 = 998)\ncrit\n\n## [1] 3.850793\n\nladd(panel.abline(v = crit, col = \"red\", lty = 1, lwd = 1))\n\n\n\n\n\n\n\n\nThe qf() function allows us to calculate critical values under an \\(F\\) distribution given the number of degrees of freedom for the regression and error.\n\nplotDist(\"f\", df1 = 1, df2 = 998, main = \"df1 = 1, df2 = 998\", xlab = \"x\", ylab = \"f(x)\",\n    xlim = c(0, 5), ylim = c(0, 1.5))\ncrit &lt;- qf(p = 0.95, df1 = 1, df2 = 998)\ncrit\n\n## [1] 3.850793\n\nladd(panel.abline(v = crit, col = \"red\", lty = 1, lwd = 1))\nladd(panel.polygon(cbind(c(crit, seq(from = crit, to = 12, length.out = 1000), 12),\n    c(0, df(seq(from = crit, to = 12, length.out = 1000), df1 = 1, df2 = 998), 0)),\n    border = \"black\", col = rgb(0, 0, 1, 0.5)))\n\n\n\n\n\n\n\n\nFor our data, the value for the F ratio statistic well exceeds this critical value!\nAlternatively, we can use the following formulation to directly estimate a p value associated with our value for the F statistic. The p value is simply the area under the \\(F\\) distribution curve to the right of the F statistic value (or 1 minus the cumulative probability up to that point):\n\npf(q = fratio, df1 = 1, df2 = 998, lower.tail = FALSE)\n\n## [1] 2.646279e-258\n\n# or\n1 - pf(q = fratio, df1 = 1, df2 = 998)\n\n## [1] 0\n\n\n‚Ä¶ and we see that the p value associated with this high of an F statistic is infintessimally small.\nAs usual, R can handle all of the calculations above for easily. The aov() function, like the lm() function, returns a model object that we can use summary() on to look at the results we want. Alternatively, we can run the function summary.aov() using the model object resulting from lm() as an argument. In either case, the results returned are the same as we calculated by hand above.\n\na &lt;- aov(data = d, height ~ weight)\nsummary(a)\n\n##              Df Sum Sq Mean Sq F value Pr(&gt;F)    \n## weight        1  12865   12865    2255 &lt;2e-16 ***\n## Residuals   998   5694       6                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary.aov(m)\n\n##              Df Sum Sq Mean Sq F value Pr(&gt;F)    \n## weight        1  12865   12865    2255 &lt;2e-16 ***\n## Residuals   998   5694       6                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nRecall that the results returned by summary() of our regression model also shows the coefficient of determination, or the ‚ÄúR-squared value‚Äù, which we defined above as the fraction of the total variation explained by the model. We can calculate this value directly from our ANOVA table as simply \\(SSR/SSY\\). The correlation coefficient, \\(\\rho\\), between our response and predictor variable is simply the square root of this value.\n\nrsq &lt;- SSR/SSY\nrsq\n\n## [1] 0.6931998\n\nrho &lt;- sqrt(rsq)\nrho\n\n## [1] 0.8325862",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Elements of Regression Analysis</span>"
    ]
  },
  {
    "objectID": "19-module.html#standard-errors-of-coefficients",
    "href": "19-module.html#standard-errors-of-coefficients",
    "title": "19¬† Elements of Regression Analysis",
    "section": "19.4 Standard Errors of Coefficients",
    "text": "19.4 Standard Errors of Coefficients\nRecall that lm() returned the standard errors associated with each of the various components of our regression model, i.e., the slope and intercept and each predicted value of y. We can calculate standard errors directly to show how R is deriving them.\nThe formula for the standard error of the regression slope, \\(\\beta_1\\), is calculated as:\n\\[SE_{\\beta_1} = \\sqrt{\\frac{MSE}{SSX}}\\]\nUsing our data‚Ä¶\n\nSSX &lt;- sum((m$model$weight - mean(m$model$weight))^2)  # how much x variation there is\nSEbeta1 &lt;- sqrt(MSE/SSX)\nSEbeta1\n\n## [1] 0.004106858\n\n\nThe standard error of the intercept, \\(\\beta_0\\), is calculated as:\n\\[SE_{\\beta_0} = \\sqrt{\\frac{MSE \\times \\sum{x^2}}{n \\times SSX}} = SE_{\\beta_1} \\times \\sqrt{\\frac{\\sum{x^2}}{n}} \\]\n\nSEbeta0 &lt;- sqrt((MSE * sum(m$model$weight^2))/(1000 * SSX))\nSEbeta0\n\n## [1] 0.5958147\n\n\nFinally, the standard error of each predicted value of y is calculated as:\n\\[SE_{\\hat{y}} = \\sqrt{MSE \\times {\\biggl[\\frac{1}{n} + \\frac{(x-\\hat{x})^2}{SSX}}{\\biggr]}}\\]\n\nSEyhat &lt;- sqrt(MSE * (1/1000 + (m$model$weight - mean(m$model$weight))^2/SSX))\nhead(SEyhat)  # just the first 6 rows\n\n## [1] 0.08978724 0.07620966 0.08414480 0.09533986 0.08904151 0.08341218\n\n\nThese same standard errors for \\(\\beta_0\\) and \\(\\beta_1\\) are exactly what are returned by the lm() function.\n\nsummary(m)\n\n## \n## Call:\n## lm(formula = height ~ weight, data = d)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -7.1519 -1.5206 -0.0535  1.5167  9.4439 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 39.565446   0.595815   66.41   &lt;2e-16 ***\n## weight       0.195019   0.004107   47.49   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.389 on 998 degrees of freedom\n## Multiple R-squared:  0.6932, Adjusted R-squared:  0.6929 \n## F-statistic:  2255 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\nNote that the t statistic is calculated as simply the estimate value divided by the corresponding SE value, and the p value just comes from comparing that statistic to a \\(t\\) distribution with the appropriate degrees of freedom (i.e., number of observations - 2). We can run another model with higher p values to see this. Here, too, we use the convenient tidy() function from the {broom} package to pull out a table of results from our linear model very easily.\n\nm &lt;- lm(zombies_killed ~ age, data = d)\ncoefficients &lt;- tidy(m)\ncoefficients &lt;- coefficients |&gt;\n    mutate(t.calc = estimate/std.error)\ncoefficients &lt;- coefficients |&gt;\n    mutate(p.calc = 2 * (1 - pt(abs(t.calc), df = nrow(m$model) - 2)))\n# the p value is 2 times the tail probability implied by the t statistic\ncoefficients\n\n## # A tibble: 2 √ó 7\n##   term        estimate std.error statistic  p.value  t.calc   p.calc\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n## 1 (Intercept)  3.01       0.378     7.97   4.35e-15  7.97   4.44e-15\n## 2 age         -0.00111    0.0187   -0.0596 9.53e- 1 -0.0596 9.53e- 1",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Elements of Regression Analysis</span>"
    ]
  },
  {
    "objectID": "19-module.html#model-checking",
    "href": "19-module.html#model-checking",
    "title": "19¬† Elements of Regression Analysis",
    "section": "19.5 Model Checking",
    "text": "19.5 Model Checking\nSo far, we‚Äôve derived a bunch of summary statistics describing our model and we have looked at parametric ways of testing whether those summary statistics are significantly different from zero. That is‚Ä¶\n\nWe have seen whether our overall regression model explains a significant portion of the variance in y by means of the F ratio test.\nWe have calculated standard errors for our \\(\\beta_1\\) and \\(\\beta_0\\) estimates and seen whether they are significantly different from zero by means of t tests.\nWe have calculated standard errors for our prediction of y (i.e., \\(\\hat{y}\\)) at each value of x.\nWe have estimated the proportion of the total variance in y explained by our model (i.e., \\(R-squared\\)).\n\nWhat we have not done yet, however, is checked our model fit critically in other ways‚Ä¶ particularly, we have not checked whether two key assumptions of linear modeling are met: that our residuals (or errors) are normally distributed and that there is constancy of variance in our y values across the range of xs (or, ‚Äúhomoscedasticity‚Äù).\nWe can investigate our residuals as one way of assessing model fit.\n\nCHALLENGE\nCalculate the residuals from the regression of zombie apocalypse survivor height on weight and plot these in relation to weight (as the x variable). There are lots of ways to do this quickly.\n\n\nShow Code\nm &lt;- lm(data = d, height ~ weight)\ne &lt;- m$residuals\nplot(x = d$weight, y = e)\n\n\n\n\n\n\n\n\n\nShow Code\n# or we could use the function `resid()`\ne &lt;- resid(m)\nplot(x = d$weight, y = e)\n\n\nNow, plot a histogram of your residuals‚Ä¶ ideally they are normally distributed!\n\n\nShow Code\nhistogram(e, xlim = c(-4 * sd(e), 4 * sd(e)), breaks = 20, main = \"Histogram of Residuals\")\n\n\n\n\n\n\n\n\n\nAn additional way to quickly examine your residuals is to use the plot() function with your model as an argument. This prints out four plots that each tell you something.\n\npar(mfrow = c(2, 2))\nplot(m)\n\n\n\n\n\n\n\n\nThe first (top left) plot of ‚ÄúResiduals vs Fitted‚Äù values of y should, like the plot residuals versus x, not show any structure. We hope to see equally spread residuals around a horizontal line without distinct patterns.\nThe second (top right) plot is ‚ÄúNormal Q-Q‚Äù plot of theoretical quantiles versus standardized quantiles for the residual values. These should fall on roughly a straight line if the residuals are normally distributed.\nThe third (bottom left) plot (‚ÄúScale-Location‚Äù) is similar to the first, but graphs the square root of the standardized residuals versus fitted values of y and shows whether or the magnitude of residuals differs across the fitted values of y. Again, it is good if you see a horizontal line with equally spread points rather than a decrease or increase in spread across the range of fitted ys, which would indicate that the error variance increases or decreases across the relationship between y and x in your model.\nThe fourth (bottom right) plot (‚ÄúResiduals vs.¬†Leverage‚Äù) highlights whether there are any particular observations with disproportionate influence on the model. In particular, we look to see if there are cases that fall in the upper or lower right portion of the plot.\nWe can also do a QQ plot of our residuals‚Ä¶\n\nqqnorm(m$residuals)\nqqline(m$residuals)\n\n\n\n\n\n\n\n\nPerhaps more helpful QQ plots can be found in the {car} and {ggpubr} package. The functions qqPlot() and ggqqplot() provides a trend line and confidence intervals that allow us to see exactly which points make the sample fall outside of normality (if any). Let‚Äôs take a look:\n\nqqPlot(m, distribution = \"norm\", id = FALSE)  # qqPlot from {car}\n\n\n\n\n\n\n\n# `id=FALSE` means that outlier observations will not be labelled\nlibrary(ggpubr)\nggqqplot(m$residuals)\n\n\n\n\n\n\n\ndetach(package:ggpubr)\n\nFinally, there are a number of tests for normality that we can run within the R framework and using other packages. A Shapiro-Wilk Normality Test is perhaps the most widely used, where a low p value would indicate deviation from normality (technically, a measure of how far the trend line of the residuals deviates from the Q-Q plot line).\n\n(s &lt;- shapiro.test(m$residuals))\n\n## \n##  Shapiro-Wilk normality test\n## \n## data:  m$residuals\n## W = 0.99713, p-value = 0.07041\n\n\nAs you can see, although there are some points at the higher quantiles that suggest non-normality, the Shapiro-Wilk test suggests that it is not quite non-normal, so our use of parametric statistics should be okay.\nSome other popular tests for normality, and the cases in which they are best used, are listed below:\n\nAnderson-Darling test from the {nortest} package\n\nVery popular, not quite as powerful as Shapiro-Wilk\nBest used when \\(n\\) ‚â• 8\nnortest:: ad.test()\n\nMartinez-Iglewicz test from the {PoweR} package\n\nTests for dispersion from the median\nVery powerful for heavy-tailed distributions\nBest with small sample sizes (\\(n\\) ‚â• 3)\nPoweR::stat0032.MartinezIglewicz()\n\nKolmogorov-Smirnov test (with Lilliefors adjustment) from the {nortest} package\n\nNot as good as Anderson-Darling, but historically popular\nRequires that \\(n\\) ‚â• 4.\nnortest::lillie.test()\n\nD-Agostino Omnibus test (based on assessment of skew and kurtosis) from the {fBasics} package\n\nRobust against identical values in distribution\nSkewness test requires \\(n\\) ‚â• 8\nKurtosis test requires \\(n\\) ‚â• 20\nfBasics::dagoTest()\n\n\nFor a good discussion/demonstration of the relative power of each of these tests (meaning the probability that the test will correctly reject the null hypothesis) at different sample sizes, check out this link or this pdf, especially the tables on pages 670-8 and 670-9 and the plots on 670-10. This can help you better understand which test is best for a given sample size, and how much faith to put in these tests given your sample!\n\n\nCHALLENGE\nLoad in the ‚ÄúKamilarAndCooper.csv‚Äù dataset and develop a linear model to look at the relationship between ‚Äúweaning age‚Äù and ‚Äúfemale body mass‚Äù. You will probably need to look at the data and variable names again to find the appropriate variables to examine.\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/KamilarAndCooperData.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\nnames(d)\n\n##  [1] \"Scientific_Name\"         \"Superfamily\"            \n##  [3] \"Family\"                  \"Genus\"                  \n##  [5] \"Species\"                 \"Brain_Size_Species_Mean\"\n##  [7] \"Brain_Size_Female_Mean\"  \"Brain_size_Ref\"         \n##  [9] \"Body_mass_male_mean\"     \"Body_mass_female_mean\"  \n## [11] \"Mass_Dimorphism\"         \"Mass_Ref\"               \n## [13] \"MeanGroupSize\"           \"AdultMales\"             \n## [15] \"AdultFemale\"             \"AdultSexRatio\"          \n## [17] \"Social_Organization_Ref\" \"InterbirthInterval_d\"   \n## [19] \"Gestation\"               \"WeaningAge_d\"           \n## [21] \"MaxLongevity_m\"          \"LitterSz\"               \n## [23] \"Life_History_Ref\"        \"GR_MidRangeLat_dd\"      \n## [25] \"Precip_Mean_mm\"          \"Temp_Mean_degC\"         \n## [27] \"AET_Mean_mm\"             \"PET_Mean_mm\"            \n## [29] \"Climate_Ref\"             \"HomeRange_km2\"          \n## [31] \"HomeRangeRef\"            \"DayLength_km\"           \n## [33] \"DayLengthRef\"            \"Territoriality\"         \n## [35] \"Fruit\"                   \"Leaves\"                 \n## [37] \"Fauna\"                   \"DietRef1\"               \n## [39] \"Canine_Dimorphism\"       \"Canine_Dimorphism_Ref\"  \n## [41] \"Feed\"                    \"Move\"                   \n## [43] \"Rest\"                    \"Social\"                 \n## [45] \"Activity_Budget_Ref\"\n\nd &lt;- select(d, WeaningAge = \"WeaningAge_d\", FemaleBodyMass = \"Body_mass_female_mean\")\n# keep select columns\nd &lt;- na.omit(d)  # get rid of NAs\nggplot(data = d, aes(x = FemaleBodyMass, y = WeaningAge)) + geom_point() + geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\nUsing the procedures outlined above and in Module 18, calculate estimates of \\(\\beta_0\\) and \\(\\beta_1\\) by hand and by using the lm() function. Are the regression coefficients estimated under a simple linear model statistically significantly different from zero?\n\n\n\nShow Code\n# by hand\nbeta1 &lt;- cor(d$FemaleBodyMass, d$WeaningAge) * (sd(d$WeaningAge)/sd(d$FemaleBodyMass))\nbeta1\n\n\n## [1] 0.02012737\n\n\nShow Code\nbeta0 &lt;- mean(d$WeaningAge) - beta1 * mean(d$FemaleBodyMass)\nbeta0\n\n\n## [1] 201.634\n\n\nShow Code\n# using lm()\nm &lt;- lm(data = d, WeaningAge ~ FemaleBodyMass)\n\n\n\nConstruct an ANOVA table by hand and compare your values to the results of running lm() and then looking at summary.aov(lm()).\n\n\n# by hand\nSSY &lt;- sum((m$model$WeaningAge - mean(m$model$WeaningAge))^2)\nSSR &lt;- sum((m$fitted.values - mean(m$model$WeaningAge))^2)\nSSE &lt;- sum((m$model$WeaningAge - m$fitted.values)^2)\nDFR &lt;- 1\nDFE &lt;- nrow(d) - DFR - 1\nDFY &lt;- nrow(d) - DFR\nMSR &lt;- SSR/DFR\nMSE &lt;- SSE/DFE\nMSY &lt;- SSY/DFY\nfratio &lt;- MSR/MSE\np &lt;- 1 - pf(q = fratio, df1 = DFR, df2 = DFE)\n(aov_table &lt;- tibble(Source = c(\"Regression\", \"Error\", \"Total\"), df = c(DFR, DFE,\n    DFY), `Sum Sq` = c(SSR, SSE, SSY), `Mean Sq` = c(MSR, MSE, MSY), `F value` = c(fratio,\n    NA, NA), p = c(p, NA, NA)))\n\n## # A tibble: 3 √ó 6\n##   Source        df `Sum Sq` `Mean Sq` `F value`     p\n##   &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n## 1 Regression     1 3668116.  3668116.      109.     0\n## 2 Error        114 3833822.    33630.       NA     NA\n## 3 Total        115 7501938.    65234.       NA     NA\n\n# using summary.aov()\nsummary.aov(m)\n\n##                 Df  Sum Sq Mean Sq F value Pr(&gt;F)    \n## FemaleBodyMass   1 3668116 3668116   109.1 &lt;2e-16 ***\n## Residuals      114 3833822   33630                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nGenerate the residuals for your linear model, plot them in relation to female body mass, and make a histogram of the residuals. Do they appear to be normally distributed?\n\n\n\nShow Code\nresiduals &lt;- m$residuals\nplot(residuals ~ d$FemaleBodyMass, xlab = \"Female Body Mass\", ylab = \"Residuals\")\n\n\n\n\n\n\n\n\n\nShow Code\nhist(residuals, breaks = 20, main = \"Histogram of Residuals\")\n\n\n\n\n\n\n\n\n\n\nRun the plot() command on the result of lm() and examine the 4 plots produced.\n\n\n\nShow Code\npar(mfrow = c(2, 2))\nplot(m)\n\n\n\n\n\n\n\n\n\nAgain, based on examination of the residuals and the results of Shapiro-Wilks test, does it look like your model has good fit?\n\n\nShow Code\npar(mfrow = c(1, 1))\nqqnorm(m$residuals)\nqqline(m$residuals)\n\n\n\n\n\n\n\n\n\nShow Code\n(s &lt;- shapiro.test(m$residuals))\n\n\n## \n##  Shapiro-Wilk normality test\n## \n## data:  m$residuals\n## W = 0.86291, p-value = 5.825e-09",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Elements of Regression Analysis</span>"
    ]
  },
  {
    "objectID": "19-module.html#data-transformations",
    "href": "19-module.html#data-transformations",
    "title": "19¬† Elements of Regression Analysis",
    "section": "19.6 Data Transformations",
    "text": "19.6 Data Transformations\nRecall, again, that for linear regression modeling to be appropriate, two important conditions need to be met: [1] our variables (and the error variance in our variables) should be approximately normally distributed and [2] there should be homogeneity of variance (‚Äúhomoscedasticity‚Äù) in our response variable around the range of our predictor variable.\nIn many cases, these conditions may not be met‚Ä¶ for example, the continuous metric data we have may not, in fact, be normally distributed. Nonetheless, we can sometimes apply some kind of mathematical transformation to our data to change their distribution to more closely approximate the normal.\nThe logarithmic or ‚Äúlog‚Äù transformation (where we take the log value of each data point) is often applied to positive numeric variables with heavy skew to dramatically reduce the overall range of the data and bring extreme observations closer to a measure of centrality. The logarithm for a number is the power to which you must raise a base value (e.g., \\(e\\), the natural log) in order to obtain that number. This is an example of a ‚Äúpower transformation‚Äù, other examples of which include the square root transformation and the reciprocal (or multiplicative inverse) transformation.\n\nCHALLENGE\nReturn to the original ‚ÄúKamilarAndCooper.csv‚Äù dataset you were looking at above, log() transform both of your variables, and then run a simple, bivariate linear model.\nDo you notice a difference between these results and those obtained using untransformed variables?\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/KamilarAndCooperData.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\nnames(d)\n\n##  [1] \"Scientific_Name\"         \"Superfamily\"            \n##  [3] \"Family\"                  \"Genus\"                  \n##  [5] \"Species\"                 \"Brain_Size_Species_Mean\"\n##  [7] \"Brain_Size_Female_Mean\"  \"Brain_size_Ref\"         \n##  [9] \"Body_mass_male_mean\"     \"Body_mass_female_mean\"  \n## [11] \"Mass_Dimorphism\"         \"Mass_Ref\"               \n## [13] \"MeanGroupSize\"           \"AdultMales\"             \n## [15] \"AdultFemale\"             \"AdultSexRatio\"          \n## [17] \"Social_Organization_Ref\" \"InterbirthInterval_d\"   \n## [19] \"Gestation\"               \"WeaningAge_d\"           \n## [21] \"MaxLongevity_m\"          \"LitterSz\"               \n## [23] \"Life_History_Ref\"        \"GR_MidRangeLat_dd\"      \n## [25] \"Precip_Mean_mm\"          \"Temp_Mean_degC\"         \n## [27] \"AET_Mean_mm\"             \"PET_Mean_mm\"            \n## [29] \"Climate_Ref\"             \"HomeRange_km2\"          \n## [31] \"HomeRangeRef\"            \"DayLength_km\"           \n## [33] \"DayLengthRef\"            \"Territoriality\"         \n## [35] \"Fruit\"                   \"Leaves\"                 \n## [37] \"Fauna\"                   \"DietRef1\"               \n## [39] \"Canine_Dimorphism\"       \"Canine_Dimorphism_Ref\"  \n## [41] \"Feed\"                    \"Move\"                   \n## [43] \"Rest\"                    \"Social\"                 \n## [45] \"Activity_Budget_Ref\"\n\n# keep select columns\nd &lt;- select(d, WeaningAge = \"WeaningAge_d\", FemaleBodyMass = \"Body_mass_female_mean\")\nd &lt;- na.omit(d)  # get rid of NAs\nd$logWeaningAge &lt;- log(d$WeaningAge)\nd$logFemaleBodyMass &lt;- log(d$FemaleBodyMass)\nggplot(data = d, aes(x = logFemaleBodyMass, y = logWeaningAge)) + geom_point() +\n    geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n# or plot(data=d, logWeaningAge ~ logFemaleBodyMass)\nm &lt;- lm(data = d, logWeaningAge ~ logFemaleBodyMass)\nsummary(m)\n\n## \n## Call:\n## lm(formula = logWeaningAge ~ logFemaleBodyMass, data = d)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.10639 -0.32736  0.00848  0.32214  1.11010 \n## \n## Coefficients:\n##                   Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)         1.7590     0.2196   8.011 1.08e-12 ***\n## logFemaleBodyMass   0.4721     0.0278  16.983  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.4532 on 114 degrees of freedom\n## Multiple R-squared:  0.7167, Adjusted R-squared:  0.7142 \n## F-statistic: 288.4 on 1 and 114 DF,  p-value: &lt; 2.2e-16\n\npar(mfrow = c(2, 2))\nplot(m)\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))\nqqPlot(m$residuals)  ## qqPlot from {car}\n\n\n\n\n\n\n\n\n## [1]  24 116\n\n(s &lt;- shapiro.test(m$residuals))\n\n## \n##  Shapiro-Wilk normality test\n## \n## data:  m$residuals\n## W = 0.99367, p-value = 0.8793\n\n\nThe following chart and graphs shows some other common numerical transformations that are often useful for changing a variable‚Äôs distribution to more closely approximate the normal.\n\n\n\n\n\n\n\n\n\n\npar(mfrow = c(1, 2))\na &lt;- 2\nb &lt;- 2\n\n# LOG X\nx &lt;- seq(from = 0, to = 100, length.out = 1000)\ny &lt;- a + b * log(x)\nplot(x, y, type = \"l\", main = \"untransformed\")\nplot(log(x), y, type = \"l\", main = \"log(x)\")\n\n\n\n\n\n\n\n# LOG Y - a log transformation for the response variable is useful when the\n# variance in the explanatory variable is not consistent across the range of\n# explanatory variable values\nx &lt;- seq(from = 0, to = 10, length.out = 1000)\ny &lt;- exp(a + b * x)\nplot(x, y, type = \"l\", main = \"untransformed\")\nplot(x, log(y), type = \"l\", main = \"log(y)\")\n\n\n\n\n\n\n\n# ASYMPTOTIC\nx &lt;- seq(from = 1, to = 100, length.out = 100)\ny &lt;- (a * x)/(1 + b * x)\nplot(x, y, type = \"l\", main = \"untransformed\")\nplot(1/x, y, type = \"l\", main = \"1/x\")\n\n\n\n\n\n\n\n# RECIPROCAL\nx &lt;- seq(from = 1, to = 100, length.out = 100)\ny &lt;- a + b/x\nplot(x, y, type = \"l\", main = \"untransformed\")\nplot(1/x, y, type = \"l\", main = \"1/x\")\n\n\n\n\n\n\n\n# POWER - a power transformation for the explanatory variable is useful # if\n# there is a curvilinear relationship between the explanatory variable and the\n# response variable\nx &lt;- seq(from = 1, to = 100, length.out = 100)\ny &lt;- a * x^b\nplot(x, y, type = \"l\", main = \"untransformed\")\nplot(x^b, y, type = \"l\", main = \"x^b\")\n\n\n\n\n\n\n\n# EXPONENTIAL\nx &lt;- seq(from = 1, to = 10, length.out = 100)\ny &lt;- a * exp(b * x)\nplot(x, y, type = \"l\", main = \"untransformed\")\nplot(x, log(y), type = \"l\", main = \"log(y)\")\n\n\n\n\n\n\n\n\nApplying a function to transform a complete dataset is the most common way of attempting to squeeze your data into matching a normal distribution so that you can use parametric statistics. This is a very common practice, and much has been written on this topic, including its drawbacks.\nFor a good discussion on common transformations, see R in Action Chapter 8.5 (‚ÄúCorrective Measures‚Äù), and please do note the section titled A caution concerning transformations. When trying out transformations, keep your normalization tests handy so that you can retest the transformed data and see if the transformation achieved its intended purpose.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Elements of Regression Analysis</span>"
    ]
  },
  {
    "objectID": "19-module.html#concept-review",
    "href": "19-module.html#concept-review",
    "title": "19¬† Elements of Regression Analysis",
    "section": "Concept Review",
    "text": "Concept Review\n\nRegression analysis partitions the variance in a response variable into components associated with the various predictor variables plus an ‚Äúerror‚Äù component, or residual\nThe significance of the overall regression model, and of individual predictors, is evaluated by comparing F ratios (the ratio of the variance explained by the predictor to the unexplained variance) against an F distribution, taking into account the number of degrees of freedom in each\nAfter running a model, we need to confirm that key assumptions for linear regression are met, including that residuals are normally distributed and observations show homogeneity of variance across the range of predictor values\nIf these assumptions are not met, data transformation of the raw variables may sometimes be helpful",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Elements of Regression Analysis</span>"
    ]
  },
  {
    "objectID": "20-module.html",
    "href": "20-module.html",
    "title": "20¬† Categorical Data Analysis",
    "section": "",
    "text": "20.1 Objectives",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "20-module.html#objectives",
    "href": "20-module.html#objectives",
    "title": "20¬† Categorical Data Analysis",
    "section": "",
    "text": "In this module, we examine how simple linear regression can be applied to datasets where our predictor variable is discrete or categorical rather than continuous. Indeed, we will see that One-Way (and Two-Way) Analysis of Variance (ANOVA) is a specific application of simple (and multiple) linear regression. We also look at other methods for basic statistical analysis of categorical data.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "20-module.html#preliminaries",
    "href": "20-module.html#preliminaries",
    "title": "20¬† Categorical Data Analysis",
    "section": "20.2 Preliminaries",
    "text": "20.2 Preliminaries\n\nInstall these packages in R: {permuco}, {dunn.test}, {conover.test}\nInstall and load this package in R: {effectsize}\nLoad {tidyverse}, {broom}, and {infer}",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "20-module.html#categorical-predictors",
    "href": "20-module.html#categorical-predictors",
    "title": "20¬† Categorical Data Analysis",
    "section": "20.3 Categorical Predictors",
    "text": "20.3 Categorical Predictors\nThus far we have used simple linear regression models involving continuous explanatory variables, but we can also use a discrete or categorical explanatory variable, made up of 2 or more groups that are coded as ‚Äúfactors‚Äù (i.e., we use integer values from 1 to \\(k\\) discrete groups as dummy values for our categorical variables). Let‚Äôs load in our zombie apocalpyse survivor data again, but this time after doing so, we will convert gender and major to factors using the function as.factor(). [We could also do this by reading the data in using read.csv(), which has as a default argument stringsAsFactors=TRUE.] Then look at the class() and summary() of the variable gender.\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/zombies.csv\"\nz &lt;- read_csv(f, col_names = TRUE)\n\n## Rows: 1000 Columns: 10\n## ‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n## Delimiter: \",\"\n## chr (4): first_name, last_name, gender, major\n## dbl (6): id, height, weight, zombies_killed, years_of_education, age\n## \n## ‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n## ‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nz$gender &lt;- factor(z$gender)  # can also use `as.factor()`\nz$major &lt;- factor(z$major)  # can also use `as.factor()`\n\nclass(z$gender)\n\n## [1] \"factor\"\n\nsummary(z$gender)\n\n## Female   Male \n##    494    506\n\n\nAs with our prior simple regression analysis, we want to evaluate the effect of a predictor variable on a response variable (e.g., height), but this time we want our predictor to be a discrete, or categorical, variable (e.g., gender) rather than a continuous one. We can start off by plotting height by gender using the same formula notation we have been using.\n\nplot(z$height ~ z$gender)\n\n\n\n\n\n\n\n# or, with {ggplot}\nggplot(data = z, aes(x = gender, y = height)) + geom_boxplot()\n\n\n\n\n\n\n\n\nThis immediately gives us a nice boxplot. Note that if we‚Äôd try to use this command after loading in gender as character rather than factor data, R would have thrown an error.\nBased on our plot, there indeed seems to be a difference in height between males and females. We can test this directly using linear regression (and, recall, we already know another way to test this, using \\(Z\\) or t tests to compare means).\n\nm &lt;- lm(data = z, height ~ gender)\n\nIf we take a look at the summary() of our model, m, we see the same kind of table of results we have seen before, but because the predictor variable in this case is a factor vector instead of a numeric vector, the coefficients are reported and interpreted a bit differently.\n\nsummary(m)\n\n## \n## Call:\n## lm(formula = height ~ gender, data = z)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -15.4642  -2.4861   0.0876   2.5425  11.0065 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  65.5983     0.1717  382.13   &lt;2e-16 ***\n## genderMale    4.0154     0.2413   16.64   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.815 on 998 degrees of freedom\n## Multiple R-squared:  0.2172, Adjusted R-squared:  0.2164 \n## F-statistic: 276.9 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\nThe coefficient for the intercept, i.e., \\(\\beta_0\\), reflects the estimate of the mean height for the first of our level variables.\n\nlevels(z$gender)\n\n## [1] \"Female\" \"Male\"\n\n\nThe estimate for \\(\\beta_1\\) is reported as ‚ÄúgenderMale‚Äù and the value for that coefficient, 4.0154, is the estimated difference in mean height associated with being a male. The regression equation is basically:\n\\[height = 65.5983 + 4.0154 \\times gender\\]\nwith males assigned a gender value of 1 and females of 0.\nIn this case, the p value associated with the t statistic for \\(\\beta_1\\) is extremely low, so we conclude that gender has a significant effect on height.\nWe can easily relevel() what is the baseline group. The result is very similar, but the sign of \\(\\beta_1\\) is changed.\n\nz$gender &lt;- relevel(z$gender, ref = \"Male\")\nm &lt;- lm(data = z, height ~ gender)\nsummary(m)\n\n## \n## Call:\n## lm(formula = height ~ gender, data = z)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -15.4642  -2.4861   0.0876   2.5425  11.0065 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   69.6137     0.1696  410.42   &lt;2e-16 ***\n## genderFemale  -4.0154     0.2413  -16.64   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.815 on 998 degrees of freedom\n## Multiple R-squared:  0.2172, Adjusted R-squared:  0.2164 \n## F-statistic: 276.9 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\nThe last line of the summary() output shows the results of the global test of significance of the regression model based on an F statistic compared to an F distribution with, in this case, 1 and 998 degrees of freedom.\n\np &lt;- 1 - pf(q = 276.9, df1 = 1, df2 = 998)\np\n\n## [1] 0\n\n\nWe can extend this approach to the case where we have more than two categories for a variable‚Ä¶ in this case we need to dummy code our factor variable into multiple binary variables. R takes care of this for us automatically, but it is good to recognize the procedure.\nLet‚Äôs explore this by re-coding the variable major into four levels. We can first use the unique() or levels() function to list all of the different majors in our dataset. The latter does this alphabetically.\n\nunique(z$major)\n\n##  [1] medicine/nursing                      criminal justice administration      \n##  [3] education                             energy studies                       \n##  [5] logistics                             psychology                           \n##  [7] botany                                communication                        \n##  [9] military strategy                     economics                            \n## [11] mechanical engineering                physical education                   \n## [13] philosophy                            biology                              \n## [15] applied sciences                      animal husbandry                     \n## [17] agricultural sciences                 culinary services                    \n## [19] city planning                         integrated water resources management\n## [21] pharmacology                          environmental science                \n## [23] human services                        epidemiology                         \n## [25] business administration               architecture                         \n## 26 Levels: agricultural sciences animal husbandry ... psychology\n\nlevels(z$major)\n\n##  [1] \"agricultural sciences\"                \n##  [2] \"animal husbandry\"                     \n##  [3] \"applied sciences\"                     \n##  [4] \"architecture\"                         \n##  [5] \"biology\"                              \n##  [6] \"botany\"                               \n##  [7] \"business administration\"              \n##  [8] \"city planning\"                        \n##  [9] \"communication\"                        \n## [10] \"criminal justice administration\"      \n## [11] \"culinary services\"                    \n## [12] \"economics\"                            \n## [13] \"education\"                            \n## [14] \"energy studies\"                       \n## [15] \"environmental science\"                \n## [16] \"epidemiology\"                         \n## [17] \"human services\"                       \n## [18] \"integrated water resources management\"\n## [19] \"logistics\"                            \n## [20] \"mechanical engineering\"               \n## [21] \"medicine/nursing\"                     \n## [22] \"military strategy\"                    \n## [23] \"pharmacology\"                         \n## [24] \"philosophy\"                           \n## [25] \"physical education\"                   \n## [26] \"psychology\"\n\n\nWe can then also do some cool batch recoding using the %in% operator and the mutate() function from {dplyr}‚Ä¶\n\nz &lt;- z |&gt;\n    mutate(occupation = case_when(major %in% c(\"agricultural sciences\", \"animal husbandry\",\n        \"applied sciences\", \"biology\", \"botany\", \"energy studies\", \"environmental science\",\n        \"epidemiology\", \"medicine/nursing\", \"pharmacology\") ~ \"natural science\",\n        major %in% c(\"business administration\", \"city planning\", \"economics\", \"human services\",\n            \"logistics\", \"military strategy\") ~ \"logistics\", major %in% c(\"architecture\",\n            \"integrated water resources management\", \"mechanical engineering\") ~\n            \"engineering\", major %in% c(\"communication\", \"criminal justice administration\",\n            \"culinary services\", \"education\", \"philosophy\", \"physical education\",\n            \"psychology\") ~ \"other\"))\nz$occupation &lt;- factor(z$occupation)  # can also use `as.factor()`\nlevels(z$occupation)\n\n## [1] \"engineering\"     \"logistics\"       \"natural science\" \"other\"\n\nz$occupation &lt;- relevel(z$occupation, ref = \"natural science\")\nlevels(z$occupation)\n\n## [1] \"natural science\" \"engineering\"     \"logistics\"       \"other\"\n\n\nAgain, we can plot our variable by group and run a multilevel linear regression. Each \\(\\beta\\) estimate reflects the difference from the estimated mean for the reference level. The lm() function also returns the results of the global significance test of our model.\n\nplot(data = z, zombies_killed ~ occupation)\n\n\n\n\n\n\n\nm &lt;- lm(data = z, zombies_killed ~ occupation)\nsummary(m)\n\n## \n## Call:\n## lm(formula = zombies_killed ~ occupation, data = z)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.3061 -1.0068 -0.0068  1.0092  8.0974 \n## \n## Coefficients:\n##                       Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)            2.90256    0.08848  32.804   &lt;2e-16 ***\n## occupationengineering  0.40356    0.19745   2.044   0.0412 *  \n## occupationlogistics    0.08826    0.14777   0.597   0.5504    \n## occupationother        0.10424    0.13496   0.772   0.4401    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.747 on 996 degrees of freedom\n## Multiple R-squared:  0.004209,   Adjusted R-squared:  0.00121 \n## F-statistic: 1.403 on 3 and 996 DF,  p-value: 0.2403\n\np &lt;- 1 - pf(q = 0.489, df1 = 3, df2 = 996)  # F test\np\n\n## [1] 0.6899872\n\n\nIn this case, we see no significant effect of occupation (based on a broad categorization of college major) on zombie killing proficiency.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "20-module.html#one-way-anova",
    "href": "20-module.html#one-way-anova",
    "title": "20¬† Categorical Data Analysis",
    "section": "20.4 One-Way ANOVA",
    "text": "20.4 One-Way ANOVA\nRegression with a single categorical predictor run as we have just done above is exactly equivalent to a ‚ÄúOne-Way‚Äù or ‚Äúone-factor‚Äù Analysis of Variance, or ANOVA. That is, ANOVA is just one type of special case of least squares regression.\nWe can, of course, run an ANOVA with one line in R. Compare the results presented in the summary() output table (or tidy() output table) from an ANOVA with that from the global test reported in summary() (or tidy()) from lm()\n\nm.aov &lt;- aov(data = z, zombies_killed ~ occupation)\nsummary(m.aov)\n\n##              Df Sum Sq Mean Sq F value Pr(&gt;F)\n## occupation    3   12.9   4.285   1.403   0.24\n## Residuals   996 3041.1   3.053\n\ntidy(m.aov)  # a neater table\n\n## # A tibble: 2 √ó 6\n##   term          df  sumsq meansq statistic p.value\n##   &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n## 1 occupation     3   12.9   4.28      1.40   0.240\n## 2 Residuals    996 3041.    3.05     NA     NA\n\nm.lm &lt;- lm(data = z, zombies_killed ~ occupation)\nsummary(m.lm)\n\n## \n## Call:\n## lm(formula = zombies_killed ~ occupation, data = z)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.3061 -1.0068 -0.0068  1.0092  8.0974 \n## \n## Coefficients:\n##                       Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)            2.90256    0.08848  32.804   &lt;2e-16 ***\n## occupationengineering  0.40356    0.19745   2.044   0.0412 *  \n## occupationlogistics    0.08826    0.14777   0.597   0.5504    \n## occupationother        0.10424    0.13496   0.772   0.4401    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.747 on 996 degrees of freedom\n## Multiple R-squared:  0.004209,   Adjusted R-squared:  0.00121 \n## F-statistic: 1.403 on 3 and 996 DF,  p-value: 0.2403\n\ntidy(m.lm)  # a neater table\n\n## # A tibble: 4 √ó 5\n##   term                  estimate std.error statistic   p.value\n##   &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)             2.90      0.0885    32.8   1.27e-160\n## 2 occupationengineering   0.404     0.197      2.04  4.12e-  2\n## 3 occupationlogistics     0.0883    0.148      0.597 5.50e-  1\n## 4 occupationother         0.104     0.135      0.772 4.40e-  1\n\npar(mfrow = c(2, 2))\nplot(m.lm)  # plot(m.aov) is equivalent\n\n\n\n\n\n\n\n\nThese two model summaries produce somewhat different tables for the same analysis. The F statistic and omnibus p value given in the aov() ANOVA table indicates whether there are differences between at least some treatments, but we do not know where those differences occur. The summary from lm() shows the effects of the categorical treatments arranged as sequential contrasts (the default in R). The first row gives the mean of the first level of the first factor specified in the model. The following rows give differences from this first mean for each subsequent factor level. Likewise the standard error in the first row is a standard error of a mean, while the entries in subsequent rows are standard errors of the differences between two means.\nIn general, in ANOVA and simple regression using a single categorical variable, we aim to test the \\(H_0\\) that the means of a variable of interest do not differ among groups, i.e., that \\(\\mu_1 = \\mu_2 = ... = \\mu_k\\) are all equal. This is an extension of our comparison of two means that we did with z and t tests.\nThe basic linear model formulation for ANOVA is:\n\\[Y_{i,j} = \\mu + \\beta_i X_i + \\epsilon_{i,j}\\]\nwhere:\n\n\\(\\mu\\) is the grand (overall) population mean\n\\(\\beta_i\\) is the deviation of the mean of treatment level \\(i\\) from the grand mean\n\\(\\epsilon_{i,j}\\) is error variance of individual points, \\(j\\), within each level, \\(i\\), from the grand mean\n\nThe assumptions of ANOVA, similar to those of simple regression, are:\n\nthat samples are independent and identically distributed (\\(iid\\))\nthat the residuals, \\(\\epsilon_{i,j}\\), are normally distributed\nthat within-group variances are similar across all groups (‚Äúhomoscedastic‚Äù)\n\nAdditionally, the following assumption of standard ANOVA makes the interpretation of results more straightforward, but it is not strictly required‚Ä¶\n\nthat our experiment/observations have a balanced design (i.e., an equal number of cases in all groups)\n\nIf this last assumption is violated, it ceases to be true that the total SS of our dataset = within group SS + between group SS, and then the calculations of our MSE and the F statistic, and our associated p value, would be off.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "20-module.html#simulation-based-inference",
    "href": "20-module.html#simulation-based-inference",
    "title": "20¬† Categorical Data Analysis",
    "section": "20.5 Simulation-Based Inference",
    "text": "20.5 Simulation-Based Inference\nNote that the ANOVA functions used above (aov(), lm()) are evaluating statistical significance and estimating p values based on mathematical theory, i.e., by comparing F statistic values to a theoretical, parametric \\(F\\) distribution to determine how likely we are to see an F statistic as extreme as our observed one by chance under the null model of no difference between groups. However, as with other tests of statistical inference, we can also use permutation/randomization methods to determine p values for ANOVA.\nOne easy way to do this is with the {infer} package. Using {infer}, we can easily permute, a large number of times, the association between our response variable the categorical explanatory variable and, each time, then calculate the associated F statistic. This generates a permutation-based null distribution of F statistics to which we can compare the F statistic generated from our original data. The permutation-based p value is then the proportion of F statistic values generated by simulation that are equal to or greater than our observed F statistic value.\n\noriginal.F &lt;- aov(data = z, zombies_killed ~ occupation) |&gt;\n    tidy() |&gt;\n    filter(term == \"occupation\")\n# show aov results for F statistic and p value for omnibus F test\noriginal.F\n\n## # A tibble: 1 √ó 6\n##   term          df sumsq meansq statistic p.value\n##   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n## 1 occupation     3  12.9   4.28      1.40   0.240\n\npermuted.F &lt;- z |&gt;\n    # specify model\nspecify(zombies_killed ~ occupation) |&gt;\n    # null hypothesis of independence\nhypothesize(null = \"independence\") |&gt;\n    # generate permutations\ngenerate(reps = 1000, type = \"permute\") |&gt;\n    # calculate the F statistic for the AOV\ncalculate(stat = \"F\")\n\n# plot our F statistic on the distribution of F statistic values generated by\n# permutation\nvisualize(permuted.F) + shade_p_value(obs_stat = original.F$statistic, direction = \"greater\")\n\n\n\n\n\n\n\n(p.value &lt;- permuted.F |&gt;\n    get_p_value(obs_stat = original.F$statistic, direction = \"greater\"))\n\n## # A tibble: 1 √ó 1\n##   p_value\n##     &lt;dbl&gt;\n## 1   0.222\n\n\nNote that, for these data, the p value estimated through simulated based inference is very similar to that calculated paramterically based on mathematical theory. This is not always the case!\n\noriginal.F$p.value\n\n## [1] 0.2403441\n\np.value\n\n## # A tibble: 1 √ó 1\n##   p_value\n##     &lt;dbl&gt;\n## 1   0.222\n\n\nAlternatively, the {permuco} package introduces the aovperm() and lmperm() functions as analogs for aov() and lm() for analysis of variance and simple linear regression, where p values are calculated by permutation.\n\nlibrary(permuco)\nm.aovperm &lt;- aovperm(data = z, zombies_killed ~ occupation)\nsummary(m.aovperm)\n\n## Anova Table\n## Resampling test using freedman_lane to handle nuisance variables and 5000 permutations.\n##                 SS  df     F parametric P(&gt;F) resampled P(&gt;F)\n## occupation   12.85   3 1.403           0.2403           0.245\n## Residuals  3041.08 996\n\nplot(m.aovperm)\n\n\n\n\n\n\n\nm.lmperm &lt;- lmperm(data = z, zombies_killed ~ occupation)\nsummary(m.lmperm)\n\n## Table of marginal t-test of the betas\n## Resampling test using freedman_lane to handle nuisance variables and 5000 permutations.\n##                       Estimate Std. Error t value parametric Pr(&gt;|t|)\n## (Intercept)            2.90256    0.08848 32.8042          1.269e-160\n## occupationengineering  0.40356    0.19745  2.0439           4.123e-02\n## occupationlogistics    0.08826    0.14777  0.5973           5.504e-01\n## occupationother        0.10424    0.13496  0.7724           4.401e-01\n##                       resampled Pr(&lt;t) resampled Pr(&gt;t) resampled Pr(&gt;|t|)\n## (Intercept)                                                               \n## occupationengineering           0.9790           0.0212             0.0396\n## occupationlogistics             0.7232           0.2770             0.5462\n## occupationother                 0.7848           0.2154             0.4440\n\ndetach(package:permuco)\n\nSimilar to {infer}, the {coin} package also implements permutation-based tests of independence, including One-Way ANOVA. Instead of F statistics, the test statistic implemented in {coin} is a Chi-Square statistic (which is essentially to equivalent to an F statistic, normalized by the residual degrees of freedom), and it is tested against a permutation-based null distribution of Chi-Square values.\n\nlibrary(coin)\nm.aov &lt;- oneway_test(data = z, zombies_killed ~ occupation, distribution = \"approximate\")\n# by default, 10000 replicates are used\nm.aov\n\n## \n##  Approximative K-Sample Fisher-Pitman Permutation Test\n## \n## data:  zombies_killed by\n##   occupation (natural science, engineering, logistics, other)\n## chi-squared = 4.2048, p-value = 0.2501\n\ndetach(package:coin)\n\n\nCHALLENGE\nLoad in the ‚Äúgibbon-femurs.csv‚Äù dataset, which contains the lengths, in centimeters, of the femurs of 400 juvenile, subadult, and adult individuals gibbons. Use both ANOVA and simple linear regression to examine the relationship between age and femur length.\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/gibbon-femurs.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\n\n## Rows: 525 Columns: 4\n## ‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n## Delimiter: \",\"\n## chr (2): age, sex\n## dbl (2): id, femur_length\n## \n## ‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n## ‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nd$age &lt;- factor(d$age, levels = c(\"inf\", \"juv\", \"subadult\", \"adult\"))\n# converts age to a factor and order the age levels so that they are in\n# ascending chronological order several ANOVA functions require that\n# categorical variables are represented as factors rather than strings\nd$sex &lt;- factor(d$sex, levels = c(\"female\", \"male\"))\n# convert sex to a factor\nhead(d)\n\n## # A tibble: 6 √ó 4\n##      id age   sex    femur_length\n##   &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;         &lt;dbl&gt;\n## 1     1 inf   female         7.28\n## 2     2 inf   male           6.3 \n## 3     3 inf   female         7.44\n## 4     4 inf   male           8.69\n## 5     5 inf   female         9.13\n## 6     6 inf   male           9.28\n\n\nBefore beginning, check for normality of data overall‚Ä¶\n\n\nCode\npar(mfrow = c(1, 2))\nhist(d$femur_length)\nqqnorm(d$femur_length)\n\n\n\n\n\n\n\n\n\nCode\n# not great overall...\n\n\nThen, do a boxplot of the data by each age group and check for normality within each group.\n\n\nShow Code\nplot(data = d, femur_length ~ age)  # boxplot with medians\n\n# calculate average and SD by group\nstats &lt;- d |&gt;\n    group_by(age) |&gt;\n    summarize(`mean(femur_length)` = mean(femur_length), `sd(femur_length)` = sd(femur_length))\n\n# add means to plot\npoints(1:4, stats$`mean(femur_length)`, pch = 4, cex = 1.5)\n\n\n\n\n\n\n\n\n\nShow Code\n# subtract relevant group mean from each data point\nmeans.centered &lt;- d$femur_length - stats[as.numeric(d$age), 2]\n\n# graphical test for normality of group-centered means\nqqnorm(means.centered$`mean(femur_length)`)  # looks good!\n\n\n\n\n\n\n\n\n\nShow Code\n# now do graphical test within each group\npar(mfrow = c(1, 2))\nhist(d$femur_length[d$age == \"inf\"], main = \"Infant\", xlab = \"Femur Length (cm)\")\nqqnorm(d$femur_length[d$age == \"inf\"])\n\n\n\n\n\n\n\n\n\nShow Code\nhist(d$femur_length[d$age == \"juv\"], main = \"Juvenile\", xlab = \"Femur Length (cm)\")\nqqnorm(d$femur_length[d$age == \"juv\"])\n\n\n\n\n\n\n\n\n\nShow Code\nhist(d$femur_length[d$age == \"subadult\"], main = \"Subadult\", xlab = \"Femur Length (cm)\")\nqqnorm(d$femur_length[d$age == \"subadult\"])\n\n\n\n\n\n\n\n\n\nShow Code\nhist(d$femur_length[d$age == \"adult\"], main = \"Adult\", xlab = \"Femur Length (cm)\")\nqqnorm(d$femur_length[d$age == \"adult\"])\n\n\n\n\n\n\n\n\n\nAlso, check whether the different groups have roughly equal variances. As a general rule of thumb, we can compare the largest and smallest within-group standard deviations; if the ratio of max-to-min is less, than 2 then the assumption of ‚Äúequal‚Äù variances is usually reasonable.\n\n\nShow Code\n# check that variances are roughly equal (ratio of max/min is &lt;2)\nmax(stats$`sd(femur_length)`)/min(stats$`sd(femur_length)`)\n\n\nShow Output\n## [1] 1.767635\n\n\n\nAll this checking done, we can again plot our data by group and then run our linear or ANOVA model‚Ä¶\n\npar(mfrow = c(1, 1))\nplot(data = d, femur_length ~ age, xlab = \"Age\", ylab = \"Femur Length (cm)\")\n\n\n\n\n\n\n\nm &lt;- lm(data = d, femur_length ~ age)\nsummary(m)\n\n## \n## Call:\n## lm(formula = femur_length ~ age, data = d)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.2052 -1.1104  0.1089  1.0396  5.6748 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   7.7104     0.1180   65.32   &lt;2e-16 ***\n## agejuv        2.3406     0.1803   12.98   &lt;2e-16 ***\n## agesubadult   6.0460     0.2045   29.57   &lt;2e-16 ***\n## ageadult      9.7147     0.2260   42.98   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.669 on 521 degrees of freedom\n## Multiple R-squared:  0.809,  Adjusted R-squared:  0.8079 \n## F-statistic: 735.8 on 3 and 521 DF,  p-value: &lt; 2.2e-16\n\nm.aov &lt;- aov(data = d, femur_length ~ age)  # femur length related to age\nsummary(m.aov)\n\n##              Df Sum Sq Mean Sq F value Pr(&gt;F)    \n## age           3   6152  2050.6   735.8 &lt;2e-16 ***\n## Residuals   521   1452     2.8                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIs the omnibus test of the relationship between age category and femur length significant? YES! Are femur lengths significantly different for juveniles versus subadults? Subadults versus adults? Juveniles versus adults?\n\nHINT: To test some of these additional bivariate options, you will need to relevel() your factors for simple linear regression. Currently, we are just testing each group relative to the first level of the factor age.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "20-module.html#post-hoc-tests-in-anova",
    "href": "20-module.html#post-hoc-tests-in-anova",
    "title": "20¬† Categorical Data Analysis",
    "section": "20.6 Post-Hoc Tests in ANOVA",
    "text": "20.6 Post-Hoc Tests in ANOVA\nAfter finding a significant omnibus F statistic in an ANOVA, we can test, post-hoc, what group means are different from one another using pairwise t tests with an appropriate p value correction for multiple tests.\n\npairwise.t.test(d$femur_length, d$age, p.adj = \"bonferroni\")\n\n## \n##  Pairwise comparisons using t tests with pooled SD \n## \n## data:  d$femur_length and d$age \n## \n##          inf    juv    subadult\n## juv      &lt;2e-16 -      -       \n## subadult &lt;2e-16 &lt;2e-16 -       \n## adult    &lt;2e-16 &lt;2e-16 &lt;2e-16  \n## \n## P value adjustment method: bonferroni\n\n\nAfter an ANOVA, we can also use a Tukey Honest Significant Differences test with the model as the argument to evaluate this.\n\nNOTE: The TukeyHSD() function is run on the output of the ANOVA (aov()) function.\n\n\n# renaming age categories so we get nice axis labels on plot...\nd &lt;- d |&gt;\n    mutate(age = case_when(age == \"adult\" ~ \"A\", age == \"subadult\" ~ \"S\", age ==\n        \"juv\" ~ \"J\", age == \"inf\" ~ \"I\"))\nm &lt;- aov(femur_length ~ age, data = d)\nposthoc &lt;- TukeyHSD(m, which = \"age\", ordered = TRUE, conf.level = 0.95)\nposthoc  # all age-sex classes differ\n\n##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n##     factor levels have been ordered\n## \n## Fit: aov(formula = femur_length ~ age, data = d)\n## \n## $age\n##         diff      lwr       upr p adj\n## J-I 2.340617 1.875875  2.805359     0\n## S-I 6.045950 5.518982  6.572918     0\n## A-I 9.714750 9.132165 10.297335     0\n## S-J 3.705333 3.149860  4.260806     0\n## A-J 7.374133 6.765643  7.982623     0\n## A-S 3.668800 3.011556  4.326044     0\n\n\nWe can get a visual summary of difference in means between each group and their confidence intervals by passing the Tukey test output to a plot() function. Confidence intervals that cross the vertical line indicate pairs of groups where the difference in mean is not significant, according to the threshold we set.\n\nplot(posthoc, xlim = c(-2, 12))  # xlim set to show zero",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "20-module.html#a-non-parametric-alternative",
    "href": "20-module.html#a-non-parametric-alternative",
    "title": "20¬† Categorical Data Analysis",
    "section": "20.7 A Non-Parametric Alternative",
    "text": "20.7 A Non-Parametric Alternative\nThe Kruskal-Wallis test is a nonparametric alternative to One-Way ANOVA that relaxes the need for normality in the distribution of data in each group (the different groups should still have roughly equal variances). Essentially, rather than testing the null hypothesis that the means for each group do not differ we are instead testing the null hypothesis that the medians do not differ. The test converts the continuous response variable to a set of RANKS (i.e., it does a uniform transformation) and then works with those ranks. The p value associated with the Kruskall-Wallis test statistic is calculated mathematically relative to a Chi-Square distribution.\n\n(m.kruskal &lt;- kruskal.test(data = d, femur_length ~ age))\n\n## \n##  Kruskal-Wallis rank sum test\n## \n## data:  femur_length by age\n## Kruskal-Wallis chi-squared = 407.13, df = 3, p-value &lt; 2.2e-16\n\n# to show that this is the same as the test using ranks...  use {dplyr} to sort\n# by femur.length...\nd &lt;- arrange(d, femur_length)\n# then use {dplyr} to add new variable of rank femur_length\nd &lt;- mutate(d, femur_rank = row(data.frame(d$femur_length)))\n(m.kruskal &lt;- kruskal.test(data = d, femur_rank ~ age))\n\n## \n##  Kruskal-Wallis rank sum test\n## \n## data:  femur_rank by age\n## Kruskal-Wallis chi-squared = 407.43, df = 3, p-value &lt; 2.2e-16\n\n\nFor a post-hoc test of which groups are different following a Kruskall-Wallis test, we can use either a pairwise Mann-Whitney U test, a Dunn test, or a Conover-Iman test. The latter two tests are implemented using dunn.test() from the {dunn.test} package or conover.test() from the {conover.test} package, which both allow us to show the calculated Krusal-Wallis test statistic as well.\n\n# pairwise Mann-Whitney U test the arguments are a vector of data, a vector\n# with the grouping factor, and the multiple test correction method...\npairwise.wilcox.test(d$femur_length, d$age, p.adjust.method = \"bonferroni\")\n\n## \n##  Pairwise comparisons using Wilcoxon rank sum test with continuity correction \n## \n## data:  d$femur_length and d$age \n## \n##   A      I      J     \n## I &lt;2e-16 -      -     \n## J &lt;2e-16 &lt;2e-16 -     \n## S &lt;2e-16 &lt;2e-16 &lt;2e-16\n## \n## P value adjustment method: bonferroni\n\n# `dunn.test()` and `conover.test()` include another argument, `kw=` which is\n# whether or not to output KW results\nlibrary(dunn.test)\ndunn.test(d$femur_length, g = d$age, method = \"bonferroni\", kw = TRUE)\n\n##   Kruskal-Wallis rank sum test\n## \n## data: x and group\n## Kruskal-Wallis chi-squared = 407.1264, df = 3, p-value = 0\n## \n## \n##                            Comparison of x by group                            \n##                                  (Bonferroni)                                  \n## Col Mean-|\n## Row Mean |          A          I          J\n## ---------+---------------------------------\n##        I |   17.43010\n##          |    0.0000*\n##          |\n##        J |   10.06368  -8.673333\n##          |    0.0000*    0.0000*\n##          |\n##        S |   3.468151  -14.94416  -6.920645\n##          |    0.0016*    0.0000*    0.0000*\n## \n## alpha = 0.05\n## Reject Ho if p &lt;= alpha/2\n\ndetach(package:dunn.test)\nlibrary(conover.test)\nconover.test(d$femur_length, g = d$age, method = \"bonferroni\", kw = TRUE)\n\n##   Kruskal-Wallis rank sum test\n## \n## data: x and group\n## Kruskal-Wallis chi-squared = 407.1264, df = 3, p-value = 0\n## \n## \n##                            Comparison of x by group                            \n##                                  (Bonferroni)                                  \n## Col Mean-|\n## Row Mean |          A          I          J\n## ---------+---------------------------------\n##        I |   36.80108\n##          |    0.0000*\n##          |\n##        J |   21.24798  -18.31245\n##          |    0.0000*    0.0000*\n##          |\n##        S |   7.322488  -31.55238  -14.61191\n##          |    0.0000*    0.0000*    0.0000*\n## \n## alpha = 0.05\n## Reject Ho if p &lt;= alpha/2\n\ndetach(package:conover.test)\n\n\n20.7.1 Effect Sizes\nWe can apply the effectsize() function from the {effectsize} package to our model objects to better interpret the meaning of our coefficients.\nFor an ANOVA, the effect sizes represent the amount of variance in the response variable explained by each of the model‚Äôs terms. For this model, we are asking what percent of the total variance in femur length is explained by the categorical variable of age class. This measure is called \\(\\eta^2\\) (‚Äúeta-squared‚Äù) and is analogous to r-squared.\n\\[\\eta^2 = \\frac{SS_{predictor}}{SS_{total}} = \\frac{SS_{predictor}}{SS_{predictor} + SS_{residuals}}\\]\n\neffectsize(m.aov)\n\n## For one-way between subjects designs, partial eta squared is equivalent\n##   to eta squared. Returning eta squared.\n\n\n## # Effect Size for ANOVA\n## \n## Parameter | Eta2 |       95% CI\n## -------------------------------\n## age       | 0.81 | [0.79, 1.00]\n## \n## - One-sided CIs: upper bound fixed at [1.00].\n\n# or `eta_squared(m.aov)` we can hand-calculate this same value from the ANOVA\n# table as 6152/(6152 + 1452)\n\nHere, 81% of the variance in femur length is accounted for by variance in age.\nApplied to a linear model object, the effectsize() function refits the model and returns standardized coefficients rather than unstandardized ones. This is equivalent to running the original linear model after standardizing all of the variables so that they are represented on the same scale, which is typically done via a \\(Z\\) transformation (i.e., by subtracting its mean from each variable observation and divide by its standard deviation): \\(z_i = \\frac{x_i - \\mu_x}{SD_x}\\)\n\neffectsize(m)\n\n## For one-way between subjects designs, partial eta squared is equivalent\n##   to eta squared. Returning eta squared.\n\n\n## # Effect Size for ANOVA\n## \n## Parameter | Eta2 |       95% CI\n## -------------------------------\n## age       | 0.81 | [0.79, 1.00]\n## \n## - One-sided CIs: upper bound fixed at [1.00].\n\n\n\nNOTE: The {datawizard} package provides the function standardize(), which will take a model and refit it using standardized parameters and return a refitted model object. The {parameters} package function standardize_parameters() will also return standardized coefficients when passed a model object. These two function are designed to replace the effectsize() function from the {effectsize} package.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "20-module.html#multiple-factor-anova",
    "href": "20-module.html#multiple-factor-anova",
    "title": "20¬† Categorical Data Analysis",
    "section": "20.8 Multiple Factor ANOVA",
    "text": "20.8 Multiple Factor ANOVA\nSometimes the data we are interested in is characterized by multiple grouping variables (e.g., age and sex). In the case of the gibbon femur length data, we are interested in the main effect of each factor on the variable of interest (e.g., do femur lengths vary by age or sex) while accounting for the effects of the other factor. We may also be interested in any interactive effects among factors. Thus, in multiple factor ANOVA we are interested in testing several null hypotheses simultaneously: [1] that each factor has no effect on the mean of our continuous response variable and [2] that there are no interactive effects of sets of factors on the mean of our continuous response variable.\nModel description and testing for multiple-factor ANOVA is a simple extension of the formula notation which we‚Äôve used for single factors. First, though, let‚Äôs quickly check that our groups have similar variance.\n\nstats &lt;- d |&gt;\n    group_by(age, sex) |&gt;\n    summarize(`mean(femur_length)` = mean(femur_length), `sd(femur_length)` = sd(femur_length))\n# first we calculate averages by combination of factors\nmax(stats$`sd(femur_length)`)/min(stats$`sd(femur_length)`)\n\n## [1] 1.882171\n\n# check that variances in each group are roughly equal (ratio of max/min is &lt;2)\np &lt;- ggplot(data = d, aes(y = femur_length, x = sex)) + geom_boxplot() + facet_wrap(~age,\n    ncol = 4) + xlab(\"Sex\") + ylab(\"Femur Length (cm)\")\n# and let's plot what the data look like p &lt;- p + geom_point() # uncommenting\n# this shows all points\np &lt;- p + stat_summary(data = d, aes(y = femur_length, x = sex), fun = base::mean,\n    color = \"darkgreen\", geom = \"point\", shape = 8, size = 6)\n# make sure we use {base} version of mean\np\n\n\n\n\n\n\n\n\nIf we look at each variable separately using ANOVA, we see there is an effect of age but not of sex.\n\nsummary(aov(data = d, femur_length ~ age))\n\n##              Df Sum Sq Mean Sq F value Pr(&gt;F)    \n## age           3   6152  2050.6   735.8 &lt;2e-16 ***\n## Residuals   521   1452     2.8                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(aov(data = d, femur_length ~ sex))\n\n##              Df Sum Sq Mean Sq F value Pr(&gt;F)\n## sex           1     28   28.33   1.956  0.163\n## Residuals   523   7576   14.48\n\n\nHowever, if we do a Two-Way ANOVA and consider the factors together, we see that there is still a main effect of age when taking sex into account and there is a main effect of sex when we take age into account.\n\nm.aov &lt;- summary(aov(data = d, femur_length ~ age + sex))\nm.aov\n\n##              Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## age           3   6152  2050.6   756.4  &lt; 2e-16 ***\n## sex           1     42    42.3    15.6 8.89e-05 ***\n## Residuals   520   1410     2.7                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTo examine whether there is an interaction effect, we would modify our model formula a bit using the colon operator (:) to specify a particular interaction term‚Ä¶\n\nm.aov &lt;- aov(data = d, femur_length ~ age + sex + age:sex)\n# the colon (:) operator includes specific interaction terms\nsummary(m)\n\n##              Df Sum Sq Mean Sq F value Pr(&gt;F)    \n## age           3   6152  2050.6   735.8 &lt;2e-16 ***\n## Residuals   521   1452     2.8                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe could also use the asterisk (*) operator, which expands to include all interactions among terms, including 3-way, 4-way, etc., interactions.\n\nm.aov &lt;- aov(data = d, femur_length ~ age * sex)\n# asterisk (*) operator includes all interaction terms\nsummary(m.aov)\n\n##              Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## age           3   6152  2050.6  816.35  &lt; 2e-16 ***\n## sex           1     42    42.3   16.84 4.72e-05 ***\n## age:sex       3    111    37.0   14.73 3.21e-09 ***\n## Residuals   517   1299     2.5                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nm &lt;- lm(data = d, femur_length ~ age * sex)\n# or using the lm() function...\nsummary(m)\n\n## \n## Call:\n## lm(formula = femur_length ~ age * sex, data = d)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.8727 -1.0088  0.1336  1.0469  4.6773 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   16.7159     0.2389  69.960  &lt; 2e-16 ***\n## ageI          -8.8909     0.2854 -31.148  &lt; 2e-16 ***\n## ageJ          -6.7395     0.2968 -22.706  &lt; 2e-16 ***\n## ageS          -4.0428     0.3360 -12.031  &lt; 2e-16 ***\n## sexmale        1.7160     0.3716   4.617 4.91e-06 ***\n## ageI:sexmale  -1.9523     0.4341  -4.498 8.48e-06 ***\n## ageJ:sexmale  -1.5538     0.4534  -3.427 0.000658 ***\n## ageS:sexmale   0.2536     0.4895   0.518 0.604641    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.585 on 517 degrees of freedom\n## Multiple R-squared:  0.8292, Adjusted R-squared:  0.8269 \n## F-statistic: 358.6 on 7 and 517 DF,  p-value: &lt; 2.2e-16\n\n\nWe can use the function interaction.plot() to visualize interactions among categorical variables.\n\ninteraction.plot(x.factor = d$age, xlab = \"Age\", trace.factor = d$sex, trace.label = \"Sex\",\n    response = d$femur_length, fun = base::mean, ylab = \"Mean Femur Length\")\n\n\n\n\n\n\n\n\nHere, it looks like there is indeed a significant main effect of each term as well as an interaction between our two categorical variables. We will return to comparing models to one another (e.g., our model with and without interactions) and to post-hoc tests of what group mean differences are significant when we get into model selection in another few lectures.\nWhen we do summary() of the results of the lm() function, we are estimating eight \\(\\beta\\) coefficients (equivalent to the number of groups we have). \\(\\beta_0\\), the intercept, is the mean femur length for the base level (in this case, ‚Äúadult females‚Äù). Then we have coefficients showing how the different factor combination groups would differ from that base level (e.g., adult males have mean femur lengths 1.716 greater than adult females, etc).\nThe {permuco} package also lets us run multiple factor ANOVA using permutation/randomization to estimate p values.\n\nlibrary(permuco)\nm.aovperm &lt;- aovperm(data = d, femur_length ~ age + sex + age:sex)\nsummary(m.aovperm)\n\n## Anova Table\n## Resampling test using freedman_lane to handle nuisance variables and 5000 permutations.\n##               SS  df      F parametric P(&gt;F) resampled P(&gt;F)\n## age       6136.3   3 814.28        0.000e+00           2e-04\n## sex         91.7   1  36.51        2.914e-09           2e-04\n## age:sex    111.0   3  14.73        3.210e-09           2e-04\n## Residuals 1298.7 517\n\ndetach(package:permuco)\n\nWhen we run effectsize() on a model object resulting from a multiple factor ANOVA, the function by default returns partial \\(\\eta^2\\) values, which is the proportion of the variance in the response variable that is accounted by each predictor when controlling for the other predictors.\n\neffectsize(m.aov)\n\n## # Effect Size for ANOVA (Type I)\n## \n## Parameter | Eta2 (partial) |       95% CI\n## -----------------------------------------\n## age       |           0.83 | [0.81, 1.00]\n## sex       |           0.03 | [0.01, 1.00]\n## age:sex   |           0.08 | [0.04, 1.00]\n## \n## - One-sided CIs: upper bound fixed at [1.00].\n\n# or effectsize(m.aov, partial = TRUE)",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "20-module.html#type-i-ii-and-iii-anova",
    "href": "20-module.html#type-i-ii-and-iii-anova",
    "title": "20¬† Categorical Data Analysis",
    "section": "20.9 Type I, II, and III ANOVA",
    "text": "20.9 Type I, II, and III ANOVA\nIt is important to recognize that the ORDER in which our factors are entered into a linear model using categorical predictors can result in different values for the entries in our ANOVA table, while the estimation of our regression coefficients is identical regardless. Take a look at this example, where the variables age and sex are entered into our ANOVA formula in different orders:\n\nm1 &lt;- aov(data = d, femur_length ~ age + sex)\nsummary(m1)\n\n##              Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## age           3   6152  2050.6   756.4  &lt; 2e-16 ***\n## sex           1     42    42.3    15.6 8.89e-05 ***\n## Residuals   520   1410     2.7                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nm2 &lt;- aov(data = d, femur_length ~ sex + age)\nsummary(m2)\n\n##              Df Sum Sq Mean Sq F value  Pr(&gt;F)    \n## sex           1     28    28.3   10.45 0.00131 ** \n## age           3   6166  2055.3  758.13 &lt; 2e-16 ***\n## Residuals   520   1410     2.7                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nm1 &lt;- lm(data = d, femur_length ~ age + sex)\nsummary(m1)\n\n## \n## Call:\n## lm(formula = femur_length ~ age + sex, data = d)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.2429 -1.1040  0.1011  1.0560  5.3403 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  17.1896     0.1993   86.27  &lt; 2e-16 ***\n## ageI         -9.7556     0.2232  -43.71  &lt; 2e-16 ***\n## ageJ         -7.4007     0.2329  -31.77  &lt; 2e-16 ***\n## ageS         -3.7467     0.2523  -14.85  &lt; 2e-16 ***\n## sexmale       0.5701     0.1443    3.95 8.89e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.647 on 520 degrees of freedom\n## Multiple R-squared:  0.8146, Adjusted R-squared:  0.8132 \n## F-statistic: 571.2 on 4 and 520 DF,  p-value: &lt; 2.2e-16\n\nm2 &lt;- lm(data = d, femur_length ~ sex + age)\nsummary(m2)\n\n## \n## Call:\n## lm(formula = femur_length ~ sex + age, data = d)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.2429 -1.1040  0.1011  1.0560  5.3403 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  17.1896     0.1993   86.27  &lt; 2e-16 ***\n## sexmale       0.5701     0.1443    3.95 8.89e-05 ***\n## ageI         -9.7556     0.2232  -43.71  &lt; 2e-16 ***\n## ageJ         -7.4007     0.2329  -31.77  &lt; 2e-16 ***\n## ageS         -3.7467     0.2523  -14.85  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.647 on 520 degrees of freedom\n## Multiple R-squared:  0.8146, Adjusted R-squared:  0.8132 \n## F-statistic: 571.2 on 4 and 520 DF,  p-value: &lt; 2.2e-16\n\n\nWhy is this? In the first model, we are looking at the variance within each age group that is explained by gender while in the second case we are looking at the variance within each gender that is explained by age‚Ä¶ but we have different numbers of observations in our different groups. This is known as an unbalanced design.\nWe can see the unbalanced design by tabulating the cases for each combination of factors.\n\ntable(d$sex, d$age)\n\n##         \n##            A   I   J   S\n##   female  44 103  81  45\n##   male    31  97  69  55\n\n\nRecall that ANOVA is based on splitting up the sums of squares, so that for a model like:\n\\[Y_{i,j,k} = \\mu + a_i + b_j + ab_{i,j} + \\epsilon_{i,j,k}\\]\nThe sums of squares add up such that:\n\\[SS_{total} = SS_a + SS_b + SS_{ab} + SS_{residual}\\]\nBy default, the aov() function uses something called Type I Sums of Squares (also called ‚Äúsequential sum of squares‚Äù), which gives greater emphasis to the first factor in the model, leaving only residual variation to the remaining factors. It should be used when you want to first control for one factor, leaving the others to explain only any remaining differences. In a Type I ANOVA, the sums of squares for the first term are calculated around the grand mean of our observations, but the next terms are calculated as residuals around the average of the grand mean and the first group mean. This sequential procedure means our results depend on which term shows up first. This can have a marked effect on the sums of squares calculated, and hence on p values, when using an unbalanced design.\nBy contrast, what are know as Type II and Type III Sums of Squares both calculate the deviations between the individual observations within each group from the grand mean rather than the group mean.\nType II Sum of Squares compares the main effects of each factor, assuming that the interaction between factors is minimal. Generally it is a much more appropriate test for comparing among different main effects and is more appropriate for us to use when there is an unbalanced design.\nType III Sum of Squares (or ‚Äúmarginal sum of squares‚Äù) is most appropriate when there is a significant interaction effect between factors. Since both Type II and Type III ANOVA calculate sums of squares around the grand mean, these are unaffected by different sample sizes across different categorical groups and do not arbitrarily give preference to one effect over another.\nTo summarize:\n\nWhen our data are balanced, our factors are orthogonal, and all three types of sums of squares give the same results.\nIf our data are strongly unbalanced, we should generally be using Type II or Type III Sums of Squares, since we are generally interested in exploring the significance of one factor while controlling for the level of the other factors.\nIn general, if there is no significant interaction effect between factors, then Type II is more powerful than Type III.\nIf an important interaction is present, then Type II analysis is inappropriate, while Type III analysis can still be used, but the results need to be interpreted with caution (in the presence of interactions, individual main effects of different factors difficult to interpret).\n\nSee this post or this post for further treatments of the issue.\nWe can use the Anova() function in the {car} package to run ANOVA with Type II and Type III Sums of Squares. In the examples below, both linear models, where the order of factors are reversed, give the same results for Type II and Type III ANOVA, but not for Type I. The Anova() function takes as its argument a model object, e.g., the result from a call to either lm() or aov() and recalculates the appropriate sums of squares. Note how the effect sizes (partial \\(\\eta^2\\) values) are slightly different depending on whether they are being calculated from Type I or Type II ANOVA objects.\n\nlibrary(car)\n\n## Loading required package: carData\n\n\n## \n## Attaching package: 'car'\n\n\n## The following object is masked from 'package:dplyr':\n## \n##     recode\n\n\n## The following object is masked from 'package:purrr':\n## \n##     some\n\nm1 &lt;- aov(data = d, femur_length ~ age + sex)\neffectsize(m1)\n\n## # Effect Size for ANOVA (Type I)\n## \n## Parameter | Eta2 (partial) |       95% CI\n## -----------------------------------------\n## age       |           0.81 | [0.79, 1.00]\n## sex       |           0.03 | [0.01, 1.00]\n## \n## - One-sided CIs: upper bound fixed at [1.00].\n\nm1 &lt;- Anova(m1, type = \"II\")\nm1\n\n## Anova Table (Type II tests)\n## \n## Response: femur_length\n##           Sum Sq  Df F value    Pr(&gt;F)    \n## age       6165.8   3 758.126 &lt; 2.2e-16 ***\n## sex         42.3   1  15.604 8.888e-05 ***\n## Residuals 1409.7 520                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nm2 &lt;- aov(data = d, femur_length ~ sex + age)\neffectsize(m2)\n\n## # Effect Size for ANOVA (Type I)\n## \n## Parameter | Eta2 (partial) |       95% CI\n## -----------------------------------------\n## sex       |           0.02 | [0.00, 1.00]\n## age       |           0.81 | [0.79, 1.00]\n## \n## - One-sided CIs: upper bound fixed at [1.00].\n\nm2 &lt;- Anova(m2, type = \"II\")\nm2\n\n## Anova Table (Type II tests)\n## \n## Response: femur_length\n##           Sum Sq  Df F value    Pr(&gt;F)    \n## sex         42.3   1  15.604 8.888e-05 ***\n## age       6165.8   3 758.126 &lt; 2.2e-16 ***\n## Residuals 1409.7 520                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\neffectsize(m1)\n\n## # Effect Size for ANOVA (Type II)\n## \n## Parameter | Eta2 (partial) |       95% CI\n## -----------------------------------------\n## age       |           0.81 | [0.79, 1.00]\n## sex       |           0.03 | [0.01, 1.00]\n## \n## - One-sided CIs: upper bound fixed at [1.00].\n\neffectsize(m2)\n\n## # Effect Size for ANOVA (Type II)\n## \n## Parameter | Eta2 (partial) |       95% CI\n## -----------------------------------------\n## sex       |           0.03 | [0.01, 1.00]\n## age       |           0.81 | [0.79, 1.00]\n## \n## - One-sided CIs: upper bound fixed at [1.00].\n\nm1 &lt;- aov(data = d, femur_length ~ age * sex)\nm1 &lt;- Anova(m1, type = \"III\")\nm1\n\n## Anova Table (Type III tests)\n## \n## Response: femur_length\n##              Sum Sq  Df  F value    Pr(&gt;F)    \n## (Intercept) 12294.6   1 4894.442 &lt; 2.2e-16 ***\n## age          2661.3   3  353.158 &lt; 2.2e-16 ***\n## sex            53.6   1   21.320  4.91e-06 ***\n## age:sex       111.0   3   14.735  3.21e-09 ***\n## Residuals    1298.7 517                       \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nm2 &lt;- aov(data = d, femur_length ~ sex * age)\nm2 &lt;- Anova(m2, type = \"III\")\nm2\n\n## Anova Table (Type III tests)\n## \n## Response: femur_length\n##              Sum Sq  Df  F value    Pr(&gt;F)    \n## (Intercept) 12294.6   1 4894.442 &lt; 2.2e-16 ***\n## sex            53.6   1   21.320  4.91e-06 ***\n## age          2661.3   3  353.158 &lt; 2.2e-16 ***\n## sex:age       111.0   3   14.735  3.21e-09 ***\n## Residuals    1298.7 517                       \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ndetach(package:car)",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "20-module.html#chi-square-tests",
    "href": "20-module.html#chi-square-tests",
    "title": "20¬† Categorical Data Analysis",
    "section": "20.10 Chi-Square Tests",
    "text": "20.10 Chi-Square Tests\nOne additional type of categorical data we will often encounter are counts of observations that fall into two or more categories (when we were dealing with \\(Z\\) tests for proportion data, we were interested in something similar, though with two categories only). We can use Chi-Square tests to evaluate statistically the distribution of observations across levels of one or more categorical variables. To use the Chi-Square test we first derive a Chi-Square statistic (\\(\\chi^2\\)) , which is calculated as‚Ä¶\n\\[\\chi^2 = \\displaystyle\\sum_{i=1}^k \\frac{(O_i - E_i)^2}{E_i}\\]\nwhere:\n\n\\(O_i\\) = number of observations in the \\(i\\)th category\n\\(E_i\\) = number of observations in the \\(i\\)th category\n\nWe then compare the value of the \\(\\chi^2\\) statistic to the Chi-Square distribution with \\(k-1\\) degrees of freedom. The Chi-Square distribution, like the \\(F\\) distribution, is a continuous probability distribution, defined for \\(x‚â•0\\). It is governed by a single parameter, \\(df\\).\n\npar(mfrow = c(1, 1))\ncurve(dchisq(x, df = 1), col = \"green\", lty = 3, lwd = 2, xlim = c(0, 20), main = \"Some Example Chi-Square Distributions\",\n    sub = \"(vertical line shows critical value for df=3)\", ylab = \"f(x)\", xlab = \"x\")\ncurve(dchisq(x, df = 3), col = \"blue\", lty = 3, lwd = 2, add = TRUE)\ncurve(dchisq(x, df = 5), col = \"red\", lty = 3, lwd = 2, add = TRUE)\ncurve(dchisq(x, df = 10), col = \"purple\", lty = 3, lwd = 2, add = TRUE)\nlegend(\"right\", c(\"df=1\", \"df=3\", \"df=5\", \"df=10\"), lty = c(3, 3, 3, 3), lwd = 2,\n    col = c(\"green\", \"blue\", \"red\", \"purple\", \"black\"), bty = \"n\", cex = 0.75)\n\ncrit &lt;- qchisq(p = 0.95, df = 3)\ncrit\n\n## [1] 7.814728\n\nabline(v = crit)\nabline(h = 0)\npolygon(cbind(c(crit, seq(from = crit, to = 20, length.out = 1000), 8), c(0, dchisq(seq(from = crit,\n    to = 20, length.out = 1000), df = 3), 0)), border = \"black\", col = rgb(0, 0,\n    1, 0.5))\n\n\n\n\n\n\n\n\n\nCHALLENGE\nLet‚Äôs return to the zombie apocalypse survivors dataset, where we defined an occupation based on major for survivors of the zombie apocalypse. Use a Chi-Square test to evaluate the hypothesis that survivors of the zombie apocalypse are more likely than expected by chance to be natural science majors. Assume that assume our null hypothesis is that the proportions of different post-apocalypse occupations are equivalent, i.e., that \\(\\pi_{natural science}\\) = \\(\\pi_{engineering}\\)= \\(\\pi_{logistics}\\) = \\(\\pi_{other}\\) = 0.25.\n\n\nShow Code\n(obs.table &lt;- table(z$occupation))\n\n\nShow Output\n## \n## natural science     engineering       logistics           other \n##             390              98             218             294\n\n\n\nShow Code\n# returns the same as summary()\n(exp.table &lt;- rep(0.25 * length(z$occupation), 4))\n\n\nShow Output\n## [1] 250 250 250 250\n\n\n\nShow Code\n# equal expectation for each of 4 categories\noccupation.matrix &lt;- data.frame(cbind(obs.table, exp.table, (obs.table - exp.table)^2/exp.table))\nnames(occupation.matrix) &lt;- c(\"Oi\", \"Ei\", \"(Oi-Ei)^2/Ei\")\noccupation.matrix\n\n\nShow Output\n##                  Oi  Ei (Oi-Ei)^2/Ei\n## natural science 390 250       78.400\n## engineering      98 250       92.416\n## logistics       218 250        4.096\n## other           294 250        7.744\n\n\n\nShow Code\n(X2 &lt;- sum(occupation.matrix[, 3]))\n\n\nShow Output\n## [1] 182.656\n\n\n\nShow Code\n(p &lt;- 1 - pchisq(q = X2, length(obs.table) - 1))\n\n\nShow Output\n## [1] 0\n\n\n\nHere, we reject the null hypothesis that the proportions of different occupations among the survivors of the zombie apocalypse is equivalent.\nWe can do all this with a 1-liner in R, too.\n\nchisq.test(x = obs.table, p = c(0.25, 0.25, 0.25, 0.25))\n\n## \n##  Chi-squared test for given probabilities\n## \n## data:  obs.table\n## X-squared = 182.66, df = 3, p-value &lt; 2.2e-16\n\n# here p is a vector of expected proportions... default is uniform\nchisq.test(x = obs.table)\n\n## \n##  Chi-squared test for given probabilities\n## \n## data:  obs.table\n## X-squared = 182.66, df = 3, p-value &lt; 2.2e-16\n\nchisq.test(x = obs.table, p = c(0.42, 0.07, 0.22, 0.29))\n\n## \n##  Chi-squared test for given probabilities\n## \n## data:  obs.table\n## X-squared = 13.416, df = 3, p-value = 0.003818\n\n# with a different set of expected proportions... fail to reject H0\n\nThe above was a Chi-Square goodness of fit test for one categorical variable‚Ä¶ what about if we have two categorical variables and we are curious if there is an association among them? Then we do a Chi-Square test of independence. In this case, our Chi-Square statistic would be the sum of \\(\\frac{(O-E)^2}{E}\\) across all cells in our table, and our degrees of freedom is (number of rows - 1) \\(\\times\\) (number of columns - 1). Let‚Äôs suppose we want to see if there is a relationship among zombie apocalypse survivors between gender and occupation.\nFirst, we determine our table of observed proportions:\n\n(obs.table = table(z$gender, z$occupation))\n\n##         \n##          natural science engineering logistics other\n##   Male               198          49       109   150\n##   Female             192          49       109   144\n\n\nWe can view our data graphically using the mosaic plot() function.\n\nmosaicplot(t(obs.table), main = \"Contingency Table\", col = c(\"darkseagreen\", \"gray\"))\n\n\n\n\n\n\n\n# the `t()` function transposes the table\n\nThen, we determine our table of expected proportions:\n\n(r &lt;- rowSums(obs.table))  # row margins\n\n##   Male Female \n##    506    494\n\n(c &lt;- colSums(obs.table))  # column margins\n\n## natural science     engineering       logistics           other \n##             390              98             218             294\n\n(nr &lt;- nrow(obs.table))  # row dimensions\n\n## [1] 2\n\n(nc &lt;- ncol(obs.table))  # column dimensions\n\n## [1] 4\n\n(exp.table &lt;- matrix(rep(c, each = nr) * r/sum(obs.table), nrow = nr, ncol = nc,\n    dimnames = dimnames(obs.table), byrow = FALSE))\n\n##         \n##          natural science engineering logistics   other\n##   Male            197.34      49.588   110.308 148.764\n##   Female          192.66      48.412   107.692 145.236\n\n# calculates the product of c*r and divides by total\n(X2 &lt;- sum((obs.table - exp.table)^2/exp.table))\n\n## [1] 0.07076686\n\n(p &lt;- 1 - pchisq(q = X2, df = (nr - 1) * (nc - 1)))\n\n## [1] 0.9950981\n\n\nAgain, we can do a one-liner for a test of independence‚Ä¶\n\nchisq.test(x = obs.table)\n\n## \n##  Pearson's Chi-squared test\n## \n## data:  obs.table\n## X-squared = 0.070767, df = 3, p-value = 0.9951",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "20-module.html#concept-review",
    "href": "20-module.html#concept-review",
    "title": "20¬† Categorical Data Analysis",
    "section": "Concept Review",
    "text": "Concept Review\n\nANOVA is conceptually equivalent linear regression, but with one or more categorical predictor variables\nANOVA effectively tests whether the means of different groups are different\nThe Kruskall-Wallis test is a nonparametric alternative to a One-Way ANOVA that relaxes the need for normality in the distribution of data in each group and tests for a difference in the medians of different groups\nANOVAs should have balanced or roughly balanced numbers of observations within each categorical group\nChi-Square (\\(\\chi^2\\)) goodness-of-fit tests are another type of factor-based categorical data analysis, where we compare between observed and expected counts of observations in different grouping categories for either a single variable or two variables",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "21-module.html",
    "href": "21-module.html",
    "title": "21¬† Multiple Regression and ANCOVA",
    "section": "",
    "text": "21.1 Objectives",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Multiple Regression and ANCOVA</span>"
    ]
  },
  {
    "objectID": "21-module.html#objectives",
    "href": "21-module.html#objectives",
    "title": "21¬† Multiple Regression and ANCOVA",
    "section": "",
    "text": "In this module, we extend our simple linear regression and ANOVA models to cases where we have more than one predictor variable. The same approach can be used with combinations of continuous and categorical variables. If all of our predictors variables are continuous, we typically describe this as ‚Äúmultiple linear regression‚Äù. If our predictors are combinations of continuous and categorical variables, we typically describe this as ‚Äúanalysis of covariance‚Äù, or ANCOVA.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Multiple Regression and ANCOVA</span>"
    ]
  },
  {
    "objectID": "21-module.html#preliminaries",
    "href": "21-module.html#preliminaries",
    "title": "21¬† Multiple Regression and ANCOVA",
    "section": "21.2 Preliminaries",
    "text": "21.2 Preliminaries\n\nInstall these packages in R: {jtools} {ggeffects}\nInstall and load this package in R: {effects}\nLoad {tidyverse}, {broom}, {car}, and {gridExtra}",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Multiple Regression and ANCOVA</span>"
    ]
  },
  {
    "objectID": "21-module.html#overview",
    "href": "21-module.html#overview",
    "title": "21¬† Multiple Regression and ANCOVA",
    "section": "21.3 Overview",
    "text": "21.3 Overview\nMultiple linear regression and ANCOVA are pretty straightforward generalizations of the simple linear regression and ANOVA approach that we have considered previously (i.e., Model I regressions, with our parameters estimated using the criterion of OLS). In multiple linear regression and ANCOVA, we are looking to model a response variable in terms of more than one predictor variable so that we can evaluate the effects of several different explanatory variables. When we do multiple linear regression, we are, essentially, looking at the relationship between each of two or more continuous predictor variables and a continuous response variable while holding the effect of all other predictor variables constant. When we do ANCOVA, we are effectively looking at the relationship between one or more continuous predictor variables and a continuous response variable within each of one or more categorical groups.\nTo characterize this using formulas analogous to those we have used before‚Ä¶\nSimple Bivariate Linear Model:\n\\[Y = \\beta_0 + \\beta_1x + \\epsilon\\]\nMultivariate Linear Model:\n\\[Y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_kx_k + \\epsilon\\]\nWe thus now need to estimate several \\(\\beta\\) coefficients for our regression model - one for the intercept plus one for each predictor variable in our model, and, instead of a ‚Äúline‚Äù‚Äù of best fit, we are determining a multidimensional ‚Äúsurface‚Äù of best fit. The criteria we typically use for estimating best fit is analogous to that we‚Äôve used before, i.e., ordinary least squares, where we want to minimize the multidimensional squared deviation between our observed and predicted values:\n\\[\\sum\\limits_{i=1}^{k}[y_i - (\\beta_0 + \\beta_1x_{1_i} + \\beta_2x_{2_i} + ... + \\beta_kx_{k_i})]^2\\]\nEstimating our set of coefficients for multiple regression is actually done using matrix algebra, which we do not need to explore thoroughly (but see the ‚Äúdigression‚Äù below). The lm() function will implement this process for us directly.\n\nNOTE: Besides the least squares approach to parameter estimation, we might also use other approaches, including maximum likelihood and Bayesian approaches. These are mostly beyond the scope of what we will discuss in this class, but the objective is the same‚Ä¶ to use a particular criterion to estimate parameter values for describing the relationship between our predictor and response variables, as well as for estimating the amount of uncertainty in those parameter values. It is worth noting that for many common population parameters (such as the mean), as well as for regression slopes and/or factor effects in certain linear models, OLS and ML estimators turn out to be exactly the same when the assumptions behind OLS are met (i.e., normally distributed variables and normally distributed error terms). When response variables or residuals are not normally distributed (e.g., where we have binary or categorical response variables), we use ML or Bayesian approaches, rather than OLS, for parameter estimation.\n\nLet‚Äôs work some examples!",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Multiple Regression and ANCOVA</span>"
    ]
  },
  {
    "objectID": "21-module.html#multiple-regression",
    "href": "21-module.html#multiple-regression",
    "title": "21¬† Multiple Regression and ANCOVA",
    "section": "21.4 Multiple Regression",
    "text": "21.4 Multiple Regression\n\nContinous Response Variable with More than One Continuous Predictor\nWe will start by taking a digression to construct a dataset ourselves of some correlated random normal continuous variables. The following bit of code will let us do that. See also this post for more information on generating toy data with a particular correlation structure. First, we define a matrix of correlations, R, among our variables (you can play with the values in this matrix, but it must be symmetric):\n\nR = matrix(cbind(1, 0.8, -0.5, 0, 0.8, 1, -0.3, 0.3, -0.5, -0.3, 1, 0.6, 0, 0.3,\n    0.6, 1), nrow = 4)\nR\n\n##      [,1] [,2] [,3] [,4]\n## [1,]  1.0  0.8 -0.5  0.0\n## [2,]  0.8  1.0 -0.3  0.3\n## [3,] -0.5 -0.3  1.0  0.6\n## [4,]  0.0  0.3  0.6  1.0\n\n\nSecond, we generate a dataset of random normal variables where each has a defined mean and standard deviation and then bundle these into a dataframe (‚Äúoriginal‚Äù):\n\nn &lt;- 1000\nk &lt;- 4\noriginal &lt;- NULL\nv &lt;- NULL\nmu &lt;- c(15, 40, 5, 23)  # vector of variable means\ns &lt;- c(5, 20, 4, 15)  # vector of variable SDs\nfor (i in 1:k) {\n    v &lt;- rnorm(n, mu[i], s[i])\n    original &lt;- cbind(original, v)\n}\noriginal &lt;- as.data.frame(original)\nnames(original) = c(\"Y\", \"X1\", \"X2\", \"X3\")\nhead(original)\n\n##          Y       X1       X2          X3\n## 1 14.21699 21.39284 9.362235  -0.7274692\n## 2 11.94653 45.00073 7.170268  14.2185200\n## 3 10.38090 37.88025 7.968537  35.2757871\n## 4 12.21405 39.91291 4.215638 -11.4862198\n## 5 18.17789 62.35475 6.249696   1.1743073\n## 6 10.54651 34.13490 7.436182  15.1271084\n\ncor(original)  # variables are uncorrelated\n\n##              Y          X1           X2         X3\n## Y   1.00000000 0.016097148 -0.059603194 0.01170286\n## X1  0.01609715 1.000000000  0.009184961 0.01309151\n## X2 -0.05960319 0.009184961  1.000000000 0.01322231\n## X3  0.01170286 0.013091511  0.013222307 1.00000000\n\n# make quick bivariate plots for each pair of variables\nplot(original)\n\n\n\n\n\n\n\n# using `pairs(original)` would do the same\n\nNow, let‚Äôs normalize and standardize our variables by subtracting the relevant means and dividing by the standard deviation. This converts them to \\(Z\\) scores from a standard normal distribution.\nNow run the following‚Ä¶ note how we use the apply() and sweep() functions here. Cool, eh?\n\nmeans &lt;- apply(original, 2, FUN = \"mean\")  # returns a vector of means, where we are taking this across dimension 2 of the array 'orig'\nmeans\n\n##         Y        X1        X2        X3 \n## 14.914897 39.610928  5.116208 22.555870\n\n# or\nmeans &lt;- unlist(summarize(original, ymean = mean(Y), x1mean = mean(X1), x2mean = mean(X2),\n    x3mean = mean(X3)))\nms\n\n## function (..., quiet = FALSE, roll = FALSE) \n## {\n##     out &lt;- .parse_hms(..., order = \"MS\", quiet = quiet)\n##     if (roll) {\n##         hms &lt;- .roll_hms(min = out[\"M\", ], sec = out[\"S\", ])\n##         period(hour = hms$hour, minute = hms$min, second = hms$sec)\n##     }\n##     else {\n##         period(minute = out[\"M\", ], second = out[\"S\", ])\n##     }\n## }\n## &lt;bytecode: 0x134a246a8&gt;\n## &lt;environment: namespace:lubridate&gt;\n\nstdevs &lt;- apply(original, 2, FUN = \"sd\")\nstdevs\n\n##         Y        X1        X2        X3 \n##  5.132015 20.458289  3.998754 14.961787\n\n# or\nstdevs &lt;- unlist(summarize(original, ySD = sd(Y), x1SD = sd(X1), x2SD = sd(X2), x3SD = sd(X3)))\nstdevs\n\n##       ySD      x1SD      x2SD      x3SD \n##  5.132015 20.458289  3.998754 14.961787\n\nnormalized &lt;- sweep(original, 2, STATS = means, FUN = \"-\")  # 2nd dimension is columns, removing array of means, function = subtract\nnormalized &lt;- sweep(normalized, 2, STATS = stdevs, FUN = \"/\")  # 2nd dimension is columns, scaling by array of sds, function = divide\nhead(normalized)  # now a dataframe of Z scores\n\n##            Y          X1         X2         X3\n## 1 -0.1359908 -0.89049929  1.0618374 -1.5561870\n## 2 -0.5784012  0.26345312  0.5136750 -0.5572429\n## 3 -0.8834728 -0.08459531  0.7133044  0.8501603\n## 4 -0.5262744  0.01476066 -0.2252125 -2.2752689\n## 5  0.6358103  1.11171658  0.2834604 -1.4290781\n## 6 -0.8512021 -0.26766790  0.5801742 -0.4965156\n\nplot(normalized)\n\n\n\n\n\n\n\nM &lt;- as.matrix(normalized)  # define M as our matrix of normalized variables\n\nWith apply(), we apply a function to a specified margin of an array or matrix (1 = row, 2 = column), and with sweep() we then perform whatever function is specified by FUN= on all of the elements in an array specified by the given margin.\nNext, we take what is called the Cholesky decomposition of our correlation matrix, R, and then multiply our normalized data matrix by the decomposition matrix to yield a transformed dataset with the specified correlation among variables. The Cholesky decomposition breaks certain symmetric matrices into two such that:\n\\(R = U \\cdot U^T\\)\n\nU = chol(R)\nnewM = M %*% U\nnew = as.data.frame(newM)\nnames(new) = c(\"Y\", \"X1\", \"X2\", \"X3\")\ncor(new)  # note that is correlation matrix is what we are aiming for!\n\n##              Y         X1         X2          X3\n## Y   1.00000000  0.8034739 -0.5346185 -0.02072712\n## X1  0.80347394  1.0000000 -0.3277508  0.28584345\n## X2 -0.53461847 -0.3277508  1.0000000  0.60135342\n## X3 -0.02072712  0.2858435  0.6013534  1.00000000\n\nplot(original)\n\n\n\n\n\n\n\nplot(new)  # note the axis scales; using `pairs(new)` would plot the same\n\n\n\n\n\n\n\n\nFinally, we can scale these back out to the mean and distribution of our original random variables.\n\nd &lt;- sweep(new, 2, STATS = stdevs, FUN = \"*\")  # scale back out to original mean...\nd &lt;- sweep(d, 2, STATS = means, FUN = \"+\")  # and standard deviation\nhead(d)\n\n##          Y       X1       X2         X3\n## 1 14.21699 26.45436 8.403052 11.1927158\n## 2 11.94653 33.37833 8.193846 24.0571208\n## 3 10.38090 24.11305 9.250238 36.2564093\n## 4 12.21405 31.17878 5.412929 -0.3778483\n## 5 18.17789 63.66329 5.549177 20.2637577\n## 6 10.54651 22.39400 8.611291 21.2491238\n\ncor(d)\n\n##              Y         X1         X2          X3\n## Y   1.00000000  0.8034739 -0.5346185 -0.02072712\n## X1  0.80347394  1.0000000 -0.3277508  0.28584345\n## X2 -0.53461847 -0.3277508  1.0000000  0.60135342\n## X3 -0.02072712  0.2858435  0.6013534  1.00000000\n\nplot(d)  # note the change to the axis scales\n\n\n\n\n\n\n\n# using `pairs(d)` would produce the same plot\n\nWe now have our own dataframe, d, comprising correlated random variables in original units!\nLet‚Äôs explore this dataset, first with single and then with multivariate regression.\n\n\nCHALLENGE\nStart off by making three bivariate scatterplots in {ggplot2} using the dataframe of three predictor variables (X1, X2, and X3) and one response variable (Y) that we generated above (or download it from ‚Äúhttps://raw.githubusercontent.com/difiore/ada-datasets/main/multiple_regression.csv‚Äù)\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/multiple_regression.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\n\n## Rows: 1000 Columns: 4\n## ‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n## Delimiter: \",\"\n## dbl (4): Y, X1, X2, X3\n## \n## ‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n## ‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nShow Code\ng1 &lt;- ggplot(data = d, aes(x = X1, y = Y)) + geom_point() + geom_smooth(method = \"lm\",\n    formula = y ~ x)\ng2 &lt;- ggplot(data = d, aes(x = X2, y = Y)) + geom_point() + geom_smooth(method = \"lm\",\n    formula = y ~ x)\ng3 &lt;- ggplot(data = d, aes(x = X3, y = Y)) + geom_point() + geom_smooth(method = \"lm\",\n    formula = y ~ x)\ngrid.arrange(g1, g2, g3, ncol = 3)\n\n\n\n\n\n\n\n\n\nThen, using simple linear regression as implemented with lm(), how does the response variable (Y) vary with each predictor variable (X1, X2, X3)? Are the \\(\\beta_1\\) coefficients for each bivariate significant? How much of the variation in Y does each predictor explain in a simple bivariate linear model?\n\n\nShow Code\nm1 &lt;- lm(data = d, formula = Y ~ X1)\nsummary(m1)\n\n\nShow Output\n## \n## Call:\n## lm(formula = Y ~ X1, data = d)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10.9848  -2.1238   0.0731   2.1175  10.7924 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 7.132691   0.213888   33.35   &lt;2e-16 ***\n## X1          0.198846   0.004785   41.55   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.012 on 998 degrees of freedom\n## Multiple R-squared:  0.6337, Adjusted R-squared:  0.6334 \n## F-statistic:  1727 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\n\nShow Code\nm2 &lt;- lm(data = d, formula = Y ~ X2)\nsummary(m2)\n\n\nShow Output\n## \n## Call:\n## lm(formula = Y ~ X2, data = d)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -16.1071  -3.0301   0.1099   2.9888  15.3328 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 17.91187    0.22465   79.73   &lt;2e-16 ***\n## X2          -0.57531    0.03579  -16.07   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.435 on 998 degrees of freedom\n## Multiple R-squared:  0.2057, Adjusted R-squared:  0.2049 \n## F-statistic: 258.4 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\n\nShow Code\nm3 &lt;- lm(data = d, formula = Y ~ X3)\nsummary(m3)\n\n\nShow Output\n## \n## Call:\n## lm(formula = Y ~ X3, data = d)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -17.2471  -3.2823   0.0706   3.2325  13.9029 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 14.906854   0.288255  51.714   &lt;2e-16 ***\n## X3           0.008163   0.010715   0.762    0.446    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.975 on 998 degrees of freedom\n## Multiple R-squared:  0.0005812,  Adjusted R-squared:  -0.0004202 \n## F-statistic: 0.5804 on 1 and 998 DF,  p-value: 0.4463\n\n\n\nIn simple linear regression, Y has a significant, positive relationship with X1, a signficant negative relationship with X2, and no significant bivariate relationship with X3.\nNow let‚Äôs move on to doing actual multiple regression. To review, with multiple regression, we are looking to model a response variable in terms of two or more predictor variables so we can evaluate the effect of each of several explanatory variables while holding the others constant.\nUsing lm() and formula notation, we can fit a model with all three predictor variables. The + sign is used to add additional predictors to our model.\n\nm &lt;- lm(data = d, formula = Y ~ X1 + X2 + X3)\ncoef(m)\n\n## (Intercept)          X1          X2          X3 \n##  9.14885797  0.20049618 -0.19794402 -0.04931104\n\nsummary(m)\n\n## \n## Call:\n## lm(formula = Y ~ X1 + X2 + X3, data = d)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -9.3943 -1.7442 -0.1112  1.8602 10.6242 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  9.148858   0.254643  35.928  &lt; 2e-16 ***\n## X1           0.200496   0.005680  35.297  &lt; 2e-16 ***\n## X2          -0.197944   0.033890  -5.841 7.03e-09 ***\n## X3          -0.049311   0.009235  -5.339 1.15e-07 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.7 on 996 degrees of freedom\n## Multiple R-squared:  0.7062, Adjusted R-squared:  0.7053 \n## F-statistic: 798.1 on 3 and 996 DF,  p-value: &lt; 2.2e-16\n\n\nNext, let‚Äôs check if our residuals are random normal‚Ä¶\n\n\nShow Code\nplot(fitted(m), residuals(m))\n\n\n\n\n\n\n\n\n\nShow Code\nhist(residuals(m))\n\n\n\n\n\n\n\n\n\nShow Code\nqqnorm(residuals(m))\nqqline(residuals(m))\n\n\n\n\n\n\n\n\n\nWhat does this output tell us? First off, the results of the omnibus F test tells us that the overall model is significant; using these three variables, we explain signficantly more of the variation in the response variable, Y, than we would using a model with just an intercept, i.e., just that \\(Y\\) = mean(\\(Y\\)).\nFor a multiple regression model, we calculate the F statistic as follows:\n\\[F = \\frac{R^2(n-p-1)}{(1-R^2)p}\\]\nwhere‚Ä¶\n\n\\(R^2\\) = multiple R squared value\n\\(n\\) = number of data points\n\\(p\\) = number of parameters estimated from the data (i.e., the number of \\(\\beta\\) coefficients, not including the intercept)\n\n\nf &lt;- (summary(m)$r.squared * (nrow(df) - (ncol(df) - 1) - 1))/((1 - summary(m)$r.squared) *\n    (ncol(df) - 1))\nf\n\n## numeric(0)\n\n\nIs this F ratio significant?\n\n1 - pf(q = f, df1 = nrow(df) - (ncol(df) - 1) - 1, df2 = ncol(df) - 1)\n\n## numeric(0)\n\n\nSecond, looking at summary() we see that the \\(\\beta\\) coefficient for each of our predictor variables (including X3) is significant. That is, each predictor is significant even when the effects of the other predictor variables are held constant. Recall that in the simple linear regression, the \\(\\beta\\) coefficient for X3 was not significant.\nThird, we can interpret our \\(\\beta\\) coefficients as we did in simple linear regression‚Ä¶ for each change of one unit in a particular predictor variable (holding the other predictors constant), our predicted value of the response variable changes \\(\\beta\\) units.\n\n\n21.4.1 Plotting Effects\nWe can use the versatile {effects} package to visualize the results of a variety of model objects, include general and generalized linear models and mixed effects models. The function predictorEffects() with no arguments will display the linear relationship between the response variable and each predictor.\n\nplot(predictorEffects(m))\n\n\n\n\n\n\n\n\nWe can also specify particular predictors we wish to look at by adding the ‚Äúpredictors = ~‚Äù argument‚Ä¶\n\nplot(predictorEffects(m, predictors = ~X1))\n\n\n\n\n\n\n\n\nSpecifying the argument ‚Äúpartial.residuals = TRUE‚Äù returns the same plots as above, plus the values of the residuals after subtracting off the contribution from all other explanatory variables as well as a loess smoothed line of best fit through the residuals. Such plots are useful for detecting, for example, nonlinear relationships or interactions between predictors that are not obvious from looking at the original data.\n\nplot(predictorEffects(m, partial.residuals = TRUE))\n\n\n\n\n\n\n\n\nThe {ggeffects} package allows us to similarly predict and plot marginal effects from a model object. The predict_response() function takes a model object and one or more focal terms and returns the predicted values for the response variable across values of the focal term, along a specified margin (‚Äúmean_reference‚Äù by default) for the nonfocal terms. These can then be plotted with the plot() function.\n\nlibrary(ggeffects)\np &lt;- predict_response(m, \"X1 [all]\")\nplot(p)\n\n\n\n\n\n\n\nplot(p, show_data = TRUE, jitter = 0.1)  # superimpose original data\n\n\n\n\n\n\n\nplot(p, show_residuals = TRUE, show_residuals_line = TRUE, jitter = 0.1)  # superimpose residuals and loess smoothed line of best fit\n\n## `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\ndetach(package:ggeffects)\n\n\n\nCHALLENGE\nLoad up the ‚Äúzombies.csv‚Äù dataset with the characteristics of zombie apocalypse survivors again and run a linear model of height as a function of both weight and age. Is the overall model significant? Are both predictor variables significant when the other is controlled for? Make effects plots for each variable, plotting the partial residuals and the loess smoothed relationship between the residuals and each predictor.\n\n\nShow Code\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/zombies.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\n\n\n## Rows: 1000 Columns: 10\n## ‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n## Delimiter: \",\"\n## chr (4): first_name, last_name, gender, major\n## dbl (6): id, height, weight, zombies_killed, years_of_education, age\n## \n## ‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n## ‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nShow Code\nhead(d)\n\n\nShow Output\n## # A tibble: 6 √ó 10\n##      id first_name last_name gender height weight zombies_killed\n##   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;          &lt;dbl&gt;\n## 1     1 Sarah      Little    Female   62.9   132.              2\n## 2     2 Mark       Duncan    Male     67.8   146.              5\n## 3     3 Brandon    Perez     Male     72.1   153.              1\n## 4     4 Roger      Coleman   Male     66.8   130.              5\n## 5     5 Tammy      Powell    Female   64.7   132.              4\n## 6     6 Anthony    Green     Male     71.2   153.              1\n## # ‚Ñπ 3 more variables: years_of_education &lt;dbl&gt;, major &lt;chr&gt;, age &lt;dbl&gt;\n\n\n\nShow Code\nm &lt;- lm(data = d, height ~ weight + age)\nsummary(m)\n\n\nShow Output\n## \n## Call:\n## lm(formula = height ~ weight + age, data = d)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.2278 -1.1782 -0.0574  1.1566  5.4117 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 31.763388   0.470797   67.47   &lt;2e-16 ***\n## weight       0.163107   0.002976   54.80   &lt;2e-16 ***\n## age          0.618270   0.018471   33.47   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.64 on 997 degrees of freedom\n## Multiple R-squared:  0.8555, Adjusted R-squared:  0.8553 \n## F-statistic:  2952 on 2 and 997 DF,  p-value: &lt; 2.2e-16\n\n\n\nShow Code\nplot(predictorEffects(m, partial.residuals = TRUE))",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Multiple Regression and ANCOVA</span>"
    ]
  },
  {
    "objectID": "21-module.html#ancova",
    "href": "21-module.html#ancova",
    "title": "21¬† Multiple Regression and ANCOVA",
    "section": "21.5 ANCOVA",
    "text": "21.5 ANCOVA\n\nContinous Response Variable with both Continuous and Categorical Predictors\nWe can use the same linear modeling approach to do analysis of covariance, or ANCOVA, where we have a continuous response variable and a combination of continuous and categorical predictor variables. Let‚Äôs return to the ‚Äúzombies.csv‚Äù dataset and now include one continuous and one categorical variable in our model‚Ä¶ we want to predict height as a function of age (a continuous variable) and gender (a categorical variable), and we want to use Type II regression (because we have an unbalanced design). What is our model formula?\n\n\nShow Code\nd$gender &lt;- factor(d$gender)\nm &lt;- lm(data = d, formula = height ~ gender + age)\nsummary(m)\n\n\nShow Output\n## \n## Call:\n## lm(formula = height ~ gender + age, data = d)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10.1909  -1.7173   0.1217   1.7670   7.6746 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 46.74251    0.56869   82.19   &lt;2e-16 ***\n## genderMale   4.00224    0.16461   24.31   &lt;2e-16 ***\n## age          0.94091    0.02777   33.88   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.603 on 997 degrees of freedom\n## Multiple R-squared:  0.6361, Adjusted R-squared:  0.6354 \n## F-statistic: 871.5 on 2 and 997 DF,  p-value: &lt; 2.2e-16\n\n\n\nShow Code\nm.aov &lt;- Anova(m, type = \"II\")\nm.aov\n\n\nShow Output\n## Anova Table (Type II tests)\n## \n## Response: height\n##           Sum Sq  Df F value    Pr(&gt;F)    \n## gender    4003.9   1  591.15 &lt; 2.2e-16 ***\n## age       7775.6   1 1148.01 &lt; 2.2e-16 ***\n## Residuals 6752.7 997                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nShow Code\nplot(fitted(m), residuals(m))\n\n\n\n\n\n\n\n\n\nShow Code\nhist(residuals(m))\n\n\n\n\n\n\n\n\n\nShow Code\nqqnorm(residuals(m))\nqqline(residuals(m))\n\n\n\n\n\n\n\n\n\nHow do we interpret these results?\n\nThe omnibus F test is significant\nBoth predictors are significant\nHeight is related to age when sex is controlled for\nControlling for age, being male adds ~4 inches to predicted height when compared to being female.\n\n\n\nVisualizing a Parallel Slopes Model\nWe can now write two equations for the relationship between height, on the one hand, and age and gender on the other:\nFor females‚Ä¶\n\\[height = 46.7251 + 0.94091 \\times age\\]\n\nIn this case, females are the first level of our ‚Äúgender‚Äù factor and thus do not have additional regression coefficients associated with them.\n\nFor males‚Ä¶\n\\[height = 46.7251 + 4.00224 + 0.94091 \\times age\\]\n\nHere, the additional 4.00224 added to the intercept term is the coefficient associated with genderMale\n\n\np &lt;- ggplot(data = d, aes(x = age, y = height)) + geom_point(aes(color = factor(gender))) +\n    scale_color_manual(values = c(\"red\", \"blue\"))\np &lt;- p + geom_abline(slope = m$coefficients[3], intercept = m$coefficients[1], color = \"darkred\")\np &lt;- p + geom_abline(slope = m$coefficients[3], intercept = m$coefficients[1] + m$coefficients[2],\n    color = \"darkblue\")\np\n\n\n\n\n\n\n\n\nNote that this model is based on all of the data collectively‚Ä¶ we are not doing separate linear models for males and females, which could result in different intercepts and slopes for each sex‚Ä¶ rather, we are modeling both sexes as having the same slope but different intercepts. This is what is known as a ‚Äúparallel slopes‚Äù model. Below, we will explore a model where we posit an interaction between age and sex, which would require estimation of four separate parameters (i.e., both a slope and an intercept for males and females rather than, as above, only different intercepts for males and females but the same slope for each sex).\nUsing the confint() function on our ANCOVA model results reveals the confidence intervals for each of the coefficients in our multiple regression, just as it did for simple regression.\n\nconfint(m, level = 0.95)\n\n##                  2.5 %     97.5 %\n## (Intercept) 45.6265330 47.8584809\n## genderMale   3.6792172  4.3252593\n## age          0.8864191  0.9954081\n\n\nWe can also generate effects plots for this ANCOVA model:\n\nplot(predictorEffects(m, partial.residuals = TRUE))",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Multiple Regression and ANCOVA</span>"
    ]
  },
  {
    "objectID": "21-module.html#multicollinearity-and-vifs",
    "href": "21-module.html#multicollinearity-and-vifs",
    "title": "21¬† Multiple Regression and ANCOVA",
    "section": "21.6 Multicollinearity and VIFs",
    "text": "21.6 Multicollinearity and VIFs\nWhen we have more than one explanatory variable in a regression model, we need to be concerned about possible high inter-correlations those variables. One way to characterize the amount of multicollinearity is by examining the variance inflation factor, or VIF, for each variable. Mathematically, the VIF for a predictor variable i is calculated as:\n\\[\\frac{1}{1-R_i^2}\\] where \\(R_i^2\\) is the R-squared value for the regression of variable i on all other predictors. A high VIF indicates that the predictor is highly collinear with other predictors in the model. As a rule of thumb, a VIF value that exceeds ~5 indicates a problematic amount of multicollinearity. The function vif() from the {car} package provides one way to calculate VIFs.\nBelow, we set up a new model with three predictors, weight, age, and gender, and calculate VIFs:\n\nm &lt;- lm(height ~ weight + age + gender, data = d)\nsummary(m)\n\n## \n## Call:\n## lm(formula = height ~ weight + age + gender, data = d)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.6235 -1.0082  0.0141  0.9845  4.6444 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 33.309791   0.437855   76.08   &lt;2e-16 ***\n## weight       0.140542   0.003083   45.59   &lt;2e-16 ***\n## age          0.662485   0.016953   39.08   &lt;2e-16 ***\n## genderMale   1.609671   0.107436   14.98   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.482 on 996 degrees of freedom\n## Multiple R-squared:  0.8821, Adjusted R-squared:  0.8818 \n## F-statistic:  2484 on 3 and 996 DF,  p-value: &lt; 2.2e-16\n\nvif(m)\n\n##   weight      age   gender \n## 1.463629 1.149158 1.313461\n\n\nWe can also calculate VIFs manually, by running models that regress each individual predictor on all of the others‚Ä¶\n\nw &lt;- lm(weight ~ gender + age, data = d)\nrsq_w &lt;- summary(w)$r.square\n(vif_w &lt;- 1/(1 - rsq_w))\n\n## [1] 1.463629\n\na &lt;- lm(age ~ gender + weight, data = d)\nrsq_a &lt;- summary(a)$r.square\n(vif_a &lt;- 1/(1 - rsq_a))\n\n## [1] 1.149158\n\n# for a categorical predictor variable, we can coerce it to numeric to use it\n# as a response variable\ng &lt;- lm(as.numeric(gender) ~ age + weight, data = d)\nrsq_g &lt;- summary(g)$r.square\n(vif_g &lt;- 1/(1 - rsq_g))\n\n## [1] 1.313461",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Multiple Regression and ANCOVA</span>"
    ]
  },
  {
    "objectID": "21-module.html#confidence-and-prediction-intervals",
    "href": "21-module.html#confidence-and-prediction-intervals",
    "title": "21¬† Multiple Regression and ANCOVA",
    "section": "21.7 Confidence and Prediction Intervals",
    "text": "21.7 Confidence and Prediction Intervals\nOne common reason for fitting a regression model is to then use that model to predict the value of the response variable under particular combinations of values of predictor variables, i.e., to predict the values of new observations. We can think of the Y value predicted by the regression model (\\(\\hat{y}\\)) as a point estimate of the response variable given particular value(s) of the predictor variable(s). This point estimate is associated with some uncertainty, which we can characterize in terms of either confidence intervals or prediction intervals around each the estimate. CIs represent uncertainty about the mean value of the response variable for a given (combination of) predictor value(s), while PIs represent our uncertainty about actual new predicted values of the response variable at those predictor value(s).\nThe predict() allows us to determine these intervals for individual responses for a given combination of predictor variables. predict() takes as arguments a model object, new values for the predictor(s) to be plugged into the model, an interval type (‚Äúconfidence‚Äù or ‚Äúprediction‚Äù), and a confidence level (e.g., ‚Äú0.95‚Äù).\n\nCHALLENGE\nLet‚Äôs return to our model of height as a function of age and gender:\n\nm &lt;- lm(height ~ age + gender, data = d)\n\n\nWhat is the predicted mean height, in inches, for 29-year old males who have survived the zombie apocalypse? What is the 95% confidence interval around this predicted mean height?\n\n\n\nShow Code\nci &lt;- predict(m, newdata = data.frame(age = 29, gender = \"Male\"), interval = \"confidence\",\n    level = 0.95)\nci\n\n\nShow Output\n##        fit      lwr      upr\n## 1 78.03124 77.49345 78.56903\n\n\n\n\nWhat is the 95% prediction interval for the individual heights of 29-year old male zombie apocalypse survivors?\n\n\n\nShow Code\npi &lt;- predict(m, newdata = data.frame(age = 29, gender = \"Male\"), interval = \"prediction\",\n    level = 0.95)\npi\n\n\nShow Output\n##        fit      lwr      upr\n## 1 78.03124 72.89597 83.16651\n\n\n\nIn general, the width of a confidence interval for \\(\\hat{y}\\) increases as the value of our predictor variables moves away from the center of their distribution. Also, although both are intervals are centered at \\(\\hat{y}\\), the predicted mean value of the response variable for the given (combination of) predictor(s), the prediction interval is invariably wider than the confidence interval. This makes intuitive sense, since the prediction interval encapsulates the possibility for individual values of Y to fluctuate away from the predicted mean value, while the confidence interval describes uncertainty in estimates of the predicted mean value. That is, the estimate of the mean of a variable is expected to be less uncertain than estimates of individual variable values, because the mean is already a statistic that summarizes a set of values.\nWe can see this by generating CIs and PIs for a range of data using the predict() function. Below, we pass predict() a new set of 1000 values ranging from the minimum to the maximum male age in our original data set‚Ä¶\n\nmales &lt;- d |&gt;\n    filter(gender == \"Male\")\nnew_x &lt;- seq(from = min(males$age), to = max(males$age), length.out = 1000)\nci &lt;- predict(m, newdata = data.frame(age = new_x, gender = \"Male\"), interval = \"confidence\",\n    level = 0.95)\nci &lt;- tibble(x = new_x, as_tibble(ci))\n\npi &lt;- predict(m, newdata = data.frame(age = new_x, gender = \"Male\"), interval = \"prediction\",\n    level = 0.95)\n\npi &lt;- as_tibble(pi)\npi &lt;- tibble(x = new_x, as_tibble(pi))\n\np &lt;- ggplot(data = d |&gt;\n    filter(gender == \"Male\"), aes(x = age, y = height)) + geom_point() + geom_line(data = ci,\n    aes(x = x, y = fit), color = \"red\") + geom_line(data = ci, aes(x = x, y = lwr),\n    color = \"blue\") + geom_line(data = ci, aes(x = x, y = upr), color = \"blue\") +\n    geom_line(data = pi, aes(x = x, y = lwr), color = \"green\") + geom_line(data = pi,\n    aes(x = x, y = upr), color = \"green\")\np\n\n\n\n\n\n\n\n\nAn easier, alternative way to generate CIs and PIs for across a range of predictor values iswith the augment() function from {broom}, as it returns a ‚Äútibble‚Äù with fitted values and interval limits appended to the original data frame.\n\nci &lt;- augment(m, data = d, interval = c(\"confidence\"))\npi &lt;- augment(m, data = d, interval = c(\"prediction\"))\n\np &lt;- ggplot(data = ci |&gt;\n    filter(gender == \"Male\"), aes(x = age, y = height)) + geom_point() + geom_line(aes(x = age,\n    y = .fitted), color = \"red\") + geom_ribbon(aes(x = age, ymin = .lower, ymax = .upper),\n    alpha = 0.5) + geom_ribbon(data = pi |&gt;\n    filter(gender == \"Male\"), aes(x = age, ymin = .lower, ymax = .upper), alpha = 0.1)\np",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Multiple Regression and ANCOVA</span>"
    ]
  },
  {
    "objectID": "21-module.html#interactions-between-predictors",
    "href": "21-module.html#interactions-between-predictors",
    "title": "21¬† Multiple Regression and ANCOVA",
    "section": "21.8 Interactions Between Predictors",
    "text": "21.8 Interactions Between Predictors\nSo far, we have only considered the joint main effects of multiple predictors on a response variable, but often there are interactive effects between our predictors. An interactive effect is an additional change in the response that occurs because of particular combinations of predictors or because the relationship of one continuous variable to a response is contingent on a particular level of a categorical variable. We explored the former case a bit when we looked at ANOVAs involving two discrete predictors. Now, we‚Äôll consider the latter case‚Ä¶ is there an interactive effect of sex AND age on height in our population of zombie apocalypse survivors?\nUsing formula notation, it is easy for us to consider interactions between predictors. The colon (:) operator allows us to specify particular interactions we want to consider. We can also use the asterisk (*) operator to specify a full model, i.e., all single terms factors and all their interactions.\nWhat are the formula and results for a linear model of height as a function of age and sex plus the interaction of age and sex?\n\n\nShow Code\nm &lt;- lm(data = d, height ~ age + gender + age:gender)\nsummary(m)\n\n\nShow Output\n## \n## Call:\n## lm(formula = height ~ age + gender + age:gender, data = d)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -9.7985 -1.6973  0.1189  1.7662  7.9473 \n## \n## Coefficients:\n##                Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)    48.18107    0.79839  60.348   &lt;2e-16 ***\n## age             0.86913    0.03941  22.053   &lt;2e-16 ***\n## genderMale      1.15975    1.12247   1.033   0.3018    \n## age:genderMale  0.14179    0.05539   2.560   0.0106 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.595 on 996 degrees of freedom\n## Multiple R-squared:  0.6385, Adjusted R-squared:  0.6374 \n## F-statistic: 586.4 on 3 and 996 DF,  p-value: &lt; 2.2e-16\n\n\n\nShow Code\n# or\nm &lt;- lm(data = d, height ~ age * gender)\nsummary(m)\n\n\nShow Output\n## \n## Call:\n## lm(formula = height ~ age * gender, data = d)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -9.7985 -1.6973  0.1189  1.7662  7.9473 \n## \n## Coefficients:\n##                Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)    48.18107    0.79839  60.348   &lt;2e-16 ***\n## age             0.86913    0.03941  22.053   &lt;2e-16 ***\n## genderMale      1.15975    1.12247   1.033   0.3018    \n## age:genderMale  0.14179    0.05539   2.560   0.0106 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.595 on 996 degrees of freedom\n## Multiple R-squared:  0.6385, Adjusted R-squared:  0.6374 \n## F-statistic: 586.4 on 3 and 996 DF,  p-value: &lt; 2.2e-16\n\n\n\nShow Code\ncoefficients(m)\n\n\nShow Output\n##    (Intercept)            age     genderMale age:genderMale \n##     48.1810741      0.8691284      1.1597481      0.1417928\n\n\n\nHere, when we allow an interaction, there is no main effect of gender, but there is an interaction effect of gender and age.\nIf we want to visualize this‚Ä¶\n\\[ female\\ height = 48.1817041 + 0.8891281 \\times age\\] \\[ male\\ height = 48.1817041 + + 1.1597481 + 0.1417928 \\times age\\]\n\np1 &lt;- ggplot(data = d, aes(x = age, y = height)) + geom_point(aes(color = factor(gender))) +\n    scale_color_manual(values = c(\"red\", \"blue\"))\np1 &lt;- p1 + geom_abline(slope = m$coefficients[2], intercept = m$coefficients[1],\n    color = \"darkred\")\np1 &lt;- p1 + geom_abline(slope = m$coefficients[2] + m$coefficients[4], intercept = m$coefficients[1] +\n    m$coefficients[3], color = \"darkblue\")\np1\n\n\n\n\n\n\n\n# or, using `geom_smooth()`...\np2 &lt;- ggplot(data = d, aes(x = age, y = height)) + geom_point(aes(color = factor(gender))) +\n    scale_color_manual(values = c(\"red\", \"blue\")) + geom_smooth(method = \"lm\", aes(color = factor(gender)))\np2\n\n## `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe predictorEffects() function from {effects} provides and alternative way to visualize interactions‚Ä¶\n\nplot(predictorEffects(m, predictors = ~age))\n\n\n\n\n\n\n\nplot(predictorEffects(m, predictors = ~gender))\n\n\n\n\n\n\n\n\nIn the case of gender, the function plots the difference in predicted height of females and males as a set of levels specified by the argument ‚Äúxlevels=‚Äù. The argument can be set to a particular number of levels (which is 5 by default, as in the example above) or it can be passed a list with elements named for the predictors, with either the number of level or a vector of level values added.\n\nplot(predictorEffects(m, predictors = ~gender, xlevels = list(age = 6)))  # 6 ages, spanning the range of ages in the dataset\n\n\n\n\n\n\n\nplot(predictorEffects(m, predictors = ~gender, xlevels = list(age = c(10, 15, 20))))  # 3 ages, specified by the user\n\n\n\n\n\n\n\n\n\nCHALLENGE\n\nLoad in the ‚ÄúKamilarAndCooper.csv‚Äù‚Äù dataset we have used previously\nReduce the dataset to the following variables: Family, Brain_Size_Female_Mean, Body_mass_female_mean, MeanGroupSize, DayLength_km, HomeRange_km2, and Move and assign Family to be a factor. Also, rename the brain size, body mass, group size, day length, and home range variables as BrainSize, BodyMass, GroupSize, DayLength, HomeRange\n\n\n\nShow Code\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/KamilarAndCooperData.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\n\n\n## Rows: 213 Columns: 45\n## ‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n## Delimiter: \",\"\n## chr (17): Scientific_Name, Superfamily, Family, Genus, Species, Brain_size_R...\n## dbl (28): Brain_Size_Species_Mean, Brain_Size_Female_Mean, Body_mass_male_me...\n## \n## ‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n## ‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nShow Code\nd$Family &lt;- factor(d$Family)\nhead(d)\n\n\nShow Output\n## # A tibble: 6 √ó 45\n##   Scientific_Name        Superfamily Family Genus Species Brain_Size_Species_M‚Ä¶¬π\n##   &lt;chr&gt;                  &lt;chr&gt;       &lt;fct&gt;  &lt;chr&gt; &lt;chr&gt;                    &lt;dbl&gt;\n## 1 Allenopithecus_nigrov‚Ä¶ Cercopithe‚Ä¶ Cerco‚Ä¶ Alle‚Ä¶ nigrov‚Ä¶                   58.0\n## 2 Allocebus_trichotis    Cercopithe‚Ä¶ Cerco‚Ä¶ Allo‚Ä¶ tricho‚Ä¶                   NA  \n## 3 Alouatta_belzebul      Ceboidea    Ateli‚Ä¶ Alou‚Ä¶ belzeb‚Ä¶                   52.8\n## 4 Alouatta_caraya        Ceboidea    Ateli‚Ä¶ Alou‚Ä¶ caraya                    52.6\n## 5 Alouatta_guariba       Ceboidea    Ateli‚Ä¶ Alou‚Ä¶ guariba                   51.7\n## 6 Alouatta_palliata      Ceboidea    Ateli‚Ä¶ Alou‚Ä¶ pallia‚Ä¶                   49.9\n## # ‚Ñπ abbreviated name: ¬π‚ÄãBrain_Size_Species_Mean\n## # ‚Ñπ 39 more variables: Brain_Size_Female_Mean &lt;dbl&gt;, Brain_size_Ref &lt;chr&gt;,\n## #   Body_mass_male_mean &lt;dbl&gt;, Body_mass_female_mean &lt;dbl&gt;,\n## #   Mass_Dimorphism &lt;dbl&gt;, Mass_Ref &lt;chr&gt;, MeanGroupSize &lt;dbl&gt;,\n## #   AdultMales &lt;dbl&gt;, AdultFemale &lt;dbl&gt;, AdultSexRatio &lt;dbl&gt;,\n## #   Social_Organization_Ref &lt;chr&gt;, InterbirthInterval_d &lt;dbl&gt;, Gestation &lt;dbl&gt;,\n## #   WeaningAge_d &lt;dbl&gt;, MaxLongevity_m &lt;dbl&gt;, LitterSz &lt;dbl&gt;, ‚Ä¶\n\n\n\nShow Code\nd &lt;- d |&gt;\n    select(Brain_Size_Female_Mean, Family, Body_mass_female_mean, MeanGroupSize,\n        DayLength_km, HomeRange_km2, Move) |&gt;\n    rename(BrainSize = Brain_Size_Female_Mean, BodyMass = Body_mass_female_mean,\n        GroupSize = MeanGroupSize, DayLength = DayLength_km, HomeRange = HomeRange_km2)\n\n\n\nFit a Model I least squares multiple linear regression model using log(HomeRange) as the response variable and log(BodyMass), log(BrainSize), GroupSize, and Move as predictor variables, and view a model summary.\nLook at and interpret the estimated regression coefficients for the fitted model and interpret. Are any of them statistically significant? What can you infer about the relationship between the response and predictors?\nReport and interpret the coefficient of determination and the outcome of the omnibus F test.\nExamine the residuals‚Ä¶ are they normally distributed?\n\n\n\nShow Code\nm &lt;- lm(data = d, log(HomeRange) ~ log(BodyMass) + log(BrainSize) + GroupSize + Move)\nsummary(m)\n\n\nShow Output\n## \n## Call:\n## lm(formula = log(HomeRange) ~ log(BodyMass) + log(BrainSize) + \n##     GroupSize + Move, data = d)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.7978 -0.6473 -0.0038  0.8807  2.1598 \n## \n## Coefficients:\n##                 Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)    -6.955853   1.957472  -3.553 0.000865 ***\n## log(BodyMass)   0.315276   0.468439   0.673 0.504153    \n## log(BrainSize)  0.614460   0.591100   1.040 0.303771    \n## GroupSize       0.034026   0.009793   3.475 0.001095 ** \n## Move            0.025916   0.019559   1.325 0.191441    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.127 on 48 degrees of freedom\n##   (160 observations deleted due to missingness)\n## Multiple R-squared:  0.6359, Adjusted R-squared:  0.6055 \n## F-statistic: 20.95 on 4 and 48 DF,  p-value: 4.806e-10\n\n\n\nShow Code\nplot(m$residuals)\n\n\n\n\n\n\n\n\n\nShow Code\nqqnorm(m$residuals)\n\n\n\n\n\n\n\n\n\nShow Code\nshapiro.test(m$residuals)\n\n\nShow Output\n## \n##  Shapiro-Wilk normality test\n## \n## data:  m$residuals\n## W = 0.96517, p-value = 0.1242\n\n\n\nWhen we plot effects for this model, note that the predictor effect plot uses untransformed predictor values on the horizontal axis, not the log-transformed variables that we used in the regression model.\n\nplot(predictorEffects(m, partial.residuals = TRUE))\n\n\n\n\n\n\n\n\nWe can use the ‚Äúaxes=‚Äù argument and the ‚Äúx=‚Äù sub-argument to transform the horizontal axis, for example to replace BodyMass by log(BodyMass). The code below specifies a lot of tweaks to the x axis‚Ä¶\n\nplot(predictorEffects(m, ~BodyMass, partial.residuals = TRUE), axes = list(x = list(rotate = 90,\n    BodyMass = list(transform = list(trans = log, inverse = exp), lab = \"log(Body Mass)\",\n        ticks = list(at = c(100, 1000, 10000, 1e+05)), lim = c(100, 1e+05)))))\n\n\n\n\n\n\n\n\n\nWhat happens if we remove the \\(Move\\) term from the model?\n\n\n\nShow Code\nm &lt;- lm(data = d, log(HomeRange) ~ log(BodyMass) + log(BrainSize) + GroupSize)\nsummary(m)\n\n\nShow Output\n## \n## Call:\n## lm(formula = log(HomeRange) ~ log(BodyMass) + log(BrainSize) + \n##     GroupSize, data = d)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.7982 -0.7310 -0.0140  0.8386  3.1926 \n## \n## Coefficients:\n##                 Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)    -5.354845   1.176046  -4.553 1.45e-05 ***\n## log(BodyMass)  -0.181627   0.311382  -0.583 0.560972    \n## log(BrainSize)  1.390536   0.398840   3.486 0.000721 ***\n## GroupSize       0.030433   0.008427   3.611 0.000473 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.228 on 103 degrees of freedom\n##   (106 observations deleted due to missingness)\n## Multiple R-squared:  0.6766, Adjusted R-squared:  0.6672 \n## F-statistic: 71.85 on 3 and 103 DF,  p-value: &lt; 2.2e-16\n\n\n\nShow Code\nplot(m$residuals)\n\n\n\n\n\n\n\n\n\nShow Code\nqqnorm(m$residuals)\n\n\n\n\n\n\n\n\n\nShow Code\nshapiro.test(m$residuals)  # no significant deviation from normal\n\n\nShow Output\n## \n##  Shapiro-Wilk normality test\n## \n## data:  m$residuals\n## W = 0.99366, p-value = 0.9058",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Multiple Regression and ANCOVA</span>"
    ]
  },
  {
    "objectID": "21-module.html#visualizing-model-results",
    "href": "21-module.html#visualizing-model-results",
    "title": "21¬† Multiple Regression and ANCOVA",
    "section": "21.9 Visualizing Model Results",
    "text": "21.9 Visualizing Model Results\nWe have already seen lots of ways to view linear modeling results, e.g., by using the summary(), glance(), and tidy() functions with model objects as arguments. The {jtools} package provides some interesting additional functions for summarizing and visualizing regression models results.\n\nThe summ() function is an alternative to summary() that provides a concise table of a model and its results. It can be used with standard lm() simple linear regression results (as we work with in this module) as well as with glm() and lmer() results (for generalized linear modeling and mixed effects modeling, respectively), as we cover in Module 23 and Module 24.\nThe effect_plot() function can be used to plot the relationship between (one of) the predictor variables and the response variable while holding the others constant. The function contains lots of possible arguments for customizing the plot, e.g., for specifying whether and what types of interval can be plotted.\nThe plot_summs() function can be used to nicely visualize coefficient estimates and CI values around those terms, including visualizing multiple models on the same plot.\n\nLet‚Äôs return to working with the zombie apocaluypse survivors dataset‚Ä¶\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/zombies.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\nlibrary(jtools)\nm1 &lt;- lm(data = d, height ~ age + weight + gender + zombies_killed)\nsumm(m1, confint = TRUE, ci.width = 0.95, digits = 3)\n\n\n\n\n\nObservations\n1000\n\n\nDependent variable\nheight\n\n\nType\nOLS linear regression\n\n\n\n\n\n\n\n\nF(4,995)\n1863.277\n\n\nR¬≤\n0.882\n\n\nAdj. R¬≤\n0.882\n\n\n\n\n\n\n\n\n\nEst.\n2.5%\n97.5%\nt val.\np\n\n\n\n\n(Intercept)\n33.219\n32.340\n34.098\n74.185\n0.000\n\n\nage\n0.662\n0.629\n0.696\n39.060\n0.000\n\n\nweight\n0.141\n0.135\n0.147\n45.586\n0.000\n\n\ngenderMale\n1.609\n1.398\n1.820\n14.974\n0.000\n\n\nzombies_killed\n0.026\n-0.027\n0.079\n0.970\n0.332\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\n\n\neffect_plot(m1, pred = age, interval = TRUE, int.type = \"confidence\", int.width = 0.95,\n    plot.points = TRUE)\n\n\n\n\n\n\n\nm2 &lt;- lm(data = d, height ~ age + weight + gender)\nm3 &lt;- lm(data = d, height ~ age + gender)\nm4 &lt;- lm(data = d, height ~ weight + gender)\nm5 &lt;- lm(data = d, height ~ gender)\n\n# plot with unstandardized beta coefficients\nplot_summs(m1)\n\n\n\n\n\n\n\n# plot with standardized beta coefficients (i.e., mean centers the variables\n# and sets SD to 1)\nplot_summs(m1, scale = TRUE)\n\n\n\n\n\n\n\n# plot with 95% CIs and distributions\nplot_summs(m1, plot.distributions = TRUE)\n\n\n\n\n\n\n\n# plot with 95% CIs and distributions scaled to same height\nplot_summs(m1, plot.distributions = TRUE, rescale.distributions = TRUE)\n\n\n\n\n\n\n\n# plot multiple models\nplot_summs(m1, m2, m3, m4, m5, scale = TRUE)\n\n\n\n\n\n\n\n# plot multiple models\nplot_summs(m1, m2, m3, m4, m5, plot.distributions = TRUE, rescale.distributions = TRUE)\n\n\n\n\n\n\n\ndetach(package:jtools)",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Multiple Regression and ANCOVA</span>"
    ]
  },
  {
    "objectID": "21-module.html#concept-review",
    "href": "21-module.html#concept-review",
    "title": "21¬† Multiple Regression and ANCOVA",
    "section": "Concept Review",
    "text": "Concept Review\n\nMultiple regression and ANCOVA are further extensions of simple linear regression/ANOVA to cases where we have more than one predictor variable\n‚ÄúMultiple regression‚Äù refers to where we have 2 or more continuous predictor variables\n‚ÄúANCOVA‚Äù refers to cases where we have one or more continuous predictor variables and one or more categorical predictor variables",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Multiple Regression and ANCOVA</span>"
    ]
  },
  {
    "objectID": "22-module.html",
    "href": "22-module.html",
    "title": "22¬† Model Selection in Regression",
    "section": "",
    "text": "22.1 Overview\nThe purpose of a model selection process in a regression analysis is to sort through our explanatory variables in a systematic fashion in order to establish which are best able to describe the response. There are different possible algorithms to use for model selection, e.g., forward and backward selection, and different ways of comparing models to one another, which may result in different parameters being included in the final model. The are also several different package that we can use to combine results from several different models.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Model Selection in Regression</span>"
    ]
  },
  {
    "objectID": "22-module.html#preliminaries",
    "href": "22-module.html#preliminaries",
    "title": "22¬† Model Selection in Regression",
    "section": "22.2 Preliminaries",
    "text": "22.2 Preliminaries\n\nInstall these packages in R: {MASS}, {AICcmodavg}, and {MuMIn}\nLoad {tidyverse}",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Model Selection in Regression</span>"
    ]
  },
  {
    "objectID": "22-module.html#nested-comparisons-and-f-tests",
    "href": "22-module.html#nested-comparisons-and-f-tests",
    "title": "22¬† Model Selection in Regression",
    "section": "22.3 Nested Comparisons and F Tests",
    "text": "22.3 Nested Comparisons and F Tests\nOne way we can compare different models is to use F ratios and what are called partial F tests. This approach looks at two or more nested models - a larger, or more complex, model that contains explanatory variables that we are interested in and smaller, less complex, models that exclude one or more of those variables. Basically, we aim to compare the total variance in the response variable that is explained by the more complex model to that explained by a ‚Äúreduced‚Äù model. If the more complex model explains a significantly greater proportion of the variation, then we conclude that predictor terms absent from the less complex model are important.\nFor example, if including an additional term with its associated \\(\\beta\\) coefficient results in significantly better fit to the observed data than we find for a model that lacks that particular terms, then this is evidence against the null hypothesis that the \\(\\beta\\) coefficient (slope) for that term equals zero.\n\nEXAMPLE:\nLet‚Äôs go back to our zombie apocalypse survivors dataset and compare a few models. We need to calculate the following partial F statistic for a full versus reduced model:\n\\[F = \\frac{(R^2_{full}-R^2_{reduced})(n-q-1)}{(1-R^2_{full})(q-p)}\\]\nwhere:\n\nThe \\(R^2\\) values are the coefficients of determination of the full model and the nested ‚Äúreduced‚Äù model\n\\(n\\) is the number of observations in the data set\n\\(p\\) is the number of predictor terms in the nested (reduced) model\n\\(q\\) is the number of predictor terms in the full model\n\nAfter we calculate this F statistic, we compare it to an \\(F\\) distribution with ‚Äúdf1 = q-p‚Äù and ‚Äúdf2 = n-q‚Äù to derive a p value. The lm() function will do this for us automatically, but we can also do it by hand.\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/zombies.csv\"\nz &lt;- read_csv(f, col_names = TRUE)\n\n## Rows: 1000 Columns: 10\n## ‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n## Delimiter: \",\"\n## chr (4): first_name, last_name, gender, major\n## dbl (6): id, height, weight, zombies_killed, years_of_education, age\n## \n## ‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n## ‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nz$gender &lt;- factor(z$gender)\nm1 &lt;- lm(data = z, height ~ age * gender)  # full model\nm2 &lt;- lm(data = z, height ~ age + gender)  # model without interaction\nm3 &lt;- lm(data = z, height ~ age)  # model with one predictor\nm4 &lt;- lm(data = z, height ~ 1)  # intercept only model\n\nOnce we have fitted the full and three nested models, we can carry out partial F tests to compare particular models using the anova() function, with the nested (reduced) and full model as arguments. The reduced model is included as the first argument and the full model is included as the second argument.\n\nanova(m2, m1, test = \"F\")\n\n## Analysis of Variance Table\n## \n## Model 1: height ~ age + gender\n## Model 2: height ~ age * gender\n##   Res.Df    RSS Df Sum of Sq     F  Pr(&gt;F)  \n## 1    997 6752.7                             \n## 2    996 6708.6  1    44.138 6.553 0.01062 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# compares the reduced model without interactions (m2) to the full model with\n# interactions (m1)\n\nWe can also calculate the F statistic by hand and compare it to the \\(F\\) distribution.\n\n(f &lt;- ((summary(m1)$r.squared - summary(m2)$r.squared) * (nrow(z) - 3 - 1))/((1 -\n    summary(m1)$r.squared) * (3 - 2)))\n\n## [1] 6.552973\n\n(p &lt;- 1 - pf(f, df1 = 3 - 2, df2 = nrow(z) - 3, lower.tail = TRUE))\n\n## [1] 0.01061738\n\n# df1 = q-p, df2 = n-q\nanova(m3, m2, test = \"F\")\n\n## Analysis of Variance Table\n## \n## Model 1: height ~ age\n## Model 2: height ~ age + gender\n##   Res.Df     RSS Df Sum of Sq      F    Pr(&gt;F)    \n## 1    998 10756.6                                  \n## 2    997  6752.7  1    4003.9 591.15 &lt; 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# compares the age only model (m3) to the age + gender model (m2)\n(f &lt;- ((summary(m2)$r.squared - summary(m3)$r.squared) * (nrow(z) - 2 - 1))/((1 -\n    summary(m2)$r.squared) * (2 - 1)))\n\n## [1] 591.147\n\n(p &lt;- 1 - pf(f, df1 = 2 - 1, df2 = nrow(z) - 2, lower.tail = TRUE))\n\n## [1] 0\n\n# df1 = q-p, df2 = n-q\n\nIn these cases, each comparison shows that the more complex model indeed results in signficantly more explantory power than the reduced model.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Model Selection in Regression</span>"
    ]
  },
  {
    "objectID": "22-module.html#forward-selection",
    "href": "22-module.html#forward-selection",
    "title": "22¬† Model Selection in Regression",
    "section": "22.4 Forward Selection",
    "text": "22.4 Forward Selection\nForward selection starts with an intercept-only model and then tests which of the predictor variables best improves the goodness-of-fit. Then the model is updated by adding that term and tests which of the remaining predictors would further and best improve the fit. The process continues until there are not any other terms that could be added to improve the fit more. The R functions add1() and update(), respectively, perform the series of tests and update your fitted regression model. Setting the ‚Äútest=‚Äù argument in add1() to ‚ÄúF‚Äù includes the partial F statistic value and its significance. The ‚Äú.~.‚Äù part of the ‚Äúscope=‚Äù argument means, basically, ‚Äúwhat is already there‚Äù, while the remainder of the ‚Äúscope=‚Äù argument is the list of additional variables you might add for the fullest possible model.\n\nm0 &lt;- lm(data = z, height ~ 1)\nsummary(m0)\n\n## \n## Call:\n## lm(formula = height ~ 1, data = z)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -13.4806  -2.9511  -0.1279   2.7530  12.8997 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  67.6301     0.1363   496.2   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.31 on 999 degrees of freedom\n\nadd1(m0, scope = . ~ . + age + weight + zombies_killed + years_of_education, test = \"F\")\n\n## Single term additions\n## \n## Model:\n## height ~ 1\n##                    Df Sum of Sq     RSS    AIC   F value  Pr(&gt;F)    \n## &lt;none&gt;                          18558.6 2922.9                      \n## age                 1    7802.0 10756.6 2379.5  723.8676 &lt; 2e-16 ***\n## weight              1   12864.8  5693.8 1743.4 2254.9310 &lt; 2e-16 ***\n## zombies_killed      1       5.7 18552.9 2924.6    0.3048 0.58100    \n## years_of_education  1      81.6 18477.0 2920.5    4.4057 0.03607 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLooking at the output of add1(), we see that adding weight, age, or years_of_education all would add explanatory power to our model, but weight is associated with the highest new F statistic and the lowest new RSS, so we should update our model by adding it as a new predictor. We do this with the update() function, passing it the old model and a ‚Äúformula=‚Äù argument of ‚Äú.~.‚Äù (what is already there) ‚Äú+ weight‚Äù (the new predictor).\n\nNOTE: The ‚Äúformula=‚Äù and ‚Äúscope=‚Äù arguments work by first identifying what was on the left-hand side (LHS) and right-hand side (RHS) of the old model. It then examines the new model and substitutes the LHS of the old model for any occurrence of ‚Äú.‚Äù on the LHS of the new model, substitutes the RHS of the old formula for any occurrence of ‚Äú.‚Äù on the RHS of new, and adds new terms to the RHS.\n\n\nm1 &lt;- update(m0, formula = . ~ . + weight)\nsummary(m1)\n\n## \n## Call:\n## lm(formula = height ~ weight, data = z)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -7.1519 -1.5206 -0.0535  1.5167  9.4439 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 39.565446   0.595815   66.41   &lt;2e-16 ***\n## weight       0.195019   0.004107   47.49   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.389 on 998 degrees of freedom\n## Multiple R-squared:  0.6932, Adjusted R-squared:  0.6929 \n## F-statistic:  2255 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\nWe then repeat this process until adding additional predictors no longer improves the explanatory power of the model.\n\nadd1(m1, scope = . ~ . + age + weight + zombies_killed + years_of_education, test = \"F\")\n\n## Single term additions\n## \n## Model:\n## height ~ weight\n##                    Df Sum of Sq    RSS     AIC   F value  Pr(&gt;F)    \n## &lt;none&gt;                          5693.8 1743.38                      \n## age                 1   3012.83 2681.0  992.17 1120.4147 &lt; 2e-16 ***\n## zombies_killed      1      5.16 5688.6 1744.47    0.9052 0.34163    \n## years_of_education  1     16.93 5676.9 1742.40    2.9728 0.08498 .  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# note: here, weight has already been added to m1, so having including it on\n# the RHS of the .~. operator is not essential... I've just kept it there for\n# ease in copying and pasting in successive `add1()` steps\nm2 &lt;- update(m1, formula = . ~ . + age)\nsummary(m2)\n\n## \n## Call:\n## lm(formula = height ~ weight + age, data = z)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.2278 -1.1782 -0.0574  1.1566  5.4117 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 31.763388   0.470797   67.47   &lt;2e-16 ***\n## weight       0.163107   0.002976   54.80   &lt;2e-16 ***\n## age          0.618270   0.018471   33.47   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.64 on 997 degrees of freedom\n## Multiple R-squared:  0.8555, Adjusted R-squared:  0.8553 \n## F-statistic:  2952 on 2 and 997 DF,  p-value: &lt; 2.2e-16\n\nadd1(m2, scope = . ~ . + age + weight + zombies_killed + years_of_education, test = \"F\")\n\n## Single term additions\n## \n## Model:\n## height ~ weight + age\n##                    Df Sum of Sq    RSS    AIC F value Pr(&gt;F)\n## &lt;none&gt;                          2681.0 992.17               \n## zombies_killed      1   2.62551 2678.3 993.20  0.9764 0.3233\n## years_of_education  1   0.77683 2680.2 993.89  0.2887 0.5912\n\n\nAfter we add weight and age, no other variable improves the fit of the model significantly, so the final, best model in this case is m2.\n\nsummary(m2)\n\n## \n## Call:\n## lm(formula = height ~ weight + age, data = z)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.2278 -1.1782 -0.0574  1.1566  5.4117 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 31.763388   0.470797   67.47   &lt;2e-16 ***\n## weight       0.163107   0.002976   54.80   &lt;2e-16 ***\n## age          0.618270   0.018471   33.47   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.64 on 997 degrees of freedom\n## Multiple R-squared:  0.8555, Adjusted R-squared:  0.8553 \n## F-statistic:  2952 on 2 and 997 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Model Selection in Regression</span>"
    ]
  },
  {
    "objectID": "22-module.html#backward-selection",
    "href": "22-module.html#backward-selection",
    "title": "22¬† Model Selection in Regression",
    "section": "22.5 Backward Selection",
    "text": "22.5 Backward Selection\nOpposite to forward selection, backward selection starts with the fullest model you want to consider and systematically drops terms that do not contribute to the explanatory value of the model. The R functions for this process are drop1() to inspect the partial F test results and update() to update the model.\n\nm0 &lt;- lm(data = z, height ~ age + weight + zombies_killed + years_of_education)\nsummary(m0)\n\n## \n## Call:\n## lm(formula = height ~ age + weight + zombies_killed + years_of_education, \n##     data = z)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.1716 -1.1751 -0.0645  1.1263  5.4526 \n## \n## Coefficients:\n##                     Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)        31.625711   0.486310  65.032   &lt;2e-16 ***\n## age                 0.617436   0.018511  33.355   &lt;2e-16 ***\n## weight              0.163197   0.002981  54.750   &lt;2e-16 ***\n## zombies_killed      0.029777   0.029721   1.002    0.317    \n## years_of_education  0.017476   0.031050   0.563    0.574    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.64 on 995 degrees of freedom\n## Multiple R-squared:  0.8557, Adjusted R-squared:  0.8551 \n## F-statistic:  1475 on 4 and 995 DF,  p-value: &lt; 2.2e-16\n\ndrop1(m0, test = \"F\")\n\n## Single term deletions\n## \n## Model:\n## height ~ age + weight + zombies_killed + years_of_education\n##                    Df Sum of Sq     RSS     AIC   F value Pr(&gt;F)    \n## &lt;none&gt;                           2677.5  994.88                     \n## age                 1    2993.7  5671.2 1743.40 1112.5240 &lt;2e-16 ***\n## weight              1    8066.3 10743.7 2382.32 2997.5633 &lt;2e-16 ***\n## zombies_killed      1       2.7  2680.2  993.89    1.0038 0.3166    \n## years_of_education  1       0.9  2678.3  993.20    0.3168 0.5737    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere, removing either weight or age would result in a significant loss of explanatory power for our model, while removing either zombies_killed or years_of_education would not. The latter variable is associated with the lowest F value and lowest increase in RSS, so we update our model by removing it. We then run the drop1() function on the new, updated model and remove the next variable whose loss does not significantly reduce the explanatory power of the model. We continue doing this until removing any additional predictors results in a significantly worse model.\n\nm1 &lt;- update(m0, . ~ . - years_of_education)\nsummary(m1)\n\n## \n## Call:\n## lm(formula = height ~ age + weight + zombies_killed, data = z)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.1715 -1.1928 -0.0615  1.1435  5.4700 \n## \n## Coefficients:\n##                 Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)    31.661862   0.481884  65.704   &lt;2e-16 ***\n## age             0.618053   0.018472  33.458   &lt;2e-16 ***\n## weight          0.163232   0.002979  54.793   &lt;2e-16 ***\n## zombies_killed  0.029348   0.029701   0.988    0.323    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.64 on 996 degrees of freedom\n## Multiple R-squared:  0.8557, Adjusted R-squared:  0.8552 \n## F-statistic:  1968 on 3 and 996 DF,  p-value: &lt; 2.2e-16\n\ndrop1(m1, test = \"F\")\n\n## Single term deletions\n## \n## Model:\n## height ~ age + weight + zombies_killed\n##                Df Sum of Sq     RSS     AIC   F value Pr(&gt;F)    \n## &lt;none&gt;                       2678.3  993.20                     \n## age             1    3010.3  5688.6 1744.47 1119.4439 &lt;2e-16 ***\n## weight          1    8073.4 10751.7 2381.07 3002.2762 &lt;2e-16 ***\n## zombies_killed  1       2.6  2681.0  992.17    0.9764 0.3233    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nm2 &lt;- update(m1, . ~ . - zombies_killed)\nsummary(m2)\n\n## \n## Call:\n## lm(formula = height ~ age + weight, data = z)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.2278 -1.1782 -0.0574  1.1566  5.4117 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 31.763388   0.470797   67.47   &lt;2e-16 ***\n## age          0.618270   0.018471   33.47   &lt;2e-16 ***\n## weight       0.163107   0.002976   54.80   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.64 on 997 degrees of freedom\n## Multiple R-squared:  0.8555, Adjusted R-squared:  0.8553 \n## F-statistic:  2952 on 2 and 997 DF,  p-value: &lt; 2.2e-16\n\ndrop1(m2, test = \"F\")\n\n## Single term deletions\n## \n## Model:\n## height ~ age + weight\n##        Df Sum of Sq     RSS     AIC F value    Pr(&gt;F)    \n## &lt;none&gt;               2681.0  992.17                      \n## age     1    3012.8  5693.8 1743.38  1120.4 &lt; 2.2e-16 ***\n## weight  1    8075.7 10756.6 2379.52  3003.2 &lt; 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAt this point, all of the explanatory variables are still significant, so the final, best model in this case is also m2.\n\nsummary(m2)\n\n## \n## Call:\n## lm(formula = height ~ age + weight, data = z)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.2278 -1.1782 -0.0574  1.1566  5.4117 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 31.763388   0.470797   67.47   &lt;2e-16 ***\n## age          0.618270   0.018471   33.47   &lt;2e-16 ***\n## weight       0.163107   0.002976   54.80   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.64 on 997 degrees of freedom\n## Multiple R-squared:  0.8555, Adjusted R-squared:  0.8553 \n## F-statistic:  2952 on 2 and 997 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Model Selection in Regression</span>"
    ]
  },
  {
    "objectID": "22-module.html#model-selection-using-aic",
    "href": "22-module.html#model-selection-using-aic",
    "title": "22¬† Model Selection in Regression",
    "section": "22.6 Model Selection Using AIC",
    "text": "22.6 Model Selection Using AIC\n\nStepwise Selection using {MASS}\nThere are two R functions that can act as further shortcuts for this process that use the Akaike Information Criterion (AIC) rather than partial F tests to determine relative model fit. We will talk in more detail about how we get AIC in our upcoming modules on GLM and mixed modeling as well.\nThe AIC is typically calculated as -2(log-likelihood) + 2K, where \\(K\\) is the number of model parameters (i.e., the number of \\(\\beta\\) coefficients we estimate, which equals the number of variables in the model, plus the intercept), and the log-likelihood is a measure of model fit (the higher, the better). [Keep in mind, though, that log-likelihood is a function of sample size, and larger samples will tend to have lower log-likelihoods regardless of fit!] We will not derive log-likelihoods by hand in class, but if you want to know more about how log-likelihood values are derived, you can check out this helpful resource.\nAs you might guess from how AIC is defined, the model with the lowest AIC is typically designated as the best fit for the data. Note that although AIC can be used to assess the relative fit of a model to a certain data set, it really does not say anything about the absolute fit or explanatory value of the model (e.g., like the \\(R^2\\) value can). Keep in mind that the best fit model according to AIC, among the models you test against each other, may actually explain very little of the overall variation.\nThere are many functions within R that will perform stepwise model reduction automatically using AIC as a criterion. One of the most popular is called stepAIC(), which is a function in the {MASS} package. To use it, you must simply specify the most complex version of the model and choose whether you would like to assess the model using backwards or forwards (or both) methods of stepwise comparison.\nLet‚Äôs try stepAIC() with our m0 from above. In the call for the function, we can ask it to run forward, backward, or both directions.\n\nlibrary(MASS)\n\n## \n## Attaching package: 'MASS'\n\n\n## The following object is masked from 'package:dplyr':\n## \n##     select\n\n# start with full model...\nm &lt;- lm(data = z, height ~ age + weight + zombies_killed + years_of_education)\n(s &lt;- stepAIC(m, scope = . ~ ., direction = \"both\"))\n\n## Start:  AIC=994.88\n## height ~ age + weight + zombies_killed + years_of_education\n## \n##                      Df Sum of Sq     RSS     AIC\n## - years_of_education  1       0.9  2678.3  993.20\n## - zombies_killed      1       2.7  2680.2  993.89\n## &lt;none&gt;                             2677.5  994.88\n## - age                 1    2993.7  5671.2 1743.40\n## - weight              1    8066.3 10743.7 2382.32\n## \n## Step:  AIC=993.2\n## height ~ age + weight + zombies_killed\n## \n##                      Df Sum of Sq     RSS     AIC\n## - zombies_killed      1       2.6  2681.0  992.17\n## &lt;none&gt;                             2678.3  993.20\n## + years_of_education  1       0.9  2677.5  994.88\n## - age                 1    3010.3  5688.6 1744.47\n## - weight              1    8073.4 10751.7 2381.07\n## \n## Step:  AIC=992.17\n## height ~ age + weight\n## \n##                      Df Sum of Sq     RSS     AIC\n## &lt;none&gt;                             2681.0  992.17\n## + zombies_killed      1       2.6  2678.3  993.20\n## + years_of_education  1       0.8  2680.2  993.89\n## - age                 1    3012.8  5693.8 1743.38\n## - weight              1    8075.7 10756.6 2379.52\n\n\n## \n## Call:\n## lm(formula = height ~ age + weight, data = z)\n## \n## Coefficients:\n## (Intercept)          age       weight  \n##     31.7634       0.6183       0.1631\n\nsummary(s)\n\n## \n## Call:\n## lm(formula = height ~ age + weight, data = z)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.2278 -1.1782 -0.0574  1.1566  5.4117 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 31.763388   0.470797   67.47   &lt;2e-16 ***\n## age          0.618270   0.018471   33.47   &lt;2e-16 ***\n## weight       0.163107   0.002976   54.80   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.64 on 997 degrees of freedom\n## Multiple R-squared:  0.8555, Adjusted R-squared:  0.8553 \n## F-statistic:  2952 on 2 and 997 DF,  p-value: &lt; 2.2e-16\n\n# ... or start with minimal model\nm &lt;- lm(data = z, height ~ 1)\n(s &lt;- stepAIC(m, scope = . ~ . + age + weight + zombies_killed + years_of_education,\n    direction = \"both\"))\n\n## Start:  AIC=2922.93\n## height ~ 1\n## \n##                      Df Sum of Sq     RSS    AIC\n## + weight              1   12864.8  5693.8 1743.4\n## + age                 1    7802.0 10756.6 2379.5\n## + years_of_education  1      81.6 18477.0 2920.5\n## &lt;none&gt;                            18558.6 2922.9\n## + zombies_killed      1       5.7 18552.9 2924.6\n## \n## Step:  AIC=1743.38\n## height ~ weight\n## \n##                      Df Sum of Sq     RSS     AIC\n## + age                 1    3012.8  2681.0  992.17\n## + years_of_education  1      16.9  5676.9 1742.40\n## &lt;none&gt;                             5693.8 1743.38\n## + zombies_killed      1       5.2  5688.6 1744.47\n## - weight              1   12864.8 18558.6 2922.93\n## \n## Step:  AIC=992.17\n## height ~ weight + age\n## \n##                      Df Sum of Sq     RSS     AIC\n## &lt;none&gt;                             2681.0  992.17\n## + zombies_killed      1       2.6  2678.3  993.20\n## + years_of_education  1       0.8  2680.2  993.89\n## - age                 1    3012.8  5693.8 1743.38\n## - weight              1    8075.7 10756.6 2379.52\n\n\n## \n## Call:\n## lm(formula = height ~ weight + age, data = z)\n## \n## Coefficients:\n## (Intercept)       weight          age  \n##     31.7634       0.1631       0.6183\n\nsummary(s)\n\n## \n## Call:\n## lm(formula = height ~ weight + age, data = z)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.2278 -1.1782 -0.0574  1.1566  5.4117 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 31.763388   0.470797   67.47   &lt;2e-16 ***\n## weight       0.163107   0.002976   54.80   &lt;2e-16 ***\n## age          0.618270   0.018471   33.47   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.64 on 997 degrees of freedom\n## Multiple R-squared:  0.8555, Adjusted R-squared:  0.8553 \n## F-statistic:  2952 on 2 and 997 DF,  p-value: &lt; 2.2e-16\n\ndetach(package:MASS)\n\nNote that the stepAIC() function, whether we start with the full model more with a null model, has converged (using AIC, in this case) on the same best model we found above through forward and backwards selection by hand - i.e., the model with just age and weight retained as predictor variables after our process of model selection.\nFinally, there‚Äôs a very helpful model selection package called {AICcmodavg}. You may have noticed the extra ‚Äúc‚Äù in AICc‚Ä¶ this is a ‚Äúcorrected‚Äù version of AIC that can account for small sample sizes (helpful for most of us in anthropology and biology). It is derived from AIC using the following equation (which you do not need to know!):\n\\[AIC_c = AIC + \\frac{2K^2 + 2K}{n-K-1}\\]\nwhere \\(n\\) is sample size and \\(K\\) is the number of parameters in the model. This is, essentially, a version of AIC with greater penalties for models with more parameters. Since the values for AICc and AIC converge as sample size increases, it has been argued that AICc should be the default model testing criterion rather than AIC.\nWhat makes this wrapper especially useful is that the table output of the function very clearly compares AIC values for different models in a way often asked for in publications. Let‚Äôs take a look using our already-defined models.\n\nlibrary(AICcmodavg)\naictab(list(m0, m1, m2), c(\"m0\", \"m1\", \"m2\"))\n\n## \n## Model selection based on AICc:\n## \n##    K    AICc Delta_AICc AICcWt Cum.Wt       LL\n## m2 4 3832.09       0.00   0.54   0.54 -1912.03\n## m1 5 3833.13       1.04   0.32   0.86 -1911.54\n## m0 6 3834.84       2.75   0.14   1.00 -1911.38\n\ndetach(package:AICcmodavg)\n\nWe get a few types of output that allow us to directly compare the relative fit of each model: the number of parameters (k), the AICc scores, and a number of other helpful outputs. Delta AICc (or \\(\\Delta_i(AIC)=AIC_i-min(AIC)\\)) is simply a comparison showing how different each ‚Äúleast-best‚Äù model is from the best model in AICc score (i.e., the difference in their AICc scores). AICcWt shows us the ‚Äúweight‚Äù of the model at each level (see here for an explanation of what this means). In this case, the best model is the first on the list and is, like in all our other model selection methods, m2.\n\n\nAutomated Subset Selection and Model Averaging using {MuMIn}\nAn alternative approach to model selection using AIC is provided by using the function dredge() in the {MuMIn} package, which explores subsets of a given global model in an automated way. With dredge(), we first run a global model that includes all of the terms we want to consider and then we run dredge() to fit subsets of that model without having to specify explicitly which submodels we want to run. The dredge() function returns a ‚Äúmodel selection‚Äù table with models ranked in order of increasing AICc (i.e., lowest, or ‚Äúbest‚Äù, first).\nWe start by defining a ‚Äúglobal.model‚Äù‚Ä¶\n\nlibrary(MuMIn)\nm &lt;- lm(data = z, height ~ age + weight + zombies_killed + years_of_education, na.action = \"na.fail\")\n\n\nNOTE: Here, we need to change the ‚Äúna.action‚Äù argument from the default of ‚Äúna.omit‚Äù to ‚Äúna.fail‚Äù to prevent dredge() from trying to fit submodels with different data sets than the global model if there are missing values.\n\n\nAlternatively, we could set that option globally for our R session before running the global model by using options(na.action = na.fail) and then omitting the na.action= argument from our call to lm().\n\n\nWe could also filter our dataset to only include rows where none of the variables of interest have missing data using, e.g., drop_na().\n\n\nThe command options()$na.action shows the current ‚Äúna.action‚Äù option.\n\nWe then run dredge() to explore a set of submodels‚Ä¶\n\nthe m.lim= argument is optional and sets lower and upper limits on the number of terms to include\nthe beta= argument specifies whether or not and how to standardize the beta coefficients; ‚Äúnone‚Äù is the default\n\n\nmods &lt;- dredge(m, beta = \"none\", m.lim = c(0, 4))\n\n## Fixed term is \"(Intercept)\"\n\nclass(mods)\n\n## [1] \"model.selection\" \"data.frame\"\n\n\nThe dredge() function returns a ‚Äúmodel.selection‚Äù object summarizing all of the models that were tested. The get.models() function can be used to return a list of these models.\n\n(mods.list &lt;- get.models(mods, subset = TRUE))\n\n## $`4`\n## \n## Call:\n## lm(formula = height ~ age + weight + 1, data = z, na.action = \"na.fail\")\n## \n## Coefficients:\n## (Intercept)          age       weight  \n##     31.7634       0.6183       0.1631  \n## \n## \n## $`12`\n## \n## Call:\n## lm(formula = height ~ age + weight + zombies_killed + 1, data = z, \n##     na.action = \"na.fail\")\n## \n## Coefficients:\n##    (Intercept)             age          weight  zombies_killed  \n##       31.66186         0.61805         0.16323         0.02935  \n## \n## \n## $`8`\n## \n## Call:\n## lm(formula = height ~ age + weight + years_of_education + 1, \n##     data = z, na.action = \"na.fail\")\n## \n## Coefficients:\n##        (Intercept)                 age              weight  years_of_education  \n##           31.73031             0.61768             0.16307             0.01668  \n## \n## \n## $`16`\n## \n## Call:\n## lm(formula = height ~ age + weight + years_of_education + zombies_killed + \n##     1, data = z, na.action = \"na.fail\")\n## \n## Coefficients:\n##        (Intercept)                 age              weight  years_of_education  \n##           31.62571             0.61744             0.16320             0.01748  \n##     zombies_killed  \n##            0.02978  \n## \n## \n## $`7`\n## \n## Call:\n## lm(formula = height ~ weight + years_of_education + 1, data = z, \n##     na.action = \"na.fail\")\n## \n## Coefficients:\n##        (Intercept)              weight  years_of_education  \n##           39.37682             0.19471             0.07771  \n## \n## \n## $`3`\n## \n## Call:\n## lm(formula = height ~ weight + 1, data = z, na.action = \"na.fail\")\n## \n## Coefficients:\n## (Intercept)       weight  \n##      39.565        0.195  \n## \n## \n## $`15`\n## \n## Call:\n## lm(formula = height ~ weight + years_of_education + zombies_killed + \n##     1, data = z, na.action = \"na.fail\")\n## \n## Coefficients:\n##        (Intercept)              weight  years_of_education      zombies_killed  \n##           39.22119             0.19487             0.07883             0.04304  \n## \n## \n## $`11`\n## \n## Call:\n## lm(formula = height ~ weight + zombies_killed + 1, data = z, \n##     na.action = \"na.fail\")\n## \n## Coefficients:\n##    (Intercept)          weight  zombies_killed  \n##       39.41922         0.19518         0.04116  \n## \n## \n## $`2`\n## \n## Call:\n## lm(formula = height ~ age + 1, data = z, na.action = \"na.fail\")\n## \n## Coefficients:\n## (Intercept)          age  \n##     48.7357       0.9425  \n## \n## \n## $`6`\n## \n## Call:\n## lm(formula = height ~ age + years_of_education + 1, data = z, \n##     na.action = \"na.fail\")\n## \n## Coefficients:\n##        (Intercept)                 age  years_of_education  \n##           48.61525             0.94036             0.05458  \n## \n## \n## $`10`\n## \n## Call:\n## lm(formula = height ~ age + zombies_killed + 1, data = z, na.action = \"na.fail\")\n## \n## Coefficients:\n##    (Intercept)             age  zombies_killed  \n##       48.85642         0.94246        -0.04006  \n## \n## \n## $`14`\n## \n## Call:\n## lm(formula = height ~ age + years_of_education + zombies_killed + \n##     1, data = z, na.action = \"na.fail\")\n## \n## Coefficients:\n##        (Intercept)                 age  years_of_education      zombies_killed  \n##            48.7343              0.9404              0.0535             -0.0387  \n## \n## \n## $`5`\n## \n## Call:\n## lm(formula = height ~ years_of_education + 1, data = z, na.action = \"na.fail\")\n## \n## Coefficients:\n##        (Intercept)  years_of_education  \n##            67.1195              0.1704  \n## \n## \n## $`13`\n## \n## Call:\n## lm(formula = height ~ years_of_education + zombies_killed + 1, \n##     data = z, na.action = \"na.fail\")\n## \n## Coefficients:\n##        (Intercept)  years_of_education      zombies_killed  \n##           67.23863             0.16936            -0.03875  \n## \n## \n## $`1`\n## \n## Call:\n## lm(formula = height ~ 1, data = z, na.action = \"na.fail\")\n## \n## Coefficients:\n## (Intercept)  \n##       67.63  \n## \n## \n## $`9`\n## \n## Call:\n## lm(formula = height ~ zombies_killed + 1, data = z, na.action = \"na.fail\")\n## \n## Coefficients:\n##    (Intercept)  zombies_killed  \n##       67.75898        -0.04308  \n## \n## \n## attr(,\"rank\")\n## function (x) \n## do.call(\"rank\", list(x))\n## &lt;environment: 0x112caba88&gt;\n## attr(,\"call\")\n## AICc(x)\n## attr(,\"class\")\n## [1] \"function\"     \"rankFunction\"\n## attr(,\"beta\")\n## [1] \"none\"\n\n\nRunning the coef() function on the output of dredge() returns a table of coefficients from each model, where the rowname is the number of the corresponding model in the ‚Äúmodel.selection‚Äù object.\n\ncoef(mods)\n\n##    (Intercept)       age    weight zombies_killed years_of_education\n## 4     31.76339 0.6182699 0.1631067             NA                 NA\n## 12    31.66186 0.6180530 0.1632323     0.02934761                 NA\n## 8     31.73031 0.6176839 0.1630710             NA         0.01667747\n## 16    31.62571 0.6174357 0.1631967     0.02977682         0.01747568\n## 7     39.37682        NA 0.1947115             NA         0.07771446\n## 3     39.56545        NA 0.1950187             NA                 NA\n## 15    39.22119        NA 0.1948748     0.04304143         0.07883279\n## 11    39.41922        NA 0.1951791     0.04115847                 NA\n## 2     48.73566 0.9425086        NA             NA                 NA\n## 6     48.61525 0.9403588        NA             NA         0.05457557\n## 10    48.85642 0.9424641        NA    -0.04006155                 NA\n## 14    48.73428 0.9403581        NA    -0.03870129         0.05350017\n## 5     67.11947        NA        NA             NA         0.17043620\n## 13    67.23863        NA        NA    -0.03874818         0.16935942\n## 1     67.63010        NA        NA             NA                 NA\n## 9     67.75898        NA        NA    -0.04307553                 NA\n\n\n\n\n22.6.1 Averaging Coefficients across Sets of Models\nThe code below averages beta coefficients for the set of models with Delta AICc &lt; 4. Here, we also need to set the argument fit=TRUE because dredge() does not actually store the model results. The model.avg() function returns a ‚Äúsummary.averaging‚Äù object.\n\n(mods.avg &lt;- summary(model.avg(mods, subset = delta &lt; 4, fit = TRUE)))\n\n## \n## Call:\n## model.avg(object = get.models(object = mods, subset = delta &lt; \n##     4))\n## \n## Component model call: \n## lm(formula = height ~ &lt;4 unique rhs&gt;, data = z, na.action = na.fail)\n## \n## Component models: \n##      df   logLik    AICc delta weight\n## 12    4 -1912.03 3832.09  0.00   0.44\n## 124   5 -1911.54 3833.13  1.04   0.26\n## 123   5 -1911.88 3833.82  1.73   0.19\n## 1234  6 -1911.38 3834.84  2.75   0.11\n## \n## Term codes: \n##                age             weight years_of_education     zombies_killed \n##                  1                  2                  3                  4 \n## \n## Model-averaged coefficients:  \n## (full average) \n##                     Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)    \n## (Intercept)        31.715277   0.479075    0.479651  66.122   &lt;2e-16 ***\n## age                 0.618011   0.018485    0.018508  33.392   &lt;2e-16 ***\n## weight              0.163143   0.002979    0.002982  54.706   &lt;2e-16 ***\n## zombies_killed      0.011014   0.023089    0.023106   0.477    0.634    \n## years_of_education  0.005046   0.018619    0.018638   0.271    0.787    \n##  \n## (conditional average) \n##                     Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)    \n## (Intercept)        31.715277   0.479075    0.479651  66.122   &lt;2e-16 ***\n## age                 0.618011   0.018485    0.018508  33.392   &lt;2e-16 ***\n## weight              0.163143   0.002979    0.002982  54.706   &lt;2e-16 ***\n## zombies_killed      0.029476   0.029707    0.029744   0.991    0.322    \n## years_of_education  0.016977   0.031046    0.031084   0.546    0.585    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nclass(mods.avg)\n\n## [1] \"summary.averaging\" \"averaging\"\n\n\nWe can use the confint() function to confidence intervals for the averaged coefficient estimates across models‚Ä¶\n\nconfint(mods.avg)\n\n##                          2.5 %      97.5 %\n## (Intercept)        30.77517771 32.65537579\n## age                 0.58173646  0.65428595\n## weight              0.15729812  0.16898796\n## zombies_killed     -0.02882057  0.08777229\n## years_of_education -0.04394591  0.07790059\n\n\n\nNOTE: The ‚Äòsubset‚Äô (or ‚Äòconditional‚Äô) average reported is only averaging over models where the term appears. The ‚Äòfull‚Äô average reported assumes that a term is included in every model, but the estimate and standard error are set to zero in models where the term is not included.\n\nThe code below averages beta coefficients for models in the 95% confidence set, i.e., for cumulative AIC weights up to 95%:\n\n(mods.avg &lt;- summary(model.avg(mods, subset = cumsum(weight) &lt;= 0.95, fit = TRUE)))\n\n## \n## Call:\n## model.avg(object = get.models(object = mods, subset = cumsum(weight) &lt;= \n##     0.95))\n## \n## Component model call: \n## lm(formula = height ~ &lt;3 unique rhs&gt;, data = z, na.action = na.fail)\n## \n## Component models: \n##     df   logLik    AICc delta weight\n## 12   4 -1912.03 3832.09  0.00   0.50\n## 124  5 -1911.54 3833.13  1.04   0.29\n## 123  5 -1911.88 3833.82  1.73   0.21\n## \n## Term codes: \n##                age             weight years_of_education     zombies_killed \n##                  1                  2                  3                  4 \n## \n## Model-averaged coefficients:  \n## (full average) \n##                     Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)    \n## (Intercept)        31.726534   0.476970    0.477545  66.437   &lt;2e-16 ***\n## age                 0.618084   0.018481    0.018503  33.404   &lt;2e-16 ***\n## weight              0.163136   0.002978    0.002982  54.711   &lt;2e-16 ***\n## zombies_killed      0.008656   0.020959    0.020974   0.413    0.680    \n## years_of_education  0.003484   0.015723    0.015739   0.221    0.825    \n##  \n## (conditional average) \n##                     Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)    \n## (Intercept)        31.726534   0.476970    0.477545  66.437   &lt;2e-16 ***\n## age                 0.618084   0.018481    0.018503  33.404   &lt;2e-16 ***\n## weight              0.163136   0.002978    0.002982  54.711   &lt;2e-16 ***\n## zombies_killed      0.029348   0.029701    0.029737   0.987    0.324    \n## years_of_education  0.016677   0.031040    0.031078   0.537    0.592    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nconfint(mods.avg)\n\n##                          2.5 %      97.5 %\n## (Intercept)        30.79056281 32.66250541\n## age                 0.58181768  0.65434939\n## weight              0.15729206  0.16898053\n## zombies_killed     -0.02893582  0.08763103\n## years_of_education -0.04423354  0.07758849\n\n\nWith the {MuMIn} package loaded, we can use the plot() function to make a plot of averaged model coefficients, similar to plot_summs() from {jtools} (which unfortunately does not work on ‚Äúsummary.averaging‚Äù objects).\n\nplot(mods.avg, full = TRUE, intercept = FALSE)\n\n\n\n\n\n\n\nplot(mods.avg, full = FALSE, intercept = FALSE)\n\n\n\n\n\n\n\n\n\nNOTE: Omitting the intercept= argument or setting it to TRUE will include the \\(\\beta0\\) coefficient. When full=TRUE, coefficient averaging is done over all models in the selected set (for models where a particular parameter does not appear, the corresponding coefficient and its variance are set to zero). When full=FALSE, coefficient averaging is done only over models in the selected set where the parameter appears.\n\nNote that, here again, the two key predictors of zombie apocalypse survivor‚Äôs height are age and weight, and the best models include both of these terms!",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Model Selection in Regression</span>"
    ]
  },
  {
    "objectID": "22-module.html#concept-review",
    "href": "22-module.html#concept-review",
    "title": "22¬† Model Selection in Regression",
    "section": "Concept Review",
    "text": "Concept Review\n\nModel selection is an iterative process, done as part of a regression analysis, where we sort through our explanatory variables and alternative models involving those variables to discern which are best able to account for variation in the response variable\nThere are various criteria for evaluating alternative models, but all rely on comparing the ‚Äúexplanatory value‚Äù of more complex models versus less complex models\nForward selection evaluates whether adding a variable or interaction among variables to a less complex model increases the explanatory value of the model significantly\nBackwards selection evaluates whether removing a variable or interaction among variables from a more complex model results in a significant loss of explanatory power\nIn comparison to F ratio tests, AIC based model selection penalizes more complex models\nThe dredge() command from the {MuMIn} package can be used to automate aspects of model selection",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Model Selection in Regression</span>"
    ]
  },
  {
    "objectID": "23-module.html",
    "href": "23-module.html",
    "title": "23¬† Generalized Linear Models",
    "section": "",
    "text": "23.1 Objectives",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "23-module.html#objectives",
    "href": "23-module.html#objectives",
    "title": "23¬† Generalized Linear Models",
    "section": "",
    "text": "In this module, we extend our discussion of regression modeling to include generalized linear models.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "23-module.html#preliminaries",
    "href": "23-module.html#preliminaries",
    "title": "23¬† Generalized Linear Models",
    "section": "23.2 Preliminaries",
    "text": "23.2 Preliminaries\n\nInstall and load this package in R: {lmtest}\nLoad {tidyverse} and {broom}",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "23-module.html#extending-the-general-model",
    "href": "23-module.html#extending-the-general-model",
    "title": "23¬† Generalized Linear Models",
    "section": "23.3 Extending the ‚ÄúGeneral‚Äù Model",
    "text": "23.3 Extending the ‚ÄúGeneral‚Äù Model\nSo far, our discussion of regression has centered on standard or ‚Äúgeneral‚Äù linear models that assume that our response variables are continuously distributed and have normally distributed error terms (residuals) from our fitted models and that these error terms show constant variance across the range of our predictor variables. If these assumptions of general linear regression are not met, we can sometimes transform our variables to meet them, but other times we cannot, e.g., when we have binary or count data for a response variable.\nIn these cases, however, we can use a different regression technique called generalized linear modeling instead of general linear modeling. Generalized linear models, then, extend traditional regression models to allow the expected value of our response variable to depend on our predictor variable(s) through what is called a link function. It allows the response variable to belong to any of a set of distributions belonging to the ‚Äúexponential‚Äù family (e.g., normal, Poisson, binomial), and it does not require errors (residuals) to be normally distributed. It also does not require homogeneity of variance across the range of predictor variable values and allows for overdispersion (when the observed variance is larger than what the model assumes).\nOne of the most important differences is that in generalized linear modeling we no longer use ordinary least squares to estimate parameter values, but rather use maximum likelihood or Bayesian approaches.\nA generalized linear model consists of three components:\n\nThe systematic or linear component, which reflects the linear combination of predictor variables in our model. As in general linear regression, these can be be continuous and/or categorical. Interactions between predictors and polynomial functions of predictors can also be included, as in general linear modeling. [Recall that ‚Äúlinear‚Äù simply means the regression model is based on linear combinations of regression coefficients, not of variables.]\nThe error structure or random component, which refers to the probability distribution of the response variable and of the residuals in the response variable after the linear component has been removed. The probability distribution in a GLM must be from the exponential family of probability distributions, which includes the normal (Gaussian), Poisson (e.g., if our response variable consists of count data), binomial (e.g., if our response variable is binary, like ‚Äúyes/no‚Äù, ‚Äúpresence/absence‚Äù), gamma, negative binomial, etc.\nA link function, which links the expected value of the response variable to the predictors. You can think of this as a transformation function. In GLM, as in general linear modeling, our linear component yields a predicted value, but this value is not the predicted value of our response variable, \\(Y\\), per se. Rather, the predicted value from the regression model needs to be transformed back into a predicted \\(Y\\) by applying the inverse of the link function.\n\n\nNOTE: Using a particular link function is not the same as transforming the response variable. With the link function, we are modeling some function of the expectation of the response variable for given values of the predictor variables, but not the value of the response itself. Link functions typically transform the discontinuous scale of a categorical response variable to a continuous scale that is unbounded and that can thus be examined using regression. Predicted responses from a GLM (and the associated regression coefficients) typically need to be back-transformed to put them into the more intuitive scale of the original response variable.\n\nCommon link functions include:\n\nThe identity link, which is used to model \\(\\mu\\), the mean value of \\(Y\\) and is what we use implicitly in standard linear models. That is, our standard (or general) linear regression model is a particular case of the generalized linear model.\nThe log link, which is typically used to model log(\\(\\lambda\\)), the log of the mean value of \\(Y\\). This link function is typically used for modeling for count data.\nThe logit link, which is log(\\(\\pi\\)/(1-\\(\\pi\\))), is typically used for modeling binary data, or ‚Äúlogistic‚Äù regression.\n\n\n\n\nModel\nError Structure\nCanonical Link Function\n\n\n\n\nlinear\nGaussian\nidentity\n\n\nlog linear\nPoisson\nlog\n\n\nlogistic\nbinomial\nlogit\n\n\ngamma\nGamma\nreciprocal (inverse)\n\n\n\nStandard or ‚Äúgeneral‚Äù linear regression can be viewed as a special case of GLM, where the random component of our model has a normal distribution and the link function is the identity link so that we are modeling an expected (mean) value for \\(Y\\).",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "23-module.html#model-fitting-in-glm",
    "href": "23-module.html#model-fitting-in-glm",
    "title": "23¬† Generalized Linear Models",
    "section": "23.4 Model Fitting in GLM",
    "text": "23.4 Model Fitting in GLM\nModel fitting and parameter estimation in GLM is commonly done using a maximum likelihood approach, which is an iterative process. To determine the fit of a given model, a GLM evaluates a linear predictor for each value of the response variable with a particular set of parameter values (i.e., \\(\\beta\\) coefficients), then back-transforms the predicted value into the scale of the \\(Y\\) variable using the inverse of the link function. These predicted values are compared with the observed values of \\(Y\\). The parameters are then adjusted, and the model is refitted on the transformed scale in an iterative procedure until the fit stops improving. In ML approaches, then, the data are taken as a given, and we are trying to find the most likely parameter values and model to fit those data. We judge the fit of the particular model of the basis of how likely the data would be if the model were correct.\nThe measure of discrepancy used in a GLM to assess the goodness of fit of the model to the data is called the deviance, which we can think of as analogous to the variance in a general linear model. Deviance is defined as 2 \\(\\times\\) (the log-likelihood of a ‚Äúfully saturated‚Äù model minus the log-likelihood of the proposed model). The former is a model that fits the data perfectly. Its likelihood is 1 and its log-likelihood is thus 0, so deviance functionally can be calculated as -2 \\(\\times\\) log-likelihood of the proposed model. Because the saturated model does not depend on any estimated parameters and has a likelihood of 1, minimizing the deviance for a particular model is the same as maximizing the likelihood. [For the ML process of parameter estimation, it is actually mathematically easier to maximize the log-likelihood, ln(L), than it is to maximize the likelihood, L, so computationally that is what is usually done.] In generalized linear regression, the sum of squared ‚Äúdeviance residuals‚Äù of all the data points is analogous to the sum of squares of the residuals in a standard linear regression (i.e., this is what the model does not explain).\nThe glm() function in R can be used for many types of generalized linear modeling, using a similar formula notation to that we‚Äôve used before, with an additional argument, family=, to specify the kind of error structure we expect in the response variable (‚Äúgaussian‚Äù, ‚Äúpoisson‚Äù, ‚Äúbinomial‚Äù, etc.): glm(y ~ x, family = \"gaussian\")\n\n\nCode\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/zombies.csv\"\nz &lt;- read_csv(f, col_names = TRUE)\n\n\n## Rows: 1000 Columns: 10\n## ‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n## Delimiter: \",\"\n## chr (4): first_name, last_name, gender, major\n## dbl (6): id, height, weight, zombies_killed, years_of_education, age\n## \n## ‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n## ‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nlm &lt;- lm(data = z, height ~ weight + age)\nsummary(lm)\n\n\n## \n## Call:\n## lm(formula = height ~ weight + age, data = z)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.2278 -1.1782 -0.0574  1.1566  5.4117 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 31.763388   0.470797   67.47   &lt;2e-16 ***\n## weight       0.163107   0.002976   54.80   &lt;2e-16 ***\n## age          0.618270   0.018471   33.47   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.64 on 997 degrees of freedom\n## Multiple R-squared:  0.8555, Adjusted R-squared:  0.8553 \n## F-statistic:  2952 on 2 and 997 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nglm &lt;- glm(data = z, height ~ weight + age, family = \"gaussian\")\nsummary(glm)\n\n\n## \n## Call:\n## glm(formula = height ~ weight + age, family = \"gaussian\", data = z)\n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 31.763388   0.470797   67.47   &lt;2e-16 ***\n## weight       0.163107   0.002976   54.80   &lt;2e-16 ***\n## age          0.618270   0.018471   33.47   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for gaussian family taken to be 2.689027)\n## \n##     Null deviance: 18559  on 999  degrees of freedom\n## Residual deviance:  2681  on 997  degrees of freedom\n## AIC: 3832.1\n## \n## Number of Fisher Scoring iterations: 2\n\n\nAs with previous models, our explanatory variable, \\(X\\), can be continuous (leading to a regression analysis) or categorical (leading to an ANOVA-like procedure called an analysis of deviance) or both.\n\n\nCode\nz$gender &lt;- factor(z$gender)\nlm &lt;- lm(data = z, height ~ weight + gender)\nsummary(lm)\n\n\n## \n## Call:\n## lm(formula = height ~ weight + gender, data = z)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -7.3328 -1.5538 -0.0057  1.4437  9.1060 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 40.714398   0.627933  64.839  &lt; 2e-16 ***\n## weight       0.183945   0.004575  40.206  &lt; 2e-16 ***\n## genderMale   0.878857   0.168299   5.222 2.15e-07 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.358 on 997 degrees of freedom\n## Multiple R-squared:  0.7014, Adjusted R-squared:  0.7008 \n## F-statistic:  1171 on 2 and 997 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nglm &lt;- glm(data = z, height ~ weight + gender, family = \"gaussian\")\nsummary(glm)\n\n\n## \n## Call:\n## glm(formula = height ~ weight + gender, family = \"gaussian\", \n##     data = z)\n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 40.714398   0.627933  64.839  &lt; 2e-16 ***\n## weight       0.183945   0.004575  40.206  &lt; 2e-16 ***\n## genderMale   0.878857   0.168299   5.222 2.15e-07 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for gaussian family taken to be 5.558875)\n## \n##     Null deviance: 18558.6  on 999  degrees of freedom\n## Residual deviance:  5542.2  on 997  degrees of freedom\n## AIC: 4558.3\n## \n## Number of Fisher Scoring iterations: 2",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "23-module.html#common-types-of-glms",
    "href": "23-module.html#common-types-of-glms",
    "title": "23¬† Generalized Linear Models",
    "section": "23.5 Common Types of GLMs",
    "text": "23.5 Common Types of GLMs\nWe will explore two types of GLMs‚Ä¶ logistic regression (used when our response variable is binary) and log-linear or Poisson regression (used when our response variable is count data).\n\nLogistic Regression\nAs alluded to above, when we have a binary response variable (i.e., a categorical variable with two levels, 0 or 1), we actually are interested in modeling \\(\\pi_i\\), which is the probability that \\(Y\\) equals 1 for a given value of \\(X\\) (\\(x_i\\)), rather than \\(\\mu_i\\), the mean value of \\(Y\\) for a given \\(X\\), which is what we typically model with general linear regression. The usual model we fit to such data is the logistic regression model, which is a nonlinear model with a sigmoidal shape. The errors or residuals from such a model are not normally distributed but rather have a binomial distribution.\nWhen we do our regression, we actually use as our response variable the natural log of the odds ratio between our two possible outcomes, i.e., the ratio of the probabilities that \\(y_i\\) = 1 versus that \\(y_i\\) = 0 for a given \\(x_i\\), a ratio which we call the logit:\n\\[g = logit(\\pi_i) = \\ln[\\frac{\\pi_i}{(1-\\pi_i)}] = \\beta_0 + \\beta_1x_i\\]\nwhere \\(\\pi_i\\) = the probability that \\(y_i\\) equals 1 for a given value of \\(x_i\\) and \\((1-\\pi)\\) = the probability that \\(y_i\\) equals 0.\nThe logit transformation, then, is the link function connecting \\(Y\\) to our predictors. The logit is useful as it converts probabilities, which lie in the range 0 to 1, into the scale of the whole real number line.\nWe can convert back from the natural log of the odds ratio to an actual odds ratio using the inverse of the logit, which is called the expit:\n\\[expit(g) = \\frac{e^{g_i}}{1+e^{g_i}} = \\frac{1}{1+e^{-g_i}}=p_i\\]\n\n\nEXAMPLE:\nSuppose we are interested in how a students‚Äô GRE scores, grade point averages (GPA), and ranking of their undergraduate institution (into quartiles, 1 to 4, from high to low), affect admission into graduate school. The response variable, ‚Äúadmitted/not admitted‚Äù, is a binary variable, scored as 1/0.\nLoad in the ‚Äúgraddata.csv‚Äù dataset, which comes from this website, and then explore it using head(), summary(), plot(), pairs(), and table().\n\n\nCode\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/graddata.csv\"\n# or f &lt;- 'https://stats.idre.ucla.edu/stat/data/binary.csv'\nd &lt;- read_csv(f, col_names = TRUE)\n\n\n## Rows: 400 Columns: 4\n## ‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n## Delimiter: \",\"\n## dbl (4): admit, gre, gpa, rank\n## \n## ‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n## ‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nd$rank &lt;- factor(d$rank)\nhead(d)\n\n\n## # A tibble: 6 √ó 4\n##   admit   gre   gpa rank \n##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;\n## 1     0   380  3.61 3    \n## 2     1   660  3.67 3    \n## 3     1   800  4    1    \n## 4     1   640  3.19 4    \n## 5     0   520  2.93 4    \n## 6     1   760  3    2\n\n\nCode\nsummary(d)\n\n\n##      admit             gre             gpa        rank   \n##  Min.   :0.0000   Min.   :220.0   Min.   :2.260   1: 61  \n##  1st Qu.:0.0000   1st Qu.:520.0   1st Qu.:3.130   2:151  \n##  Median :0.0000   Median :580.0   Median :3.395   3:121  \n##  Mean   :0.3175   Mean   :587.7   Mean   :3.390   4: 67  \n##  3rd Qu.:1.0000   3rd Qu.:660.0   3rd Qu.:3.670          \n##  Max.   :1.0000   Max.   :800.0   Max.   :4.000\n\n\nCode\n# first, some exploratory visualization\np &lt;- ggplot() + geom_point(data = d, aes(x = gpa, y = admit), color = \"green\") +\n    xlab(\"GPA\") + ylab(\"Admit\") + ggtitle(\"Pr(Y) versus GPA\")\nq &lt;- ggplot() + geom_point(data = d, aes(x = gpa, y = admit), color = \"blue\") + xlab(\"GRE\") +\n    ylab(\"Admit\") + ggtitle(\"Pr(Y) versus GRE\")\nlibrary(patchwork)\np + q\n\n\n\n\n\n\n\n\n\nCode\ndetach(package:patchwork)\npairs(d)\n\n\n\n\n\n\n\n\n\nCode\n(t &lt;- group_by(d, admit, rank) |&gt;\n    summarize(n = n()) |&gt;\n    pivot_wider(names_from = rank, values_from = n))\n\n\n## `summarise()` has grouped output by 'admit'. You can override using the\n## `.groups` argument.\n\n\n## # A tibble: 2 √ó 5\n## # Groups:   admit [2]\n##   admit   `1`   `2`   `3`   `4`\n##   &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n## 1     0    28    97    93    55\n## 2     1    33    54    28    12\n\n\nTo use logistic regression to look at how the odds of admission is influenced by GRE scores, we can call the glm() function with a model where admit is our response variable and gre is our predictor variable.\n\n# glm of admit~gre\nglm &lt;- glm(data = d, admit ~ gre, family = \"binomial\")\nsummary(glm)\n\n## \n## Call:\n## glm(formula = admit ~ gre, family = \"binomial\", data = d)\n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept) -2.901344   0.606038  -4.787 1.69e-06 ***\n## gre          0.003582   0.000986   3.633  0.00028 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 499.98  on 399  degrees of freedom\n## Residual deviance: 486.06  on 398  degrees of freedom\n## AIC: 490.06\n## \n## Number of Fisher Scoring iterations: 4\n\n\nHere is the equation representing the results of our model: \\[logit(\\pi_i) = -2.901344 + 0.003582 \\times {gre}\\]\n\nInterpretation and Hypothesis Testing\nWhen we get a \\(\\beta_1\\) estimate from a logistic regression, it represents the change in the log(odds ratio) of the outcome for an increase in one unit of our predictor variable, \\(X\\).\nLooking at the coefficient for gre, we see it is positive and, while low, it is significantly different from 0. Thus, increasing GRE score results in an increase in the log(odds ratio) of admission (i.e., students with higher scores are more likely to be admitted). Here, for every one unit change in gre, the log odds of admission (versus non-admission) increases by 0.018172.\nUsing the predict() function, we can plot our data and fit the change in the admitted/not admitted ratio across the range of GRE scores.\n\nx &lt;- seq(from = min(d$gre), to = max(d$gre), by = 1)\nx &lt;- seq(from = 100, to = 900, by = 1)\nlogOR &lt;- predict(glm, newdata = data.frame(gre = x))\n# this function will predict the log(odds ratio)... but if we add the argument\n# `type='response'`, the `predict()` function will return the expected response\n# on the scale of the Y variable, i.e., Pr(Y) running the `exp()` function,\n# i.e., `exp(logOR)` will yield the actual odds ratio!\ny &lt;- predict(glm, newdata = data.frame(gre = x), type = \"response\")\nOR &lt;- exp(logOR)\nres &lt;- tibble(x = x, logOR = logOR, pr_admit = y, OR = OR)\nhead(res)\n\n## # A tibble: 6 √ó 4\n##       x logOR pr_admit     OR\n##   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n## 1   100 -2.54   0.0729 0.0786\n## 2   101 -2.54   0.0731 0.0789\n## 3   102 -2.54   0.0734 0.0792\n## 4   103 -2.53   0.0736 0.0795\n## 5   104 -2.53   0.0739 0.0798\n## 6   105 -2.53   0.0741 0.0800\n\ntail(res)\n\n## # A tibble: 6 √ó 4\n##       x logOR pr_admit    OR\n##   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n## 1   895 0.305    0.576  1.36\n## 2   896 0.308    0.576  1.36\n## 3   897 0.312    0.577  1.37\n## 4   898 0.315    0.578  1.37\n## 5   899 0.319    0.579  1.38\n## 6   900 0.323    0.580  1.38\n\np &lt;- ggplot() + geom_point(data = d, aes(x = gre, y = admit)) + xlab(\"GRE Score\") +\n    ylab(\"Pr(admit)\") + ggtitle(\"Pr(admit) versus GRE Score\") + geom_line(data = res,\n    aes(x = x, y = pr_admit))\np\n\n\n\n\n\n\n\n\nBy exponentiating the regression coefficient, \\(\\beta_1\\), we can get the actual odds ratio change, as opposed to the log(odds ratio) change, associated with a 1 unit change in GRE score.\n\n(ORchange &lt;- exp(glm$coefficients[2]))\n\n##      gre \n## 1.003589\n\n# a 1 unit increase in gre results in a 0.36% increase in the odds of admission\n\nAs in simple (general) linear regression, the key null hypothesis, \\(H_0\\), of relevance when fitting a simple logistic regression model is that our regression coefficient, \\(\\beta\\), is zero, i.e.¬†that there is no relationship between the binary response variable and the predictor variable.\nThere are two common ways to test this \\(H_0\\). The first is to calculate the Wald statistic for the predictor variable and compare this to the standard normal or \\(z\\) distribution. This is like a maximum likelihood-based version of a t test. The Wald statistic is, like a t statistic, our \\(\\beta\\) parameter estimate divided by the standard error of that estimate: \\(\\frac{\\beta_1}{SE( \\beta_1)}\\)). This is most appropriate when sample sizes are large. The ‚Äúz value‚Äù in the summary table for the model shows us the value of the Wald statistic.\n\nsummary(glm)\n\n## \n## Call:\n## glm(formula = admit ~ gre, family = \"binomial\", data = d)\n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept) -2.901344   0.606038  -4.787 1.69e-06 ***\n## gre          0.003582   0.000986   3.633  0.00028 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 499.98  on 399  degrees of freedom\n## Residual deviance: 486.06  on 398  degrees of freedom\n## AIC: 490.06\n## \n## Number of Fisher Scoring iterations: 4\n\n\nIf we wanted to calculate the Wald statistic by hand, we can do so easily. Below, we use the convenient tidy() function from the {broom} package to pull out a table of results from our GLM very easily. Note that tidy() works on lots of different linear model objects, e.g., those created by lm() and glm() (though not lmer()).\n\n(results &lt;- tidy(glm))\n\n## # A tibble: 2 √ó 5\n##   term        estimate std.error statistic    p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n## 1 (Intercept) -2.90     0.606        -4.79 0.00000169\n## 2 gre          0.00358  0.000986      3.63 0.000280\n\nresults$wald &lt;- results$estimate/results$std.error\nresults$p &lt;- 2 * (1 - pnorm(abs(results$wald)))\n# 2 tailed p value associated with the Wald statistic\nresults\n\n## # A tibble: 2 √ó 7\n##   term        estimate std.error statistic    p.value  wald          p\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n## 1 (Intercept) -2.90     0.606        -4.79 0.00000169 -4.79 0.00000169\n## 2 gre          0.00358  0.000986      3.63 0.000280    3.63 0.000280\n\n\nJust as when running an lm(), we can get the confidence intervals around our glm() model estimates several different ways. First, we can use the confint() function, giving our GLM model results as an argument, which yields a CI based on a maximum likelihood approach.\n\nalpha &lt;- 0.05\n(CI &lt;- confint(glm, level = 1 - alpha))\n\n##                    2.5 %       97.5 %\n## (Intercept) -4.119988259 -1.739756286\n## gre          0.001679963  0.005552748\n\n# this function returns a CI based on log-likelihood, an iterative ML process\n\nAlternatively, we can use the confint.default() function, which returns a CI based on standard errors, the way we have calculated them by hand previously.\n\n(CI &lt;- confint.default(glm, level = 1 - alpha))\n\n##                    2.5 %       97.5 %\n## (Intercept) -4.089156159 -1.713532380\n## gre          0.001649677  0.005514746\n\n# which is equivalent to how we have calculated CIs previously\n\nresults$lower &lt;- results$estimate - qnorm(1 - alpha/2) * results$std.error\nresults$upper &lt;- results$estimate + qnorm(1 - alpha/2) * results$std.error\n(CI &lt;- results[, c(\"term\", \"lower\", \"upper\")])\n\n## # A tibble: 2 √ó 3\n##   term           lower    upper\n##   &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;\n## 1 (Intercept) -4.09    -1.71   \n## 2 gre          0.00165  0.00551\n\n\n\n\n\nCHALLENGE\nRepeat the logistic regression above, but using gpa rather than gre as the predictor variable.\n\nIs gpa a significant predictor of the odds of admission?\nWhat is the estimate of \\(\\beta_1\\) and the 95% CI around that estimate?\nHow much does an increase of 1 unit in gpa increase the actual odds ratio (as opposed to the log(odds ratio) for admission?\nWhat is the 95% CI around this odds ratio?\n\n\nHINT: For the latter two questions, you will need to apply the exp() function and convert the log(odds ratio) into an actual odds ratio.\n\n\n\nCode\nglm &lt;- glm(data = d, admit ~ gpa, family = \"binomial\")\nsummary(glm)\n\n\n## \n## Call:\n## glm(formula = admit ~ gpa, family = \"binomial\", data = d)\n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)  -4.3576     1.0353  -4.209 2.57e-05 ***\n## gpa           1.0511     0.2989   3.517 0.000437 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 499.98  on 399  degrees of freedom\n## Residual deviance: 486.97  on 398  degrees of freedom\n## AIC: 490.97\n## \n## Number of Fisher Scoring iterations: 4\n\n\nCode\n(logOR &lt;- glm$coefficients)  # pull out coefficients\n\n\n## (Intercept)         gpa \n##   -4.357587    1.051109\n\n\nCode\n(CI &lt;- confint(glm, level = 1 - alpha))  # calculate CIs\n\n\n##                  2.5 %    97.5 %\n## (Intercept) -6.4334239 -2.367430\n## gpa          0.4742282  1.647959\n\n\nCode\n# exponentiate to convert to odds ratio scale instead of log(odds ratio)\n(OR &lt;- exp(logOR))\n\n\n## (Intercept)         gpa \n##  0.01280926  2.86082123\n\n\nCode\n(CI_OR &lt;- exp(CI))  # exponentiate to convert to odds ratio scal\n\n\n##                   2.5 %     97.5 %\n## (Intercept) 0.001606939 0.09372126\n## gpa         1.606773662 5.19636490\n\n\n\nGraph the probability of admission, i.e., Pr(admit) or \\(\\pi_i\\), for students with GPAs between 2.0 and 4.0 GPAs, superimposed on a point plot of admission (0 or 1) by gpa.\n\n\nHINT: Use the predict() function with type=\"response\" to yield \\(\\pi_i\\) directly.\n\n\n\nCode\nx &lt;- data.frame(gpa = seq(from = 2, to = 4, length.out = 100))\nprediction &lt;- cbind(gpa = x, predict(glm, newdata = x, type = \"response\", se.fit = TRUE))\n# IMPORTANT: Using type='response' returns predictions on the scale of the Y\n# variable, in this case Pr(admit). Using the default for the `type=` argument\n# would return a prediction on the logit scale, i.e., the log(odds ratio),\n# which equals log(Pr(admit)/(1-Pr(admit)))\nhead(prediction)\n\n\n##        gpa        fit     se.fit residual.scale\n## 1 2.000000 0.09488728 0.03825202              1\n## 2 2.020202 0.09672673 0.03840310              1\n## 3 2.040404 0.09859796 0.03854537              1\n## 4 2.060606 0.10050136 0.03867851              1\n## 5 2.080808 0.10243733 0.03880223              1\n## 6 2.101010 0.10440626 0.03891621              1\n\n\nCode\np &lt;- ggplot(data = prediction, aes(x = gpa, y = fit)) + geom_line() + xlab(\"GPA\") +\n    ylab(\"Pr(admit)\") + ggtitle(\"Pr(admit) versus GPA\") + geom_point(data = d, aes(x = gpa,\n    y = admit), alpha = 0.5)\np\n\n\n\n\n\n\n\n\n\nThe predict() function can also be used to get confidence intervals around our estimate of the log(odds of admission) (if the type= argument is unspecified or set to ‚Äúlink‚Äù), or around the estimate of the probability of admission (if the type= argument is set to ‚Äúresponse‚Äù), by using the argument se.fit=TRUE.\n\nalpha &lt;- 0.05\nprediction &lt;- cbind(gpa = x, predict(glm, newdata = x, type = \"response\", se.fit = TRUE))\nprediction$lower &lt;- prediction$fit - qnorm(1 - alpha/2) * prediction$se.fit\nprediction$upper &lt;- prediction$fit + qnorm(1 - alpha/2) * prediction$se.fit\nhead(prediction)\n\n##        gpa        fit     se.fit residual.scale      lower     upper\n## 1 2.000000 0.09488728 0.03825202              1 0.01991469 0.1698599\n## 2 2.020202 0.09672673 0.03840310              1 0.02145803 0.1719954\n## 3 2.040404 0.09859796 0.03854537              1 0.02305043 0.1741455\n## 4 2.060606 0.10050136 0.03867851              1 0.02469287 0.1763098\n## 5 2.080808 0.10243733 0.03880223              1 0.02638636 0.1784883\n## 6 2.101010 0.10440626 0.03891621              1 0.02813189 0.1806806\n\np &lt;- ggplot(prediction, aes(x = gpa, y = fit)) + geom_line() + xlab(\"GPA\") + ylab(\"Pr(admit)\") +\n    ggtitle(\"Pr(admit) versus GPA\") + geom_ribbon(aes(ymin = lower, ymax = upper),\n    alpha = 0.2) + geom_point(data = d, aes(x = gpa, y = admit), alpha = 0.5)\np\n\n\n\n\n\n\n\n\n\nLikelihood Ratio Tests\nTo evaluate the significance of an overall model in a logistic regression, we can compare the fit of a more complex model to that of a nested, reduced model, just as when we discussed model selection approaches for simple linear regression using partial F tests.\nFor example, to test the null hypothesis of \\(\\beta_1\\) = 0 for a simple logistic regression model with a single predictor, we would compare the log-likelihood of the full model to that of the reduced, intercept only model. This is called a likelihood ratio test. Likelihood ratio tests are very similar to partial F tests in that they compare the full model with a nested, reduced model where the explanatory variables of interest are omitted. Now, however, instead of using a test statistic based on a ratio of variances explained by the two models and interpreting that by comparison to an \\(F\\) distribution, we create a test statistic that is a ratio of the log-likelihoods of the two models and interpret that statistic by comparison to a \\(\\chi^2\\) distribution with a given number of degrees of freedom. Still, the underlying idea is conceptually the same.\nA likelihood ratio test comparing a proposed and reduced model can be performed using the anova() function where we specify the test= argument to be either LRT or Chisq (these are equivalent).\nRecall that when we used the anova() function to run F ratio tests to compare the proportion of variance explained by two general linear models, we specified that `test = ‚ÄúF‚Äù and generated an F statistic which was considered relative to an F distribution to calculate a p value. With generalized linear models, because we are working with deviances and likelihood ratios, the test statistic we generate is ratio of likelihoods (which is equivalent to a difference in deviances, see below) rather than a ratio of variances. That statistic is then considered relative to a \\(\\chi^2\\) distribution to determine the p value. The relevant degrees of freedom for the likelihood ratio test is the # of parameters in the proposed model minus the # parameters in the nested model (here, df = 2 - 1 = 1).\n\nproposed &lt;- glm(data = d, admit ~ gpa, family = \"binomial\")\nreduced &lt;- glm(data = d, admit ~ 1, family = \"binomial\")\nanova(reduced, proposed, test = \"LRT\")  # or `test = 'Chisq')\n\n## Analysis of Deviance Table\n## \n## Model 1: admit ~ 1\n## Model 2: admit ~ gpa\n##   Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)    \n## 1       399     499.98                         \n## 2       398     486.97  1   13.009  0.00031 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWith this low p value, we would reject the null hypothesis that removing the variable of interest (gpa) from our model does not result in a loss of fit.\nAlternatively, we could use the function lrtest() from the {lmtest} package to do the same likelihood ratio test of the two models.\n\nlrtest(reduced, proposed)\n\n## Likelihood ratio test\n## \n## Model 1: admit ~ 1\n## Model 2: admit ~ gpa\n##   #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n## 1   1 -249.99                         \n## 2   2 -243.48  1 13.009    0.00031 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe can also perform a likelihood ratio test by hand by taking the difference between the deviances of the two models. The deviance for a generalized linear model is analogous to the the residual sum of squares (i.e., unexplained variance) in a general linear model - i.e, lower deviance and lower RSS both mean better models. Deviance is calculated as a kind of ‚Äúdistance‚Äù of given model from a fully ‚Äúsaturated‚Äù model, i.e., a model where each data point has its own parameters. The likelihood of a fully ‚Äúsaturated‚Äù model is 1, thus its log-likelihood is log(1) = 0.\n\nDeviance = 2 \\(\\times\\) (log-likelihood of the saturated model - log-likelihood of the proposed model)\nDeviance = 2 \\(\\times\\) (0 - log-likelihood of the proposed model)\nDeviance = -2 \\(\\times\\) (log-likelihood of the proposed model)\n\nWe can get the deviance associated with a given glm model object by accessing its $deviance slot or by using the deviance() function with the model object as an argument.\n\n# deviance of intercept only model\n(D_reduced &lt;- reduced$deviance)\n\n## [1] 499.9765\n\n# or...\n(D_reduced &lt;- deviance(reduced))\n\n## [1] 499.9765\n\n# deviance of proposed model with intercept and one predictor\n(D_proposed &lt;- proposed$deviance)\n\n## [1] 486.9676\n\n# or...\n(D_proposed &lt;- deviance(proposed))\n\n## [1] 486.9676\n\n\nThe \\(\\chi^2\\) statistic is then just the difference of the deviances‚Ä¶\n\n(chisq &lt;- D_reduced - D_proposed)\n\n## [1] 13.0089\n\n# this is a measure of how much the fit improves by adding in the predictor\n(p &lt;- 1 - pchisq(chisq, df = 1))\n\n## [1] 0.0003100148\n\n# df = difference in number of parameters in the proposed versus reduced model\n\nThe $null.deviance slot of a model object returns, for any model, the deviance associated with an intercept only null model.\n\n(x2 &lt;- reduced$null.deviance - reduced$deviance)\n\n## [1] 5.115908e-12\n\n# this is essentially 0 because here the `reduced` model *is* the intercept\n# only model!\n(p &lt;- 1 - pchisq(q = 0, df = 0))\n\n## [1] 1\n\n(x2 &lt;- proposed$null.deviance - proposed$deviance)\n\n## [1] 13.0089\n\n(p &lt;- 1 - pchisq(q = x2, df = 1))\n\n## [1] 0.0003100148\n\n# df = difference in number of parameters in the proposed versus reduced model\n\nHere‚Äôs some additional useful information about interpreting the results of summary() from running a logistic regression based on this post from StackExchange and this post from The Analysis Factor\n\nLet \\(n\\) = the number of observations in the data set and \\(K\\) = the number of predictor terms in a proposed model.\n\nThe Saturated Model is a model that assumes each data point has its own parameter, which means we have \\(n\\) parameters to describe the data\nThe Null Model is a model that assumes exactly the opposite, i.e., one parameter describes all of the data points, which means only 1 parameter, the intercept, is estimated from the data\nThe Proposed Model is the model we are fitting by GLM, which has \\(K\\) parameters terms (1 for the intercept, plus 1 for each predictor variable/interaction term)\nNull Deviance = deviance of the Null Model, calculated as 2(log-likelihood of the Saturated Model \\(-\\) log-likelihood of a Null Model), where df = \\(n\\) - 1\nResidual Deviance = 2(log-likelihood of the saturated model \\(-\\) log-likelihood of the proposed model), where df = \\(n\\) - \\(K\\)\n\nIf the null deviance is really small, it means that the Null Model explains the data pretty well. Likewise, if the residual deviance is really small, then the Proposed Model explains the data pretty well.\nIf you want to compare a Proposed Model against a Null, or intercept-only model, then you can look at Null Deviance \\(-\\) Residual Deviance for that model, which yields a difference that can be examined against a \\(\\chi^2\\) distribution with \\(K\\) degrees of freedom.\nIf you want to compare one Proposed Model against a nested Reduced Model, then you can look at Residual Deviance for the Reduced Model \\(-\\) Residual Deviance for the Proposed Model, which again yields a difference that can be examined against a \\(\\chi^2\\) distribution with df = (\\(K\\) for Proposed Model) - (\\(K\\) for Reduced Model).\nThis deviance-based, \\(\\chi^2\\)-like statistic is also referred to as a G Square or G statistic. If the p value associated with this statistic is less than the alpha level, it means that the the fuller model is associated with significantly reduced deviance relative to the nested reduced model, and thus has a better fit. We would thus reject the null hypothesis that the fuller model is not better than the reduced one.\n\n\n\nMultiple Logistic Regression\nLogistic regression can be easily extended to situations with multiple predictor variables, including both continuous and categorical variables, as in our discussion of multiple regression under the general linear model.\n\n\n\nCHALLENGE\nUsing the same ‚Äúgraddata.csv‚Äù dataset, run a multiple logistic regression analysis using gpa, gre, and rank to look at student admissions to graduate school. Do not, at first, include any interaction terms.\n\nWhat variables are significant predictors of the log(odds ratio) of admission?\nWhat is the value of the log(odds ratio) coefficient and the 95% CIs around that value for the two continuous variable (gpa and gre), when taking the effects of the other and of rank into account? What do these translate into on the actual odds ratio scale?\n\n\n\nCode\n# model with 3 predictors\nglmGGR &lt;- glm(data = d, formula = admit ~ gpa + gre + rank, family = binomial)\nsummary(glmGGR)\n\n\n## \n## Call:\n## glm(formula = admit ~ gpa + gre + rank, family = binomial, data = d)\n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept) -3.989979   1.139951  -3.500 0.000465 ***\n## gpa          0.804038   0.331819   2.423 0.015388 *  \n## gre          0.002264   0.001094   2.070 0.038465 *  \n## rank2       -0.675443   0.316490  -2.134 0.032829 *  \n## rank3       -1.340204   0.345306  -3.881 0.000104 ***\n## rank4       -1.551464   0.417832  -3.713 0.000205 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 499.98  on 399  degrees of freedom\n## Residual deviance: 458.52  on 394  degrees of freedom\n## AIC: 470.52\n## \n## Number of Fisher Scoring iterations: 4\n\n\nCode\nlog_coeff &lt;- glmGGR$coefficients  # extract coefficients...\nlog_coeff_CI &lt;- confint(glmGGR, level = 1 - alpha)\n# and calculate 95% CIs around them\n(results &lt;- cbind(log_coeff, log_coeff_CI))  # none include 0!\n\n\n##                log_coeff         2.5 %       97.5 %\n## (Intercept) -3.989979073 -6.2716202334 -1.792547080\n## gpa          0.804037549  0.1602959439  1.464142727\n## gre          0.002264426  0.0001375921  0.004435874\n## rank2       -0.675442928 -1.3008888002 -0.056745722\n## rank3       -1.340203916 -2.0276713127 -0.670372346\n## rank4       -1.551463677 -2.4000265384 -0.753542605\n\n\nCode\n# convert results out of log scale\ncoeff &lt;- exp(log_coeff)  # convert results out of log scale\ncoeff_CI &lt;- exp(log_coeff_CI)\n(results &lt;- cbind(coeff, coeff_CI))\n\n\n##                 coeff       2.5 %    97.5 %\n## (Intercept) 0.0185001 0.001889165 0.1665354\n## gpa         2.2345448 1.173858216 4.3238349\n## gre         1.0022670 1.000137602 1.0044457\n## rank2       0.5089310 0.272289674 0.9448343\n## rank3       0.2617923 0.131641717 0.5115181\n## rank4       0.2119375 0.090715546 0.4706961\n\n\n\nIs the model including all three predictors better than models that include just two predictors?\n\n\n# Compare 2 versus 3 factor models\nglmGG &lt;- glm(data = d, formula = admit ~ gpa + gre, family = binomial)\nglmGR &lt;- glm(data = d, formula = admit ~ gpa + rank, family = binomial)\nglmRG &lt;- glm(data = d, formula = admit ~ gre + rank, family = binomial)\nlrtest(glmGG, glmGGR)  # or anova(glmGG, glmGGR, test='Chisq')\n\n## Likelihood ratio test\n## \n## Model 1: admit ~ gpa + gre\n## Model 2: admit ~ gpa + gre + rank\n##   #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n## 1   3 -240.17                         \n## 2   6 -229.26  3 21.826  7.088e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nlrtest(glmGR, glmGGR)  # or anova(glmGR, glmGGR, test='Chisq')\n\n## Likelihood ratio test\n## \n## Model 1: admit ~ gpa + rank\n## Model 2: admit ~ gpa + gre + rank\n##   #Df  LogLik Df  Chisq Pr(&gt;Chisq)  \n## 1   5 -231.44                       \n## 2   6 -229.26  1 4.3578    0.03684 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nlrtest(glmRG, glmGGR)  # or anova(glmRG, glmGGR, test='Chisq')\n\n## Likelihood ratio test\n## \n## Model 1: admit ~ gre + rank\n## Model 2: admit ~ gpa + gre + rank\n##   #Df  LogLik Df  Chisq Pr(&gt;Chisq)  \n## 1   5 -232.27                       \n## 2   6 -229.26  1 6.0143    0.01419 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe 3-factor model is indeed better and any of the 2-factor models!\n\n\nCompare a model that includes the three predictors with no interactions to one that includes the three predictors and all possible interactions.\n\n\n# Compare model with and model without interactions\nglmNO &lt;- glm(data = d, admit ~ rank + gpa + gre, family = \"binomial\")\nglmALL &lt;- glm(data = d, admit ~ rank * gpa * gre, family = \"binomial\")\nlrtest(glmNO, glmALL)  # or anova(glmNO,glmALL, test='Chisq')\n\n## Likelihood ratio test\n## \n## Model 1: admit ~ rank + gpa + gre\n## Model 2: admit ~ rank * gpa * gre\n##   #Df  LogLik Df  Chisq Pr(&gt;Chisq)\n## 1   6 -229.26                     \n## 2  16 -225.19 10 8.1317      0.616\n\n\n\nAdding interaction terms to the 3-factor model does not significantly decrease the deviance!\n\n\n\nLog-Linear or Poisson Regression\nSometimes, we want to model a response variable that is in the form of count data (e.g., species richness on an island in terms of distance from the mainland, number of plants of a particular species found in a sampling plot in relation to altitude). Many discrete response variables have counts as possible outcomes. Binomial counts are the number of successes \\(x\\) in a fixed number of trials, \\(n\\). Poisson counts are the number occurrences of some event in a certain interval of time (or space). While binomial counts only take values between 0 and n, Poisson counts have no upper bound. We are going to focus on Poisson counts here.\nAs we have discussed before, for Poisson-distributed variables, the mean and the variance are equal and represented by a single parameter (\\(\\lambda\\)), and therefore linear models based on normal distributions may not be appropriate. We have seen that sometimes we can simply transform a response variable with some kind of power transformation to make it appear more normally distributed, but an alternative is to use a GLM with a Poisson error term and use a log transformation as the link function, resulting in in a log-linear model. Thus, when we do Poisson regression, our regression model tries to predict the natural log of the expected value of \\(Y\\), i.e., \\(\\lambda_i\\).\nThe link function is thus‚Ä¶\n\\[g=log(\\mu_i)\\]\n‚Ä¶ and the inverse link function is‚Ä¶\n\\[\\lambda_i = e^{\\lambda_i}\\]\nOur regression model formulation is the same as in logistic regression, above, except we use family=\"Poisson\", e.g, glm(y ~ x, family = \"poisson\")\n\n\nEXAMPLE:\nResearchers studied the reproductive success of a set of male woolly monkeys over a period of 8 years. The age of each monkey at the beginning of the study and the number of successful matings they had during the 8 years were recorded, and they were also scored into ranks of ‚Äúhigh‚Äù, ‚Äúmedium‚Äù, and ‚Äúlow‚Äù. We assume the number of matings follows a Poisson distribution, and we are interested in exploring whether mating success depends on the age of the monkey in question.\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/woollydata.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\n\n## Rows: 60 Columns: 4\n## ‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n## Delimiter: \",\"\n## chr (2): name, rank\n## dbl (2): success, age\n## \n## ‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n## ‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(d)\n\n## # A tibble: 6 √ó 4\n##   name    success   age rank  \n##   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; \n## 1 Aaron        15  9.8  medium\n## 2 Adam         14 12.1  medium\n## 3 Agustin      22 16.3  high  \n## 4 Alan          9  8.79 low   \n## 5 Andrew       12  6.81 low   \n## 6 Anthony      11 14.8  low\n\nd$name &lt;- factor(d$name)\nd$rank &lt;- factor(d$rank)\nsummary(d)\n\n##       name       success           age             rank   \n##  James  : 2   Min.   : 5.00   Min.   : 5.740   high  :24  \n##  Aaron  : 1   1st Qu.: 9.00   1st Qu.: 9.555   low   :24  \n##  Adam   : 1   Median :12.00   Median :10.765   medium:12  \n##  Agustin: 1   Mean   :12.12   Mean   :10.884              \n##  Alan   : 1   3rd Qu.:14.00   3rd Qu.:12.285              \n##  Andrew : 1   Max.   :24.00   Max.   :16.330              \n##  (Other):53\n\n# first, some exploratory visualization\n(p &lt;- ggplot(data = d, aes(x = age, y = success)) + geom_point() + xlab(\"Age\") +\n    ylab(\"Mating Success\"))\n\n\n\n\n\n\n\npairs(d)\n\n\n\n\n\n\n\ntable(d$rank, d$success)\n\n##         \n##          5 6 7 8 9 10 11 12 13 14 15 16 17 19 20 21 22 24\n##   high   1 1 1 1 3  1  2  3  2  2  2  1  1  1  0  0  1  1\n##   low    0 1 1 4 4  2  2  5  2  1  1  0  1  0  0  0  0  0\n##   medium 0 0 0 0 1  1  2  1  1  2  1  0  0  1  1  1  0  0\n\n# glm of success ~ age\nglm &lt;- glm(data = d, success ~ age, family = \"poisson\")\nsummary(glm)\n\n## \n## Call:\n## glm(formula = success ~ age, family = \"poisson\", data = d)\n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)  1.84007    0.17696  10.398  &lt; 2e-16 ***\n## age          0.05920    0.01541   3.841 0.000122 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 75.972  on 59  degrees of freedom\n## Residual deviance: 61.179  on 58  degrees of freedom\n## AIC: 322.88\n## \n## Number of Fisher Scoring iterations: 4\n\n# we can generate a nice summary table of results with `tidy()` and by setting\n# 'conf.int = TRUE' we use an ML approach to estimate the CI\n(results &lt;- tidy(glm, conf.int = TRUE, conf.level = 0.95))\n\n## # A tibble: 2 √ó 7\n##   term        estimate std.error statistic  p.value conf.low conf.high\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)   1.84      0.177      10.4  2.52e-25   1.49      2.18  \n## 2 age           0.0592    0.0154      3.84 1.22e- 4   0.0290    0.0894\n\n\nIn the summary(), note that the residual deviance is slighly higher than the residual degrees of freedom, which suggests that our data are slightly overdispersed (i.e., there is some extra, unexplained variation in the response, where the variance is greater than the mean). If this were dramatic, we might use ‚Äúquasipoisson‚Äù for the family instead, but we will stick with ‚Äúpoisson‚Äù.\nNow, let‚Äôs fit a ‚Äúline‚Äù of best fit through our data, along with 95% CI around this ‚Äúline‚Äù. We want to plot the relationship between success, rather than log(success), and age, so this relationship will not actually be linear but log-linear.\n\nalpha &lt;- 0.05\nx &lt;- data.frame(age = seq(from = 5, to = 17, by = 1))\nprediction &lt;- cbind(age = x, predict(glm, newdata = x, type = \"response\", se.fit = TRUE))\n# IMPORTANT: Using the argument `type='response'` makes our prediction be in\n# units of our actual Y variable, success, rather than log(success)\nprediction$lower &lt;- prediction$fit - qnorm(1 - alpha/2) * prediction$se.fit\nprediction$upper &lt;- prediction$fit + qnorm(1 - alpha/2) * prediction$se.fit\nhead(prediction)\n\n##   age       fit    se.fit residual.scale     lower    upper\n## 1   5  8.465986 0.8711054              1  6.758651 10.17332\n## 2   6  8.982273 0.7966819              1  7.420806 10.54374\n## 3   7  9.530046 0.7145099              1  8.129632 10.93046\n## 4   8 10.111223 0.6274232              1  8.881497 11.34095\n## 5   9 10.727844 0.5422279              1  9.665096 11.79059\n## 6  10 11.382067 0.4739422              1 10.453158 12.31098\n\np &lt;- p + geom_line(data = prediction, aes(x = age, y = fit)) + geom_ribbon(data = prediction,\n    aes(x = age, y = fit, ymin = lower, ymax = upper), alpha = 0.2)\np  # note the curvilinear 'line' of best fit\n\n\n\n\n\n\n\n\nIs this model better than an intercept-only model? YES!\nWe can see this by doing a likelihood ratio test.\n\nproposed &lt;- glm(data = d, success ~ age, family = \"poisson\")\nreduced &lt;- glm(data = d, success ~ 1, family = \"poisson\")\n# using the `lrtest()` function\nlrtest(reduced, proposed)\n\n## Likelihood ratio test\n## \n## Model 1: success ~ 1\n## Model 2: success ~ age\n##   #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n## 1   1 -166.84                         \n## 2   2 -159.44  1 14.794  0.0001199 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# or, using the `anova()` function\nanova(reduced, proposed, test = \"Chisq\")\n\n## Analysis of Deviance Table\n## \n## Model 1: success ~ 1\n## Model 2: success ~ age\n##   Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n## 1        59     75.972                          \n## 2        58     61.179  1   14.794 0.0001199 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# or, based on the deviance difference between the reduced and proposed models\n(x2 &lt;- reduced$deviance - proposed$deviance)\n\n## [1] 14.79362\n\n(p &lt;- 1 - pchisq(x2, df = 1))\n\n## [1] 0.0001199407\n\n# or, based on hand calculating model deviances using the `logLik()` function,\n# which returns the log-likelihood of a model\n(D_reduced = -2 * logLik(reduced))\n\n## 'log Lik.' 333.6736 (df=1)\n\n(D_proposed = -2 * logLik(proposed))\n\n## 'log Lik.' 318.8799 (df=2)\n\n(x2 &lt;- D_reduced - D_proposed)\n\n## 'log Lik.' 14.79362 (df=1)\n\n(p &lt;- 1 - pchisq(x2, df = 1))\n\n## 'log Lik.' 0.0001199407 (df=1)\n\n# df = difference in # of parameters in the proposed versus reduced model\n\nAs mentioned in Module 22, the Akaike Information Criterion, or AIC, is another way of evaluating and comparing related models. For similar models, those with lower AIC models are preferred over those with higher AIC. The AIC value is based on the deviance associated with the model, but it penalizes model complexity. Much like an adjusted \\(R^2\\), its intent is to prevent you from including irrelevant predictors when choosing among similar models. Models with low AICs represent a better fit to the data, and if many models have similarly low AICs, you should choose the one with the fewest model terms. For both continuous and categorical predictors, we prefer comparing full and reduced models against one another to test individual terms rather than comparing the fit of all possible models and trying and select a single ‚Äúbest‚Äù one. Thus, AIC values are useful for comparing models, but they are not interpretable on their own. The logLik() function returns the log-likelihood associated with a particular model and can be used to calculate AIC values by hand.\n\n(AICproposed &lt;- 2 * 2 - 2 * logLik(proposed))\n\n## 'log Lik.' 322.8799 (df=2)\n\n# AIC = (2 * # parameters estimated() minus (2 * log-likelihood of model) for\n# the proposed model, we estimated 2 parameters\n(AICreduced &lt;- 2 * 1 - 2 * logLik(reduced))\n\n## 'log Lik.' 335.6736 (df=1)\n\n# for the reduced model, we estimated 1 parameter\n\nHere, the log-likelihood of the model including age as a predictor is much lower than the log-likelihood of the reduced (intercept only) model, so we prefer the former.\n\n\nCHALLENGE\nUsing the woolly monkey mating success data set, explore multiple Poisson regression models of [a] mating success in relation to rank and [b] mating success in relation to age + rank, and [c] mating success in relation to age + rank + their interaction on your own. What conclusions can you come to about the importance of rank and rank in combination with age versus age alone?\n\nMating Success in Relation to Rank\n\n\nCode\n# glm of success ~ rank\nglm1 &lt;- glm(data = d, success ~ rank, family = \"poisson\")\nsummary(glm1)\n\n\n## \n## Call:\n## glm(formula = success ~ rank, family = \"poisson\", data = d)\n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)  2.53568    0.05745  44.138   &lt;2e-16 ***\n## ranklow     -0.17247    0.08498  -2.029   0.0424 *  \n## rankmedium   0.10931    0.09601   1.139   0.2549    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 75.972  on 59  degrees of freedom\n## Residual deviance: 67.229  on 57  degrees of freedom\n## AIC: 330.93\n## \n## Number of Fisher Scoring iterations: 4\n\n\nCode\ntidy(glm1, conf.int = TRUE, conf.level = 0.95)\n\n\n## # A tibble: 3 √ó 7\n##   term        estimate std.error statistic p.value conf.low conf.high\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)    2.54     0.0574     44.1   0        2.42     2.65   \n## 2 ranklow       -0.172    0.0850     -2.03  0.0424  -0.340   -0.00621\n## 3 rankmedium     0.109    0.0960      1.14  0.255   -0.0807   0.296\n\n\n\n\nMating Success in Relation to Age + Rank\n\n\nCode\n# glm of success~age+rank\nglm2 &lt;- glm(data = d, success ~ age + rank, family = \"poisson\")\nsummary(glm2)\n\n\n## \n## Call:\n## glm(formula = success ~ age + rank, family = \"poisson\", data = d)\n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)  1.89985    0.18016  10.546  &lt; 2e-16 ***\n## age          0.05848    0.01547   3.780 0.000157 ***\n## ranklow     -0.18419    0.08504  -2.166 0.030324 *  \n## rankmedium   0.07981    0.09633   0.829 0.407344    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 75.972  on 59  degrees of freedom\n## Residual deviance: 52.905  on 56  degrees of freedom\n## AIC: 318.61\n## \n## Number of Fisher Scoring iterations: 4\n\n\nCode\ntidy(glm2, conf.int = TRUE, conf.level = 0.95)\n\n\n## # A tibble: 4 √ó 7\n##   term        estimate std.error statistic  p.value conf.low conf.high\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)   1.90      0.180     10.5   5.32e-26   1.54      2.25  \n## 2 age           0.0585    0.0155     3.78  1.57e- 4   0.0282    0.0888\n## 3 ranklow      -0.184     0.0850    -2.17  3.03e- 2  -0.351    -0.0178\n## 4 rankmedium    0.0798    0.0963     0.829 4.07e- 1  -0.111     0.267\n\n\n\n\nMating Success in Relation to Age + Rank + Their Interaction\n\n\nCode\n# glm of success ~ age + rank + age:rank\nglm3 &lt;- glm(data = d, success ~ age * rank, family = \"poisson\")\nsummary(glm3)\n\n\n## \n## Call:\n## glm(formula = success ~ age * rank, family = \"poisson\", data = d)\n## \n## Coefficients:\n##                Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)     1.68099    0.27350   6.146 7.93e-10 ***\n## age             0.07821    0.02398   3.261  0.00111 ** \n## ranklow         0.35253    0.39958   0.882  0.37763    \n## rankmedium      0.19221    0.47173   0.407  0.68368    \n## age:ranklow    -0.04820    0.03514  -1.372  0.17009    \n## age:rankmedium -0.01052    0.04037  -0.261  0.79432    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 75.972  on 59  degrees of freedom\n## Residual deviance: 50.925  on 54  degrees of freedom\n## AIC: 320.63\n## \n## Number of Fisher Scoring iterations: 4\n\n\nCode\ntidy(glm3, conf.int = TRUE, conf.level = 0.95)\n\n\n## # A tibble: 6 √ó 7\n##   term           estimate std.error statistic  p.value conf.low conf.high\n##   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)      1.68      0.273      6.15  7.93e-10   1.14      2.21  \n## 2 age              0.0782    0.0240     3.26  1.11e- 3   0.0311    0.125 \n## 3 ranklow          0.353     0.400      0.882 3.78e- 1  -0.433     1.13  \n## 4 rankmedium       0.192     0.472      0.407 6.84e- 1  -0.742     1.11  \n## 5 age:ranklow     -0.0482    0.0351    -1.37  1.70e- 1  -0.117     0.0208\n## 6 age:rankmedium  -0.0105    0.0404    -0.261 7.94e- 1  -0.0894    0.0690\n\n\n\nlibrary(effects)\n\n## Loading required package: carData\n\n\n## lattice theme set by effectsTheme()\n## See ?effectsTheme for details.\n\nplot(predictorEffects(glm3, partial.residuals = TRUE))\n\n\n\n\n\n\n\nplot(allEffects(glm3, partial.residuals = TRUE))",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "23-module.html#concept-review",
    "href": "23-module.html#concept-review",
    "title": "23¬† Generalized Linear Models",
    "section": "Concept Review",
    "text": "Concept Review\n\nGeneralized linear modeling extends general linear modeling to accommodate response variables that are not continuously distributed (e.g., binary responses, count data, proportion data)\nWhat is being predicted in generalized linear model is some function of the expected value of an observed response, not the observed response per se\n\nThe link/inverse link function is what connects the response predicted by the GLM back to the scale of the original observed response\n\nFor binary response variables, we use logistic regression, where the response variable is natural log of the odds ratio between two possible outcomes:\n\n\\[\\ln[\\frac{\\pi_i}{(1-\\pi_i)}] = \\beta_0 + \\beta_1x_i\\]\n\nFor response variables that are open-ended counts, we use log-linear or Poisson regression, where the response variable is the natural log of the expected value of the count\n\n\\[\\lambda_i = \\beta_0 + \\beta_1x_i\\]\n\nAs with simple (or general) linear models, we can create generalized linear models with more than one predictor variable, with a combination of continuous and categorial predictors, and with interactions between predictors",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "24-module.html",
    "href": "24-module.html",
    "title": "24¬† Mixed Effects or Multilevel Modeling",
    "section": "",
    "text": "24.1 Objectives",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Mixed Effects or Multilevel Modeling</span>"
    ]
  },
  {
    "objectID": "24-module.html#objectives",
    "href": "24-module.html#objectives",
    "title": "24¬† Mixed Effects or Multilevel Modeling",
    "section": "",
    "text": "In this module, we extend our discussion of regression modeling even further to include ‚Äúmixed effects‚Äù or ‚Äúmultilevel‚Äù models.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Mixed Effects or Multilevel Modeling</span>"
    ]
  },
  {
    "objectID": "24-module.html#preliminaries",
    "href": "24-module.html#preliminaries",
    "title": "24¬† Mixed Effects or Multilevel Modeling",
    "section": "24.2 Preliminaries",
    "text": "24.2 Preliminaries\n\nInstall and load these packages in R: {lme4}, {redres}, {glmmTMB}, {sjPlot}, and {mixedup}\nLoad {tidyverse} and {effects}\nBe sure you have the packages {car}, {cowplot}, {lmtest}, and {MASS} installed",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Mixed Effects or Multilevel Modeling</span>"
    ]
  },
  {
    "objectID": "24-module.html#overview-of-mixed-models",
    "href": "24-module.html#overview-of-mixed-models",
    "title": "24¬† Mixed Effects or Multilevel Modeling",
    "section": "24.3 Overview of ‚ÄúMixed‚Äù Models",
    "text": "24.3 Overview of ‚ÄúMixed‚Äù Models\nA final extension of linear regression modeling that we will talk about is so-called ‚Äúmultilevel‚Äù or ‚Äúmixed effects‚Äù modeling. This is a very complex topic, and we will only scratch the surface!\nIn a (general or generalized) linear mixed model, we have a reponse variable, \\(Y\\), and observations that fall into different factor categories each with some set of levels (e.g., ‚Äúsex‚Äù with levels ‚Äúmale‚Äù and ‚Äúfemale‚Äù), and we are interested in the effects of the various factors and factor levels on the response variable. Generically, if \\(\\mu\\) = a population mean response and \\(\\mu_A\\) = mean response for observations belonging to factor level A, then the effect of A is given by \\(\\mu\\) - \\(\\mu_A\\). We have already dealt with factors and factor levels in our linear regression models when we looked at categorical predictors (e.g., sex, rank category) in our discussions of ANOVA and ANCOVA.\nWe can conceptualize these factor effects as being either fixed or random. Fixed factors are those that reflect all levels of interest in our study, while random effects are those that represent only a sample of the levels of interest. For example, if we include sex as a factor in a model with the factor levels ‚Äúmale‚Äù and ‚Äúfemale‚Äù, this (typically) will cover the entire gamut of levels of interest our study, thus we would consider sex a fixed factor. When we were doing ANOVA and ANCOVA analyses previously, we were implicitly looking at the effects of such fixed factors.\nHowever, if our observational data were to consist of repeated observations of the same sampling unit, e.g., measurements taken on the same set of individuals on different dates, individual ID would be considered a random factor because it is unlikely that we will have collected data from all possible ‚Äúlevels of interest‚Äù, i.e., from all possible individual subjects. We have not yet dealt with such random factors as an additional source of variance in our modeling.\nSo-called mixed models, then, are those that include BOTH fixed and random effects. Including random effects in addition to fixed effects in our models has several ramifications:\n\nUsing random effects broadens the scope of inference. That is, we can use statistical methods to infer something about the population from which the levels of the random factor have been drawn.\nUsing random effects naturally incorporates dependence in the model and helps us account for pseudoreplication in our dataset. Observations that share the same level of the random effects are explicitly modeled as being correlated. This makes mixed effect modeling very useful for dealing with time series data, spatially correlated data, or situations where we have repeated observations/measures from the same subjects or sampling unit.\nUsing random factors often gives more accurate parameter estimates.\nIncorporating random factors, however, does require the use of more sophisticated estimation and fitting methods.\n\nNot surprisingly, there several different varieties of mixed modeling approaches, which are supported in a variety of different packages in R:\n\nStandard or general Linear Mixed Models (LMM), analogous to standard or general linear regression - used when we are dealing with normally distributed variables and error structures.\nGeneralized Linear Mixed Models (GLMM), analogous to generalized linear regresseion - used when we are dealing with various other variable types and error structure (e.g., binary, proportion, or count data).\nNonlinear Mixed Models (NLMM), analogous to nonlinear regression - used if we are dealing with situations where our response variable is best modeled by a nonlinear combination of predictor variables.\n\n\nNOTE: We have not talked at all in this course about general or generalized NONLINEAR modeling, but it is worth knowing that such approaches are also possible. NONLINEAR modeling is where our regression equation is a nonlinear function of the model parameters.\n\nWe will explore ‚Äúmixed effects‚Äù modeling using an example based on this excellent tutorial.\n\nEXAMPLE:\nSuppose we have measured the amount of grooming received by female chimpanzees when they are either in their periovulatory period (i.e., the window of 2-3 days around the likely time of ovulation) or duing other portions of their reproductive cycle. We collected data on the duration of grooming bouts received and scored a female‚Äôs reproductive condition at the time as a categorical factor with two levels: ‚ÄúPOP‚Äù versus ‚ÄúNONPOP‚Äù. On top of that, we also recorded data on female parity at the time of the grooming bout, i.e., whether the female had given birth previously (was ‚Äúparous‚Äù, or ‚ÄúP‚Äù) or had not yet had an offspring (was ‚Äúnulliparous‚Äù, or ‚ÄúN‚Äù).\nIf we are interested in how reproductive condition and parity influence how much grooming a female receives, our simple regression model would look like this:\n\\[grooming\\ duration \\sim reproductive\\ condition + parity + \\epsilon\\]\nAlso imagine that our study design was such that we took multiple observations per subject. That is, our data set includes records of multiple grooming bouts received by each subject. This situation violates the assumption of independence of observations that we make for standard linear regression: multiple responses/measures from the same subject cannot be regarded as independent from each other.\nUsing a mixed effects model, we can deal with this situation by adding subject ID as a random effect in our model. Doing so allows us to address the nonindependence issue by estimating a different set of parameters for each level of the factor ‚Äúsubject‚Äù. We can either estimate a different intercept for each subject (which would correspond to each female having a different ‚Äúbaseline‚Äù level of grooming received) or estimate a different intercept and slope (where individual subjects are presumed to differ both in the baseline level of grooming received and the strength of the relationship between grooming duration, on the one hand, and reproductive condition and parity, on the other). Our mixed effects model estimates these individual level parameters in addition to the main effects of each variable.\nThis is why a ‚Äúmixed effects‚Äù model is called a mixed model. The models that we have considered so far have been fixed effects only models and included only one or more ‚Äúfixed‚Äù predictor variables and a general error term. We essentially divided the world into things that we somehow understand or that are systematic (the fixed effects, or the explanatory variables) and things that we could not control for or do not understand (the general error, or \\(\\epsilon\\)). These fixed effects models did not examine possible structure within the error term.\nIn a mixed model, by contrast, we add one or more random effects to our fixed effects that may explain a portion of the variance in our error term.\n\n\nCHALLENGE\nLet‚Äôs explore these idea using some actual data. First, load in the dataset ‚Äúchimpgrooming.csv‚Äù and do some exploratory data analysis:\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/chimpgrooming.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\nhead(d)\n\n## # A tibble: 6 √ó 5\n##   subject parity season reprocondition duration\n##   &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;             &lt;dbl&gt;\n## 1 Nina    P      WS     NONPOP             214.\n## 2 Nina    P      WS     POP                206.\n## 3 Nina    P      WS     NONPOP             294.\n## 4 Nina    P      WS     POP                269.\n## 5 Nina    P      WS     NONPOP             205.\n## 6 Nina    P      WS     POP                287.\n\nsummary(d)\n\n##    subject             parity             season          reprocondition    \n##  Length:84          Length:84          Length:84          Length:84         \n##  Class :character   Class :character   Class :character   Class :character  \n##  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n##                                                                             \n##                                                                             \n##                                                                             \n##     duration     \n##  Min.   : 86.34  \n##  1st Qu.:132.36  \n##  Median :205.41  \n##  Mean   :197.81  \n##  3rd Qu.:252.27  \n##  Max.   :316.23\n\n\n\nPlot grooming received duration in relation to subject ID‚Ä¶\n\n\n\nShow Code\n(p &lt;- ggplot(data = d, aes(x = subject, y = duration)) + geom_boxplot() + geom_jitter(width = 0.2,\n    alpha = 0.5) + xlab(\"subject\") + theme(axis.text.x = element_text(angle = 90)))\n\n\n\n\n\n\n\n\n\n\nPlot grooming received in relation to reproductive condition‚Ä¶\n\n\n\nShow Code\n(p &lt;- ggplot(data = d, aes(x = reprocondition, y = duration)) + geom_boxplot() +\n    xlab(\"reproductive condition\") + geom_jitter(width = 0.2, alpha = 0.5) + theme(axis.text.x = element_text(angle = 90)))\n\n\n\n\n\n\n\n\n\n\nPlot grooming received in relation to reproductive condition and parity‚Ä¶\n\n\n\nShow Code\n(p &lt;- ggplot(data = d, aes(x = reprocondition, y = duration, fill = factor(parity))) +\n    geom_boxplot() + xlab(\"reproductive condition\") + labs(fill = \"parity\") + theme(axis.text.x = element_text(angle = 90)))\n\n\n\n\n\n\n\n\n\nShow Code\n# or\n\n(p &lt;- ggplot(data = d, aes(x = parity, y = duration, fill = factor(reprocondition))) +\n    geom_boxplot() + xlab(\"parity\") + labs(fill = \"reprocondition\") + theme(axis.text.x = element_text(angle = 90)))\n\n\n\n\n\n\n\n\n\nFrom these plots, we can see lots of [a] individual variation in how much grooming is received (some females seem to receive more than others, overall), [b] variation in response to reproductive condition (where grooming in the POP seems to be higher, overall), [c] limited variation due to parity, and [d] variation in response to this combination of fixed effects.\n\nFinally, plot grooming received into relation to both a fixed effect (reprocondition) and a random effect (subject)‚Ä¶\n\n\n\nShow Code\n(p &lt;- ggplot(data = d, aes(x = reprocondition, y = duration, fill = factor(subject))) +\n    geom_boxplot() + xlab(\"reproductive condition\") + labs(fill = \"subject\") + theme(axis.text.x = element_text(angle = 90)))\n\n\n\n\n\n\n\n\n\nShow Code\n# or\n\n(p &lt;- ggplot(data = d, aes(x = subject, y = duration, fill = factor(reprocondition))) +\n    geom_boxplot() + xlab(\"subject\") + labs(fill = \"reproductive condition\") + theme(axis.text.x = element_text(angle = 90)))\n\n\n\n\n\n\n\n\n\nWhat patterns do you see? There is, again, lots of apparent variation, and some of it seems to be associated with female ID and/or reproductive condition.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Mixed Effects or Multilevel Modeling</span>"
    ]
  },
  {
    "objectID": "24-module.html#random-intercept-models",
    "href": "24-module.html#random-intercept-models",
    "title": "24¬† Mixed Effects or Multilevel Modeling",
    "section": "24.4 Random Intercept Models",
    "text": "24.4 Random Intercept Models\nWe will now perform an initial mixed effects analysis where we look in more detail at how reproductive condition and parity (as fixed effects) affect grooming duration, where we include individual subject ID as a random effect.\nHere is a first mixed effects model that we will fit, using one extension of formula notation that is commonly used in R:\n\\[grooming\\ duration \\sim reproductive\\ condition + parity + (1|subject) + \\epsilon\\]\nHere, the 1 refers to the fact that we want to estimate an intercept and the pipe operator(‚Äú|‚Äù) following the ‚Äú1‚Äù signifies that we want to estimate a different intercept for each subject. Note that this generic formula still contains a general error term, \\(\\epsilon\\), to highlight that there will still be unexplained ‚Äúerror‚Äù variance after accounting for both fixed and random effects in the model.\nWe can think of this formula as saying that we expect our dataset to include multiple observations of the response variable per subject, and these responses will depend, in part, on each subject‚Äôs baseline level. This effectively accounts the nonindependence that stems from having multiple responses by the same subject.\nThe {lme4} package in R is commonly used for mixed effects modeling, and the function lmer() is the mixed model equivalent of the function lm(). In the formula syntax for mixed effects models using the {lme4} package, fixed effects are included without parentheses while random effects are included in parentheses (the error, \\(\\epsilon\\), is understood and is not included explicitly).\n\nNOTE: We could also use the package {nlme} for mixed effects modeling (which requires a slightly different formula syntax than that used here). That same package also allows us to do nonlinear mixed effects modeling, which we will not be talking about. It is important to note that {lme4} uses, by default, a slightly different parameter estimation algorithm than {nlme}. Unless otherwise specified, {lme4} uses ‚Äúrestricted maximum likelihood‚Äù (REML) rather than ordinary maximum likelihood estimation, which is what is used in {nlme}. In practice, these often give very similar results. We will see below that when we want to compare different models using {lme4}, we will need to tell {lme4} to use ordinary maximum likelihood.\n\nThe code block below implements this first ‚Äúmixed effects‚Äù model:\n\nm &lt;- lmer(data = d, duration ~ reprocondition + parity + (1 | subject))\nsummary(m)\n\n## Linear mixed model fit by REML ['lmerMod']\n## Formula: duration ~ reprocondition + parity + (1 | subject)\n##    Data: d\n## \n## REML criterion at convergence: 796.1\n## \n## Scaled residuals: \n##     Min      1Q  Median      3Q     Max \n## -2.2611 -0.5349 -0.1890  0.3918  3.1994 \n## \n## Random effects:\n##  Groups   Name        Variance Std.Dev.\n##  subject  (Intercept) 611.9    24.74   \n##  Residual             847.3    29.11   \n## Number of obs: 84, groups:  subject, 6\n## \n## Fixed effects:\n##                   Estimate Std. Error t value\n## (Intercept)        132.841     15.305   8.680\n## reproconditionPOP   20.293      6.352   3.195\n## parityP            109.650     21.173   5.179\n## \n## Correlation of Fixed Effects:\n##             (Intr) rprPOP\n## rprcndtnPOP -0.208       \n## parityP     -0.692  0.000\n\n\nLet‚Äôs focus on the output for the random effects first. Have a look at the column Std.Dev.. The entry for subject shows us how much variability in grooming duration (apart from that explained by the fixed effects) is due to subject ID. The entry for Residual summarizes the remaining variability in grooming duration that is not due to subject or to our fixed effects. The latter is our \\(\\epsilon\\), the ‚Äúrandom‚Äù deviations from the predicted values that are not due to either subject or our fixed effects.\nThe fixed effects output mirrors the coefficient tables that we have seen previously in our linear models that have focused only on fixed effects. The coefficient ‚ÄúreproconditionPOP‚Äù is the \\(\\beta\\) coefficient for the categorical effect of reproductive condition. The positive sign for the coefficient means that grooming duration is GREATER by 20.293 units for POP than for NONPOP females. Then, there is a standard error associated with this slope, and a t value, which is simply the estimate divided by the standard error.\nThe coefficient ‚ÄúparityP‚Äù is the \\(\\beta\\) coefficient for the categorical effect of parity. The grooming duration associated with being parous versus nulliparous is GREATER by 109.65 units.\nThe INTERCEPT in this case is the grooming duration associated with being an average, nulliparous, NONPOP female. Like the lm() function, the lmer() took whatever factor level came first in the alphabet to be the reference level for each fixed effect variable.\nLet‚Äôs also look at the coefficients coming out of the model and their confidence intervals.\n\ncoefficients(m)\n\n## $subject\n##       (Intercept) reproconditionPOP parityP\n## Luna    142.76558          20.29286  109.65\n## Maya    137.37902          20.29286  109.65\n## Nina    118.37755          20.29286  109.65\n## Nipa    160.80050          20.29286  109.65\n## Sofia    99.80117          20.29286  109.65\n## Vita    137.92047          20.29286  109.65\n## \n## attr(,\"class\")\n## [1] \"coef.mer\"\n\n# or, for just the fixed effects...  coefficients(summary(m))\nconfint(m, level = 0.95, method = \"boot\")\n\n## Computing bootstrap confidence intervals ...\n\n\n## \n## 11 message(s): boundary (singular) fit: see help('isSingular')\n\n\n##                        2.5 %    97.5 %\n## .sig01              2.390756  41.48460\n## .sigma             24.224461  33.98986\n## (Intercept)       102.909448 161.44522\n## reproconditionPOP   7.277285  33.27166\n## parityP            71.577446 149.21201\n\n\n\nNOTE: For the confint() function, we can set method = argument to ‚Äúboot‚Äù (to generate a CI based on bootstrapping), ‚Äúprofile‚Äù (to generate a CI based on likelihoods), or ‚ÄúWald‚Äù (which generates a CI for the fixed effects only). Also, in the output from running confint(), ‚Äú.sigXX‚Äù is the square root of the variance associated with each of the random parameter levels and ‚Äú.sigma‚Äù is the square root of the variance associated with the residuals of the model.\n\nWe can see the separate intercepts, i.e., the ‚Äúbaseline‚Äù level of grooming received, for each female when they are (presumably) nulliparous and in a NONPOP reproductive condition.\n\nNOTE: Some females may not ever be seen both parity or reproductive conditions!\n\n\n24.4.1 Visualizing Model Results\nAs in earlier modules, we can use the predictorEffects() function from the {effects} package to visualize our mixed model results in terms of how the response variable is expected to differ across levels of each predictor variable when the other predictor and the random effect of subject are controlled for.\n\nplot(predictorEffects(m, partial.residuals = TRUE))\n\n\n\n\n\n\n\n\nSimilar effects plots for our predictors can be created using plot_model() from the {sjPlot} package.\n\np1 &lt;- plot_model(m, type = \"eff\", terms = c(\"reprocondition\"))\np2 &lt;- plot_model(m, type = \"eff\", terms = c(\"parity\"))\ncowplot::plot_grid(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\nWe can also use plot_model() to visualize estimates and confidence intervals for the fixed effect coefficients and for the random effects.\n\np1 &lt;- plot_model(m, type = \"est\")\np2 &lt;- plot_model(m, type = \"re\")\ncowplot::plot_grid(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\nThe package {mixedup}, installable from here, is a useful package that provides several functions for extracting relevant summary information from mixed models.\n\nmixedup::summarize_model(m)  # clean model summary\n\n##     Group    Effect Variance    SD SD_2.5 SD_97.5 Var_prop\n##   subject Intercept   611.94 24.74  10.57   41.75     0.42\n##  Residual             847.28 29.11  24.91   34.13     0.58\n\n\n##               Term  Value    SE    t P_value Lower_2.5 Upper_97.5\n##          Intercept 132.84 15.30 8.68    0.00    102.84     162.84\n##  reproconditionPOP  20.29  6.35 3.19    0.00      7.84      32.74\n##            parityP 109.65 21.17 5.18    0.00     68.15     151.15\n\nmixedup::extract_fixed_effects(m)  # table of fixed effects coefficients\n\n## # A tibble: 3 √ó 7\n##   term              value    se     t p_value lower_2.5 upper_97.5\n##   &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n## 1 Intercept         133.  15.3   8.68   0        103.        163. \n## 2 reproconditionPOP  20.3  6.35  3.20   0.001      7.84       32.7\n## 3 parityP           110.  21.2   5.18   0         68.2       151.\n\nmixedup::extract_random_coefficients(m)  # table of coefficients for random effects\n\n## # A tibble: 6 √ó 7\n##   group_var effect    group value    se lower_2.5 upper_97.5\n##   &lt;chr&gt;     &lt;chr&gt;     &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n## 1 subject   Intercept Luna  143.   17.0     109.        176.\n## 2 subject   Intercept Maya  137.   17.0     104.        171.\n## 3 subject   Intercept Nina  118.   17.0      85.0       152.\n## 4 subject   Intercept Nipa  161.   17.0     127.        194.\n## 5 subject   Intercept Sofia  99.8  17.0      66.5       133.\n## 6 subject   Intercept Vita  138.   17.0     105.        171.\n\nmixedup::extract_random_effects(m)  # table of coefficients for each level of random effect\n\n## # A tibble: 6 √ó 7\n##   group_var effect    group  value    se lower_2.5 upper_97.5\n##   &lt;chr&gt;     &lt;chr&gt;     &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n## 1 subject   Intercept Luna    9.93  7.42     -4.62     24.5  \n## 2 subject   Intercept Maya    4.54  7.42    -10.0      19.1  \n## 3 subject   Intercept Nina  -14.5   7.42    -29.0       0.082\n## 4 subject   Intercept Nipa   28.0   7.42     13.4      42.5  \n## 5 subject   Intercept Sofia -33.0   7.42    -47.6     -18.5  \n## 6 subject   Intercept Vita    5.08  7.42     -9.46     19.6\n\n\n\n\n24.4.2 Residual Analysis\nWe can use functions from the {redres} package to conduct some analyses on the residuals resulting from a mixed effects model. The plot_resqq() function creates a QQ plot of the overall residuals from the model. Ideally, these should fall along a line, and we can see that ours do not‚Ä¶\n\nplot_resqq(m)\n\n## Loading required namespace: testthat\n\n\n\n\n\n\n\n\n# or\nqqnorm(compute_redres(m))\nqqline(compute_redres(m))\n\n\n\n\n\n\n\n# or\ncar::qqPlot(compute_redres(m), id = FALSE)\n\n\n\n\n\n\n\n\n‚Ä¶ rather, the residuals from this model appear to deviate from a normal distribution, thus the appropriatess of using linear regression here is suspect!\nThe plot_redres() function plots the residuals with respect to fitted values of the response variable.\nWe can also pass an ‚Äúxvar =‚Äù argument to plot_redres() to view our residuals by our different predictor variables, e.g., reprocondition, parity, or subject.\nThe code below generates and prints all of these plots‚Ä¶\n\np1 &lt;- plot_redres(m)\np2 &lt;- plot_redres(m, xvar = \"reprocondition\")\np3 &lt;- plot_redres(m, xvar = \"parity\")\np4 &lt;- plot_redres(m, xvar = \"subject\")\ncowplot::plot_grid(p1, p2, p3, p4, ncol = 2)\n\n\n\n\n\n\n\n\nIn the plot of residuals by fitted values (upper left), the residuals should roughly form a horizonal belt around the zero line, with the points randomly scattered above and below 0. Such a pattern would be consistent with ‚Äúhomoskedasticity‚Äù, or equal variance, in the error term across values of the predictor terms and the response. Here, things look pretty good (although perhaps the width of the ‚Äúbelt‚Äù of points looks like it may be wider at larger values of the fitted response).\nIn the other plots, we are looking to see whether the scatter (variance) above and below the zero line is similar across levels of the predictor variables.\nFinally, we can use the plot_ranef() function to plot estimate modal values for the random effects (i.e., in our model, for each subject conditioned on the fixed effects) against quantiles of the normal distribution. For linear regression to be appropriate, the random effects should not deviate from normal expectations.\n\nplot_ranef(m)\n\n\n\n\n\n\n\n\nAlternatively, we can use the {sjPlot} function plot_model() with the ‚Äútype =‚Äù argument set to ‚Äúdiag‚Äù to produce a similar set of diagnostic plots.\n\np &lt;- plot_model(m, type = \"diag\")\ncowplot::plot_grid(p[[1]], p[[3]], p[[4]], ncol = 1)\n\n\n\n\n\n\n\n\n\n\nInference using LRTs\nIn mixed effects models, it is not as straightforward as it is for standard linear models to determine p values associated with either overall models or individual coefficients. However, using likelihood ratio tests, which we previously used for comparing among generalized linear models, is one common approach. Likelihood is the probability of seeing the data we have actually collected given a particular model. The logic of the likelihood ratio test is to compare the likelihood of two models with each other, i.e., a model that includes the factor that we are interested in versus a reduced, nested model with that factor excluded.\nSo‚Ä¶ if we are interested in the effect of reproductive condition on grooming duration, we could compare a more complex model‚Ä¶\n\\[grooming\\ duration \\sim reproductive\\ condition + parity + (1|subject) + \\epsilon\\]\nto a nested, less complex model‚Ä¶\n\\[grooming\\ duration \\sim parity + (1|subject) + \\epsilon\\]\nIn R, we would do this as follows:\n\nfull &lt;- lmer(data = d, duration ~ reprocondition + parity + (1 | subject), REML = FALSE)\n# note the additional `REML=` argument\nsummary(full)\n\n## Linear mixed model fit by maximum likelihood  ['lmerMod']\n## Formula: duration ~ reprocondition + parity + (1 | subject)\n##    Data: d\n## \n##       AIC       BIC    logLik -2*log(L)  df.resid \n##     825.7     837.9    -407.9     815.7        79 \n## \n## Scaled residuals: \n##     Min      1Q  Median      3Q     Max \n## -2.2996 -0.5283 -0.1783  0.4032  3.2285 \n## \n## Random effects:\n##  Groups   Name        Variance Std.Dev.\n##  subject  (Intercept) 388.6    19.71   \n##  Residual             836.4    28.92   \n## Number of obs: 84, groups:  subject, 6\n## \n## Fixed effects:\n##                   Estimate Std. Error t value\n## (Intercept)        132.841     12.625  10.522\n## reproconditionPOP   20.293      6.311   3.215\n## parityP            109.650     17.288   6.343\n## \n## Correlation of Fixed Effects:\n##             (Intr) rprPOP\n## rprcndtnPOP -0.250       \n## parityP     -0.685  0.000\n\nreduced &lt;- lmer(data = d, duration ~ parity + (1 | subject), REML = FALSE)\nsummary(reduced)\n\n## Linear mixed model fit by maximum likelihood  ['lmerMod']\n## Formula: duration ~ parity + (1 | subject)\n##    Data: d\n## \n##       AIC       BIC    logLik -2*log(L)  df.resid \n##     833.4     843.2    -412.7     825.4        80 \n## \n## Scaled residuals: \n##     Min      1Q  Median      3Q     Max \n## -2.4996 -0.6492 -0.1198  0.6594  2.7072 \n## \n## Random effects:\n##  Groups   Name        Variance Std.Dev.\n##  subject  (Intercept) 380.6    19.51   \n##  Residual             947.3    30.78   \n## Number of obs: 84, groups:  subject, 6\n## \n## Fixed effects:\n##             Estimate Std. Error t value\n## (Intercept)   142.99      12.22  11.697\n## parityP       109.65      17.29   6.343\n## \n## Correlation of Fixed Effects:\n##         (Intr)\n## parityP -0.707\n\n\n\nNOTE: Here, we added the argument ‚ÄúREML = FALSE‚Äù to the lmer() function. This is necessary to do when we want to compare models using the likelihood ratio test. Basically, REML uses a different algorithm to determine likelihood values than ordinary likelihood, and, if we want to use these likelihoods to execute an LRT, we need to use ordinary likelihood. See this site for a more complete explanation of this issue.\n\nWe then perform the likelihood ratio test using the anova() function or the lrtest() function from {lmtest}.\n\nanova(reduced, full)\n\n## Data: d\n## Models:\n## reduced: duration ~ parity + (1 | subject)\n## full: duration ~ reprocondition + parity + (1 | subject)\n##         npar    AIC    BIC  logLik -2*log(L)  Chisq Df Pr(&gt;Chisq)   \n## reduced    4 833.43 843.15 -412.72    825.43                        \n## full       5 825.72 837.88 -407.86    815.72 9.7089  1   0.001834 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# or\nlmtest::lrtest(reduced, full)\n\n## Likelihood ratio test\n## \n## Model 1: duration ~ parity + (1 | subject)\n## Model 2: duration ~ reprocondition + parity + (1 | subject)\n##   #Df  LogLik Df  Chisq Pr(&gt;Chisq)   \n## 1   4 -412.72                        \n## 2   5 -407.86  1 9.7089   0.001834 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThese results tell us that the model containing both reproductive condition and parity as fixed effects fits the data better than a model lacking reproductive condition and containing only parity as a fixed effect, while accounting for subject as a random effect.\n\n\nCHALLENGE\nNow, compare a model containing reproductive condition and parity to one containing just reproductive condition.\n\n\nShow Code\nfull &lt;- lmer(data = d, duration ~ reprocondition + parity + (1 | subject), REML = FALSE)\nreduced &lt;- lmer(data = d, duration ~ reprocondition + (1 | subject), REML = FALSE)\n\nanova(reduced, full)\n\n\nShow Output\n## Data: d\n## Models:\n## reduced: duration ~ reprocondition + (1 | subject)\n## full: duration ~ reprocondition + parity + (1 | subject)\n##         npar    AIC    BIC  logLik -2*log(L)  Chisq Df Pr(&gt;Chisq)    \n## reduced    4 835.97 845.70 -413.99    827.97                         \n## full       5 825.72 837.88 -407.86    815.72 12.251  1   0.000465 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nShow Code\nplot(predictorEffects(full, partial.residuals = TRUE))\n\n\n\n\n\n\n\n\n\nBased on this result, including parity as well as reproductive condition as fixed effects also significantly improves the fit of our model.\n\n\nCHALLENGE\nConstruct a model that includes an interaction of reproductive condition and parity and compare it to a model without the interaction term. Is the interaction of these two fixed effects significant?\n\n\nShow Code\nfull &lt;- lmer(data = d, duration ~ reprocondition * parity + (1 | subject), REML = FALSE)\nreduced &lt;- lmer(data = d, duration ~ reprocondition + parity + (1 | subject), REML = FALSE)\n\nanova(reduced, full)\n\n\nShow Output\n## Data: d\n## Models:\n## reduced: duration ~ reprocondition + parity + (1 | subject)\n## full: duration ~ reprocondition * parity + (1 | subject)\n##         npar    AIC    BIC  logLik -2*log(L)  Chisq Df Pr(&gt;Chisq)\n## reduced    5 825.72 837.88 -407.86    815.72                     \n## full       6 826.41 840.99 -407.20    814.41 1.3124  1      0.252\n\n\n\nShow Code\nplot(predictorEffects(full, partial.residuals = TRUE))\n\n\n\n\n\n\n\n\n\nIn this case, adding the interaction of reproductive condition and parity does not significantly improve the explanatory power of the model.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Mixed Effects or Multilevel Modeling</span>"
    ]
  },
  {
    "objectID": "24-module.html#random-slope-models",
    "href": "24-module.html#random-slope-models",
    "title": "24¬† Mixed Effects or Multilevel Modeling",
    "section": "24.5 Random Slope Models",
    "text": "24.5 Random Slope Models\nIn the exercise above, we included only estimation of a separate INTERCEPT for each female and presumed that the same relationships (SLOPES) between grooming duration and reproductive condition + parity obtained for all females. But we can also allow that relationship to vary from subject to subject. We would indicate this model in formula notation as follows:\n\\[grooming\\ duration \\sim reproductive\\ condition + parity\\ +\\] \\[(1 + reproductive\\ condition|subject) + (1 + parity|subject) + \\epsilon\\]\n\nm &lt;- lmer(data = d, duration ~ reprocondition + parity + (1 + reprocondition | subject) +\n    (1 + parity | subject), REML = FALSE)\nsummary(m)\n\n## Linear mixed model fit by maximum likelihood  ['lmerMod']\n## Formula: duration ~ reprocondition + parity + (1 + reprocondition | subject) +  \n##     (1 + parity | subject)\n##    Data: d\n## \n##       AIC       BIC    logLik -2*log(L)  df.resid \n##     833.6     857.9    -406.8     813.6        74 \n## \n## Scaled residuals: \n##     Min      1Q  Median      3Q     Max \n## -2.4514 -0.6093 -0.1559  0.3851  3.2152 \n## \n## Random effects:\n##  Groups    Name              Variance Std.Dev. Corr \n##  subject   (Intercept)        84.020   9.166        \n##            reproconditionPOP   1.469   1.212   -1.00\n##  subject.1 (Intercept)       628.419  25.068        \n##            parityP           627.945  25.059   -1.00\n##  Residual                    835.801  28.910        \n## Number of obs: 84, groups:  subject, 6\n## \n## Fixed effects:\n##                   Estimate Std. Error t value\n## (Intercept)        132.530     16.293   8.134\n## reproconditionPOP   20.293      6.328   3.207\n## parityP            110.272     17.258   6.390\n## \n## Correlation of Fixed Effects:\n##             (Intr) rprPOP\n## rprcndtnPOP -0.211       \n## parityP     -0.902  0.000\n## optimizer (nloptwrap) convergence code: 0 (OK)\n## boundary (singular) fit: see help('isSingular')\n\n\nHere, we have changed the random effects, which now look a little more complicated. The notation ‚Äú(1 + reprocondition | subject)‚Äù tells the model to estimate differing baseline levels of grooming duration (the intercept, represented by 1) as well as differing responses to the main factor in question, which is reproductive condition in this case. We do the same for parity.\n\nNOTE: It is equivalent to use just ‚Äú(reprocondition | subject)‚Äù rather than ‚Äú(1 + reprocondition | subject)‚Äù to specify that we want our analysis to estimate both an intercept and slope coefficient for each subject. We use the latter to be explicit, though, as we could used ‚Äú(0 + reprocondition | subject)‚Äù to return a single intercept but different slopes for each subject, which is sometimes referred to as a ‚Äúfixed intercept‚Äù model.\n\nLooking at the coefficients of the new model, we see the effects. Each female now has a different intercept and a different coefficient for the slopes of grooming duration as a function of both reproductive condition and parity.\n\ncoefficients(m)\n\n## $subject\n##       (Intercept) reproconditionPOP   parityP\n## Luna     144.8594          19.47768 110.25259\n## Maya     137.6452          19.95465 110.26271\n## Nina     113.0874          21.57830 110.30006\n## Nipa     139.1961          19.85212  84.78083\n## Sofia    125.3318          20.76876 140.06436\n## Vita     135.0590          20.12564 105.97018\n## \n## attr(,\"class\")\n## [1] \"coef.mer\"\n\nconfint(m, level = 0.95, method = \"boot\")\n\n##                           2.5 %      97.5 %\n## .sig01             0.0000000000  19.9952106\n## .sig02            -1.0000000000   1.0000000\n## .sig03             0.0001437061  17.5293904\n## .sig04             0.0000000000  41.4780440\n## .sig05            -1.0000000000   0.1345876\n## .sig06             0.0001416102  41.3781807\n## .sigma            23.7605847918  32.8980359\n## (Intercept)       98.7935109139 163.7931314\n## reproconditionPOP  7.2085451103  32.4516570\n## parityP           74.7609782262 143.3194450\n\n\nFinally, effects plots of how the expected amount of grooming received varies with the fixed effects (reprocondition and parity) under our new random slope model is the same as that under the random intercept model.\n\nplot(predictorEffects(m, partial.residuals = TRUE))\n\n\n\n\n\n\n\n# or\n\nplot_model(m, type = \"eff\", term = \"reprocondition\")\n\n\n\n\n\n\n\nplot_model(m, type = \"eff\", term = \"parity\")\n\n\n\n\n\n\n\n\n\nmixedup::summarize_model(m)\n\n##      Group            Effect Variance    SD SD_2.5 SD_97.5 Var_prop\n##    subject         Intercept    84.02  9.17   0.00    0.10     0.04\n##    subject reproconditionPOP     1.47  1.21   0.00   15.20     0.00\n##  subject.1         Intercept   628.42 25.07   0.00    0.10     0.29\n##  subject.1           parityP   627.95 25.06   0.00  177.15     0.29\n##   Residual                     835.80 28.91  24.90   34.12     0.38\n\n\n##               Term  Value    SE    t P_value Lower_2.5 Upper_97.5\n##          Intercept 132.53 16.29 8.13    0.00    100.60     164.46\n##  reproconditionPOP  20.29  6.33 3.21    0.00      7.89      32.70\n##            parityP 110.27 17.26 6.39    0.00     76.45     144.10\n\nmixedup::extract_fixed_effects(m)\n\n## # A tibble: 3 √ó 7\n##   term              value    se     t p_value lower_2.5 upper_97.5\n##   &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n## 1 Intercept         133.  16.3   8.13   0        101.        164. \n## 2 reproconditionPOP  20.3  6.33  3.21   0.001      7.89       32.7\n## 3 parityP           110.  17.3   6.39   0         76.4       144.\n\n\n\nNOTE: Here, running the functions extract_random_coefficients() and extract_random_effect() would throw an error because we would be generating two (Intercept) terms per random effect level, one for each of two predictors.\n\n\n24.5.1 Residual Analysis\nOnce again, we can use the {redres} package to do some visual residual analysis‚Ä¶\n\nplot_resqq(m)\n\n\n\n\n\n\n\nplot_redres(m)\n\n\n\n\n\n\n\nplot_redres(m, xvar = \"subject\")\n\n\n\n\n\n\n\n\n\nNOTE: Here, running the functions plot_ranef() would throw an error because we would be generating two (Intercept) terms per random effect level, one for each of two predictors. If we wanted, we could run the ranef() function on the model, look at the $subject dataframe, and pull out the columns we wanted to plot against normal quantiles using qqPlot() or qqnorm().\n\n\n\nInference using LRTs\nTo then get p values associated with each of the fixed factors, we can use likelihood ratio tests‚Ä¶\n\n# random effects only\nnull &lt;- lmer(data = d, duration ~ (1 + reprocondition | subject) + (1 + parity |\n    subject), REML = FALSE)\n\n# full model with both fixed effects\nfull &lt;- lmer(data = d, duration ~ reprocondition + parity + (1 + reprocondition |\n    subject) + (1 + parity | subject), REML = FALSE)\n\n# model without reproductive condition as a fixed effect\nminusRC &lt;- lmer(data = d, duration ~ parity + (1 + reprocondition | subject) + (1 +\n    parity | subject), REML = FALSE)\n\n# model without parity as a fixed effect\nminusP &lt;- lmer(data = d, duration ~ reprocondition + (1 + reprocondition | subject) +\n    (1 + parity | subject), REML = FALSE)\n\n# p value for reproductive condition as a fixed effect\nanova(minusRC, full)\n\n## Data: d\n## Models:\n## minusRC: duration ~ parity + (1 + reprocondition | subject) + (1 + parity | subject)\n## full: duration ~ reprocondition + parity + (1 + reprocondition | subject) + (1 + parity | subject)\n##         npar    AIC    BIC  logLik -2*log(L)  Chisq Df Pr(&gt;Chisq)   \n## minusRC    9 840.23 862.11 -411.11    822.23                        \n## full      10 833.63 857.94 -406.82    813.63 8.5963  1   0.003368 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# p value for parity as a fixed effect\nanova(minusP, full)\n\n## Data: d\n## Models:\n## minusP: duration ~ reprocondition + (1 + reprocondition | subject) + (1 + parity | subject)\n## full: duration ~ reprocondition + parity + (1 + reprocondition | subject) + (1 + parity | subject)\n##        npar    AIC    BIC  logLik -2*log(L)  Chisq Df Pr(&gt;Chisq)   \n## minusP    9 840.08 861.96 -411.04    822.08                        \n## full     10 833.63 857.94 -406.82    813.63 8.4473  1   0.003656 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nplot(predictorEffects(full, partial.residuals = TRUE))\n\n\n\n\n\n\n\n\n\n\nInference using AIC\nFor a long time, the appropriateness of our mixed models was assessed as above - i.e., by evaluating the significance of each fixed effect using LRTs. As information theoretic approaches have become more popular, it is increasingly common to assess model fit by comparing the AIC values of different models, acknowledging the caveat that AIC can only tell us about the relative fit of alternative models, but not whether a particular model is a good fit, overall. Recall that AIC values are a way of inverting and scaling model log-likelihoods that penalizes models with greater numbers of parameters.\nThe aictab() function from {AICcmodavg} neatly prints out tables with AIC, Delta AIC, and log-likelihood values, along with AIC ‚Äúweights‚Äù.\n\nlibrary(AICcmodavg)\n(aic_table &lt;- aictab(list(full, minusRC, minusP, null), modnames = c(\"full\", \"minusRC\",\n    \"minusP\", \"null\")))\n\n## \n## Model selection based on AICc:\n## \n##          K   AICc Delta_AICc AICcWt Cum.Wt      LL\n## full    10 836.65       0.00   0.91   0.91 -406.82\n## minusP   9 842.51       5.87   0.05   0.95 -411.04\n## minusRC  9 842.66       6.02   0.04   1.00 -411.11\n## null     8 851.47      14.82   0.00   1.00 -416.77\n\ndetach(package:AICcmodavg)\n\n\nNOTE: Here we are printing out AICc values, rather than AIC values. AICc is simply a version of AIC with a correction added for small sample sizes. To print the uncorrected AIC values, which appear in the anova() LRT output, we can add the argument ‚Äúsecond.ord = TRUE‚Äù to the aictab() function.\n\n\\[AIC_c = AIC + \\frac{2K^2 + 2K}{n-K-1}\\]\nwhere \\(K\\) is the number of parameters and \\(n\\) is the sample size.\nIn the table, for each model, \\(K\\) is the number of model parameters, the \\(Delta\\  AICc\\) value is the difference between that model‚Äôs AICc and the best model‚Äôs AICc (again, here, that is the full model), and the \\(AICc\\ weight\\) is the relative likelihood of that model. [The weights for a particular set of models sum to 1, with each weight equal to the model‚Äôs likelihood divided by the summed likelihoods across all models.]\nWhere the best model has a very high Akaike weight, e.g., &gt;0.9, it is reasonable to base inferences about the included variables on that single most parsimonious model, but when several models rank highly (e.g., several models have Delta AICc values &lt;2 to 4), it is common to model-average effect sizes for the variables that have the most support across that set of models. That is, ‚Äúmodel averaging‚Äù means making inferences based on a set of candidate models, instead of on a single ‚Äúbest‚Äù model.\nHere, note that the full model containing both reproductive condition and parity has the highest likelihood (the least negative log-likelihood value) and a much lower AICc than any of the less complex alternative models tested. It also has a very high Akaike weight (0.91).\n\nNOTE: When running a number of the above models and/or in doing the likelihood ratio tests, we saw a significant result but we also got either a warning that our null models ‚Äúfailed to converge‚Äù or we saw a warning about a ‚Äúboundary (singular) fit‚Äù. Both of these warning are due to having a LOT of parameters we are trying to estimate relative to the number of observations we have. Dealing with lack of convergence in fitting maximum likelihood models is beyond what we can cover here, but I encourage you to explore that on your own!\n\n\n\nOther Methods for Assessing Fit\nUsing AIC values for evaluating and expressing how well a particular model fits a dataset has some critical limitations:\n\nWhile AIC provides an estimate of the relative fit of various models, it does not say anything about the absolute fit\nAIC does not address the amount of variance in a response variable explained by a model\nAIC is not comparable across datasets, and so fit is not generalizable\n\nNakagawa & Schielzeth (2013) and Nakagawa et al.¬†(2017) have published a simple and effective method for calculating a type of pseudo-\\(R^2\\) (or coefficient of determination) value for generalized linear mixed models, and because linear mixed models are a specific type of GLMM, this method can be used with LMMs as well.\n\nSee:\n\nNakagawa, S., & Schielzeth, H. (2013). A general and simple method for obtaining R2 from generalized linear mixed-effects models. Methods in Ecology and Evolution, 4, 133‚Äì142.\nNakagawa, S., Johnson, P. C. D., & Schielzeth, H. (2017). The coefficient of determination R2 and intra-class correlation coefficient from generalized linear mixed-effects models revisited and expanded. Journal of the Royal Society Interface, 14, article 20170213.\n\n\nIn these papers, two measures are proposed for characterizing the amount of ‚Äúvariance explained‚Äù for mixed effects models:\n\n\\(Marginal\\ R2GLMM(m)^2\\) is the amount of variance explained on the latent (or link) scale rather than the original scale. We can interpret this as a measure of the variance explained by only the fixed effects.\n\\(Conditional\\ R2GLMM(c)^2\\) is the amount of variance explained by both fixed and random effects, i.e., by the entire model.\n\nThere is an easy way to calculate these two R2GLMM values in R using the r.squaredGLMM() function in the package {MuMIn}.\n\n\nCHALLENGE\nCompare the full, the two reduced, and the null mixed effects models from our random slope exercise using an information theoretic approach. Is the best model (full) the one that explains the greatest amount of variance in the dataset? In the full model, how much more of the total variance is explained by the random effects than by the fixed effects alone?\n\naic_table  # re-print the AIC table\n\n## \n## Model selection based on AICc:\n## \n##          K   AICc Delta_AICc AICcWt Cum.Wt      LL\n## full    10 836.65       0.00   0.91   0.91 -406.82\n## minusP   9 842.51       5.87   0.05   0.95 -411.04\n## minusRC  9 842.66       6.02   0.04   1.00 -411.11\n## null     8 851.47      14.82   0.00   1.00 -416.77\n\nlibrary(MuMIn)\nr.squaredGLMM(full)\n\n##            R2m       R2c\n## [1,] 0.7221769 0.8102366\n\nr.squaredGLMM(minusRC)\n\n##            R2m    R2c\n## [1,] 0.6778132 0.7898\n\nr.squaredGLMM(minusP)\n\n##             R2m       R2c\n## [1,] 0.01443171 0.8841458\n\nr.squaredGLMM(null)\n\n##      R2m       R2c\n## [1,]   0 0.8714428\n\ndetach(package:MuMIn)",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Mixed Effects or Multilevel Modeling</span>"
    ]
  },
  {
    "objectID": "24-module.html#generalized-lmms",
    "href": "24-module.html#generalized-lmms",
    "title": "24¬† Mixed Effects or Multilevel Modeling",
    "section": "24.6 Generalized LMMs",
    "text": "24.6 Generalized LMMs\nJust as we extended our standard linear modeling approach to include non-normally distributed response variables/error structures, so too can we extend our mixed effects modeling to such situations. This is referred to as generalized linear mixed modeling, or GLMM. There are several R packages we can use to do this under either a maximum likelihood (e.g., {lme4}, {glmmML}, the no-longer-maintained {glmmboot}) or Bayesian (e.g., {MCMCglmm}, {glmmTMB}, {brms}) framework. The methods for generating maximum likelihood and Bayesian parameter estimates under GLMMs are more complicated, but conceptually, the process is an extension of what we have talked about already. Below, we explore such a scenario.\n\nCHALLENGE\nBoden-Parry et al.¬†(2020) studied the effect of food type and abundance on the begging and food sharing behavior of otters in captivity. Here, we work with the dataset provided in the article below and replicate one of their models.\n\nBowden-Parry, M.; Postma, E.; and Boogert, N.J. (2020). Effects of food type and abundance on begging and sharing in Asian small-clawed otters (Aonyx cinereus). PeerJ 8: e10369.\n\n\nStep 1\n\nRead in the data set and look at the variables included.\n\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/Bowden-ParryOtterdata.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\nhead(d)\n\n## # A tibble: 6 √ó 22\n##   zoo   ID    fooddensity foodtype trialorder forcedclaim unsuccessfulforcedcl‚Ä¶¬π\n##   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;         &lt;dbl&gt;       &lt;dbl&gt;                  &lt;dbl&gt;\n## 1 T     Feet  medium      trout             7           0                      0\n## 2 T     Feet  a_low       trout             8           0                      0\n## 3 T     Feet  high        trout             9           0                      0\n## 4 T     Feet  medium      crab              1           0                      0\n## 5 T     Feet  a_low       crab              2           0                      0\n## 6 T     Feet  high        crab              3           0                      0\n## # ‚Ñπ abbreviated name: ¬π‚Äãunsuccessfulforcedclaim\n## # ‚Ñπ 15 more variables: activebeg &lt;dbl&gt;, passivebeg &lt;dbl&gt;, totalbeg &lt;dbl&gt;,\n## #   collectnear &lt;dbl&gt;, relaxedclaim &lt;dbl&gt;, foodgiving &lt;dbl&gt;,\n## #   totalshareproonly &lt;dbl&gt;, attackfromowner &lt;dbl&gt;, trialduration &lt;dbl&gt;,\n## #   agecategory &lt;chr&gt;, sex &lt;chr&gt;, begreceived &lt;dbl&gt;,\n## #   ownersharebecauseharassed &lt;dbl&gt;, TotalSharebothharasspro &lt;dbl&gt;,\n## #   sharingbybegonly &lt;dbl&gt;\n\n\n\n\nStep 2\n\nCreate a new random variable, trial, that joins together the variables zoo and trialorder. This will be used as a random effect.\nCreate a new random variable, obs, that is simply an observation number. This will also be used as a random effect.\n\n\nHINT: Check out the useful function row_to_column_id()\n\n\nRename the variable TotalSharebothharasspro as Shared.\nRename the variable begreceived as BegRec.\n\n\n\nShow Code\nd &lt;- d |&gt;\n    mutate(trial = paste0(zoo, trialorder))\nd &lt;- rowid_to_column(d, var = \"obs\")\nd &lt;- d |&gt;\n    rename(Shared = TotalSharebothharasspro, BegRec = begreceived)\n\n\n\n\nStep 3\n\nPlot total food transferred (Shared) in relation to begging received (BegReceived).\n\n\n\nShow Code\nggplot(d, aes(x = BegRec, y = Shared)) + geom_jitter() + xlab(\"Begging Received (total number)\") +\n    ylab(\"Food Transfers (total number)\") + theme(text = element_text(size = 12))\n\n\n\n\n\n\n\n\n\n\n\nStep 4\n\nDoes the amount of begging received influence sharing frequency?\n\nTo test this, the researchers used a GLMM with Poisson error distribution to examine how the number of ‚Äúfood transfers‚Äù between otters (Shared) in trials was related to the total number of instances of ‚Äúbegging received‚Äù (BegRec) as a fixed effect.\nWe also need to consider that the response variable, Shared, is implicitly expressed as a rate, i.e., it represents a count of the number of events that occured in over a particular length of time (the variable, trial duration). This allows the observation time to vary among records in the dataset. Our regression model, then, needs to contain time on the right-hand side of the regression formula in some way to acknowledge that counts are in some way dependent on the time over which they are collected (i.e., counts are expected to only increase with time).\nAdditionally, recall that, in Poisson regression, given that our response variable is a count of events that occur within a given amount of time, what we are actually modeling as a function of our predictors is log(count)‚Ä¶ so we need to include log(time) as what known as an offset term on the right-hand side of the formula. In R, we use the function offset() to indicate this in our regression formula. The offset term is, essentially, modeled an additional fixed effect though we will not generate a coefficient for it.\n\\[log(count) \\sim offset(log(time))\\ +\\ ...\\ other\\ predictors\\] Finally, the researchers included three random effects in their model: otter ID, trial, and an observation-level random effect. The first of these appears in the author‚Äôs data frame, and latter two are our variables trial and obs.\n\nNOTE: To me, it makes little sense to include the third of these random effects as there is no variance associated with this term, but to replicate the researchers‚Äô model, we include it.\n\n\n\nShow Code\nm1 &lt;- glmer(Shared ~ BegRec + offset(log(trialduration/60)) + (1 | ID) + (1 | trial) +\n    (1 | obs), data = d, family = poisson(link = \"log\"))\nsummary(m1)\n\n\nShow Output\n## Generalized linear mixed model fit by maximum likelihood (Laplace\n##   Approximation) [glmerMod]\n##  Family: poisson  ( log )\n## Formula: Shared ~ BegRec + offset(log(trialduration/60)) + (1 | ID) +  \n##     (1 | trial) + (1 | obs)\n##    Data: d\n## \n##       AIC       BIC    logLik -2*log(L)  df.resid \n##     666.9     682.9    -328.5     656.9       175 \n## \n## Scaled residuals: \n##      Min       1Q   Median       3Q      Max \n## -1.34822 -0.80347 -0.04456  0.48868  2.15072 \n## \n## Random effects:\n##  Groups Name        Variance Std.Dev.\n##  obs    (Intercept) 0.23311  0.4828  \n##  ID     (Intercept) 0.06723  0.2593  \n##  trial  (Intercept) 0.14429  0.3799  \n## Number of obs: 180, groups:  obs, 180; ID, 20; trial, 18\n## \n## Fixed effects:\n##             Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept) -2.65525    0.14568 -18.226  &lt; 2e-16 ***\n## BegRec       0.09057    0.02543   3.562 0.000368 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Correlation of Fixed Effects:\n##        (Intr)\n## BegRec -0.372\n\n\n\nShow Code\nm2 &lt;- glmer(Shared ~ offset(log(trialduration/60)) + (1 | ID) + (1 | trial) + (1 |\n    obs), data = d, family = poisson(link = log))\n\nanova(m1, m2, test = \"F\")\n\n\nShow Output\n## Data: d\n## Models:\n## m2: Shared ~ offset(log(trialduration/60)) + (1 | ID) + (1 | trial) + (1 | obs)\n## m1: Shared ~ BegRec + offset(log(trialduration/60)) + (1 | ID) + (1 | trial) + (1 | obs)\n##    npar    AIC    BIC  logLik -2*log(L)  Chisq Df Pr(&gt;Chisq)    \n## m2    4 678.11 690.88 -335.05    670.11                         \n## m1    5 666.91 682.88 -328.46    656.91 13.193  1   0.000281 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nShow Code\n# or\nlmtest::lrtest(m1, m2)\n\n\nShow Output\n## Likelihood ratio test\n## \n## Model 1: Shared ~ BegRec + offset(log(trialduration/60)) + (1 | ID) + \n##     (1 | trial) + (1 | obs)\n## Model 2: Shared ~ offset(log(trialduration/60)) + (1 | ID) + (1 | trial) + \n##     (1 | obs)\n##   #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n## 1   5 -328.46                         \n## 2   4 -335.05 -1 13.193   0.000281 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nLooking at these results, we would conclude that the model with BegRec has a lower AIC, higher log likelihood, lower deviance than the model without BegRec, thus variance in the food sharing is associated with the amount of begging received. The coefficent for BegRec (~0.09) is positive, indicating that sharing increases with the amount of begging received.\nAgain, the {mixedup} package allows us to neatly extract summary information from mixed effects models.\n\nmixedup::summarize_model(m1)\n\n##  Group    Effect Variance   SD Var_prop\n##    obs Intercept     0.23 0.48     0.52\n##     ID Intercept     0.07 0.26     0.15\n##  trial Intercept     0.14 0.38     0.32\n\n\n##       Term Value   SE      Z P_value Lower_2.5 Upper_97.5\n##  Intercept -2.66 0.15 -18.23    0.00     -2.94      -2.37\n##     BegRec  0.09 0.03   3.56    0.00      0.04       0.14\n\nmixedup::extract_fixed_effects(m1)\n\n## # A tibble: 2 √ó 7\n##   term       value    se      z p_value lower_2.5 upper_97.5\n##   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n## 1 Intercept -2.66  0.146 -18.2        0    -2.94       -2.37\n## 2 BegRec     0.091 0.025   3.56       0     0.041       0.14\n\nmixedup::extract_random_coefficients(m1) |&gt;\n    head()\n\n## # A tibble: 6 √ó 7\n##   group_var effect    group value    se lower_2.5 upper_97.5\n##   &lt;chr&gt;     &lt;chr&gt;     &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n## 1 obs       Intercept 1     -2.81 0.475     -3.74      -1.88\n## 2 obs       Intercept 2     -2.80 0.475     -3.74      -1.87\n## 3 obs       Intercept 3     -2.46 0.458     -3.35      -1.56\n## 4 obs       Intercept 4     -2.90 0.461     -3.80      -1.99\n## 5 obs       Intercept 5     -2.78 0.479     -3.72      -1.85\n## 6 obs       Intercept 6     -2.77 0.482     -3.71      -1.82\n\nmixedup::extract_random_effects(m1) |&gt;\n    head()\n\n## # A tibble: 6 √ó 7\n##   group_var effect    group  value    se lower_2.5 upper_97.5\n##   &lt;chr&gt;     &lt;chr&gt;     &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n## 1 obs       Intercept 1     -0.154 0.452    -1.04       0.732\n## 2 obs       Intercept 2     -0.15  0.452    -1.04       0.736\n## 3 obs       Intercept 3      0.198 0.434    -0.652      1.05 \n## 4 obs       Intercept 4     -0.241 0.437    -1.10       0.616\n## 5 obs       Intercept 5     -0.13  0.456    -1.02       0.763\n## 6 obs       Intercept 6     -0.114 0.459    -1.01       0.785\n\n\nThe glmmTMB() function from {glmmTMB} can also be used for mixed effects modeling and gives very similar results‚Ä¶\n\nm3 &lt;- glmmTMB(Shared ~ BegRec + offset(log(trialduration/60)) + (1 | ID) + (1 | trial) +\n    (1 | obs), data = d, family = poisson(link = \"log\"))\n\nmixedup::summarise_model(m3)\n\n## \n## Variance Components:\n\n\n##  Group    Effect Variance   SD SD_2.5 SD_97.5 Var_prop\n##     ID Intercept     0.07 0.26   0.11    0.61     0.15\n##  trial Intercept     0.14 0.38   0.22    0.65     0.32\n##    obs Intercept     0.23 0.48   0.32    0.74     0.52\n\n\n## \n## Fixed Effects:\n\n\n##       Term Value   SE      Z P_value Lower_2.5 Upper_97.5\n##  Intercept -2.66 0.15 -18.23    0.00     -2.94      -2.37\n##     BegRec  0.09 0.03   3.56    0.00      0.04       0.14\n\nm4 &lt;- glmmTMB(Shared ~ offset(log(trialduration/60)) + (1 | ID) + (1 | trial) + (1 |\n    obs), data = d, family = poisson(link = \"log\"))\n\nanova(m3, m4, test = \"F\")\n\n## Data: d\n## Models:\n## m4: Shared ~ offset(log(trialduration/60)) + (1 | ID) + (1 | trial) + , zi=~0, disp=~1\n## m4:     (1 | obs), zi=~0, disp=~1\n## m3: Shared ~ BegRec + offset(log(trialduration/60)) + (1 | ID) + , zi=~0, disp=~1\n## m3:     (1 | trial) + (1 | obs), zi=~0, disp=~1\n##    Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    \n## m4  4 678.10 690.87 -335.05   670.10                             \n## m3  5 666.91 682.88 -328.46   656.91 13.185      1  0.0002823 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFinally, the {MASS} package provides yet another function, glmmPQL(), that can be used for mixed effects modeling. The structure for specifying the model is different than we have used before, with a list of the random effects being presented as a separate argument from the model including the fixed effects. Note the absence of obs as a random effect in the model below‚Ä¶ including it causes the model to fail to run due to singular convergence, i.e., the variance in that random effect variable is essentially zero.\n\nm5 &lt;- MASS::glmmPQL(Shared ~ BegRec + offset(log(trialduration/60)), random = list(ID = ~1,\n    trial = ~1), data = d, family = poisson(link = \"log\"))\n\nmixedup::summarise_model(m5)\n\n##     Group    Effect Variance   SD Var_prop\n##        ID Intercept     0.00 0.00     0.00\n##     trial Intercept     0.00 0.00     0.00\n##  Residual        NA     2.08 1.44     1.00\n\n\n##       Term Value   SE      Z P_value Lower_2.5 Upper_97.5\n##  Intercept -2.51 0.09 -27.76    0.00     -2.69      -2.33\n##     BegRec  0.08 0.02   4.83    0.00      0.05       0.11\n\n\n\nNOTE: The coefficient values estimated by glmmPQL() are similar to, but not the same, as those estimated by glmer() and glmmTMB().",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Mixed Effects or Multilevel Modeling</span>"
    ]
  },
  {
    "objectID": "24-module.html#concept-review",
    "href": "24-module.html#concept-review",
    "title": "24¬† Mixed Effects or Multilevel Modeling",
    "section": "Concept Review",
    "text": "Concept Review\n\n‚ÄúMixed‚Äù modeling extends general and generalized linear modeling to consider cases where we have additional ‚Äúrandom‚Äù factors that are another source of possible variation in our response variable\nThe approach allows us to estimate either different intercepts or different slopes and intercepts for each level of the random factor\nAs in generalized linear modeling, likelihood ratio tests and information criteria approaches can be used to compare the explanatory power of different models\nThough we do not address it in this class, regression modeling can be extended further to consider NONLINEAR relationships among predictor variables\n\nThe R packages {lme4} and {nlme} include functions for performing for ‚Äúnonlinear mixed effects modeling‚Äù\n\n\nThe website Mixed Models with R is great resource for information about mixed effects modeling.",
    "crumbs": [
      "Part II - Statistics and Inference",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Mixed Effects or Multilevel Modeling</span>"
    ]
  },
  {
    "objectID": "99-maximum_likelihood.html",
    "href": "99-maximum_likelihood.html",
    "title": "Maximum Likelihood Estimation",
    "section": "",
    "text": "Objectives\nMaximum Likelihood Estimation (MLE) is a method for estimating population parameters (such as the mean and variance for a normal distribution, the rate parameter (lambda) for a Poisson distribution, the regression coefficients for a linear model) from sample data such that the combined probability (likelihood) of obtaining the observed data is maximized. Here, we first explore the calculation of likelihood values generally and then use MLE to estimate the best parameters for defining from what particular normal distribution a set of data is most likely drawn.",
    "crumbs": [
      "Part III - Miscellany",
      "Maximum Likelihood Estimation"
    ]
  },
  {
    "objectID": "99-maximum_likelihood.html#objectives",
    "href": "99-maximum_likelihood.html#objectives",
    "title": "Maximum Likelihood Estimation",
    "section": "",
    "text": "This module provides a simple introduction to Maximum Likelihood Estimation.",
    "crumbs": [
      "Part III - Miscellany",
      "Maximum Likelihood Estimation"
    ]
  },
  {
    "objectID": "99-maximum_likelihood.html#preliminaries",
    "href": "99-maximum_likelihood.html#preliminaries",
    "title": "Maximum Likelihood Estimation",
    "section": "Preliminaries",
    "text": "Preliminaries\n\nInstall and load these packages in R: {bbmle} and {maxLik}\nLoad {tidyverse} and {mosaic}",
    "crumbs": [
      "Part III - Miscellany",
      "Maximum Likelihood Estimation"
    ]
  },
  {
    "objectID": "99-maximum_likelihood.html#probability-density-functions-and-likelihoods",
    "href": "99-maximum_likelihood.html#probability-density-functions-and-likelihoods",
    "title": "Maximum Likelihood Estimation",
    "section": "Probability Density Functions and Likelihoods",
    "text": "Probability Density Functions and Likelihoods\nA Probability Density Function (PDF) is a function describing the density of a continuous random variable (a probability mass function, or PMF, is an analogous function for discrete random variables). The function‚Äôs value can be calculated at any given point in the sample space (i.e., the range of possible values that the random variable might take) and provides the relative likelihood of obtaining that value on a draw from the given distribution.\nThe PDF for a normal distribution defined by the parameters \\(\\mu\\) and \\(\\sigma\\) is‚Ä¶\n\\[f(x | \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^\\frac{-(x-\\mu)^2}{2\\sigma^2}\\]\nWhere: - \\(\\mu\\) is the mean (or expected value) of the distribution - \\(\\sigma^2\\) is the variance of the distribution. - \\(\\sigma\\) the standard deviation. - \\(e\\) is the base of the natural logarithm (approximately 2.71828) - \\(\\pi\\) is a mathematical constant (approximately 3.14159)\nSolving this function for any value of \\(x\\) yields the relative likelihood (\\(L\\)) of drawing that value from the distribution.",
    "crumbs": [
      "Part III - Miscellany",
      "Maximum Likelihood Estimation"
    ]
  },
  {
    "objectID": "99-maximum_likelihood.html#calculating-likelihoods",
    "href": "99-maximum_likelihood.html#calculating-likelihoods",
    "title": "Maximum Likelihood Estimation",
    "section": "Calculating Likelihoods",
    "text": "Calculating Likelihoods\nAs an example, we can calculate the relative likelihood of drawing the number 45.6 out of a normal distribution with a mean (\\(\\mu\\)) of 50 and a standard deviation (\\(\\sigma\\)) of 10‚Ä¶\n\nval &lt;- 45.6\nmean &lt;- 50\nsd &lt;- 10\n(l &lt;- 1/sqrt(2 * pi * sd^2) * exp((-(val - mean)^2)/(2 * sd^2)))\n\n## [1] 0.03621349\n\n\nThe dnorm() function returns this relative likelihood directly‚Ä¶\n\n(l &lt;- dnorm(val, mean, sd))\n\n## [1] 0.03621349\n\n\nTo further explore calculating likelihoods, let‚Äôs create and plot a sample of 100 random variables from a normal distribution with a \\(\\mu\\) of 50 and a \\(\\sigma\\) of 10:\n\nd &lt;- tibble(val = rnorm(100, mean = 50, sd = 10))\np &lt;- ggplot(d) + geom_histogram(aes(x = val, y = after_stat(density)))\np\n\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThe mean and standard deviation of this sample are pretty are close to the parameters that we pass to the rnorm() function, even though (given the limited size of our sample) the distribution is not smooth‚Ä¶\n\n(mean &lt;- mean(d$val))\n\n## [1] 49.03848\n\n(sd &lt;- sd(d$val))\n\n## [1] 8.397534\n\n\nNow, on top of the histogram of our sample data, let‚Äôs plot two different normal distributions, one with the distribution our sample was drawn from (\\(\\mu\\) = 50 and \\(\\sigma\\) = 10, in red) and another with a different \\(\\mu\\) (65) and \\(\\sigma\\) (20).\n\np &lt;- p + stat_function(fun = function(x) dnorm(x, mean = 50, sd = 10), color = \"red\",\n    linewidth = 1) + stat_function(fun = function(x) dnorm(x, mean = 65, sd = 20),\n    color = \"blue\", linewidth = 1)\np\n\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nBased on the figure, it should be pretty clear that our observed sample (our set of 100 random draws) is more likely to have been drawn from the former distribution than the latter.\n\nCHALLENGE\nWhat is the relative likelihood of seeing a value of 41 on a random draw from a normal distribution with \\(\\mu\\) = 50 and \\(\\sigma\\) = 10?\n\nval &lt;- 41\nmean &lt;- 50\nsd &lt;- 10\n# or\n(l1 &lt;- dnorm(val, mean, sd))\n\n## [1] 0.02660852\n\n\n\n\nCHALLENGE\nWhat is the relative likelihood of seeing a value of 41 on a random draw from a normal distribution with \\(\\mu\\) = 65 and \\(\\sigma\\) = 20?\n\nval &lt;- 41\nmean &lt;- 65\nsd &lt;- 10\n(l2 &lt;- dnorm(val, mean, sd))\n\n## [1] 0.002239453\n\n\nNote the difference in these two likelihoods, l1 and l2‚Ä¶ there is a much higher likelhood of an observation of 41 coming from the first of the two normal distributions we considered. In this case, the likelihood ratio is l1/l2.\n\nl1/l2\n\n## [1] 11.88171\n\n\nWe can also calculate the relative likelihood of drawing a particular set of observations from a given distribution as the product of the likelihoods of each observation as the probability of n independent events is simply the product (\\(‚àè\\)) of their independent probabilities. An easy way to calculate the product of a set of numbers is by taking the natural log of each number, summing those values, and then exponentiating the sum. For this reason, likelihood calculations are often operationalized in terms of log likelihoods.\nFor example, the following operations are equivalent‚Ä¶\n\n5 * 8 * 9 * 4\n\n## [1] 1440\n\nexp(log(5) + log(8) + log(9) + log(4))\n\n## [1] 1440\n\nexp(sum(log(c(5, 8, 9, 4))))\n\n## [1] 1440\n\n\n\n\nCHALLENGE\nWhat is the relative likelihood of drawing the set of three numbers 41, 70, and 10 from a normal distribution with \\(\\mu\\) = 50 and \\(\\sigma\\) = 10?\n\nval &lt;- c(41, 70, 10)\nmean &lt;- 65\nsd &lt;- 10\n(l &lt;- dnorm(val, mean, sd))  # vector of likelihoods of each value\n\n## [1] 2.239453e-03 3.520653e-02 1.076976e-08\n\n(l[1] * l[2] * l[3])  # product of likelihoods\n\n## [1] 8.491242e-13\n\n(ll &lt;- log(l))  # log likelihoods of each value\n\n## [1]  -6.101524  -3.346524 -18.346524\n\n(ll &lt;- sum(ll))  # summed log likelihood\n\n## [1] -27.79457\n\n(l &lt;- exp(ll))  # convert back to likelihood\n\n## [1] 8.491242e-13\n\n\n\nNOTE: Likelihoods are always going to be zero or greater, but log likelihoods can be negative. A less negative log likelihood is nonetheless more likely than a more negative one.\n\n\n\nCHALLENGE\nWhat are the log likelihood and likelihood of drawing the sample, d, we constructed above from a normal distribution with \\(\\mu\\) = 50 and \\(\\sigma\\) = 10? How do these compare to the log likelihood and likelihood of that same sample being drawn from a normal distribution with \\(\\mu\\) = 65 and \\(\\sigma\\) = 20?\n\nval &lt;- d$val\nmean &lt;- 50\nsd &lt;- 10\nl &lt;- dnorm(val, mean, sd)\nll &lt;- log(l)\nll &lt;- sum(ll)\n(l &lt;- exp(ll))  # a very tiny number!\n\n## [1] 5.376061e-156\n\nmean &lt;- 65\nsd &lt;- 20\nl &lt;- dnorm(val, mean, sd)\nll &lt;- log(l)\nll &lt;- sum(ll)\n(l &lt;- exp(ll))  # an even tinier number\n\n## [1] 2.330268e-188\n\n\nThe log likelihood and likelihood are both HIGHER (more likely to have been drawn) from the first distribution.",
    "crumbs": [
      "Part III - Miscellany",
      "Maximum Likelihood Estimation"
    ]
  },
  {
    "objectID": "99-maximum_likelihood.html#maximum-likelihood-estimation",
    "href": "99-maximum_likelihood.html#maximum-likelihood-estimation",
    "title": "Maximum Likelihood Estimation",
    "section": "Maximum Likelihood Estimation",
    "text": "Maximum Likelihood Estimation\nNow that we have an understanding of how likelihood calculations are done, we can consider the process of maximum likelihood estimation (MLE). MLE allows us to estimate what values of population parameters (e.g., \\(\\mu\\) and \\(\\sigma\\)) are most likely given a dataset and a probability distribution function for the process generating the data (e.g., a Gaussian process).\nFirst, let‚Äôs create a function for calculating the negative log likelihood for a set of data under a particular normal distribution. To convert the log likelihood to a negative, we simply multiply it negative 1. The reason we do this is because most optimization algorithms function by searching for parameter values that yield the minimum negative log likelihood rather than the maximum likelihood directly.\n\nverbose_nll &lt;- function(val, mu, sigma, verbose = FALSE) {\n    l &lt;- 0  # set initial likelihood to 0 to define variable\n    ll &lt;- 0  # set initial log likelihood to 0 to define variable\n    for (i in 1:length(val)) {\n        l[[i]] = dnorm(val[[i]], mean = mu, sd = sigma)  # likelihoods\n        ll[[i]] &lt;- log(l[[i]])  # log likelihoods\n        if (verbose == TRUE) {\n            message(paste0(\"x=\", round(val[[i]], 4), \" mu=\", mu, \" sigma=\", sigma,\n                \" L=\", round(l[[i]], 4), \" logL=\", round(ll[[i]], 4)))\n        }\n    }\n    nll &lt;- -1 * sum(ll)\n    return(nll)\n}\n\nTesting our function‚Ä¶\n\n# if we include verbose = TRUE as an argument to `verbose_nll`, the function\n# will print each likelihood and log likelihood value before returning the\n# summed negative log likelihood\nval &lt;- d$val\nmean &lt;- 50\nsd &lt;- 10\nverbose_nll(val, mean, sd)\n\n## [1] 357.5213\n\nmean &lt;- 65\nsd &lt;- 20\nverbose_nll(val, mean, sd)\n\n## [1] 432.04\n\n\n‚Ä¶ we see that negative log likelihood of our data under the first distribution is smaller than under the second, indicating that it is more likely that are data are drawn from the first distribution.\nTo use maximum likelihood estimation, we need to create a simpler version of this function that does not have the data as argument, but rather only involves the parameter we want to estimate. The function below does this. Note that unlike the function above, this version does not use a loop and has the vector variable val included as the first argument to the dnorm() function. We also include the argument log = TRUE within the dnorm() function to calculate the log likelihood directly, instead of including that as an extra step.\n\nsimple_nll &lt;- function(mu, sigma, verbose = TRUE) {\n    ll = sum(dnorm(val, mean = mu, sd = sigma, log = TRUE))\n    nll &lt;- -1 * ll\n    return(nll)\n}\n\nTesting our function, we should get the same results as above‚Ä¶\n\nval &lt;- d$val\nmean &lt;- 50\nsd &lt;- 10\nsimple_nll(mean, sd)\n\n## [1] 357.5213\n\nval &lt;- d$val\nmean &lt;- 65\nsd &lt;- 20\nsimple_nll(mean, sd)\n\n## [1] 432.04\n\n\nSo far, we have just use this function to calculate the (negative) log likelihood of our data being generated by a Gaussian (normal) process with specific \\(\\mu\\) and \\(\\sigma\\) parameters‚Ä¶ but what we really want to do is estimate values for \\(\\mu\\) and \\(\\sigma\\) that have highest likelihood for generating our data. To do this, we will use the mle2() function from the {bbmle} package. This function takes the following arguments‚Ä¶\n\nminuslogl, which is a user-defined function for generating negative log likelihoods‚Ä¶ this is what we created above\nstart, which is a list of initial values for the parameters we want estimate\nmethod, which is the particular algorithm used for optimization of the parameter estimates (mle2() and the optim() function, which it calls, are highly customizable)\n\n\nNOTE: Be aware that to run the function below, we need to have the variable val, a vector of values, specified outside of the function. We previously set val to d, our random sample of 100 draws from a normal distribution with \\(\\mu\\)=50 and \\(\\sigma\\)=10‚Ä¶ feel free to play around with alternatives and see how well the MLE process estimates these parameters.\n\n\nval &lt;- rnorm(1000, mean = 50, sd = 10)\nmle_norm &lt;- mle2(minuslogl = simple_nll, start = list(mu = 0, sigma = 1), method = \"SANN\"  # simulated annealing method of optimization, one of several options\n)\n# note that we may sometimes see a warning about NaNs being produced... this is\n# okay!\nmle_norm\n\n## \n## Call:\n## mle2(minuslogl = simple_nll, start = list(mu = 0, sigma = 1), \n##     method = \"SANN\")\n## \n## Coefficients:\n##       mu    sigma \n## 50.26084 10.37185 \n## \n## Log-likelihood: -3757.87\n\n# note the estimates for `mu` and `sigma` are very close to the population\n# level parameters of the normal distribution that our vector `val` was drawn\n# from\n\nAn alternative for MLE is the maxLik() function from the {maxLik} package. To use function, we again first define our own log likelihood function, but this time we do not want an negative log likelihood, so we do not multiply it by minus 1. This is because the first argument to maxLik() is a log likelihood function, not a negtive log likelihood function. Our function should also have only one argument‚Ä¶ a vector of parameter values‚Ä¶ rather than separate arguments for each parameter.\n\nsimple_ll &lt;- function(parameters) {\n    # `parameters` is a vector of parameter values\n    mu &lt;- parameters[1]\n    sigma &lt;- parameters[2]\n    ll = sum(dnorm(val, mean = mu, sd = sigma, log = TRUE))\n    return(ll)\n}\n\n# test...\nsimple_ll(c(50, 10))  # this should be very close to the log likelihood returned above\n\n## [1] -3759.54\n\n# an alternative function\nsimple_ll &lt;- function(parameters) {\n    mu &lt;- parameters[1]\n    sigma &lt;- parameters[2]\n    N &lt;- length(val)\n    ll &lt;- -1 * N * log(sqrt(2 * pi)) - N * log(sigma) - 0.5 * sum((val - mu)^2/sigma^2)\n    return(ll)\n}\n\n# test...\nsimple_ll(c(50, 10))\n\n## [1] -3759.54\n\n\nAs for mle2(), for maxLik() we need to specify several arguments:\n\nlogLik, which is our user-defined function for generating log likelihoods (not negative log likelihoods)\nstart, which is a named vector of initial values for the parameters we want estimate\nmethod, which is the particular algorithm used for optimization of the parameter estimates\n\n\nmle_norm &lt;- maxLik(logLik = simple_ll, start = c(mu = 0, sigma = 1), method = \"NM\")\nsummary(mle_norm)\n\n## --------------------------------------------\n## Maximum Likelihood estimation\n## Nelder-Mead maximization, 85 iterations\n## Return code 0: successful convergence \n## Log-Likelihood: -3757.871 \n## 2  free parameters\n## Estimates:\n##       Estimate Std. error t value Pr(&gt; t)    \n## mu     50.2312     0.3236  155.23  &lt;2e-16 ***\n## sigma  10.3891     0.2261   45.94  &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## --------------------------------------------\n\n\nThe results should be very comparable to those generated using mle2().",
    "crumbs": [
      "Part III - Miscellany",
      "Maximum Likelihood Estimation"
    ]
  },
  {
    "objectID": "99-maximum_likelihood.html#mle-for-simple-linear-regression",
    "href": "99-maximum_likelihood.html#mle-for-simple-linear-regression",
    "title": "Maximum Likelihood Estimation",
    "section": "MLE for Simple Linear Regression",
    "text": "MLE for Simple Linear Regression\nNow let‚Äôs work through an example of using MLE for simple linear regression. Load in our zombie apocalypse survivors dataset‚Ä¶\n\nlibrary(tidyverse)\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/zombies.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\n\n## Rows: 1000 Columns: 10\n## ‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n## Delimiter: \",\"\n## chr (4): first_name, last_name, gender, major\n## dbl (6): id, height, weight, zombies_killed, years_of_education, age\n## \n## ‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n## ‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Standard OLS linear regression model with our zombie apocalypse survivor\n# dataset\nm &lt;- lm(data = d, height ~ weight)\nsummary(m)\n\n## \n## Call:\n## lm(formula = height ~ weight, data = d)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -7.1519 -1.5206 -0.0535  1.5167  9.4439 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 39.565446   0.595815   66.41   &lt;2e-16 ***\n## weight       0.195019   0.004107   47.49   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.389 on 998 degrees of freedom\n## Multiple R-squared:  0.6932, Adjusted R-squared:  0.6929 \n## F-statistic:  2255 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n# Compare to...  Likelihood based linear regression with `mle2()`... now we\n# likelihood function that has three instead of two parameters: b0, b1, and\n# sigma, or the standard deviation in (presumably normally distributed!)\n# residuals...\nsimple_nll &lt;- function(b0, b1, sigma, verbose = TRUE) {\n    # here, we say how our expectation for how our response variable, y, is\n    # calculated based on parameters\n    y &lt;- b0 + b1 * d$weight\n    N &lt;- nrow(d)\n    ll &lt;- -1 * N * log(sqrt(2 * pi)) - N * log(sigma) - 0.5 * sum((d$height - y)^2/sigma^2)\n    nll &lt;- -1 * ll\n    return(nll)\n}\n\nmle_norm &lt;- mle2(minuslogl = simple_nll, start = list(b0 = 10, b1 = 0, sigma = 1),\n    method = \"Nelder-Mead\"  # implements the Nelder-Mead algorithm for likelihood maximization\n)\nmle_norm\n\n## \n## Call:\n## mle2(minuslogl = simple_nll, start = list(b0 = 10, b1 = 0, sigma = 1), \n##     method = \"Nelder-Mead\")\n## \n## Coefficients:\n##         b0         b1      sigma \n## 39.5429077  0.1951592  2.3746502 \n## \n## Log-likelihood: -2288.65\n\nsummary(mle_norm)\n\n## Maximum likelihood estimation\n## \n## Call:\n## mle2(minuslogl = simple_nll, start = list(b0 = 10, b1 = 0, sigma = 1), \n##     method = \"Nelder-Mead\")\n## \n## Coefficients:\n##        Estimate Std. Error z value     Pr(z)    \n## b0    39.542908   0.592347  66.756 &lt; 2.2e-16 ***\n## b1     0.195159   0.004083  47.798 &lt; 2.2e-16 ***\n## sigma  2.374650   0.052716  45.046 &lt; 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## -2 log L: 4577.301\n\n# Compare to...  Likelihood based linear regression with `maxLik()`\nloglik &lt;- function(parameters) {\n    b0 &lt;- parameters[1]\n    b1 &lt;- parameters[2]\n    sigma &lt;- parameters[3]\n    N &lt;- nrow(d)\n    # here, we say how our expectation for how our response variable, y, is\n    # calculated based on parameters\n    y &lt;- b0 + b1 * d$weight  # estimating mean height as a function of weight\n    # then use this mu in a similar fashion as previously\n    ll &lt;- -1 * N * log(sqrt(2 * pi)) - N * log(sigma) - 0.5 * sum((d$height - y)^2/sigma^2)\n    return(ll)\n}\n\nmle_norm &lt;- maxLik(loglik, start = c(beta0 = 0, beta1 = 0, sigma = 1), method = \"NM\")  # implements the Nelder-Mead algorithm for likelihood maximization\nsummary(mle_norm)\n\n## --------------------------------------------\n## Maximum Likelihood estimation\n## Nelder-Mead maximization, 184 iterations\n## Return code 0: successful convergence \n## Log-Likelihood: -2288.645 \n## 3  free parameters\n## Estimates:\n##        Estimate Std. error t value Pr(&gt; t)    \n## beta0 39.680322   0.950924   41.73  &lt;2e-16 ***\n## beta1  0.194221   0.006522   29.78  &lt;2e-16 ***\n## sigma  2.387501   0.053872   44.32  &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## --------------------------------------------\n\n# results of both of these should be similar!",
    "crumbs": [
      "Part III - Miscellany",
      "Maximum Likelihood Estimation"
    ]
  },
  {
    "objectID": "99-python-from-R.html",
    "href": "99-python-from-R.html",
    "title": "Using Python from R",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "Part III - Miscellany",
      "Using ***Python*** from ***R***"
    ]
  },
  {
    "objectID": "99-python-from-R.html#objectives",
    "href": "99-python-from-R.html#objectives",
    "title": "Using Python from R",
    "section": "",
    "text": "In this module, we look at how to integrate Python code and functions into R.",
    "crumbs": [
      "Part III - Miscellany",
      "Using ***Python*** from ***R***"
    ]
  },
  {
    "objectID": "99-python-from-R.html#preliminaries",
    "href": "99-python-from-R.html#preliminaries",
    "title": "Using Python from R",
    "section": "Preliminaries",
    "text": "Preliminaries\n\nInstall and load this package in R: {reticulate}\nLoad {tidyverse} and {broom}\nMake sure you have a working Python installation on your computer and a package manager for Python\n\n\nNOTE: Recent versions of the MacOS, Windows, and Linux operating systems typically include a Python installation by default, but that may not be an up-to-date version and the included package manager may not be comprehensive. Thus, for most users, it is worthwhile to download and install an updated version of Python along with a software system for managing Python libraries and environments.\n\nAnaconda is one such such system; it is a comprehensive, open-source distribution for Python (as well as other scientific computing software, including R, and RStudio, and Jupyter notebooks), and it provides an easy way to get an up-to-date version of Python installed. Binary Anaconda installers are available for most computer operating systems, including MacOS, Windows, and Linux. Running the installer basically installs a command-line package manager, conda, for all of these different pieces of software, as well as a graphical version of that manager and software launcher, named Anaconda-Navigator. Unless you specify otherwise, the current distribution of Anaconda should install (on MacOS) to a directory called ~/anaconda3 at the root of your user folder.\nAlternatively, you can download and install a pared-down version of Anaconda called [Miniconda]((https://www.anaconda.com/download), which consists of just the conda package manager, Python, and a limited set of additional packages. You can download and install Miniconda yourself from this URL, in which case it should install to this directory on MacOS: ~/opt/miniconda3.\n\nNOTE: There is no need to install both Anaconda and Miniconda‚Ä¶ choose one or the other! Both will write a series of PATH configuration lines to your shell profile, and, if you do install both, the latter one to be installed will overwrite the PATH details of the former in your default shell profile.\n\nFinally, you can download and install Miniconda from within {reticulate}. In fact, when you first run a function from library(reticulate), you may see a prompt asking if you want to install Miniconda‚Ä¶\nNo non-system installation of Python could be found.\nWould you like to download and install Miniconda?\nMiniconda is an open source environment management system for Python.\nSee https://docs.conda.io/en/latest/miniconda.html for more details.\nIf so, you can answer ‚ÄúY‚Äù and let the installation run. In doing so, the installer will create a directory at the base of your user folder called ~/Library/r-miniconda with a conda installation in it. If you are not prompted to install Miniconda, you can install it manually using the {reticulate} command install_miniconda().\n\ninstall_miniconda()\n\n\nNOTE: This process installs Miniconda to a different location and does not add PATH configuration lines to your shell script. Thus, if you choose this route for Miniconda installation, it will be less easy to use the terminal to run conda commands as you would then need to manually configure your PATH.",
    "crumbs": [
      "Part III - Miscellany",
      "Using ***Python*** from ***R***"
    ]
  },
  {
    "objectID": "99-python-from-R.html#background",
    "href": "99-python-from-R.html#background",
    "title": "Using Python from R",
    "section": "Background",
    "text": "Background\nOne of the wonderful things about R is that it offers interoperability with several other programming languages useful for statistical analysis, data science, and data visualization, including Python, Stan, Julia, JavaScript, and C++. Here, we explore the {reticulate} package, which essentially embeds a Python session within your R session, allowing you the opportunity to smoothly integrate Python code into R workflows and analyses through the RStudio IDE. For additional details, see the information on the {reticulate} package‚Äôs website R Interface to Python website.\nWith {reticulate}, we can call Python code from R in several ways:\n\nBy including Python code chunks within an R Markdown document\nBy importing Python modules into R and accessing their functions\nBy sourcing entire Python scripts (‚Äú.py‚Äù files)\nBy using Python interactively within an R session\n\nWe can also use {reticulate} to convert between R and Python objects (e.g., between R and Python {pandas} data frames).",
    "crumbs": [
      "Part III - Miscellany",
      "Using ***Python*** from ***R***"
    ]
  },
  {
    "objectID": "99-python-from-R.html#getting-started-with-reticulate",
    "href": "99-python-from-R.html#getting-started-with-reticulate",
    "title": "Using Python from R",
    "section": "Getting Started with {reticulate}",
    "text": "Getting Started with {reticulate}\n\nConfiguring Python\nMany computers may have several different iterations of Python installed in different places. Upon loading {reticulate} and first calling the Python function import, either directly or implicitly , R tries to find and ‚Äúattach‚Äù to one of these versions. To control the process, it is recommended that you specify or build a desired Python version and then pass this suggested instance to {reticulate}. Restarting R will unbind an instance and allow you to specify a different one.\nThe R commands Sys.which(\"python\") and Sys.which(\"python3\") will return the path to the current version of Python (for pre-version 3 installations) and Python3, respectively that R has bound. Unless you specify a Python instance from a particular ‚Äúenvironment‚Äù (see below), these will usually be ‚Äú/usr/bin/python‚Äù and ‚Äú/usr/bin/python3‚Äù or ‚Äú/usr/local/bin/python‚Äù and ‚Äú/usr/local/bin/python3‚Äù. [Sometimes, the path returned by these functions is actually a symlink, a kind of pointer, to the actual executable file.]\n\nNOTE: If your computer does not have an older, pre-3.0 version of Python installed, Sys.which(\"python\") may return a blank path or the path to a 3.0 version of Python.\n\n\nSys.which(\"python\")\nSys.which(\"python3\")\n\nAfter loading {reticulate}, if you run the function py_config(), it will return information on the version of and path to the Python binary (executable program) and associated libraries that {reticulate} is currently bound to, It also returns the version and location of {numpy}, an important Python library for maths, if it is installed.\n\npy_config()\n\n## python:         /opt/anaconda3/bin/python3.11\n## libpython:      /opt/anaconda3/lib/libpython3.11.dylib\n## pythonhome:     /opt/anaconda3:/opt/anaconda3\n## version:        3.11.7 (main, Dec 15 2023, 12:09:56) [Clang 14.0.6 ]\n## numpy:          /opt/anaconda3/lib/python3.11/site-packages/numpy\n## numpy_version:  1.26.4\n## \n## NOTE: Python version was forced by RETICULATE_PYTHON_FALLBACK\n\n\nYou can specify or change the instance of Python that {reticulate} uses via the RStudio IDE by going either the Preferences (MacOS) or Global Options (MacOS or PC) dialog box and selecting the Python section‚Ä¶\n‚Ä¶ and then choosing the desired Python interpreter from the dialog boxes. RStudio will try to highlight all available Python interpreters for you, either ones installed in the system (e.g., distributed with MacOS) or ones associated with different virtual or conda ‚Äúenvironments‚Äù (see below) that you have created. You can also type the path to the version of the Python interpreter you wish to using directly into the dialog box. After you click, ‚ÄúApply‚Äù you will be prompted to restart R to allow {reticulate} be able to rebind to a different interpreter.\nAfter R restarts, reload {reticulate}, then call py_config() to verify that {reticulate} is connected to the desired Python executable.\n\nNOTE: After changing the bound version of Python in this manner and running py_config(), the last line of the output should read as follows:\nNOTE: Python version was forced by use_python() function\n\nNote that specification of the Python interpreter in this manner is ‚Äústicky‚Äù. That is, if an interpreter is specified in Global Options, then that takes precedence over assigning an interpreter with use_python() or by assigning an environment (see below). That is, if you specify a version of Python to use in Global Options, you will not be able change to a different interpreter or specify what environment to use via code.\nIf we do not use Global Options or Preferences to specify an interpreter, we can instead directly suggest what Python interpreter that {reticulate} should use from R by calling the use_python() function with the path to the interpreter as an argument. For example, as noted above, the current Anaconda distribution typically installs its own version of Python at ‚Äú~/anaconda3/bin/python‚Äù, thus we could specify that we want {reticulate} to bind to that version as follows:\n\nuse_python(\"~/anaconda3/bin/python\", required = TRUE)\npy_config()  # confirm our selection\n\nSimilarly, as described above, the Miniconda version of the conda package manager installed from within {reticulate} places its version of Python inside a user‚Äôs Library directory, at ‚Äú~/Library/r-miniconda/bin/python‚Äù. We can set {reticulate} to bind that version using the following code:\n\nuse_python(\"~/Library/r-miniconda/bin/python\", required = TRUE)\npy_config()  # confirm our selection\n\nHowever, it is important to remember that if you want to modify the particular instance of Python that {reticulate} should to attach to, it is necessary to first RESTART R, which you can do either in RStudio (by choosing that option from the Session menu) or by using the function .rs.restartR() at a command prompt), then load {reticulate} again, and then run the use_python() function. After doing this, you can run py_config() again to confirm that the Python configuration has indeed changed as desired, and you can check with Sys.which(\"python\") or Sys.which(\"python3\") to confirm what instance of Python is attached. For example‚Ä¶\n\n.rs.restartR()\nlibrary(reticulate)\n\n\nuse_python(\"~/Library/r-miniconda/bin/python\", required = TRUE)\npy_config()  # confirm our selection\nSys.which(\"python3\")\n\n\nNOTE: After changing the bound version of Python in this manner and running py_config(), the last line of the output should read as follows:\nNOTE: Python version was forced by use_python function\n\nNote that changing the intepreter this way is not ‚Äústicky.‚Äù That is, if you were to restart R again, load {reticulate}, and run py_config() without specifying a configuration using use_python(), {reticulate} will scan along your PATH and other possible places where Python might be installed to find the first appropriate instance, giving preference to one where {numpy} is present as that library is very important for integrating Python and R. Importantly, this may find a version of Python not associated with any particular ‚Äúenvironment‚Äù (see below).\n\n\nWorking with Environments\nWhenever we work with Python, it is best to do so in the context of an ‚Äúenvironment‚Äù. An environment (such as the ‚Äúr-reticulate‚Äù environment created by default by installing Miniconda) is basically a folder that contains all the necessary executables and Python modules (which are akin to R packages) that a particular Python project needs. An environment is kind of analogous to a repository containing an R Project, except that in R Projects, the R executable and all of the packages used in a project are not physically replicated in each repository the way that a Python executable and referenced modules are (often) replicated within the environment. When we install any specific module that we want to are going to use in a script, we typically install it to a specific working environment.\nUsing environments is a way of isolating and keeping different versions of modules independent so as not to ‚Äúbreak‚Äù our code when modules are updated. They also allow us to experiment with different Python setups.\nPython and {reticulate} support two different types of environments, conda environments, which are managed by conda and ‚Äúvirtual environments‚Äù. We will focus first on conda environments.\n\nconda Environments\nThe {reticulate} function conda_list() can be used to list all of the different conda environments that are registered within conda installations on our computer, along with a path to the Python binary associated with the environment.\n\nNOTE: Sometimes the path returned by conda_list() is a symlink to the actual executable file. That is, the symlink is located in the environment folder while the actual executable file resides in a different folder like /usr/bin/ or /usr/local/bin.\n\n\nconda_list()\n\n##           name                                      python\n## 1         base                   /opt/anaconda3/bin/python\n## 2   DEEPLABCUT   /opt/anaconda3/envs/DEEPLABCUT/bin/python\n## 3 r-reticulate /opt/anaconda3/envs/r-reticulate/bin/python\n\n\nAfter running this command, you should see two or three environments listed, depending on whether you installed Anaconda, Miniconda through {reticulate}, or both. Installing Anaconda should have created an environment called ‚Äúanaconda3‚Äù by default, while installing Miniconda through {reticulate} should have created two environments, one called ‚Äúbase‚Äù and one called ‚Äúr-reticulate‚Äù.\nYou can set up other new environments by running the function conda_create() and specifying a different environment name as an argument.\n\nNOTE: By default, if no environments have yet been defined, the conda_create() function creates one named ‚Äúr-reticulate‚Äù. This is what the Miniconda installer ran behind the scenes to create the default ‚Äúr-reticulate‚Äù environment. But we can also provide an alternative name within the particular active conda installation.\n\nWhen you create a new environment with conda_create(), it will be set up inside of the envs/ directory in which our conda installation resides (again, if using Miniconda, this should be within the ~/Library/r-miniconda/ directory. Environments created with this function will include a default suite of Python packages that {reticulate} will have access to. They will also be associated with the particular instance of Python that is currently bound to {reticulate}, unless we specify otherwise using the ‚Äúpython_version=‚Äù argument.\n\n# create a new **conda** environment named 'my-environment'\nconda_create(\"my-environment\")\nconda_list()\n\nThe {reticulate} function conda_remove() will delete a particular conda environment.\n\nconda_remove(\"my-environment\")\nconda_list()\n\n\n\n\nSpecifying an Environment\nIf we have more than one conda environment set up on our machines, perhaps with different Python modules installed and/or using different versions of Python, we can use the functions use_miniconda(condaenv=) or use_condaenv(condaenv=) to ‚Äúactivate‚Äù a particular environment, i.e., to specify which one you want to bind to a particular R and {reticulate} session. [The ‚Äúr-reticulate‚Äù environment set up by Miniconda, if present, is the one that will be used by default if no other environment is specified.] Given that each environment contains a Python executable (or path to an executable), this is an alternative to use_python() as a way of also specifying a Python version for a particular project. Specifying an environment to use is analogous to opening a particular R Project from a repository.\nAs for use_python(), the way to force use of a non-default environment is to restart R, load {reticulate}, and run the function use_miniconda() (or, equivalently, use_condaenv()) as below, after which running py_config() should confirm the configuration.\n\n.rs.restartR()\nlibrary(reticulate)\n\n\nuse_miniconda(condaenv = \"r-reticulate\", required = TRUE)\n# or use_condaenv(condaenv = 'r-reticulate', required = TRUE)\npy_config()\nSys.which(\"python3\")\n\n\nNOTE: It is a good idea to include the argument required=TRUE, which ensures that an error will be thrown if the specified environment is not found.\n\nIt is possible to also run all of these conda functions at the command prompt in a terminal‚Ä¶\n# to list your conda environments and show which one is active\n$ conda info --envs\n\n# OR\n$ conda env list\n\nNOTE: Doing this from the command line shows an asterisk next to an environment called ‚Äúbase‚Äù that is a default environment that was created when we first installed Anaconda (as opposed to Miniconda) and is associated with your base conda installation. This is typically at the path: ‚Äú~/anaconda3‚Äù in current Anaconda installations.\n\n# to create a conda environment\n$ conda create --name my-environment\n\n# to create a conda environment with a specific version of Python\n# this will install an executable for the specified version\n# of Python within the environment\n$ conda create --name my-environment python=3.9\n\n# to activate an environment\n$ conda activate my-environment\n\nNOTE: Running conda env list after activating a different environment shows the asterisk next to the newly activated environment.\n\n# to list the packages in the active environment\n$ conda list\n\n# to deactivate an active environment and return to *base*\n$ conda deactivate\n\n# to remove an environment\n$ conda remove --name my-environment --all\n\nNOTE: An active environment needs to be deactivated before it can be removed.\n\nThe conda documentation provides further details on how to use the conda package and environment management system that is central to the Anaconda and Miniconda distributions.\n\n‚ÄúVirtual‚Äù Environments\nAs noted above, {reticulate} actually supports using two different Python environment flavors: **conda*‚Äú** environments (as we set up above) and‚Äùvirtual‚Äù environments. The former are stored inside of a directory called envs/ that sits within a particular conda installation. The latter type of environments are typically stored in a hidden directory called .virtualenvs/ at the root of your user folder.\nTo create and use ‚Äúvirtual‚Äù rather than conda environments with {reticulate}, the commands are very similar. Unless otherwise specified, the environment created will be bound to the Python interpreter associated with the current {reticulate} and R session.\n\nvirtualenv_create(envname = \"r-reticulate\")\nvirtualenv_create(envname = \"my-environment\")\nvirtualenv_list()\nvirtualenv_remove(envname = \"my-environment\")\nuse_virtualenv(virtualenv = \"r-reticulate\")\n\nOnce we have bound {reticulate} to a particular conda or ‚Äúvirtual‚Äù environment, we can check/confirm how our setup is configured by running py_config().\n\npy_config()\n\n\n\n\nAdding Python Modules\nThe {reticulate} package includes the simple function py_install() that can be used to install one or more Python modules into a specified environment (either a conda environment, as we created above, or a ‚Äúvirtual environment‚Äù if we are using that flavor of environment instead). With py_install(), new modules, by default, will be installed into the environment named ‚Äúr-reticulate‚Äù (though this can be changed by specifying the ‚Äúenvname=‚Äù argument).\n\nNOTE: Alternatively, the function conda_install(envname, packages) will install python modules into a particular, specified conda environment, and the function virtualenv_install(envname, packages) will install packages into into a particular virtual environment, but py_install() is sufficient for our purposes.\n\nBelow, we install the python modules {scipy}, {numpy}, {pandas}, {matplotlib}, and {seaborn}, which are used in the subsequent Python coding examples, into the ‚Äúr-reticulate‚Äù Miniconda environment.\n\nNOTE: In the function below, the envname= argument does not need to specified, but I have included it anyway.\n\n\np &lt;- c(\"numpy\", \"scipy\", \"pandas\", \"matplotlib\", \"seaborn\", \"statsmodels\")\npy_install(envname = \"r-reticulate\", packages = p)\n\n## + /opt/anaconda3/bin/conda install --yes --name r-reticulate -c conda-forge numpy scipy pandas matplotlib seaborn statsmodels\n\n# or conda_install(envname = 'r-reticulate', packages = p) or\n# virtualenv_install(envname = 'r-reticulate', packages = p, required = TRUE)\n\nYou can also use standard shell installation utilities at a terminal prompt (e.g., conda or pip) to activate an environment and to install Python modules.\n# to activate an environment...\n$ conda activate r-reticulate\n\n# to install a module into an active conda environment...\n$ conda install scipy\n\n# or to install into a different, named environment, e.g., r-reticulate\n$ conda install --name r-reticulate scipy\n\n# to install a module into system level Python (i.e., globally, instead of in a specified environment)\n$ sudo pip install scipy",
    "crumbs": [
      "Part III - Miscellany",
      "Using ***Python*** from ***R***"
    ]
  },
  {
    "objectID": "99-python-from-R.html#python-code",
    "href": "99-python-from-R.html#python-code",
    "title": "Using Python from R",
    "section": "Using Python Code in R",
    "text": "Using Python Code in R\nAs mentioned in the introduction above, there are multiple ways to integrate Python code into your R projects and workflow. You can‚Ä¶\n\nInclude Python code chunks in R Markdown documents\nR Markdown now includes a language engine that supports bi-directional communication between R and Python (R chunks can access Python objects and vice-versa).\nImport Python modules to access their functions directly from R\nThe import() function from {reticulate} enables you to import any Python module and call its functions directly from R.\nSource a Python script from R\nThe source_python() function from {reticulate} enables you to source a Python script the same way you would source() an R script. Python functions and objects defined within the script become directly available to the R session.\nUse a Python REPL (‚ÄúRead-Eval-Print Loop‚Äù)\nThe repl_python() function embeds an interactive Python console within R that you can use interactively, as you would the R console. Objects you create within Python are available to your R session, and vice-versa.\n\nMore detail on each of these approaches, and examples, are given below.\n\nPython Code Chunks\nThe following example Python code chunk, which can be included in an ‚Äú.Rmd‚Äù document, imports the zombie apocalypse survivors datafile from GitHub and plots a linear model of height~age and a boxplot of height~gender. Note that the chunk type is {python} not {r}!\n\n# This is a Python code chunk\nimport pandas\nimport seaborn\nimport matplotlib\nf = \"https://raw.githubusercontent.com/difiore/ada-datasets/main/zombies.csv\"\nz = pandas.read_csv(f)\nprint(z.head())\n\n##    id first_name  ...                            major        age\n## 0   1      Sarah  ...                 medicine/nursing  17.642745\n## 1   2       Mark  ...  criminal justice administration  22.589513\n## 2   3    Brandon  ...                        education  21.912760\n## 3   4      Roger  ...                   energy studies  18.190582\n## 4   5      Tammy  ...                        logistics  21.103986\n## \n## [5 rows x 10 columns]\n\nz = z[['gender', 'height', 'weight', 'age', 'major']]\n# create a plot using the python seaborn module\np = seaborn.pairplot(\n  data = z,\n  hue = 'gender',\n  kind = 'scatter',\n  plot_kws = dict(alpha = 0.2)\n)\n\n## /opt/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n##   with pd.option_context('mode.use_inf_as_na', True):\n## /opt/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n##   with pd.option_context('mode.use_inf_as_na', True):\n## /opt/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n##   with pd.option_context('mode.use_inf_as_na', True):\n\n# plot the active plot object\nmatplotlib.pyplot.show(p)\n\n\n\n\n\n\n\nmatplotlib.pyplot.clf()\np = seaborn.boxplot(x=\"gender\", y=\"height\", data=z)\n# plot the active plot object\nmatplotlib.pyplot.show(p)\n\n\n\n\n\n\n\n# type `exit` at the Python prompt to exit the REPL\nexit\n\n## Use exit() or Ctrl-D (i.e. EOF) to exit\n\n\n\n\nImporting and Using Modules\nRunning the import() command in R will load functions and other data from within Python modules and classes directly into R. The functions and data included in the Python module can then be accessed via the $ operator, analogous to the way they would when we interact with an R object.\nImported Python modules also support code completion and inline help, again paralleling what we see with R packages.\n\npandas &lt;- import(\"pandas\")\nseaborn &lt;- import(\"seaborn\")\nmatplotlib &lt;- import(\"matplotlib\")\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/zombies.csv\"\nz &lt;- pandas$read_csv(f)\nhead(z)\n\n##   id first_name last_name gender   height   weight zombies_killed\n## 1  1      Sarah    Little Female 62.88951 132.0872              2\n## 2  2       Mark    Duncan   Male 67.80277 146.3753              5\n## 3  3    Brandon     Perez   Male 72.12908 152.9370              1\n## 4  4      Roger   Coleman   Male 66.78484 129.7418              5\n## 5  5      Tammy    Powell Female 64.71832 132.4265              4\n## 6  6    Anthony     Green   Male 71.24326 152.5246              1\n##   years_of_education                           major      age\n## 1                  1                medicine/nursing 17.64275\n## 2                  3 criminal justice administration 22.58951\n## 3                  1                       education 21.91276\n## 4                  6                  energy studies 18.19058\n## 5                  3                       logistics 21.10399\n## 6                  4                  energy studies 21.48355\n\n# create a plot using the python seaborn module\np &lt;- seaborn$lmplot(x = \"age\", y = \"height\", data = z)\n\nBecause there is sometimes some awkwardness with using the {matplotlib} Python module from within R, we can switch back to a {python} chunk to print out the figure created‚Ä¶\n\nimport matplotlib\nr.p\n\n\n\n\n\n\n\n# plot the active plot object created with the R code above\nmatplotlib.pyplot.show(p)\n\n\n\n\n\n\n\n# type `exit` at the Python prompt to exit the REPL\nexit\n\n## Use exit() or Ctrl-D (i.e. EOF) to exit\n\n\n\n\nSourcing a Python Script\nWe can use the source_python() function in R to source a Python script the same way we would source() an R script. We give the function the path to the file to load in and run as an argument. This process assumes the ‚Äú.py‚Äù script is stored in a directory called src/ located in the current working directory. The script loads in the zombie apocalypse survivors dataset from GitHub and then plots a boxplot of weight~gender.\n\nf &lt;- \"python_script.py\"\nsource_python(paste0(getwd(), \"/src/\", f))\n\n##    first_name last_name  ...                            major        age\n## id                       ...                                            \n## 1       Sarah    Little  ...                 medicine/nursing  17.642745\n## 2        Mark    Duncan  ...  criminal justice administration  22.589513\n## 3     Brandon     Perez  ...                        education  21.912760\n## 4       Roger   Coleman  ...                   energy studies  18.190582\n## 5       Tammy    Powell  ...                        logistics  21.103986\n## \n## [5 rows x 9 columns]\n\n\nAs above, we switch to a {python} chunk to print out the figures created‚Ä¶\n\nimport matplotlib\nr.p\n# plot the active plot object created with the sourced python code\nmatplotlib.pyplot.show(p)\n\n\n\n\n\n\n\n# type `exit` at the Python prompt to exit the REPL\nexit\n\n## Use exit() or Ctrl-D (i.e. EOF) to exit\n\n\n\n\nUsing Python Interactively\nFinally, if you want to work with Python interactively, you can call the repl_python() function from R, which provides a Python REPL (‚ÄúRead‚ÄìEval‚ÄìPrint Loop‚Äù) session embedded within your R session.\n\nrepl_python()\n\nTo test this out, copy and paste each of the following lines singly at the Python console prompt (&gt;&gt;&gt;).\nimport pandas\nimport seaborn\nimport matplotlib\nf = \"https://raw.githubusercontent.com/difiore/ADA-datasets/master/zombies.csv\"\nz = pandas.read_csv(f)\nprint(z.head())\nseaborn.lmplot(x=\"weight\", y=\"height\", data=z)\nmatplotlib.pyplot.show()\nexit",
    "crumbs": [
      "Part III - Miscellany",
      "Using ***Python*** from ***R***"
    ]
  },
  {
    "objectID": "99-python-from-R.html#converting-data-objects",
    "href": "99-python-from-R.html#converting-data-objects",
    "title": "Using Python from R",
    "section": "Converting Data Objects",
    "text": "Converting Data Objects\nOne of the great things about {reticulate} allows us to share variables and states of Python objects across Python code chunks within the same R Markdown document, the same way we can with R code chunks.\nIt also allows us to access objects created within Python code chunks from R using the py object (e.g., we can use py$age would access a variable, age created within Python from R).\nLikewise, {reticulate} allows us to access objects created within R chunks from Python using the r object (e.g., we could use r.sex to access a variable named sex that was created within R from inside a Python code chunk).\n{reticulate} supports built-in conversion between Python and R for many Python objects, including {numpy} arrays and {pandas} data frames.\nEXAMPLE:\nHere, we run a {python} chunk and load in a familiar data file from over the web‚Ä¶\n\nimport pandas\nf = \"https://raw.githubusercontent.com/difiore/ada-datasets/main/woollydata.csv\"\nd = pandas.read_csv(f)\nprint(d.head())\n\n##       name  success    age    rank\n## 0    Aaron       15   9.80  medium\n## 1     Adam       14  12.06  medium\n## 2  Agustin       22  16.33    high\n## 3     Alan        9   8.79     low\n## 4   Andrew       12   6.81     low\n\nexit\n\n## Use exit() or Ctrl-D (i.e. EOF) to exit\n\n\n‚Ä¶ then we run an {r} chunk that accesses and uses the Python object created in the chunk above, after which we load a different data set from the web and run a linear model on it using R‚Ä¶\n\nlibrary(tidyverse)\nlibrary(broom)\nhead(py$d)\n\n##      name success   age   rank\n## 1   Aaron      15  9.80 medium\n## 2    Adam      14 12.06 medium\n## 3 Agustin      22 16.33   high\n## 4    Alan       9  8.79    low\n## 5  Andrew      12  6.81    low\n## 6 Anthony      11 14.84    low\n\nboxplot(py$d$success ~ py$d$rank, xlab = \"Rank\", ylab = \"Success\")\n\n\n\n\n\n\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/master/zombies.csv\"\nz &lt;- read_csv(f, col_names = TRUE)\nhead(z)\n\n## # A tibble: 6 √ó 10\n##      id first_name last_name gender height weight zombies_killed\n##   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;          &lt;dbl&gt;\n## 1     1 Sarah      Little    Female   62.9   132.              2\n## 2     2 Mark       Duncan    Male     67.8   146.              5\n## 3     3 Brandon    Perez     Male     72.1   153.              1\n## 4     4 Roger      Coleman   Male     66.8   130.              5\n## 5     5 Tammy      Powell    Female   64.7   132.              4\n## 6     6 Anthony    Green     Male     71.2   153.              1\n## # ‚Ñπ 3 more variables: years_of_education &lt;dbl&gt;, major &lt;chr&gt;, age &lt;dbl&gt;\n\nm &lt;- lm(data = z, height ~ weight)\ntidy(m)\n\n## # A tibble: 2 √ó 5\n##   term        estimate std.error statistic   p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)   39.6     0.596        66.4 0        \n## 2 weight         0.195   0.00411      47.5 2.65e-258\n\n\n‚Ä¶ then we again run a {python} chunk, now accessing the R data frame created in the {r} chunk above and using Python to run and print a summary of the same linear model.\n\nprint(r.z.head())\n\n##     id first_name  ...                            major        age\n## 0  1.0      Sarah  ...                 medicine/nursing  17.642745\n## 1  2.0       Mark  ...  criminal justice administration  22.589513\n## 2  3.0    Brandon  ...                        education  21.912760\n## 3  4.0      Roger  ...                   energy studies  18.190582\n## 4  5.0      Tammy  ...                        logistics  21.103986\n## \n## [5 rows x 10 columns]\n\nimport pandas\n# statsmodels package for OLS using formulas\nimport statsmodels.formula.api as smf\n# regression of height ~ weight with intercept\nm = smf.ols(formula = 'height ~ weight', data=r.z)\nprint(m.fit().summary())\n\n##                             OLS Regression Results                            \n## ==============================================================================\n## Dep. Variable:                 height   R-squared:                       0.693\n## Model:                            OLS   Adj. R-squared:                  0.693\n## Method:                 Least Squares   F-statistic:                     2255.\n## Date:                Tue, 22 Apr 2025   Prob (F-statistic):          2.65e-258\n## Time:                        15:21:29   Log-Likelihood:                -2288.6\n## No. Observations:                1000   AIC:                             4581.\n## Df Residuals:                     998   BIC:                             4591.\n## Df Model:                           1                                         \n## Covariance Type:            nonrobust                                         \n## ==============================================================================\n##                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n## ------------------------------------------------------------------------------\n## Intercept     39.5654      0.596     66.406      0.000      38.396      40.735\n## weight         0.1950      0.004     47.486      0.000       0.187       0.203\n## ==============================================================================\n## Omnibus:                        8.716   Durbin-Watson:                   1.938\n## Prob(Omnibus):                  0.013   Jarque-Bera (JB):                9.617\n## Skew:                           0.159   Prob(JB):                      0.00816\n## Kurtosis:                       3.360   Cond. No.                     1.14e+03\n## ==============================================================================\n## \n## Notes:\n## [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n## [2] The condition number is large, 1.14e+03. This might indicate that there are\n## strong multicollinearity or other numerical problems.\n\n# del [pandas, seaborn, matplotlib, ggplot, smf]\n# delete refs to imported modules\nexit\n\n## Use exit() or Ctrl-D (i.e. EOF) to exit",
    "crumbs": [
      "Part III - Miscellany",
      "Using ***Python*** from ***R***"
    ]
  },
  {
    "objectID": "99-r-packages.html",
    "href": "99-r-packages.html",
    "title": "Building Custom R Packages",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "Part III - Miscellany",
      "Building Custom ***R*** Packages"
    ]
  },
  {
    "objectID": "99-r-packages.html#objectives",
    "href": "99-r-packages.html#objectives",
    "title": "Building Custom R Packages",
    "section": "",
    "text": "This module provides an overview of how to build and distribute your own, custom R package. An R package bundles together code, data, and documentation [1] using a set of standard conventions for organizing the components of the bundle and [2] in a way that is easily reusable, shareable, and distributable. Below, we go through some examples of how to create bare-minimum R packages. This module is based heavily and shamelessly on material from the following sources:\n\n\nWriting an R Package from Scratch, by Hilary Parker\nMaking your First R Package, by Fong Chun Chan\nWriting an R Package from Scratch, by Thomas Westlake\nR Package Primer, by Karl Broman",
    "crumbs": [
      "Part III - Miscellany",
      "Building Custom ***R*** Packages"
    ]
  },
  {
    "objectID": "99-r-packages.html#preliminaries",
    "href": "99-r-packages.html#preliminaries",
    "title": "Building Custom R Packages",
    "section": "Preliminaries",
    "text": "Preliminaries\n\nInstall and load these packages: {devtools} and {roxygen2}\nInstall this package: {withr}\nLoad {tidyverse} and {usethis}\nAdditionally, make sure you have a C compiler installed on your computer. Many computers do, by default, but to be sure do the following:\n\nOn Windows, install RTools\nOn MacOS, make sure you have either the Mac software development IDE, XCode, or the latest ‚ÄúCommand-Line Tools for XCode‚Äù installed\n\nTo do the latter, open a Terminal window, run the command xcode-select --install, and accept the license agreement\n\n\n\n\nNOTE: These are not R packages, but rather additional bits of software!\n\nOnce you have done all of this, switch to the R console and run the command has_devel(). If all is well, it should return the message ‚ÄúYour system is ready to build packages!‚Äù",
    "crumbs": [
      "Part III - Miscellany",
      "Building Custom ***R*** Packages"
    ]
  },
  {
    "objectID": "99-r-packages.html#steps-for-creating-a-package",
    "href": "99-r-packages.html#steps-for-creating-a-package",
    "title": "Building Custom R Packages",
    "section": "Steps for Creating a Package",
    "text": "Steps for Creating a Package\n\nCreate the Package Directory\nFirst, we are going to create a directory with the bare minimum structure needed for an R package. As an illustration, we are going to make a package for a single custom function I created for Module 17 on statistical power and effect size. We can create a new package one of the following ways:\n\nMethod 1:\n\nThe create_packages() command from the {usethis} package is the most expedient way to begin development of a new package. Specify as an argument to this function a &lt;PATH NAME&gt; to the package directory you want to create.\n\n\nusethis::create_package(\"~/Development/Repos/powerplot\")\n\nRunning this function will open a new RStudio session with the working directory set to the newly created directory for the package.\nYou should find, again, that R has created a new directory with several subdirectories and files, including a DESCRIPTION file, and a NAMESPACE file, and an R directory. Note that the man directory created in Method 2 below is not made here, nor is a git repository set up automatically, but a Build tab is added to your RStudio IDE workspace.\n\n\n\n\n\n\n\n\n\n\n\nMethod 2:\nAlternatively, you can accomplish a very similar setup for new package development by hand. Choose ‚ÄúFile &gt; New Project‚Äù and then following the dialog boxes to create a new project‚Ä¶\n\n\n\n\n\n\n\n\n\n\nCreate the project in a new directory‚Ä¶\n\n\n\n\n\n\n\n\n\n\n\n‚Ä¶ choose the form of the project to be a ‚Äúpackage‚Äù‚Ä¶\n\n\n\n\n\n\n\n\n\n\n\n‚Ä¶ and name your package (for this example, use the name ‚Äúpowerplot‚Äù), choose to create a git repository for the project, and choose to open it in a new session.\n\n\n\n\n\n\n\n\n\n\nOnce you create a new package project by this process, a new RStudio session will open, with the working directory set to the newly created directory for the package. you should see the following components:\n\nAn R/ directory, which will hold your code files\nA basic DESCRIPTION file, which is a text file that contains metadata about your package\nA NAMESPACE file\nAn .Rbuildignore file, which like .gitignore is a list of files and directories that you do not want included with your package when you eventually ‚Äúbuild‚Äù it (i.e., bundle it up into a single archive for distribution)\nA man/ directory, which will hold the manual pages for your package\n\n\nNOTE: This method differs from Method 1 in that it creates a man/ directory and sets up your package development repository as a git repository.\n\nWhichever method you use to create a new package, make sure you now work in the R session where your new project is active. Confirm that your working directory is set to be the new project directory using getwd().\n\ngetwd()\n\nIf you need to set the working directory, use setwd()\n\nsetwd(\"~/Development/Repos/powerplot\")\n\n\n\n\nEdit the DESCRIPTION File\nOpen up the newly created DESCRIPTION file, which was created as a template by either of the methods above. This is basically a structured text file that contains metadata for your package. You can edit this by hand as you see fit and then save it.\nThe Package line displays the name of the package. The Version line indicates a version number for your package and should be updated as you revise your package. The Title and Description lines give a broad understanding of the purpose of your package. The Author and Maintainer lines (created by Method 1) and the Author@R line (created by Method 2) include information on who the author (‚Äúaut‚Äù) and maintainer (‚Äúcre‚Äù) of the package are and should be replaced with your personal information. The License line indicates under what conditions your code can be used. For the package we are creating, you can use the following:\nPackage: powerplot\nTitle: Power Plot Package\nVersion: 0.0.1.0\nAuthors@R: \n    person(given = \"First\",\n           family = \"Last\",\n           role = c(\"aut\", \"cre\"),\n           email = \"first.last@example.com\",\n           comment = c(ORCID = \"YOUR-ORCID-ID\"))\nDescription: The function included in this package lets you explore power and effect size. When used with the {manipulate} package, you can use the sliders to set mu_0 and mu_A for a one-sample test (or, alternatively, think about these as mu_1 and mu_2 for a two-sample test), sigma, alpha, and n, and you can choose whether you are testing a one-sided hypothesis of mu_A being \"greater\" or \"less\" than mu_0 or are testing the two-sided hypothesis that mu_A != mu_0 (\"two.tailed\"). The graph will output Power and Effect Size.\nDepends: R (&gt;= 4.3)\nLicense: GPL-3\nEncoding: UTF-8\nLazyData: true\n\nNOTE: Be sure to end the DESCRIPTION file with a carriage return or you may get errors in running the ‚ÄúProcess the Documentation Files‚Äù). Also not that you might want to edit the Depends: R (&gt;=...) line to an earlier version of R (e.g., Depends: R (&gt;= 3.5)) if you have not updated your version of R recently.\n\n\n\nAdd Function Code\nYou are now ready to add code to your package, which you will do by adding files to the R/ subdirectory inside the package directory created by one of the methods above. Code you will include in packages is a bit different than the scripts you have been writing or ‚Äú.Rmd‚Äù documents‚Ä¶ usually, you will be using code in packages to create custom functions or objects.\nWhen you build a package, all of the code in the R/ directory is executed and saved in a format that R can read. Then, when you load in the package, the saved (or ‚Äúcached‚Äù) results are available for you to use.\nIf you used Method 1, you can simply edit the ‚Äúhello.R‚Äù file adding the code for the function(s) you want to bundle into your package (changing the name of the file if you like). If you used Method 2, you can simply create a new ‚Äú.R‚Äù script file containing the function(s) you want to bundle into your package inside the R folder within your package directory. For example, I designed the following function, power.plot() for Module 17 that lets a user explore the relationship between sample size, alpha level, mean, standard deviation, and power. I used the code below and saved this function in a file called ‚Äúpowerplot.R‚Äù.\n\nNOTE: In the code below, I have prefaced all {ggplot2} library function calls with ‚Äúggplot2::‚Äù as these are not included in {base} R. This will become important in a minute.\n\n\npower.plot &lt;- function(sigma, mu0, muA, n, alpha, alternative = \"two.tailed\") {\n    pow &lt;- 0\n    z &lt;- (muA - mu0)/(sigma/sqrt(n))\n    g &lt;- ggplot2::ggplot(data.frame(mu = c(min(mu0 - 4 * sigma/sqrt(n), muA - 4 *\n        sigma/sqrt(n)), max(mu0 + 4 * sigma/sqrt(n), muA + 4 * sigma/sqrt(n)))),\n        ggplot2::aes(x = mu)) + ggplot2::ggtitle(\"Power Calculations for Z Tests\")\n    g &lt;- g + ggplot2::ylim(c(0, max(dnorm(mu0, mu0, sigma/sqrt(n)) + 0.1, dnorm(muA,\n        muA, sigma/sqrt(n)) + 0.1)))\n    g &lt;- g + ggplot2::stat_function(fun = dnorm, geom = \"line\", args = list(mean = mu0,\n        sd = sigma/sqrt(n)), size = 1, col = \"red\", show.legend = TRUE)\n    g &lt;- g + ggplot2::stat_function(fun = dnorm, geom = \"line\", args = list(mean = muA,\n        sd = sigma/sqrt(n)), size = 1, col = \"blue\", show.legend = TRUE)\n\n    if (alternative == \"greater\") {\n        if (z &gt; 0) {\n            xcrit = mu0 + qnorm(1 - alpha) * sigma/sqrt(n)\n            g &lt;- g + ggplot2::geom_segment(x = xcrit, y = 0, xend = xcrit, yend = max(dnorm(mu0,\n                mu0, sigma/sqrt(n)) + 0.025, dnorm(muA, muA, sigma/sqrt(n)) + 0.025),\n                size = 0.5, linetype = 3)\n            g &lt;- g + ggplot2::geom_polygon(data = data.frame(cbind(x = c(xcrit, seq(from = xcrit,\n                to = muA + 4 * sigma/sqrt(n), length.out = 100), muA + 4 * sigma/sqrt(n)),\n                y = c(0, dnorm(seq(from = xcrit, to = muA + 4 * sigma/sqrt(n), length.out = 100),\n                  mean = muA, sd = sigma/sqrt(n)), 0))), ggplot2::aes(x = x, y = y),\n                fill = \"blue\", alpha = 0.5)\n            pow &lt;- pnorm(muA + 4 * sigma/sqrt(n), muA, sigma/sqrt(n)) - pnorm(xcrit,\n                muA, sigma/sqrt(n))\n        }\n    }\n    if (alternative == \"less\") {\n        if (z &lt; 0) {\n            xcrit = mu0 - qnorm(1 - alpha) * sigma/sqrt(n)\n            g &lt;- g + ggplot2::geom_segment(x = xcrit, y = 0, xend = xcrit, yend = max(dnorm(mu0,\n                mu0, sigma/sqrt(n)) + 0.025, dnorm(muA, muA, sigma/sqrt(n)) + 0.025),\n                size = 0.5, linetype = 3)\n            g &lt;- g + ggplot2::geom_polygon(data = data.frame(cbind(x = c(muA - 4 *\n                sigma/sqrt(n), seq(from = muA - 4 * sigma/sqrt(n), to = xcrit, length.out = 100),\n                xcrit), y = c(0, dnorm(seq(from = muA - 4 * sigma/sqrt(n), to = xcrit,\n                length.out = 100), mean = muA, sd = sigma/sqrt(n)), 0))), ggplot2::aes(x = x,\n                y = y), fill = \"blue\", alpha = 0.5)\n            pow &lt;- pnorm(xcrit, muA, sigma/sqrt(n)) - pnorm(muA - 4 * sigma/sqrt(n),\n                muA, sigma/sqrt(n))\n        }\n    }\n    if (alternative == \"two.tailed\") {\n        if (z &gt; 0) {\n            xcrit = mu0 + qnorm(1 - alpha/2) * sigma/sqrt(n)\n            g &lt;- g + ggplot2::geom_segment(x = xcrit, y = 0, xend = xcrit, yend = max(dnorm(mu0,\n                mu0, sigma/sqrt(n)) + 0.025, dnorm(muA, muA, sigma/sqrt(n)) + 0.025),\n                size = 0.5, linetype = 3)\n            g &lt;- g + ggplot2::geom_polygon(data = data.frame(cbind(x = c(xcrit, seq(from = xcrit,\n                to = muA + 4 * sigma/sqrt(n), length.out = 100), muA + 4 * sigma/sqrt(n)),\n                y = c(0, dnorm(seq(from = xcrit, to = muA + 4 * sigma/sqrt(n), length.out = 100),\n                  mean = muA, sd = sigma/sqrt(n)), 0))), ggplot2::aes(x = x, y = y),\n                fill = \"blue\", alpha = 0.5)\n            pow &lt;- pnorm(muA + 4 * sigma/sqrt(n), muA, sigma/sqrt(n)) - pnorm(xcrit,\n                muA, sigma/sqrt(n))\n        }\n        if (z &lt; 0) {\n            xcrit = mu0 - qnorm(1 - alpha/2) * sigma/sqrt(n)\n            g &lt;- g + ggplot2::geom_segment(x = xcrit, y = 0, xend = xcrit, yend = max(dnorm(mu0,\n                mu0, sigma/sqrt(n)) + 0.025, dnorm(muA, muA, sigma/sqrt(n)) + 0.025),\n                size = 0.5, linetype = 3)\n            g &lt;- g + ggplot2::geom_polygon(data = data.frame(cbind(x = c(muA - 4 *\n                sigma/sqrt(n), seq(from = muA - 4 * sigma/sqrt(n), to = xcrit, length.out = 100),\n                xcrit), y = c(0, dnorm(seq(from = muA - 4 * sigma/sqrt(n), to = xcrit,\n                length.out = 100), mean = muA, sd = sigma/sqrt(n)), 0))), ggplot2::aes(x = x,\n                y = y), fill = \"blue\", alpha = 0.5)\n            pow &lt;- pnorm(xcrit, muA, sigma/sqrt(n)) - pnorm(muA - 4 * sigma/sqrt(n),\n                muA, sigma/sqrt(n))\n        }\n    }\n    g &lt;- g + ggplot2::annotate(\"text\", x = max(mu0, muA) + 2 * sigma/sqrt(n), y = max(dnorm(mu0,\n        mu0, sigma/sqrt(n)) + 0.075, dnorm(muA, muA, sigma/sqrt(n)) + 0.075), label = paste(\"Effect Size = \",\n        round((muA - mu0)/sigma, digits = 3), \"\\nPower = \", round(pow, digits = 3),\n        sep = \"\"))\n    g &lt;- g + ggplot2::annotate(\"text\", x = min(mu0, muA) - 2 * sigma/sqrt(n), y = max(dnorm(mu0,\n        mu0, sigma/sqrt(n)) + 0.075, dnorm(muA, muA, sigma/sqrt(n)) + 0.075), label = \"Red = mu0\\nBlue = muA\")\n    g\n}\n\n\nNOTE: We can either create separate R script files (‚Äú.R‚Äù files) for each function we want to include in our package or we can include multiple functions in each of one or more files. If we do the latter, we just need to include function documentation comments (described below) before each function.\n\n\nImportant Caveats about Package Code\n\nDo not include ‚Äútop level‚Äù code‚Ä¶\n\nTop level code (e.g., code outside of a function) is only run at build time, which can create problems if the code in your package functions utilizes or ‚Äúdepends upon‚Äù a function from another package. For example, if you want to create a function that creates a special kind of formatting for a {ggplot2} plot object and your function calls ggplot(), that is an example of a ‚Äúdependency‚Äù. But, you should NOT include library(ggplot2) as top-level code in your package, because it will give you a problem at run time if {ggplot2} is not already loaded. Rather, you should describe all of the packages your code needs to run in the DESCRIPTION file (see Step [5] below).\n\nBe cautious about including in your package functions code that messes with the R environment. In general‚Ä¶\n\nDo not use library() or require()\nDo not use source()\nDo not modify a global option using options() unless you store the original setting(s) and then reset them on.exit() from your package function\n\n\nEXAMPLE:\nWithin your function, you could include the following‚Ä¶\n\n# code to save the default value for number of significant digits to print\noriginal &lt;- options(digits = 7)\n# code to change that global option\noptions(digits = 3)\n# code change the global option back on exit\non.exit(options(original), add = TRUE)\n\n\nDo not modify the working directory, e.g., with setwd() within your function, unless you set it back on.exit() from your package function\n\nEXAMPLE:\nIn your function, you could include the following‚Ä¶\n\n# code to save the default value for working directory\noriginal &lt;- setwd(tempdir())\n# code to change that global option\nsetwd(\"~/Desktop\")\n# code to change the global option back on exit\non.exit(setwd(original), add = TRUE)\n\n\n\n\nAdd Function Documentation\nWe next add documentation to our R script containing the power.plot() function by adding special structured comments to the begining of the ‚Äú.R‚Äù file. These comments begin with #' and then include certain parameter tags, marked with @. These comments will be read and compiled into the correct format for showing in the help/documentation files for our package when we build the package using {devtools} and {roxygen2}. Details for common parameter tags can be found in online {roxygen2} documentation, e.g., here or here.\nMinimally, we want to add the following tags to the start of our function file.\n\n@title\n@description\n@param - one line for each parameter in our function\n@keywords - to aid in searching functions\n@export - makes the function available for others to use when your package is loaded\n@examples - sample code\n\nFor my file, I added the text below just before the power.plot() function.\n#' @title An interactive power plotting function\n#' @description This function allows you explore the relationship between sample size, alpha level, mean, standard deviation, and power\n#' @param sigma standard deviation\n#' @param mu0 mean value under the null hypothesis\n#' @param muA mean value under the alternative hypothesis\n#' @param n sample size\n#' @param alpha alpha level\n#' @param alternative type of test to run, defaults to \"two-tailed\"\n#' @keywords power\n#' @export\n#' @examples\n#' power.plot(sigma = 0.5, mu0 = 1, muA = 1, n = 25, alpha = 0.05, alternative = \"two.tailed\")\n\n\nAdd Dependencies\nBecause I know that the function I just created uses functions from {ggplot2}, I want to add a reference to that package as a dependency to my DESCRIPTION file. To do this, I can run the following at the command line where I am developing my package:\n\nusethis::use_package(\"ggplot2\")\n\nThis function adds the following lines to the DESCRIPTION file:\nImports:\n  ggplot2\nAlso, because I tend to use this function inside of a function from {manipulate} (see Module 17), I also run the following‚Ä¶\n\nusethis::use_package(\"manipulate\", \"Suggests\")\n\n‚Ä¶ to add a ‚ÄúSuggests‚Äù block to the DESCRIPTION file:\nSuggests: \n  manipulate\n\nNOTE: You should open the DESCRIPTION file with the RStudio text editor to confirm that these ‚ÄúImports‚Äù and ‚ÄúSuggests‚Äù statements have been added.\n\n\n\nCreate Documentation Files\nNext, we create an ‚Äú.R‚Äù script file that includes {roxygen2} comments that will produce documentation about the package itself, and we place that file in the R subdirectory with all of our other ‚Äú.R‚Äù files. The use_package_doc() function from {usethis} will create a template for this file for us, with the default name of ‚Äúpackage name-package.R‚Äù\n\nusethis::use_package_doc()\n\n\n\nProcess Documentation Files\nTo now automatically create help/documentation files for our packages and the functions it contain from the comment annotations we included in the ‚Äú.R‚Äù files in the R directory, we run the document() function from {devtools}.\n\ndevtools::document()\n\nThis function automatically creates one or more ‚Äú.Rd‚Äù (RDocument) files and places them in the man (i.e., ‚Äúmanual‚Äù) directory, creating that directory if it does not already exist. It also adds a NAMESPACE file to the main package directory, if that does not already exist, or rewrites that file if it does.\n\n\nBuild the Package\nThe next thing we do is to bundle up all of the elements in our package directory into a single file and create a compressed version of that file. We do this by runnning the build() function from {devtools}. If we are in the package directory itself, then we do not need to specify an argument to build(), but we can provide build() with a path to the package directory if needed.\n\ndevtools::build()  # or\ndevtools::build(\"~/Development/Repos/powerplot\")\n\n\nNOTE: When we run build(), we might get the message ‚ÄúWarning: invalid uid value replaced by that for user ‚Äònobody‚Äô‚Äù. You can safely ignore that warning.\n\nRunning build() creates a ‚Äú.tar.gz‚Äù file for your package in the parent directory of your package directory. This file is basically a compressed (‚Äú.gz‚Äù) version of a bundled (‚Äú.tar‚Äù) file that contains all of the important components of your R package‚Ä¶ the function files in the R folder, the DESCRIPTION and NAMESPACE files, and the man folder containining RDocument document files (‚Äú.Rd‚Äù) for your package‚Äôs functions.\n\n\nInstall and Load the Package\nWe can now install this package into our working environment and check out our new documentation. The {devtools} function load_all(), executed from inside the package directory, will load your package into memory.\n\nNOTE: With this command, your package will not be installed into your system or user library and will disappear when you restart R. This also will not load any vignettes you have created (see below).\n\n\ndevtools::load_all()\n\nNow try out your new function! First, use library(powerplot) to load in your library. Then, here is what you should see in the ‚ÄúHelp‚Äù tab when you type ‚Äú?power.plot‚Äù at console line.\n\n\n\n\n\n\n\n\n\nYou can also install your package to your system or user library by using the {devtools} function install(). Here, I am installing to my user library.\n\ndevtools::install()\n# OR\nlibrary(withr)\nwith_libpaths(\"/Users/ad26693/Library/R/arm64/4.5/library\", code = devtools::install())\n# the code about puts my new {powerplot} package into my user library by\n# default... if I wanted to install it to my system library, I could provide\n# the path to that location instead\ndetach(package:withr)\n\nYou can also use the {base} function install.packages() function with the path to the ‚Äú.tar.gz‚Äù file you created with build() as an argument. Here, we use repos = NULL because we are installing from a local file and we set type = source as we‚Äôre installing a bundled, compressed source file. In the code below, I have set lib = to be the location of my system library, but you can instead use a user library (as we did above).\n\nNOTE: You may need to change the path depending on your operating system (the path below is a common one for MacOS users) and on where your system library is located. You may also need to have administrator privileges to successfully install to that location.\n\n\ninstall.packages(\"~/Development/Repos/powerplot_0.0.1.0.tar.gz\", repos = NULL, type = \"source\",\n    lib = \"/Library/Frameworks/R.framework/Versions/4.5/Resources/library\")\n\n\nNOTE: You may occasionally need to restart R after installing (or removing) a package in order to be able to access it. If, after installing your package and running library(), you cannot visualize the documentaton in the Help tab, try restarting R with the following command: .rs.restartR()\n\n\n\nCheck the Package for Errors\nIt is always a good idea to run certain tests on your package to make sure that it is constructed correctly before distributing it to others to install, either as a built file or via the Comprehensive R Archive Network (CRAN). The terminal function R CMD check is one way to run a series of automated checks. To do this, open a terminal window and navigate to the directory containing the ‚Äú.tar.gz‚Äù file (which should be the parent directory for your R package), and then run:\nR CMD check powerplot_0.0.1.0.tar.gz\nThis can also be done from R using {devtools} from inside the package directory.\n\ndevtools::check()\n\nEXAMPLE: Using the Package you Just Created\n\nlibrary(manipulate)\nmanipulate(power.plot(sigma, muA, mu0, n, alpha, alternative), sigma = slider(1,\n    4, step = 0.1, initial = 2), muA = slider(-10, 10, step = 0.1, initial = 2),\n    mu0 = slider(-10, 10, step = 0.1, initial = 0), n = slider(1, 50, step = 1, initial = 16),\n    alpha = slider(0.01, 0.1, step = 0.01, initial = 0.05), alternative = picker(\"two.tailed\",\n        \"greater\", \"less\"))\ndetach(package:manipulate)",
    "crumbs": [
      "Part III - Miscellany",
      "Building Custom ***R*** Packages"
    ]
  },
  {
    "objectID": "99-r-packages.html#writing-a-vignette",
    "href": "99-r-packages.html#writing-a-vignette",
    "title": "Building Custom R Packages",
    "section": "Writing a Vignette",
    "text": "Writing a Vignette\nTo include an R Markdown document as a vignette in your R package, all you need to do is:\n\nCreate a vignettes subdirectory within the package directory.\nPut your vignette ‚Äú.Rmd‚Äù file in that directory.\nWithin the YAML header at the top of the ‚Äú.Rmd‚Äù file, include code like the following:\n\n---\ntitle: \"Put the title of your vignette here\"\noutput: rmarkdown::html_vignette\nvignette: &gt;\n  %\\VignetteIndexEntry{Put the title of your vignette here}\n  %\\VignetteEngine{knitr::rmarkdown}\n  \\usepackage[utf8]{inputenc}\n---\n\nAnd add the following lines to your package‚Äôs DESCRIPTION file:\n\nSuggests: knitr, rmarkdown\nVignetteBuilder: knitr\nThe command use_vignette() from {usethis} will do all of this for us and give us a template ‚Äú.Rmd‚Äù file we can edit for our vignette‚Ä¶\n\nusethis::use_vignette(\"powerplot\", title = \"Power Plot Vignette\")\n\nOnce you have your ‚Äú.Rmd‚Äù file completed, can build (i.e., ‚Äúknit‚Äù) your vignette using‚Ä¶\n\ndevtools::build_vignettes()\n\nThe resulting rendered ‚Äú.html‚Äù version of your vignette will be placed into the inst/doc folder inside the package directory.\nAfter creating your vignette, you should run build() again to bundle the vignette into your ‚Äú.tar.gz‚Äù package file.\n\ndevtools::build()\n\nAlternatively, you can run R CMD build from a terminal prompt from inside the package directory to build a ‚Äú.tar.gz‚Äù file that contains the ‚Äú.html‚Äù version of the vignette.\nThen, you can run install() (or install_packages()), and library() again to pull the new package with vignette into R.\n\nNOTE: If you use install(), be sure to include the argument ‚Äúbuild_vignettes = TRUE‚Äù to ensure that the vignette is bundled with your package and installed.\n\n\ndevtools::install(build_vignettes = TRUE)\n# OR\nlibrary(withr)\nwith_libpaths(\"/Library/Frameworks/R.framework/Versions/4.5/Resources/library\", code = devtools::install(build_vignettes = TRUE))\n# again, this puts powerplot in my system library instead of my user library...\ndetach(package:withr)\n# OR\ninstall.packages(\"~/Development/Repos/powerplot_0.0.1.0.tar.gz\", repos = NULL, type = \"source\",\n    lib = \"/Library/Frameworks/R.framework/Versions/4.5/Resources/library\")\nlibrary(powerplot)\n\nCheck that your package now appears in the Packages tab and that clicking on its link takes you to the package documentation page with a hyperlink for ‚ÄúUser guides, package vignettes, and other documentation‚Äù. If the package does not show up there, you may need to restart R (using .rs.restartR()).",
    "crumbs": [
      "Part III - Miscellany",
      "Building Custom ***R*** Packages"
    ]
  },
  {
    "objectID": "99-r-packages.html#including-a-custom-dataset",
    "href": "99-r-packages.html#including-a-custom-dataset",
    "title": "Building Custom R Packages",
    "section": "Including a Custom Dataset",
    "text": "Including a Custom Dataset\nIt is often useful to include example datasets in your R package to use in examples or vignettes or to illustrate a data format.\nTo include a dataset in your package, you would create a data subdirectory inside of the package directory and place your dataset there, in ‚Äú.RData‚Äù format (using the extension ‚Äú.RData‚Äù or ‚Äú.rda‚Äù).\nYou can use the use_data() function from {use_this} to create the data directory and populate it with data from a dataframe.\n\nlibrary(tidyverse)\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/zombies.csv\"\nzombies &lt;- read_csv(f, col_names = TRUE)\nusethis::use_data(zombies, overwrite = TRUE)\n# overwrite argument replaces the data if it already exists\n\nYou can also use the save() function to create such files as follows:\n\nsave(zombies, file = \"data/zombies.RData\")\n# OR\nsave(zombies, file = \"data/zombies.rda\")\n\nThen, run document() and build()again as above to create the package anew with the data included in it, and run install() and library() to load it into your workspace. Vo√≠la! You can confirm that your dataset loaded by running the following‚Ä¶\n\nhead(zombies)",
    "crumbs": [
      "Part III - Miscellany",
      "Building Custom ***R*** Packages"
    ]
  },
  {
    "objectID": "99-r-packages.html#sharing-your-package",
    "href": "99-r-packages.html#sharing-your-package",
    "title": "Building Custom R Packages",
    "section": "Sharing Your Package",
    "text": "Sharing Your Package\nTo share your package with another user, simply pass them the ‚Äú.tar.gz‚Äù file and let them run install_packages() with the argument ‚Äútype=source‚Äù.\n\ninstall.packages(\"~/Development/Repos/powerplot_0.0.1.0.tar.gz\", repos = NULL, type = \"source\",\n    lib = \"/Library/Frameworks/R.framework/Versions/4.5/Resources/library\")",
    "crumbs": [
      "Part III - Miscellany",
      "Building Custom ***R*** Packages"
    ]
  },
  {
    "objectID": "99-r-packages.html#an-example-with-vignette-and-data",
    "href": "99-r-packages.html#an-example-with-vignette-and-data",
    "title": "Building Custom R Packages",
    "section": "An Example with Vignette and Data",
    "text": "An Example with Vignette and Data\nFor your group project, you may find that you do not need to write your own custom functions, but you will still need to create a vignette and a dataset to distribute, and I do want you to include at least two functions in your vignetter to make sure you get experience with including those in a package. Here, I demonstrate creating such a package based on Module 21 on multiple regression and ANCOVA.\n\nCreate the Package Directory\n\nusethis::create_package(\"~/Development/Repos/multireg\")\n\nThis should open a new RStudio session‚Ä¶ then, in that session‚Ä¶\n\n\nEdit the DESCRIPTION File\nJust open the file, edit, and save (make sure the last line remains a line break)!\nPackage: multireg\nTitle: A module for demonstrating multiple regression and ANCOVA\nVersion: 0.0.1.0\nAuthors@R: \n  person(given = \"Anthony\",\n         family = \"Di Fiore\",\n         role = c(\"aut\", \"cre\"),\n         email = \"anthony.difiore@austin.utexas.edu\",\n         comment = c(ORCID = \"0000-0001-8893-9052\"))\nDescription: This module is used to demonstrate building a package that contains a vignette about running multiple regression using an included dataset.\nLicense: GPL-3\nEncoding: UTF-8\nLazyData: true\n\n\nAdd Dependencies\n\nusethis::use_package(\"car\")\nusethis::use_package(\"jtools\")\nusethis::use_package(\"sjPlot\")\n\n\n\nCreate Documentation Files\n\nusethis::use_package_doc()\n\n\n\nAdd a Vignette Template\n\nusethis::use_vignette(\"vignette\", title = \"Multiple Regression and ANCOVA Vignette\")\n\nNow, edit the vignette ‚Äú.Rmd‚Äù file as you see fit!\nAs an example, you can copy and paste the text and code below to the end of the vignette ‚Äú.Rmd‚Äù file. Note that the R code should be included in a code block.\nThe following code uses the **zombies** dataset\nbundled in the {multireg} package to run a linear\nmodel of height as a function of weight, age, and\ngender among survivors of the zombie apocalypse.\n\nIs the overall model significant? Do both predictor\nvariables remain significant when the other is\ncontrolled for?\n\n```{r}\nlibrary(car)\nlibrary(ggplot2)\nlibrary(jtools)\nlibrary(sjPlot)\nhead(zombies)\nzombies$gender &lt;- factor(zombies$gender)\np &lt;- ggplot(data = zombies,\n            aes(x = age, y = height)) +\n     geom_point(aes(color = gender)) +\n     scale_color_manual(values = c(\"red\", \"blue\"))\np\nm &lt;- lm(formula = height ~ weight + age + gender,\n        data = zombies)\nsummary(m)\nvif(m)\nplot(fitted(m), residuals(m))\nhist(residuals(m))\nplot_summs(m,\n           plot.distributions = TRUE,\n           rescale.distributions = TRUE)\ntab_model(m)\n```\n\n\nAdd a Data Directory and Data\n\nlibrary(tidyverse)\n# first, get the data to use...\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/zombies.csv\"\nzombies &lt;- read_csv(f, col_names = TRUE)\n# then, write this out as an R data file\nusethis::use_data(zombies)\n\n\n\nBuild Vignettes\n\ndevtools::build_vignettes()\n\n\n\nProcess the Documentation Files\n\ndevtools::document()\n\n\n\nBuild the Package\n\ndevtools::build()\n\n\n\nInstall the Package\n\ndevtools::install(build_vignettes = TRUE)\n# or\ninstall.packages(\"~/Development/Repos/multireg_0.0.1.0.tar.gz\", repos = NULL, type = \"source\",\n    lib = \"/Library/Frameworks/R.framework/Versions/4.1/Resources/library\")\n\n\n\nCheck the Package for Errors\n\ndevtools::check()\n\n\n\nLoad and Use the Package\n\nlibrary(multireg)\nhead(zombies)\nutils::vignette(package = \"multireg\")\n\nThen, CHECK OUT YOUR VIGNETTE AND DATA!!!\n\nNOTE: You may need to restart R before you library() your package to get the package manager to refresh with your newly built package and any vignettes to appear properly.",
    "crumbs": [
      "Part III - Miscellany",
      "Building Custom ***R*** Packages"
    ]
  },
  {
    "objectID": "99-shiny.html",
    "href": "99-shiny.html",
    "title": "Building Interactive Web Apps",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "Part III - Miscellany",
      "Building Interactive Web Apps"
    ]
  },
  {
    "objectID": "99-shiny.html#objectives",
    "href": "99-shiny.html#objectives",
    "title": "Building Interactive Web Apps",
    "section": "",
    "text": "In this module, we explore using the {shiny} package to build interactive web applications for data analysis and visualization. The following is based heavily and shamelessly on material from this excellent tutorial and related articles on using {shiny} from RStudio, as well as on this blog post by developer Zev Ross.",
    "crumbs": [
      "Part III - Miscellany",
      "Building Interactive Web Apps"
    ]
  },
  {
    "objectID": "99-shiny.html#preliminaries",
    "href": "99-shiny.html#preliminaries",
    "title": "Building Interactive Web Apps",
    "section": "Preliminaries",
    "text": "Preliminaries\n\nInstall and load these packages in R: {shiny} and {DT}\nLoad {tidyverse}",
    "crumbs": [
      "Part III - Miscellany",
      "Building Interactive Web Apps"
    ]
  },
  {
    "objectID": "99-shiny.html#getting-started",
    "href": "99-shiny.html#getting-started",
    "title": "Building Interactive Web Apps",
    "section": "Getting Started",
    "text": "Getting Started\n{shiny} is an R package that makes it easy to build interactive web applications straight from R.\nA typical {shiny} app contained in a single R script called ‚Äúapp.R‚Äù. You can create a Shiny app by making a new directory and saving an ‚Äúapp.R‚Äù file inside it. It is recommended that each ‚Äúapp.R‚Äù script is stored in its own unique directory.\n\nNOTE: You do not need to call your app script ‚Äúapp.R‚Äù‚Ä¶ any ‚Äú.R‚Äù file with the appropriate {shiny} structure can be an app.\n\nYou can run a {shiny} app by passing the name of its directory to the function runApp(), i.e., by setting the path to the directory as an argument. If your app directory is inside your working directory, then the argument is just the name of the directory for your app. For example if your {shiny} app is in a directory called ‚Äúmy_app‚Äù, which is located in your working directory, you can run it with the following code: runApp(\"my_app\").\nIf your ‚Äúapp.R‚Äù script is open in RStudio, you can also run it using the ‚ÄúRunApp‚Äù button at the top of the editor window.\n\nNOTE: Your R session will be busy while any running {shiny} app is active, so you will not be able to run any other R commands. Basically, R is running behind the scenes, monitoring the app and executing the app‚Äôs reactions. To return to your R session, hit  or click the stop sign icon found in the upper right corner of the RStudio console panel.\n\nEvery ‚Äúapp.R‚Äù script has three components:\n\na user interface object (called ui)\na server function\na call to the shinyApp() function\n\nThe user interface (ui) object controls the layout and appearance of your app. The server function contains the instructions that your computer needs to build your app. Finally the shinyApp() function creates Shiny app objects from an explicit UI/server pair.\nYour ‚Äúapp.R‚Äù R script must also begin by loading the {shiny} package. Below is the skeleton of a typical {shiny} app.\n\nlibrary(shiny)\n\nui &lt;- ...\n\nserver &lt;- ...\n\nshinyApp(ui = ui, server = server)\n\nTo see an example app in action, run one of the following:\n\nrunExample(\"01_hello\")  # a histogram\nrunExample(\"02_text\")  # tables and data frames\nrunExample(\"03_reactivity\")  # a reactive expression\nrunExample(\"04_mpg\")  # global variables\nrunExample(\"05_sliders\")  # slider bars\nrunExample(\"06_tabsets\")  # tabbed panels\nrunExample(\"07_widgets\")  # help text and submit buttons\nrunExample(\"08_html\")  # Shiny app built from HTML\nrunExample(\"09_upload\")  # file upload wizard\nrunExample(\"10_download\")  # file download wizard\nrunExample(\"11_timer\")  # an automated timer",
    "crumbs": [
      "Part III - Miscellany",
      "Building Interactive Web Apps"
    ]
  },
  {
    "objectID": "99-shiny.html#building-a-user-interface",
    "href": "99-shiny.html#building-a-user-interface",
    "title": "Building Interactive Web Apps",
    "section": "Building a User Interface",
    "text": "Building a User Interface\nNow that you understand the structure of a {shiny} app, it‚Äôs time to build our own first app from scratch!\n\nCreate a Skeleton App\nYou can do this in a file called ‚Äúapp.R‚Äù by entering the following code:\n\nlibrary(shiny)\n\n# Define the UI ----\nui &lt;- fluidPage()\n\n# Define server logic ----\nserver &lt;- function(input, output) {\n}\n\n# Run the app ----\nshinyApp(ui = ui, server = server)\n\n{shiny} uses the function fluidPage() to create a display that automatically adjusts to the dimensions of your user‚Äôs browser window. You lay out the user interface of your app by placing elements in the fluidPage() function.\nFor example, the ui function below creates a user interface that has a title panel element (titlePanel()) and a sidebar layout (sidebarLayout()) format. The sidebar layout defines includes a sidebar panel (sidebarPanel()) and a main panel (mainPanel()). Note that all these elements are placed within the fluidPage() function.\n\nNOTE: {shiny} offers many other options for designing the user interface besides fluidPage() and various *Panels. You can explore these in the {shiny} documentation.\n\nModify your ‚Äúapp.R‚Äù file to the following and then run the app:\n\nui &lt;- fluidPage(\n  titlePanel(\"title panel\"),\n\n  sidebarLayout(\n    sidebarPanel(\"sidebar panel\"),\n    mainPanel(\"main panel\")\n  )\n)\n\ntitlePanel() and sidebarLayout() create a basic layout for your {shiny} app, but you can also create more advanced layouts. For example, you can use navbarPage() to give your app a multi-page user interface that includes a navigation bar. Or you can use fluidRow() and column() to build your layout up from a grid system. The Shiny Application Layout Guide provides further details about how you can modify the layout of your app.\nYou can add content to your {shiny} app by placing it inside one of the various *Panel() functions (e.g., sidebarPanel()). For example, the app above displays a character string in each of its panels. The words ‚Äúsidebar panel‚Äù appear in the sidebar panel because we added that string as an argument to the sidebarPanel() function, e.g.¬†sidebarPanel(\"sidebar panel\").\n\n\nAdd HTML Context\nTo add more advanced text content, use one of {shiny}‚Äôs HTML tag functions. These functions parallel common HTML5 tags.\n\n\n\n\n\n\n\n\n\n{shiny} HTML Tag Function\nHTML5 Equivalent\nCreates\n\n\n\n\np\n&lt;p&gt;\nA paragraph of text\n\n\nh1\n&lt;h1&gt;\nA first level header\n\n\nh2\n&lt;h2&gt;\nA second level header\n\n\nh3\n&lt;h3&gt;\nA third level header\n\n\nh4\n&lt;h4&gt;\nA fourth level header\n\n\nh5\n&lt;h5&gt;\nA fifth level header\n\n\nh6\n&lt;h6&gt;\nA sixth level header\n\n\na\n&lt;a&gt;\nA hyper link\n\n\nbr\n&lt;br&gt;\nA line break (e.g.¬†a blank line)\n\n\ndiv\n&lt;div&gt;\nA division of text with a uniform style\n\n\nspan\n&lt;span&gt;\nAn in-line division of text with a uniform style\n\n\npre\n&lt;pre&gt;\nText ‚Äòas is‚Äô in a fixed width font\n\n\ncode\n&lt;code&gt;\nA formatted block of code\n\n\nimg\n&lt;img&gt;\nAn image\n\n\nstrong\n&lt;strong&gt;\nBold text\n\n\nem\n&lt;em&gt;\nItalicized text\n\n\nHTML\n\nDirectly passes a character string as HTML code\n\n\n\n\nTo place a text element of one of these types in your app, pass the appropriate {shiny} function and argument as an argument to one of the *Panel() functions in the ui section of your app. The text will appear in the corresponding panel of your web page. You can place multiple elements in the same panel if you separate them with a comma.\n\n\nCHALLENGE\n\nReplace the ‚Äútitle panel‚Äù text in the titlePanel() of your app with an h1 element that says ‚ÄúMy First Web App‚Äù.\nReplace the ‚Äúmain panel‚Äù text in the mainPanel() of your app with an h3 element that says ‚ÄúWow, I‚Äôm creating a webpage and web server!‚Äù.\nAdd an h4 element to the mainPanel() that says ‚ÄúThis is really cool.‚Äù.\n\n\n\nShow Code\nlibrary(shiny)\n\n# Define the UI ----\nui &lt;- fluidPage(titlePanel(h1(\"My First Web App\")), sidebarLayout(sidebarPanel(\"sidebar panel\"),\n    mainPanel(h3(\"Wow, I'm creating a webpage and web server!\"), h4(\"This is really cool.\"))))\n\n# Define server logic ----\nserver &lt;- function(input, output) {\n\n}\n\n# Run the app ----\nshinyApp(ui = ui, server = server)\n\n\n\n\nStyle HTML Content\nYou can use html tag attribute ‚Äústyle‚Äù to style the HTML text element.\nFor example, replace this line in your ui code‚Ä¶\nh4(\"This is really cool.\")\nwith‚Ä¶\nh4(\"This is really cool.\", style = \"color:blue; text-align:center\")\n‚Ä¶ and then rerun your app.\nTo insert an image into your webpage, you can use the img() function and give the name of your image file as the ‚Äúsrc‚Äù argument (e.g., img(src = \"my_image.png\")). You must spell out this argument since img() passes your input to an HTML tag, and ‚Äúsrc‚Äùis what the tag expects.\nYou can also include other HTML-friendly parameters for your image such as height= and width= - e.g., img(src = \"my_image.png\", height = 72, width = 72), where height and width numbers will refer to pixels.\nThe img() function from {shiny} looks for your image file in a specific place. Your file must be in a folder named www located in the same directory as the ‚Äúapp.R‚Äù script. {shiny} treats this directory in a special way‚Ä¶ it will share any file placed here with your user‚Äôs web browser, which makes the www a great place to store images, css (‚Äúcascading style sheet‚Äù) files, and other things the web browser will need to build web components of your app.\n\n\nCHALLENGE\n\nInsert an image into the sidebarPanel() of your app, replacing the existing text. Also pass a ‚Äústyle‚Äù to the HTML element to center it.\n\nReplace the sidebar panel line with‚Ä¶\nsidebarPanel(img(src = \"pvz-zombie1.png\", width = 100), style = \"text-align:center\")\n‚Ä¶ and then rerun your app.\n\n\nAdding Control Widgets\nSo far, all our app does is create a static web page, but we can add control widgets and elements whose display updates when we change a value in a control to make our webpage interactive. A widget is a web element that users can interact with. They collect a value from the user and, when a user changes the widget, the value will change as well. Widgets thus provide a way for your users to send messages to the {shiny} app.\n{shiny} comes with a family of pre-built widgets, each created with a correspondingly named R function (and additional packages exist that you can use to extend the family of widgets). For example, {shiny} provides a function named actionButton() that creates an button and a function named sliderInput() that creates a slider bar.\n\n\n\n\n\n\n\n\n\nYou can add widgets to your webpage in the same way that you added other types of HTML content. To add a widget, place its corresponding function in one of the *Panel elements in your ui object.\nEach widget function requires several arguments. The first two arguments for each widget are:\n\nA name for the widget:The user will not see this name, but you can use it to access the widget‚Äôs value. The name should be a character string.\nA label: This label will appear with the widget in your app. It should be a character string, but it can be an empty string ‚Äú‚Äú, e.g., label = \"\".\n\nIn this example, the name is ‚Äúsubmit‚Äù and the label is ‚ÄúSUBMIT‚Äù:\nactionButton(\"submit\", label = \"SUBMIT\")\nThe remaining arguments vary from widget to widget, depending on what the widget needs to do its job. They include things like initial values, ranges, and increments. You can find the exact arguments needed by a widget on the widget function‚Äôs help page, (e.g., ?selectInput()).\n\n\nCHALLENGE\nAdd a button labeled ‚ÄúSUBMIT‚Äù to the sidebarPanel of your app below the image you already added and then rerun your app.\n\nNOTE: To get your button to show up BELOW the image, you will need to also add one or more br() (line break) elements to the sidebarPanel() element.\n\nReplace the sidebar panel line with‚Ä¶\nsidebarPanel(img(src = \"pvz-zombie1.png\", width = 100), style = \"text-align:center\"),\n    br(),\n    br(),\n    actionButton(\"submit\", \"SUBMIT\"),\n    style = \"text-align:center\")\n‚Ä¶ and then rerun your app.\nThe Shiny Widgets Gallery provides templates that you can use to quickly add widgets to your {shiny} apps.\nTo use a template, visit the gallery. The gallery displays each of {shiny}‚Äòs widgets, and demonstrates how the widgets‚Äô values change in response to your input.\nSelect the widget that you want and then click the ‚ÄúSee Code‚Äù button below the widget. The gallery will take you to an example app that describes the widget. To use the widget, copy and paste the code in the example‚Äôs ‚Äúapp.R‚Äù file to your ‚Äúapp.R‚Äù file in the desired place in your ui object.",
    "crumbs": [
      "Part III - Miscellany",
      "Building Interactive Web Apps"
    ]
  },
  {
    "objectID": "99-shiny.html#displaying-reactive-values",
    "href": "99-shiny.html#displaying-reactive-values",
    "title": "Building Interactive Web Apps",
    "section": "Displaying Reactive Values",
    "text": "Displaying Reactive Values\nIf we now want our web app to conduct some analysis or display some data visualization that responds to user input, we need to set up the ui and the associated server function to create and reactive output. This involves two basic steps:\n\nAdding an R object to our user interface.\nTelling {shiny} how to build that object in the server() function. The object will be reactive if the code that builds it calls a widget value.\n\n\nStep 1: Adding an R Object to the UI\n{shiny} provides a family of functions that turn R objects into output for your user interface. Each function creates a specific type of output.\n\n\n\n\n{shiny} Output Function\nCreates\n\n\n\n\ndataTableOutput\nDataTable\n\n\nhtmlOutput\nraw HTML\n\n\nimageOutput\nimage\n\n\nplotOutput\nplot\n\n\ntableOutput\ntable\n\n\ntextOutput\ntext\n\n\nuiOutput\nraw HTML\n\n\nverbatimTextOutput\ntext\n\n\n\n\nYou can add output to the user interface in the same way that we added HTML elements and widgets, by placing the desired *Output() function inside one of the *Panel elements in the ui.\n\n\nCHALLENGE\nLet‚Äôs add a selectInput() popdown menu element to the sidebarPanel() of your app. This function takes up to four arguments, the element name, the element label, a vector of choices to select from, and the name of the default choice.\nReplace the actionButton() you added to the sidebarPanel with the following and rerun your app.\nselectInput(\n  \"favorite_monster\",\n  label = \"Choose one of the following...\",\n  choices = c(\"Zombie\", \"Vampire\", \"Alien\", \"Werewolf\"),\n  selected = \"Zombie\"\n  )\nNow add a reactive textOutput() function to the mainPanel element of your app‚Ä¶ it should now look like this‚Ä¶\nmainPanel(\n  h3(\"Wow, I'm creating a webpage and web server!\"),\n  h4(\"This is really cool.\"),\n  textOutput(\"favorite_monster\")\n  )\nNotice that textOutput() takes an argument, in this case the character string ‚Äúfavorite_monster‚Äù. Each of the *Output() functions require a single argument: a character string that {shiny} uses as the name of your reactive element. Your users will not see this name, but you will use it later.\n\nStep 2: Provide R Code to Build the Reactive Object\nPlacing a function in the ui tells {shiny} where to display your object. Next, you need to tell {shiny} how to build the object.\nWe do this by providing the R code that builds the object in the server() function.\nThe server() function builds a list-like object named ‚Äúoutput‚Äù that contains all of the code needed to update the R objects in your app. Each R object needs to have its own entry in the list.\nYou can create an entry by defining a new element for output within the server() function, like below. The element name should match the name of the reactive element that you created in the ui.\nIn the server function below, output$favorite_monster matches textOutput(\"favorite_monster\") in your ui.\nModify the server() function in your app to look like the code below and then run your app.\nserver &lt;- function(input, output) {\n  output$favorite_monster &lt;- renderText({\"argument\"})\n}\nEach render*() function takes a single argument: an R expression surrounded by braces, {}. The expression can be one simple line of text, or it can involve many lines of code, as if it were a complicated function call.\nThink of this R expression as a set of instructions that you give {shiny} to store for later. {shiny} will these instructions when you first launch your app and then will re-run the instructions every time it needs to update your object.\nFor this to work, your expression should return the object you have in mind (a piece of text, a plot, a data frame, etc.). You will get an error if the expression does not return an object, or if it returns the wrong type of object.\nThus far, the text returned is not reactive. It will not change even if you manipulate the selectInput() widget of the app. But we can make the text reactive by asking {shiny} to call a widget value when it builds the text.\nNotice that the server() function has two arguments, input= and output=. Like output, input is also a list-like object. It stores the current values of all of the widgets in your app. These values will be saved under the names that you gave the widgets in your ui. We have a widget named ‚Äúfavorite_monster‚Äù in our ui, and this value is stored as input$favorite_monster and gets updated every time we change the value of the widget.\n{shiny} will automatically make an object reactive if the object incorporates an input value. For example, we can make our server() function create a reactive line of text by calling the value of the selectInput() widget to build the text.\nAgain, modify the server() function in your app to look like the code below and then run your app.\nserver &lt;- function(input, output) {\n  output$favorite_monster &lt;- renderText({paste0(\"You have selected... \", input$favorite_monster)})\n}\n{shiny} tracks which outputs depend on which widgets. When a user changes a widget, {shiny} rebuilds all of the outputs that depend on that widget, using the new value of the widget as it goes.\nThis is how you create reactivity with {shiny}, by connecting the values of ‚Äúinput‚Äù to the objects in ‚Äúoutput‚Äù. {shiny} takes care of all of the other details.",
    "crumbs": [
      "Part III - Miscellany",
      "Building Interactive Web Apps"
    ]
  },
  {
    "objectID": "99-shiny.html#loading-in-a-data-from-a-file",
    "href": "99-shiny.html#loading-in-a-data-from-a-file",
    "title": "Building Interactive Web Apps",
    "section": "Loading in a Data from a File",
    "text": "Loading in a Data from a File\nTo load in data from file to use in a {shiny} visualization, we have to have our app execute some kind of read*() function specifying the path to the file. We also have to have {shiny} load any libraries we might need to visualize our data. Below, we are going to use the {DT} library to make a nicely formatted table of data and {ggplot2} to build graphs. {shiny} will execute the commands we give it apart from the ui and server sections of our script, but where we place those commands will determine when and how many times they are run (or rerun), which can affect the performance of our app.\n\n{shiny} will run the whole script the first time we call runApp(). This causes {shiny} to execute the server function. Code put before the ui step will thus run one time.\n\n\n\n\n\n\n\n\n\n\nAlternatively we can put code inside the server() function. For each time that a new user visits the app, {shiny} will run the server function again. The server() function helps {shiny} build a distinct set of reactive objects for each user, but it is not run repeatedly.\n\n\n\n\n\n\n\n\n\nFinally, we can put code inside the render*() function. With this setup, {shiny} will run the function every time a user changes the value of a widget.\n\n\n\n\n\n\n\n\n\nThese patterns of behavior suggest that you should do the following:\n\nPut code for sourcing scripts, loading libraries, and reading data sets at the beginning of app.R outside of the server() function. {shiny} will only run this code once, which is all you need to set your server up to run the R expressions contained in server().\nDefine user-specific objects inside the server() function, but outside of any render*() calls. These would be objects that you think each user will need their own personal copy of, e.g., an object that records the user‚Äôs session information. This code will be run once per user.\nOnly place code that {shiny} must rerun to build an object inside of a render*() function. {shiny} will rerun all of the code in a render*() chunk every time a user changes a widget mentioned in the chunk. This can be quite often.\n\nYou should generally avoid placing code inside a render*() function that does not need to be there. Doing so will slow down the entire app.",
    "crumbs": [
      "Part III - Miscellany",
      "Building Interactive Web Apps"
    ]
  },
  {
    "objectID": "99-shiny.html#example-linear-model-visualizer-app",
    "href": "99-shiny.html#example-linear-model-visualizer-app",
    "title": "Building Interactive Web Apps",
    "section": "Example Linear Model Visualizer App",
    "text": "Example Linear Model Visualizer App\nWe are now going to use some of the above techniques to build a functional web app that loads in data from the ‚Äúzombies.csv‚Äù file we have been using, displays the data in a table, and then lets us interactively explore simple and multiple regression linear model to generate a table of beta coefficients and plot bivariate scatterplots and boxplots. A full version of this app can be viewed or downloaded from here.\n\nCHALLENGE\n\nStep 1\n\nCreate a new {shiny} app called ‚Äúlm.R‚Äù with the three standard sections of a {shiny} app:\n\nui &lt;- fluidPage()\nserver &lt;- function(input,output){}\nshinyApp(ui = ui, server = server)\n\n\nStep 2\n\nAdd to the ui a titlePanel() with an h1 element you call ‚ÄúSimple LM Visualizer‚Äù and a sidebarLayout() with sidebarPanel() and mainPanel() elements. Also add a dataTableOutput() and plotOutput() element to your mainPanel(). Set the names for these to ‚Äúdatatable‚Äù and ‚Äúplot‚Äù, respectively. We also set the width of the sidebarPanel() and mainPanel() to 5 and 7 units, respectively. This is because the layout system that {shiny} uses is based on dividing the browser window into a grid with 12 columns.\n\nui &lt;- fluidPage(\n  titlePanel(h1(\"Simple LM Visualizer\")),\n  sidebarLayout(\n    sidebarPanel(width = 5,\n    ),\n    mainPanel(width = 7,\n      dataTableOutput(\"datatable\"),\n      plotOutput(\"plot\")\n    )\n  )\n)\n\n\nStep 3\n\nAdd code to the start of your app to load in the {DT} and {tidyverse} (and, by extension, the {dplyr} and {ggplot2}) libraries and to read our zombie apocalypse survivors dataset into the app as a variable named d when the app starts up.\n\nlibrary(shiny)\nlibrary(DT)\nlibrary(tidyverse)\nlibrary(broom)\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/zombies.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\n\n\nStep 4\n\nUse the {dplyr} verb select() to winnow the dataset to the following five variables from the dataset: height, weight, age, gender, and major. Convert the variables gender and major to be factors. Also, create two variables r and p that are vectors of the possible quantitative ‚Äúresponse‚Äù variables (height, weight, and age) and possible ‚Äúpredictor‚Äù variables (all of the possible variables, both quantitative and categorical).\n\nd &lt;- select(d, height, weight, age, gender, major)\nd$gender &lt;- factor(d$gender)\nd$major &lt;- factor(d$major)\nr &lt;- c(\"height\", \"weight\", \"age\")\np &lt;- names(d)\n\n\nStep 5\nModify the default server() function as follows‚Ä¶\nserver &lt;- function(input, output) {\n  output$datatable &lt;-\n    renderDataTable(d, options = list(\n      paging = TRUE,\n      lengthMenu = list(c(5, 10, 25, -1), c('5', '10', '25', 'All')),\n      pageLength = 5\n    ))\nNow run your app‚Ä¶ what does it do?\n\n\nStep 6\n\nNext, modify your sidebar to include two selectInput() popdown menus. The first popdown allows you to select a variable to be a response variable:\n\nselectInput(\n  \"response\",\n  label = \"Choose a response variable...\",\n  choices = c(\"\", r)\n)\n\nNOTE: The ‚Äú‚Äù is needed in choices = c() to allow NO VARIABLE to be the default value\n\nThe second popdown allows you to select one or more variables as predictor variables:\nselectInput(\n  \"predictors\",\n  label = \"Choose one or more predictor variables...\",\n  choices = p,\n  multiple = TRUE\n)\n\nAlso add two other output variables to the sidebar. The first will be used to display the linear model we construct‚Ä¶\n\ntextOutput(\"model\")\n‚Ä¶ and the second will be used to display the results of the model.\ntableOutput(\"modelresults\")\n\n\nStep 7\n\nAdd the following code chunks to your server() function:\n\nThe first code chunk creates a reactive variable, m(), the value of which will be updated every time that the input$response or input$predictors values change as a user interacts with our app. The value returned by the m() reactive function is either NULL (if no response or predictor variable are chosen by the user) or a text version of a linear model formula (e.g., ‚Äúheight ~ weight + age‚Äù).\n\nm &lt;- reactive({\n    mod &lt;- NULL\n    if (input$response == \"\" | length(input$predictors) == 0) {\n        return(mod)\n    }\n    mod &lt;- paste0(input$response, \" ~ \", input$predictors[1])\n    if (length(input$predictors) &gt; 1) {\n        for (i in 2:length(input$predictors)) {\n            mod &lt;- paste0(mod, \" + \", input$predictors[i])\n        }\n    }\n    return(mod)\n})\n\nA second code chunk prints out the lm() being fitted.\n\noutput$model &lt;- renderText({\n    paste0(\"Model: \", print(m()))\n})\n\nCode chunk 3 outputs a table of coefficients resulting from the linear model formula stored in m(), and it updates every time the user changes input$response or input$predictors because doing that updates the value of m() the output function first confirms that a valid linear model has been constructed.\n\noutput$modelresults &lt;- renderTable({\n    if (!is.null(m())) {\n        res &lt;- lm(data = d, formula = m())\n        res &lt;- as.data.frame(coefficients(res))\n        names(res) &lt;- \"Beta\"\n        res\n    }\n}, width = \"100%\", rownames = TRUE, striped = TRUE, spacing = \"s\", bordered = TRUE,\n    align = \"c\", digits = 3)\n\nFinally, the last code chunk uses {ggplot2} to graph the relationship between the variables we have selected as input$response and input$predictors.\nNote that this output function first confirms that a valid linear model has been constructed and whether there is one or more than one predictor variable. What output gets plotted depends on the number and type(s) of the predictor variable(s).\nWith one predictor, the output may be either a scatterplot with a fitted regression line (if input$predictors is a continuous variable) or a violin + scatter plot (if input$predictors is a factor variable).\nWith two predictors, at least one of which is a factor, the output may be scatterplots or violin + scatter plots, faceted by (one of) the factors.\nWith two continuous predictor variables, or with more than two predictor variables, no plot is created.\n\noutput$plot &lt;- renderPlot({\n    if (!is.null(m()) & length(input$predictors) == 1) {\n        y &lt;- input$response\n        x &lt;- input$predictors\n        if (class(d[[x]]) != \"factor\") {\n            p &lt;- ggplot(data = d, aes(x = .data[[x]], y = .data[[y]])) + geom_point() +\n                geom_smooth(method = lm)\n        } else {\n            p &lt;- ggplot(data = d, aes(x = .data[[x]], y = .data[[y]])) + geom_violin() +\n                geom_jitter(width = 0.2, alpha = 0.5)\n        }\n        p &lt;- p + xlab(x) + ylab(y) + theme(axis.text.x = element_text(angle = 90,\n            hjust = 1))\n        p\n    } else if (!is.null(m()) & length(input$predictors) == 2) {\n        y &lt;- input$response\n        x &lt;- input$predictors\n        if (class(d[[x[1]]]) == \"factor\" & class(d[[x[2]]]) == \"factor\") {\n            p &lt;- ggplot(data = d, aes(x = .data[[x[1]]], y = .data[[y]])) + geom_violin() +\n                geom_jitter(width = 0.2, alpha = 0.5) + facet_wrap(~d[[x[2]]])\n            p &lt;- p + xlab(x[1]) + ylab(y)\n        } else if (class(d[[x[1]]]) != \"factor\" & class(d[[x[2]]]) == \"factor\") {\n            p &lt;- ggplot(data = d, aes(x = .data[[x[1]]], y = .data[[y]])) + geom_point() +\n                geom_smooth(method = lm) + facet_wrap(~d[[x[2]]])\n            p &lt;- p + xlab(x[1]) + ylab(y)\n        } else if (class(d[[x[1]]]) == \"factor\" & class(d[[x[2]]]) != \"factor\") {\n            p &lt;- ggplot(data = d, aes(x = .data[[x[2]]], y = .data[[y]])) + geom_point() +\n                geom_smooth(method = lm) + facet_wrap(~d[[x[1]]])\n            p &lt;- p + xlab(x[2]) + ylab(y)\n        } else {\n            p &lt;- NULL\n        }\n        p &lt;- p + theme(axis.text.x = element_text(angle = 90, hjust = 1))\n        p\n    }\n})\n\n\n\nStep 8\n\nRun and play with your app!",
    "crumbs": [
      "Part III - Miscellany",
      "Building Interactive Web Apps"
    ]
  },
  {
    "objectID": "99-using-packages.html",
    "href": "99-using-packages.html",
    "title": "Working with and Managing Packages",
    "section": "",
    "text": "Preliminaries",
    "crumbs": [
      "Part III - Miscellany",
      "Working with and Managing Packages"
    ]
  },
  {
    "objectID": "99-using-packages.html#preliminaries",
    "href": "99-using-packages.html#preliminaries",
    "title": "Working with and Managing Packages",
    "section": "",
    "text": "Restarting an R Session with a Clean Environment\n\nrm(list = ls())  # removes all objects from the Global Environment\n\nrstudioapi::executeCommand(\"restartR\")  # starts a fresh R session with no packages loaded, but reloads the current the Global Environment objects\n\nFor the purposes of illustrating how we can work with sets of packages, we will create a character vector of package names‚Ä¶\n\n# These are all core {tidyverse} packages\npackages &lt;- c(\"dplyr\", \"forcats\", \"ggplot2\", \"lubridate\", \"purrr\", \"readr\", \"stringr\",\n    \"tibble\", \"tidyr\")",
    "crumbs": [
      "Part III - Miscellany",
      "Working with and Managing Packages"
    ]
  },
  {
    "objectID": "99-using-packages.html#package-management-with-base-r",
    "href": "99-using-packages.html#package-management-with-base-r",
    "title": "Working with and Managing Packages",
    "section": "Package Management with Base R",
    "text": "Package Management with Base R\n\nInstalling, Removing, and Updating Packages\n\n# To install a single package...\ninstall.packages(\"dplyr\")\n\nPackages are, by default, installed into the first element that appears in the .libPaths() function from the {base} package, which is typically a user‚Äôs personal User Library. Alternatively, the library into which new packages are installed can be specified using the lib argument to install.packages().\n\n# To remove a single package...\nremove.packages(\"dplyr\")\n\n# To install one or more packages...\ninstall.packages(packages)\n\n# To remove one or more packages...\nremove.packages(packages)\n\n# To generate a data frame of all currently installed packages and their\n# versions...\ninstalled_packages &lt;- data.frame(package = installed.packages()[, 1], version = installed.packages()[,\n    3])\n\n# To check whether any of a vector of package names is not yet installed we\n# create a boolean vector equal in length to our vector of package names...\ninstalled &lt;- packages %in% rownames(installed.packages())\n\n# ... and then install any that are not installed\nif (any(installed == FALSE)) {\n    install.packages(packages[!installed])\n}\n\n# To update any installed packages...\nupdate.packages()\n\n\nNote: The functions install.packages() and remove.packages() both take a single package name in quotation marks or a character vector of package names as an argument. The function update.packages() does not take an argument as it checks for updates for all currently installed packages in all library locations.\n\n\n\nLoading Packages\nInstalling packages simply places them into a standard location (the System Library or default User Library, or some other directory that you specify) on your computer. To actually use the functions they contain in an R session, you need to also load them into the R workspace.\nTo load a package, we run the function library() or require() with the package name (either in quotation marks or as a ‚Äúbare‚Äù name) as an argument. These functions can only take a single package name as an argument, so loading multiple packages in base R requires multiple library() or require() calls. See below, however, for more efficient ways to install and/or load multiple packages at once using the {easypackages}, {pacman}, {librarian}, or {Require} packages.\nThe require() function is nearly identical to the library() function except that the latter throws an error if a package is not installed, while the former returns a value of TRUE or FALSE depending on whether the package was found by R and could be loaded successfully. Thus, require() is safer to use inside of functions because it will not throw an error if a package is not installed.\nTo remove a package from the workspace and make its functionality unavailable, we use the function detach() with the argument name set to package: followed by the name of the package to remove and with the unload argument set to ‚ÄúTRUE‚Äù. If the latter is set to ‚ÄúFALSE‚Äù or if left unspecified, then the package‚Äôs namespace will not be unloaded, and the functions in the package may or may not remain accessible. In general, we want to always unload a package‚Äôs namespace when we detach the package!\n\n# To load a single package...\nlibrary(dplyr)\n\n# To detach and unload a single package...\ndetach(package:dplyr, unload = TRUE)\n\nThe functions library(), require(), and detach() all take a single package name (or, in the case of detach(), a single value for package:packagename as an argument) and allow the package name to be passed either in quotation marks or as a ‚Äúbare‚Äù name.\n\nNote: Removing (i.e., uninstalling) a package after it has been loaded into R does not formally detach or unload its namespace or its functionality, but that functionality may be compromised if the package no longer exists! It is good practice to not remove a package without detaching and unloading its namespace first.\n\n\n\nShortcut to Installing and Loading a Package\nThe following code snippet uses the require() function to check if a package is already installed on your computer and, if not, to install and load it. The snippet uses the negation operator (!) to install the package when the require() function returns a value of FALSE, which indicates that the package was not found by R and is not already installed. As noted above, if the package is already installed and loads successfully, then the require() function returns a value of TRUE as the action of this function is to search for and load the specified package.\n\n# To install a package if it is not already installed and load it...\nif (!require(\"dplyr\")) {\n    install.packages(\"dplyr\", dependencies = TRUE)\n    library(dplyr)\n}\n\n\nNote: If the package is not already installed, this snippet will throw a warning that ‚Äúthere is no package called ‚Äù. That is to be expected and is not a cause for concern. If the package is already installed, then no warning will be generated.",
    "crumbs": [
      "Part III - Miscellany",
      "Working with and Managing Packages"
    ]
  },
  {
    "objectID": "99-using-packages.html#other-packages-for-package-management",
    "href": "99-using-packages.html#other-packages-for-package-management",
    "title": "Working with and Managing Packages",
    "section": "Other Packages for Package Management",
    "text": "Other Packages for Package Management\n\nThe {easypackages} package\nThe {easypackages} packages facilitates installing and loading multiple packages by introducing two helper functions that allow you to either install (via the packages() function) or load (via the libraries() function) multiple packages at once. Both functions take as arguments either a comma-separated set of package names, in quotation marks, or a character vector of package names.\n\nrstudioapi::executeCommand(\"restartR\")\n\nif (!require(\"easypackages\")) {\n    install.packages(\"easypackages\", dependencies = TRUE)\n    library(easypackages)\n}\n\n# To install multiple packages...\neasypackages::install_packages(packages)\n\n# To load multiple packages...\neasypackages::libraries(packages)\n\n\nNote: When called, the easypackages::packages() function, like the install.packages() function, reinstalls packages even if they already have been installed. This is often unnecessary and a waste of time and computing energy.\n\n\n\nThe {pacman} package\nThe {pacman} package contains tools to perform common tasks associated with add-on packages. It wraps library loading and detaching and other package-related functions together and names them in a consistent fashion. You can pass either a character vector of package names (as you can with {easypackage}) or a comma-separated bare package names to the {pacman} functions, but if passing a character vector of names, you need to include the argument character.only = TRUE.\n\nrstudioapi::executeCommand(\"restartR\")\n\nif (!require(\"pacman\")) {\n    install.packages(\"pacman\", dependencies = TRUE)\n    library(pacman)\n}\n\n# To list the functions in a package...\npacman::p_funs(\"pacman\")\n\n# To list all loaded packages...\npacman::p_loaded()  # excluding base R packages\npacman::p_loaded(all = TRUE)  # including base R packages\n\n# To check for whether specific packages are loaded...\npacman::p_loaded(packages, character.only = TRUE)\n\n# To install and load a character vector of package names...  If a particular\n# package is not installed, p_load() will first install it and then load it If\n# a package is already installed, the installation step is skipped\npacman::p_load(packages, character.only = TRUE)\n\n# To unload a character vector of package names...\npacman::p_unload(packages, character.only = TRUE)\n\n# To detach and unload all currently loaded packages, except for those in base\n# R...\npacman::p_unload(pacman::p_loaded(), character.only = TRUE)\n\n\n\nThe {librarian} package\nThe {librarian} package is very similar to {pacman} and and also contains tools to perform common tasks associated with add-on packages. Like {pacman}, it wraps library loading and detaching and other package-related functions together and names them in a consistent fashion. You can pass either a character vector of package names or comma-separated bare package names to the {librarian} functions.\nSee https://github.com/DesiQuintans/librarian for more details.\n\nrstudioapi::executeCommand(\"restartR\")\n\nif (!require(\"librarian\")) {\n    install.packages(\"librarian\", dependencies = TRUE)\n    library(librarian)\n}\n\n# To list loaded packages, including base R packages\nlibrarian::check_attached()\n\n# To check for whether specific packages are loaded...\nlibrarian::check_attached(packages)\n\n# To install and load a character vector of package names...  If a package is\n# not installed, shelf() will first install it and then load it If a package is\n# already installed, the installation step is skipped\nlibrarian::shelf(packages)\n\n# To install, but not load, a character vector of package names...  Again, if a\n# package is already installed, the installation step is skipped\nlibrarian::stock(packages)\n\n# To unload a character vector of package names...\nlibrarian::unshelf(packages)\n\n# To unload all currently loaded packages, except base R packages...\nlibrarian::unshelf(everything = TRUE)\n\n\n\nThe {Require} package\nThe {Require} package also provides some similar functionality. With the Require() function, packages passed to the function in a character vector will be installed if not already installed and will also be loaded, unless the require argument is set to FALSE, in which case they will only be installed.\n\nrstudioapi::executeCommand(\"restartR\")\n\nif (!require(\"Require\")) {\n    install.packages(\"Require\", dependencies = TRUE)\n    library(Require)\n}\n\n# To install multiple packages...  If a package is not installed, Require()\n# will first install it and then load it If a package is already installed, the\n# installation step is skipped\nRequire::Require(packages, require = FALSE)\n\n# To install and load multiple packages...\nRequire::Require(packages)\n\n\nNote: The {Require} package has a detachAll() function, but for a variety of reasons it does a poorer job of detaching packages successfully than either pacman::p_unload() or librarian::unshelf().",
    "crumbs": [
      "Part III - Miscellany",
      "Working with and Managing Packages"
    ]
  },
  {
    "objectID": "99-using-packages.html#conflicts",
    "href": "99-using-packages.html#conflicts",
    "title": "Working with and Managing Packages",
    "section": "Conflicts",
    "text": "Conflicts\nWhen packages are loaded into R, ‚Äúconflicts‚Äù can occur because they contain functions with the same name loaded in different namespaces. For example, both the base {stats} package and the {dplyr} package contain a function called filter(). When you load {dplyr} after starting an R session, this creates a conflict, but depending on how the package is loaded, you may or may not notice that a conflict has occurred.\nFor example, if we load the {tidyverse} package with pacman::p_load(tidyverse) or librarian::shelf(tidyverse), we are not warned of a conflict between the filter() function in {dplyr} and base {stats}‚Ä¶\n\nrstudioapi::executeCommand(\"restartR\")\n\nif (!require(\"tidyverse\")) {\n    install.packages(\"tidyverse\", dependencies = TRUE)\n    library(tidyverse)\n}\n\npacman::p_unload(tidyverse)  # first make sure the package is unloaded...\npacman::p_load(tidyverse)  # ... and then load it\n\nlibrarian::unshelf(tidyverse)  # first make sure the package is unloaded...\nlibrarian::shelf(tidyverse)  # ... and then load it\n\n‚Ä¶ but if we unload and then load the same package with library(tidyverse), we are given a warning message about the conflicting function names:\n\npacman::p_unload(tidyverse)\n# or\nlibrarian::unshelf(tidyverse)\nlibrary(tidyverse)\n\n\nNote: R automatically gives precedence to the version of a function that comes from the more-recently loaded package and namespace, and this is often a source of confusion and error.\n\nThe {conflicted} package can be used to look at and manage conflicts more explicitly. To use it, load the package at the start of your R session, before loading other packages. Then, after loading your other packages, when you try to run a function with a conflicted name, R will report the error. For example‚Ä¶\n\nif (!require(\"conflicted\")) {\n    install.packages(\"conflicted\", dependencies = TRUE)\n    library(conflicted)\n}\n\nlibrary(conflicted)\nlibrary(dplyr)\nfilter(mtcars, cyl == 8)\n\nWith {conflicted} loaded, You can assign which version of a function you want to give precedence with the conflicts_prefer() function. Below, we tell R to give explict precedence to the filter() function from {dplyr} for the current R session:\n\nconflicts_prefer(dplyr::filter)\nfilter(mtcars, cyl == 8)\n\nAlternatively, we can explicitly redefine the function for use during the R session:\n\nfilter &lt;- dplyr::filter\nfilter(mtcars, cyl == 8)\n\nSee https://conflicted.r-lib.org/index.html for more details.",
    "crumbs": [
      "Part III - Miscellany",
      "Working with and Managing Packages"
    ]
  },
  {
    "objectID": "exercise-01.html",
    "href": "exercise-01.html",
    "title": "Exercise 01",
    "section": "",
    "text": "Send Emails Programmatically",
    "crumbs": [
      "Exercises",
      "Exercise 01"
    ]
  },
  {
    "objectID": "exercise-01.html#learning-objectives",
    "href": "exercise-01.html#learning-objectives",
    "title": "Exercise 01",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nInstalling and loading/attaching packages\nIntroducing functions and arguments\nIntroducing the pipe (%&gt;% or |&gt;) operator",
    "crumbs": [
      "Exercises",
      "Exercise 01"
    ]
  },
  {
    "objectID": "exercise-01.html#preliminary-steps",
    "href": "exercise-01.html#preliminary-steps",
    "title": "Exercise 01",
    "section": "Preliminary Steps",
    "text": "Preliminary Steps\n\nNOTE: The most challenging thing about the exercise below is to get the authentication correct for accessing an SMTP server to send a message. It is often easy to get the R code to work correctly to contact a servers, but then to have the server not be able to send out an email.\n\nThe example below should allow you to send emails via R using credentials associated with a personal or university Google Gmail account.\nIf you have a Google Gmail account (either a personal one, e.g., &lt;username&gt;@gmail.com, or one associated with UT, e.g., a &lt;username&gt;@utexas.edu account), you should follow the steps below to create an ‚Äúapp password‚Äù that you can use for authentication.\n\nNOTE: For this process to work smoothly, you should also have ‚Äú2-Step Verification‚Äù established for the account.\n\n\nLog into your Google account or your UT Gmail account on a web browser, click on your user icon and choose ‚ÄúManage your Google Account‚Äù. For example‚Ä¶\n\n\n\n\n\n\n\n\n\n\n‚Ä¶ OR ‚Ä¶\n\n\n\n\n\n\n\n\n\n\nSelect ‚ÄúSecurity‚Äù‚Ä¶\n\n\n\n\n\n\n\n\n\n\n‚Ä¶ and in the window that opens scroll down to the section on ‚Äú2-Step Verification‚Äù and click the arrow at the right.\n\n\n\n\n\n\n\n\n\nAt this point, you may need to provide information to verify your identity, e.g., by entering a password and/or entering a code you receive from Google. Once you have verified your identity, in the ‚ÄúSecurity‚Äù section, scroll down to the bottom of the page to the section on ‚ÄúApp passwords‚Äù and click the ‚ÄúApp passwords‚Äù button.\n\n\n\n\n\n\n\n\n\n\nCreate a ‚Äúnew app specific password‚Äù by typing a name for the password in the grey field and then click ‚ÄúCreate‚Äù. The name does not matter, but here I have called it ‚ÄúR‚Äù.\n\n\n\n\n\n\n\n\n\n\nGoogle will now create new 16-digit password that you can use in lieu of your actual Google password when sending emails via Gmail in the {emayili} and {blastula} functions below. Copy and save this code in a safe place!\n\n\n\n\n\n\n\n\n\n\nNOTE: The code will contain spaces between each set of 4 letters‚Ä¶ regardless of whether you cut out or include those white spaces, your app password should function just the same.\n\n\nFor ease, then set up a variable to hold your app password‚Ä¶\n\n\npwd_gmail &lt;- \"\"  # enter your app password between the quotation marks",
    "crumbs": [
      "Exercises",
      "Exercise 01"
    ]
  },
  {
    "objectID": "exercise-01.html#sending-an-email-version-1-using-the-emayaili-package",
    "href": "exercise-01.html#sending-an-email-version-1-using-the-emayaili-package",
    "title": "Exercise 01",
    "section": "Sending an Email Version 1: Using the {emayaili} package",
    "text": "Sending an Email Version 1: Using the {emayaili} package\nThe {emayili} package makes sending simple text emails very easy.\n\nStep 1\n\nInstall the {emayili} and {tidyverse} packages.\n\n\nNote that {tidyverse} is only needed if we are going to use the %&gt;% operator (see below).\n\nWe can do this at the R console prompt‚Ä¶\n\ninstall.packages(\"emayili\")\ninstall.packages(\"tidyverse\")\n\n# or to install both packages together...\n\ninstall.packages(c(\"emayili\", \"tidyverse\"))\n\nWe can also do this by using the package manager in RStudio.\n\n\n\n\n\n\n\n\n\n\n\nStep 2\n\nLoad and attach these packages to the search path so you can call functions they contain.\n\n\nlibrary(emayili)\nlibrary(tidyverse)\n\n\nNOTE: We can also use require() in lieu of library().\n\n\n\nStep 3\n\nCreate a new email ‚Äúmessage‚Äù with the envelope() function from the {emayili} package.\n\nThis function takes several intuitive arguments (from, to, subject, and text) that we can assign values to directly within the function using the = operator.\n\nNOTE: from= should typically be the email address you are sending the from, although many SMTP transactions will accept alternatives, such as just a name. You can also use a vector of email addresses for the to= argument to send a message to more than one recipient. The from and to arguments are required‚Ä¶ subject and text are optional.\n\n\nmessage_gmail &lt;- envelope(from = \"anthony.difiore@utexas.edu\", to = \"anthony.difiore@gmail.com\",\n    subject = \"Sending a message using {emayili}\", text = \"Hello! This is a plain text message sent to my personal Gmail account from my UT Gmail account.\")\n\n\nNOTE: R essentially ignores all the whitespace (spaces and tab characters) and also allows you to have code continue from one line to the next. It is even pretty forgiving about where you put carriage returns, although it is good form to place them following a comma or before/after a parenthesis or brace.\n\nUsing {tidyverse} syntax, we can set up the same email as follows using the ‚Äúpipe‚Äù operator (%&gt;%):\n\nmessage_gmail &lt;- envelope() %&gt;%\n    from(\"anthony.difiore@utexas.edu\") %&gt;%\n    to(\"anthony.difiore@gmail.com\") %&gt;%\n    subject(\"Sending a message using {emayili}\") %&gt;%\n    text(\"Hello! This is a plain text message sent to my personal Gmail account from my UT Gmail account.\")\n\nAs of R version 4.1, we can alternatively use R‚Äôs ‚Äúnative‚Äù pipe operator, |&gt;:\n\nmessage_gmail &lt;- envelope() |&gt;\n    from(\"anthony.difiore@utexas.edu\") |&gt;\n    to(\"anthony.difiore@gmail.com\") |&gt;\n    subject(\"Sending a message using {emayili}\") |&gt;\n    text(\"Hello! This is a plain text message sent to my personal Gmail account from my UT Gmail account.\")\n\nThe difference here is that we are first creating an empty ‚Äúmessage‚Äù object and then ‚Äúpiping‚Äù that object using %&gt;% or |&gt; into different helper functions (from(), to(), etc.) to create the details of the message.\n\n\nStep 4\n\nCreate an SMTP (or ‚ÄúSimple Mail Transfer Protocol‚Äù) ‚Äúserver‚Äù object that includes details about how to send a message, i.e., by specifying the email service‚Äôs outgoing server host name, a communications port number to use, and user and password information for authenticating use of the server.\nSMTP transmission can often use any of several standard port numbers (25, 465, 587, and 2525), but 587 and 465 are the most commonly used and support TLS encryption. To use Google‚Äôs outgoing email server, set the host to ‚Äúsmtp.gmail.com‚Äù and the port to either 465 or 587.\n\n\nNOTE: In the function below, max_times= sets the number of attempts the function will make to send the message. The default (if the argument is omitted) is 5, but I have set it to 1 to exit quickly if the server is not contacted. For username= enter your login ID for the service you are sending from, and for password= either use the variable you set up above to hold the ‚Äúapp password‚Äù for your Google account, or enter your password directly here, in quotation marks it, in lieu of the variable gmail_pwd\n\n\nsmtp_gmail &lt;- server(host = \"smtp.gmail.com\", port = 465, max_times = 1, username = \"anthony.difiore@utexas.edu\",\n    password = pwd_gmail)\n\nAlternatively, you can use the gmail() function to create a Gmail server object more easily‚Ä¶\n\nsmtp_gmail &lt;- gmail(username = \"anthony.difiore@utexas.edu\", password = pwd_gmail)\n\n\n\nStep 5\n\nSend your message by passing it as an argument to the server object. To confirm that things are working, send a message to yourself and then CHECK YOUR EMAIL to confirm that you receive the message. A copy of the message should appear in the Sent folder of your email client!\n\n\n# send using Google's Gmail server...\nsmtp_gmail(message_gmail, verbose = TRUE)\n\n\n\nNext Steps?\nUse the RStudio Help tab to browse the documentation associated with the {emayali} package to see how you can customize your message, e.g., by adding cc or bcc arguments, by using a different ‚Äúreply to‚Äù address, by adding attachments, or by encrypting your message.",
    "crumbs": [
      "Exercises",
      "Exercise 01"
    ]
  },
  {
    "objectID": "exercise-01.html#sending-an-email-version-2-using-the-mailr-package",
    "href": "exercise-01.html#sending-an-email-version-2-using-the-mailr-package",
    "title": "Exercise 01",
    "section": "Sending an Email Version 2: Using the {mailR} package",
    "text": "Sending an Email Version 2: Using the {mailR} package\nThe {mailR} package also allows you to easily send simple emails with a single function, send.mail(). Here, we need to specify details about the SMTP server more explicitly than if we use {emayili} (above) or {blastula} (below).\n\nStep 1\n\nInstall the {mailR} package.\n\n\ninstall.packages(\"mailR\")\n\n\n\nStep 2\n\nLoad the {mailR} package.\n\n\nlibrary(mailR)\n\n\n\nStep 3\nCreate and send the email\n\nUsing Google‚Äôs Gmail server‚Ä¶\n\nTo use the Gmail server with ports other than 587 (e.g., 465) the argument ssl= must be set to TRUE, which initiates a secure connection and allows required encryption\nIf port 587 is used, either the argument ssl= or tls= should be set to TRUE\n\n\n\nsend.mail(from = \"anthony.difiore@utexas.edu\", to = \"anthony.difiore@gmail.com\",\n    subject = \"Sending a message using {mailR}\", body = \"Hello! This is a plain text message sent from a Gmail account.\",\n    smtp = list(host.name = \"smtp.gmail.com\", port = 587, user.name = \"anthony.difiore@utexas.edu\",\n        passwd = pwd_gmail, ssl = TRUE), authenticate = TRUE, send = TRUE)",
    "crumbs": [
      "Exercises",
      "Exercise 01"
    ]
  },
  {
    "objectID": "exercise-01.html#sending-an-email-version-3-using-the-blastula-package",
    "href": "exercise-01.html#sending-an-email-version-3-using-the-blastula-package",
    "title": "Exercise 01",
    "section": "Sending an Email Version 3: Using the {blastula} package",
    "text": "Sending an Email Version 3: Using the {blastula} package\nThe {blastula} package allows us to create and send more complex HTML and formatted emails using markdown. Using it is a bit more complicated (but more flexible) than using the procedures above.\nSimilar to {emayili}, {blastula} provides two main functions: [1] compose_email() for constructing various parts of a message and [2] smtp_send() for specifying email server settings and passing the message to the server to send. However, {blastula} also adds in some helper functions that allow you to store authentication information in either a separate ‚Äúcredentials‚Äù text file that is referenced when you run the function to send a message or in your computer‚Äôs keychain. {blastula} also stores some default information on commonly-used email providers and services (e.g., on Google‚Äôs Gmail server), which may make configuring the server setup easier than using {emayili} or {mailR}.\n\nStep 1\n\nInstall the {blastula} package.\n\n\ninstall.packages(\"blastula\")\n\n\n\nStep 2\n\nLoad the {blastula} package.\n\n\nlibrary(blastula)\n\n\n\nStep 3\n\nCreate a new email ‚Äúmessage‚Äù with the compose_email() function.\n\nThis function takes several intuitive arguments that we can assign values to directly within the function.\n\n# Compose the message\nmessage &lt;- compose_email(body = \"Hello! This is a simple HTML message.\")\n\n# Preview the message.  This will open the HTML message in a browser window or\n# in the RStudio Viewer tab\nmessage\n\nWe can add some formatting to our email by using the md() function and markdown syntax. Here, we pipe (%&gt;% or |&gt;) the body argument to the md() function to convert it to markdown.\n\n# Compose the message\nmessage &lt;- compose_email(body = \"# Hello!\\nThis is a simple **HTML** message with some *markdown* syntax.\" |&gt;\n    md())\n\n# Preview the message\nmessage\n\nWe can also spice up our email with an image from a local file. Here, I use an image stored in a folder called ‚Äúimg‚Äù inside the my current working directory.\n\n# Create text as html\ntext &lt;- \"# Hello!\\nThis is a simple **HTML** message with some *markdown* syntax... and a cool picture!\"\n\n# Create image as html\nimage &lt;- add_image(file = \"img/batnet.jpg\", width = 300, align = \"center\")\n\n# Compose the message\nmessage &lt;- compose_email(body = c(text, image) |&gt;\n    md())\n\n# Preview the message\nmessage\n\n\n\nStep 4\n\nCreate a credentials file.\n\nThe following code block will create a text file (in JSON format) in the current working directory that contains default information for your outgoing mail server, e.g., Google‚Äôs Gmail server (host = \"smtp.gmail.com\"), along with your email address and your password. When you run the following lines of code, you will be asked to enter your password, which will then be stored in the credentials file. For a Gmail account, you can use the app password you created above.\n\nNOTE: Be aware that if you create a credentials file like this, your password will be stored, unencrypted, in the file you create!\n\n\n# credentials file for a Gmail account\ncreate_smtp_creds_file(file = \"my_gmail_creds.txt\", user = \"anthony.difiore@utexas.edu\",\n    provider = \"gmail\")\n\n\nNOTE: If you omit the provider= argument, you should then pass the host=, port=, and use_ssl= arguments yourself to the create_smtp_creds_file() function. The use_ssl= argument allows the use of STARTTLS, which initiates secured (encrypted) TLS or SSL connection, which many email servers require.\n\n\n\nStep 5\n\nSend the message via STMP using a credentials file.\n\n\n# send using Google's servers...\nsmtp_send(email = message, from = \"anthony.difiore@utexas.edu\", to = \"anthony.difiore@gmail.com\",\n    subject = \"Sending a message using {blastula} and a credentials file\", credentials = creds_file(file = \"my_gmail_creds.txt\"))\n\n\n\nStep 6\n\nWe could also send the message by specifying our credentials manually within the smtp_send() function.\n\nThe following will prompt us for our password to send the message:\n\n# send using Google's servers...\nmessage |&gt;\n    smtp_send(from = \"anthony.difiore@utexas.edu\", to = \"anthony.difiore@gmail.com\",\n        subject = \"Sending a message using {blastula} and entering credentials manually\",\n        credentials = creds(user = \"anthony.difiore@utexas.edu\", provider = \"gmail\"))",
    "crumbs": [
      "Exercises",
      "Exercise 01"
    ]
  },
  {
    "objectID": "exercise-01.html#sending-an-email-version-4-using-the-sendmailr-package",
    "href": "exercise-01.html#sending-an-email-version-4-using-the-sendmailr-package",
    "title": "Exercise 01",
    "section": "Sending an Email Version 4: Using the {sendmailR} package",
    "text": "Sending an Email Version 4: Using the {sendmailR} package\nThe {sendmailR} is yet another that you can use to send emails from R.\n\ninstall.packages(\"sendmailR\")\nlibrary(sendmailR)\nsendmail(from = \"anthony.difiore@utexas.edu\", to = \"anthony.difiore@gmail.com\", subject = \"Sending a message using {sendmailR}\",\n    msg = \"Hello! This is a plain text message sent to my personal Gmail account from my UT Gmail account.\",\n    engine = \"curl\", engineopts = list(username = \"anthony.difiore@utexas.edu\", password = pwd_gmail),\n    control = list(smtpServer = \"smtp://smtp.gmail.com\", smtpPortSMTP = 587))\n# can also include the port as part of the smtpServer argument...  e.g.,\n# `control = list(smtpServer='smtp://smtp.gmail.com:587')`\n\n\nNext Steps?\nAgain, use the RStudio Help tab to browse the documentation associated with the {blastula} package to see how you can customize your message, e.g., with attachments.\n\n\nEven More Next Steps?\nFinally, it is a bit more complicated, but note that we can also use the {keyring} package along with {blastula} to set up a credentials ‚Äúkey‚Äù in our computer‚Äôs keychain and refer to that to specify our credentials for sending a message. When you create the keys, you will be asked to provide your password.\n\ninstall.packages(\"keyring\")\n\n\nlibrary(keyring)\n\n\n# create a Gmail key with a user-specified name, stored with the `id=` argument\ncreate_smtp_creds_key(id = \"my_gmail_key\", user = \"anthony.difiore@utexas.edu\", provider = \"gmail\",\n    overwrite = TRUE  # this argument is only needed if you have an existing key\n)\n\n\n# View all keys\nview_credential_keys()\n\n\n# send a message with credentials from a keychain using Google's servers...\nmessage |&gt;\n    smtp_send(from = \"anthony.difiore@utexas.edu\", to = \"anthony.difiore@gmail.com\",\n        subject = \"Sending a message using {blastula} and credentials from a keychain\",\n        credentials = creds_key(id = \"my_gmail_key\"))",
    "crumbs": [
      "Exercises",
      "Exercise 01"
    ]
  },
  {
    "objectID": "exercise-01.html#clean-up-steps",
    "href": "exercise-01.html#clean-up-steps",
    "title": "Exercise 01",
    "section": "Clean-Up Steps",
    "text": "Clean-Up Steps\n\n# delete all keys\ndelete_all_credential_keys()",
    "crumbs": [
      "Exercises",
      "Exercise 01"
    ]
  },
  {
    "objectID": "exercise-02.html",
    "href": "exercise-02.html",
    "title": "Exercise 02",
    "section": "",
    "text": "Practice Reproducible Research Workflows",
    "crumbs": [
      "Exercises",
      "Exercise 02"
    ]
  },
  {
    "objectID": "exercise-02.html#learning-objectives",
    "href": "exercise-02.html#learning-objectives",
    "title": "Exercise 02",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nIntegrating an RStudio project, a local git repository, and a GitHub remote repository",
    "crumbs": [
      "Exercises",
      "Exercise 02"
    ]
  },
  {
    "objectID": "exercise-02.html#task-1-begin-with-a-remote-repo",
    "href": "exercise-02.html#task-1-begin-with-a-remote-repo",
    "title": "Exercise 02",
    "section": "Task 1: Begin with a remote repo‚Ä¶",
    "text": "Task 1: Begin with a remote repo‚Ä¶\nFollow the instructions outlined as Method 1 in Module 6 to do the following:\n\nStep 1\n\nSet up a new GitHub repo in your GitHub workspace named ‚Äúexercise-02a‚Äù.\n\n\n\n\n\n\n\n\n\n\n\n\nStep 2\n\nAdd me as a collaborator on the repo by going to ‚ÄúSettings‚Äù, choosing ‚ÄúCollaborators‚Äù, then ‚ÄúAdd People‚Äù, and searching for my GitHub username (‚Äúdifiore‚Äù).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 3\n\nClone your repo to your local computer.\n\n\n\nStep 4\n\nCreate an new RMarkdown or Quarto document locally with notes you take in class this week - maybe call it ‚Äúnotes.qmd‚Äù or ‚Äúnotes.Rmd‚Äù? - including at least some expository text written in Markdown and at least one R code block. If you make a ‚Äú.qmd‚Äù document, I recommend unchecking ‚ÄúUse visual markdown editor‚Äù, though that is your choice.\n\n\n\n\n\n\n\n\n\n\n‚Ä¶ OR ‚Ä¶\n\n\n\n\n\n\n\n\n\n\n\nStep 5\n\nUse Render (for ‚Äú.qmd‚Äù) or Knit (for ‚Äú.Rmd‚Äù) to publish your document as HTML.\n\n\n\n\n\n\n\n\n\n\n‚Ä¶ OR ‚Ä¶\n\n\n\n\n\n\n\n\n\n\n\nStep 6\n\nStage (or ‚Äúadd‚Äù) and ‚Äúcommit‚Äù the changes to your repo locally and then ‚Äúpush‚Äù those up to GitHub.\n\n\n\nStep 7\n\nVisit your repo on GitHub to confirm that you have done the above steps successfully.\n\n\n\nStep 8\n\nSubmit the URL for your repo in Canvas",
    "crumbs": [
      "Exercises",
      "Exercise 02"
    ]
  },
  {
    "objectID": "exercise-02.html#task-2-begin-with-a-local-repo",
    "href": "exercise-02.html#task-2-begin-with-a-local-repo",
    "title": "Exercise 02",
    "section": "Task 2: Begin with a local repo‚Ä¶",
    "text": "Task 2: Begin with a local repo‚Ä¶\nThis is an alternative workflow to that described above, which also works well. It is outlined in more detail in Module 6 as ‚ÄúOption 3‚Äù under Connecting a Local Repo to GitHub.\n\nStep 1\n\nFrom within RStudio, create a new project called ‚Äúexercise-2b‚Äù in a brand new directory also called ‚Äúexercise-2b‚Äù.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNOTE Be sure to check the box marked ‚ÄúCreate a git repository‚Äù pressing the ‚ÄúCreate Project‚Äù button.\n\n\n\nStep 2\n\nConfigure a GitHub Personal Access Token (PAT) as described in Module 5 and Module 6, making sure that R and RStudio can access your cached credentials.\n\n\n\nStep 3\n\nAs outlined in Module 6 Method 3 Option 1, open your project locally and run usethis::use_github(protocol=\"https\") from the R console within your project‚Äôs working directory.\n\n\n\nStep 4\n\nAnswer the question about whether the suggested name for the remote repository is adequate and hit .\n\nThis will create a new remote repository on GitHub, add it as a remote origin/main, and open the GitHub page for the repository in your browser. Switch back to RStudio and continue with Steps 4 to 8 from above.",
    "crumbs": [
      "Exercises",
      "Exercise 02"
    ]
  },
  {
    "objectID": "exercise-03.html",
    "href": "exercise-03.html",
    "title": "Exercise 03",
    "section": "",
    "text": "Explore and Wrangle Data",
    "crumbs": [
      "Exercises",
      "Exercise 03"
    ]
  },
  {
    "objectID": "exercise-03.html#learning-objectives",
    "href": "exercise-03.html#learning-objectives",
    "title": "Exercise 03",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nIntegrating an RStudio project, a local git repository, and a GitHub remote repository\nPerforming basic exploratory data analyses and visualization\nUsing {dplyr} and the pipe operator (|&gt; or %&gt;%) for data wrangling\n\n\nPreliminaries\n\nSet up a new GitHub repo in your GitHub workspace called ‚Äúexercise-03‚Äù and clone that down your computer as a new RStudio project. The instructions outlined as Method 1 in Module 6 will be helpful.\nLoad the ‚Äúdata-wrangling.csv‚Äù dataset from this URL as a tabular data structure named d and look at the variables it contains, which are a subset of those in the Kamilar and Cooper dataset on primate ecology, behavior, and life history that we have used previously.\n\n\nHINT: You can use the names() function to see a list of variable names in the dataset\n\n\n\nStep 1\nCreate a new Quarto document called ‚ÄúEDA-challenge.qmd‚Äù within the project and use it to complete the following challenges. Be sure to include both descriptive text about what you are doing and code blocks - i.e., follow a ‚Äúliterate programming‚Äù approach to be sure that a reader understands what you are doing. For the plots, feel free to use either {base} R graphics or {ggplot2}, as you prefer.\n\nCreate a new variable named BSD (body size dimorphism) which is the ratio of average male to female body mass.\nCreate a new variable named sex_ratio, which is the ratio of the number of adult females to adult males in a typical group.\nCreate a new variable named DI (for ‚Äúdefensibility index‚Äù), which is the ratio of day range length to the diameter of the home range.\n\n\nHINT: You will need to calculate the latter for each species from the size of its home range! For the purposes of this assignment, presume that the home range is a circle and use the formula for the area of a circle: \\(AREA = \\pi \\times r^2\\). The variable pi (note, without parentheses!) will return the value of \\(\\pi\\) as a constant built-in to R, and the function sqrt() can be used to calculate the square root (\\(\\sqrt{}\\))\n\n\nPlot the relationship between day range length (y axis) and time spent moving (x axis), for these primate species overall and by family (i.e., a different plot for each family, e.g., by using faceting: + facet_wrap()). Do species that spend more time moving travel farther overall? How about within any particular primate family? Should you transform either of these variables?\nPlot the relationship between day range length (y axis) and group size (x axis), overall and by family. Do species that live in larger groups travel farther overall? How about within any particular primate family? Should you transform either of these variables?\nPlot the relationship between canine size dimorphism (y axis) and body size dimorphism (x axis) overall and by family. Do taxa with greater size dimorphism also show greater canine dimorphism?\nCreate a new variable named diet_strategy that is ‚Äúfrugivore‚Äù if fruits make up &gt;50% of the diet, ‚Äúfolivore‚Äù if leaves make up &gt;50% of the diet, and ‚Äúomnivore‚Äù if diet data are available, but neither of these is true (i.e., these values are not NA). Then, do boxplots of group size for species with different dietary strategies, omitting the category NA from your plot. Do frugivores live in larger groups than folivores?\n\n\nHINT: To create this new variable, try combining mutate() with ifelse() or case_when() statements‚Ä¶ check out the notes on ‚ÄúConditional Expressions‚Äù in Module 11. Take a peak at the code snippet below if you get stuck!\n\n\n\nCode\nmutate(d, diet_strategy = ifelse(Fruit &gt;= 50, \"frugivore\", ifelse(Leaves &gt;= 50, \"folivore\",\n    ifelse(Fruit &lt; 50 & Leaves &lt; 50, \"omnivore\", NA))))\n\nmutate(d, diet_strategy = case_when(Fruit &gt;= 50 ~ \"frugivore\", Leaves &gt;= 50 ~ \"folivore\",\n    Fruit &lt; 50 & Leaves &lt; 50 ~ \"omnivore\", TRUE ~ NA))\n\n\n\nIn one line of code, using {dplyr} verbs and the forward pipe (|&gt; or %&gt;%) operator, do the following:\n\n\nAdd a variable, Binomial to the data frame d, which is a concatenation of the Genus and Species variables‚Ä¶\nTrim the data frame to only include the variables Binomial, Family, Brain_size_species_mean, and Body_mass_male_mean‚Ä¶\nGroup these variables by Family‚Ä¶\nCalculate the average value for Brain_Size_Species_Mean and Body_mass_male_mean per Family (remember, you may need to specify na.rm = TRUE)‚Ä¶\nArrange by increasing average brain size‚Ä¶\nAnd print the output to the console\n\n\n\nStep 2\n\nRender your work to HTML or (if you feel ambitious!) as a PDF. üòÉ\n\n\n\nStep 3\n\nCommit your changes locally and push your repo, including your ‚Äú.qmd‚Äù file and knitted results, up to GitHub.\n\n\n\nStep 4\n\nSubmit the URL for your repo in Canvas\n\n\n\nOptional Next Steps?\nLoad in a dataset of your own and write R code to do the following:\n\nCreate and print a vector of the variable names in your dataset\nCount the total number of rows (observations) in your dataset\nFor each numeric variable, calculate the number of observations, the mean and standard deviation, and the five-number summary\nMake box-and-whiskers plots for each numerical variable\nFor each categorical variable, generate a table of counts/proportions of alternative variable values",
    "crumbs": [
      "Exercises",
      "Exercise 03"
    ]
  },
  {
    "objectID": "exercise-04.html",
    "href": "exercise-04.html",
    "title": "Exercise 04",
    "section": "",
    "text": "Program a Word Game",
    "crumbs": [
      "Exercises",
      "Exercise 04"
    ]
  },
  {
    "objectID": "exercise-04.html#learning-objectives",
    "href": "exercise-04.html#learning-objectives",
    "title": "Exercise 04",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nLoading data files into R\nBreaking a programming challenge down into discrete steps\nWriting novel functions\nUsing arguments in functions\nUsing set operation functions\nUsing loops and conditional statements\nWorking with different data structures (vectors, tabular data)\nPracticing data wrangling\nDealing with keyboard input",
    "crumbs": [
      "Exercises",
      "Exercise 04"
    ]
  },
  {
    "objectID": "exercise-04.html#wordle-puzzle-challenge",
    "href": "exercise-04.html#wordle-puzzle-challenge",
    "title": "Exercise 04",
    "section": "Wordle Puzzle Challenge",
    "text": "Wordle Puzzle Challenge\nThe rules of Wordle are simple: A player has SIX tries to guess a 5-letter word that has been selected at random from a list of possible words. Guesses need to be valid 5-letter words. After each guess, a player receives feedback about how close their guess was to the word, which provides information they can use to try to solve the puzzle. This feedback lets you know whether each letter your guess is either [1] in the solution word and in the correct spot, [2] in the solution word but in the wrong position, or [3] not in the solution word. In app/browser versions of Wordle, this feedback is provided visually using colors, but that need not be the case.\nThis week‚Äôs programming exercise is to work with a partner to get as far along as you can writing R code that allows you to play Wordle to your heart‚Äôs content using R/RStudio!\nThe assignment and steps below were inspired by this fun blog post‚Ä¶ but before you reference it or other online sites, try to tackle this coding exercise on your own.\n\nPreliminaries\n\nSet up a new GitHub repo in you or your partner‚Äôs GitHub workspace named ‚Äúexercise-04‚Äù and clone that down to both of your computers as a new RStudio project. The instructions outlined as Method 1 in Module 6 will be helpful. Make sure that both you and your partner are collaborators on the repo and that you add me as a collaborator as well (my GitHub username is ‚Äúdifiore‚Äù).\nGo to https://github.com/difiore/ada-datasets, download the following two data files, and add them to your repo:\n\ncollins-scrabble-words-2019.txt\ngoogle-10000-english-usa-no-swears.txt\n\n\nThe first of these files (279,497 lines long) contains a list of ‚ÄúOfficial Scrabble Words‚Äù in the English language based on the Collins English Dictionary published by HarperCollins. The first line in the file is the word ‚Äúwords‚Äù and is used as a header.\nThe second of these files (9885 lines long) contains a list of ~10,000 of the most common words in the English language, based on data compiled by Google, and omitting common swear words. Again, the first line is the word ‚Äúwords‚Äù and is used as a header.\n\nCreate a new Quarto or RMarkdown document, called ‚Äúwordle.qmd‚Äù or ‚Äúwordle.Rmd‚Äù. In this, you do your best to recreate all of the wordplay used in Wordle.\n\n\n\nIntroduction\nBefore we begin programming, an important first step is to break down the problem we are trying to down into discrete pieces‚Ä¶ What do we need to do to set up a Wordle game? What steps does game play need to follow? What has to be evaluated at each step? How does the game end?\nAs an early step, for example, we need to choose a mystery ‚Äúsolution‚Äù word that players will try to guess. We will use the list of the most common words in the English language to do this as a possible source of solution words for the puzzle.\nAdditionally, also need to establish a dictionary of ‚Äúvalid‚Äù words that players can guess, and for that we will use the list of Official Scrabble Words.\n\nNOTE: The list of possible ‚Äúsolution‚Äù words used in the original Wordle puzzle consists of ~2100 5-letter words, while the list of ‚Äúvalid‚Äù words that can be used as guesses totals ~13,000. How many 5-letter words are in each of the two data files you have downloaded?\n\n\n\nStep 1\n\nCreate your own custom function called load_dictionary() that takes a single argument, ‚Äúfilename‚Äù, that can be used to read in either of the two data files your downloaded.\nOnce you have created your function, use that function to create two variables, solution_list and valid_list, that, respectively contain vectors of possible solution words and valid words to guess. That is, you should be able to run the following to create these two vectors:\n\n\nvalid_list &lt;- load_dictionary(&lt;filename here&gt;)\nsolution_list &lt;- load_dictionary(&lt;filename here&gt;)\n\nRunning str(valid_list) should return‚Ä¶\n\nchr [1:279496] \"AA\" \"AAH\" \"AAHED\" \"AAHING\"...\n\nRunning str(solution_list) should return‚Ä¶\n\nchr [1:8336] \"THE\" \"OF\" \"AND\" \"TO\"...\n\n\n\nStep 2\n\nWinnow your variable solution_list to only include words that are included in valid_list. There are multiple ways that you could do this, but the set operation function, intersection() is an easy way. Use R help to look at the documentation for the intersection() function to see if you can get that to work. How many words are in your updated solution_list vector?\n\n\n\nStep 3\n\nWrite a custom function called pick_solution() that [1] removes all words from solution_list that are not 5 letters in length, [2] then randomly chooses a single word from those that remain, and [3] then splits that word into a vector of single-character elements. You should be able to pass your solution_list vector as the argument to the function.\n\n\nHINT: For [1], you will want to subset the solution_list vector according to some criterion of word length. For [2], you may find the sample() function useful (use R help to look up documentation on that function). For [3], you may want to look at the functions strsplit() from {base} R or str_split() from the {stringr} package (part of {tidyverse}). Pay attention to the data structures that those functions return, because you will need to carefully reference one of the elements that is returned in order to wind up with a vector!\n\nAs a bonus, you might include a second argument for your pick_solution() function called ‚Äúword_length‚Äù that makes your function flexible enough to select a solution word that is something other than 5 characters long.\nOnce your function works, run it and assign the result to a variable called solution.\n\nsolution &lt;- pick_solution(solution_list)\n\n\n\nStep 4\nNow, to tackle the bulk of the problem, create two more functions. The first should be called play_wordle() and it should take three arguments: [1] the answer to the puzzle (the value of your solution variable), [2] a list of valid guesses (the contents of your valid_list variable), and [3] a value for ‚Äúnumber of guesses‚Äù, which you should set to the original Wordle game default of 6.\n\nHINT: Here is possible skeleton code for that function.\n\n\nplay_wordle &lt;- function(solution, valid_list, num_guesses=6){\n      &lt;function code here&gt;\n    }\n\nThink carefully about what the play_wordle() function needs to do. It should:\n\nAt the onset, tell the player the rules of the game, e.g., ‚ÄúYou have ‚Ä¶ chances to guess a word of length ‚Ä¶‚Äù\nDisplay what letters the player has not yet guessed (at the onset, this would be all 26 letters of the alphabet), e.g., ‚ÄúLetters left: ‚Ä¶‚Äù\n\n\nHINT: There is a built-in dataset in R called LETTERS, which contains the 26 capital letters in the English alphabet and another, letters, that contains all the lowercase letters. You will probably want to use either the toupper() or tolower() functions to ensure that you are always working with the same formatted letters in words and guesses.\n\n\n# using the LETTERS built-in vector\nLETTERS\n\n##  [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\" \"K\" \"L\" \"M\" \"N\" \"O\" \"P\" \"Q\" \"R\" \"S\"\n## [20] \"T\" \"U\" \"V\" \"W\" \"X\" \"Y\" \"Z\"\n\n# using the `toupper()` function\ntoupper(\"change my case\")\n\n## [1] \"CHANGE MY CASE\"\n\n\n\nPrompt the player for a guess, e.g., ‚ÄúEnter guess number ‚Ä¶‚Äù, read in their guess, and check that their guess is valid (i.e., that it contains the correct number of letters and is a word included in the ‚Äúvalid‚Äù word list).\n\n\nHINT: The readline() function, which can take a character string as an argument, will provide a ‚Äúprompt‚Äù entering a line of numeric or character data. Hitting &lt;enter&gt; or &lt;return&gt; signals the end of the line.\n\n\nguess &lt;- readline(\"Enter some data here, then press &lt;enter&gt;: \")\n\n\nCompare the guess to the solution word and generate the necessary feedback, e.g., * for in the word and in the correct position, + for in the word but in the wrong position, and - for not in the word. For this step, try writing a separate ‚Äúhelper‚Äù function, evaluate_guess(), called from within play_wordle(). This function should take, as arguments, the player‚Äôs guess and the value of the solution variable. This is probably the trickiest part of the problem to program, and there are lots of approaches you might take to evaluating guesses. After you work on this for a while, I can share one solution.\nUpdate the list of letters not yet guessed.\n\n\nHINT: Again, consider using set operations to update the list of letters not yet guessed. setdiff() is a function that returns the difference between two vectors.\n\n\nCheck if the puzzle was solved. If so, the function should indicate that the player WON the game and print out their guess and feedback history. If not, the function should prompt the player for another guess, unless they have already hit the maximum number of guesses allowed.\nIf all guesses are exhausted, the function should indicate that the player LOST the game and, again, print out their guess and feedback history.\n\n\n\nOptional Next Steps?\n\nTry modifying your code to mimic the ‚Äúhard mode‚Äù in Wordle, where information about the letters in the solution and their positions revealed in prior guesses has to be used in subsequent guesses.\nTry spicing up the feedback given using colors or alternative formatting. One way to do this would be to use the {huxtable} package, which is a package for creating text tables that can be styled for display in the R console and can also output to HTML, PDF, and a variety of other formats.\nHave R keep track of the date and not let you play more than one Wordle game per day.\nHave R keep track of your performance across multiple games of Wordle.\nAllow R to post your Wordle results to a social media platform of your choosing. For this, check out, e.g., the {Rfacebook} or {rtweet} packages.\nConvert your code to an interactive {shiny} app to have it run in a web brower. Later modules will introduce you to programming with {shiny}.",
    "crumbs": [
      "Exercises",
      "Exercise 04"
    ]
  },
  {
    "objectID": "exercise-04-solution.html",
    "href": "exercise-04-solution.html",
    "title": "Exercise 04 Solution",
    "section": "",
    "text": "‚Ä¢ Solution\nUse the Show Code button to peek at one solution to each of the four functions. Note that needed libraries are specified with the require() at the top of each function. You could use library() as an alternative. You could also have these loaded in the global environment, but these would not be as ‚Äúportable‚Äù across users and systems. Each of these functions includes some commented out print() and/or glimpse() statements that are useful for viewing intermediate values for debugging.\n\nNOTE: The play_wordle() and evaluate_guess() functions include an additional argument, ‚Äúoutput_type‚Äù, that lets the user specify whether they would like to see simple text output or formatted (‚Äúgraphic‚Äù) output, which is made possible by using the {huxtable} library. A substantial chunk of the play_wordle() and evaluate_guess() functions are devoted to producing this graphic output and are not necessary.\n\n\nFunctions\n\nload_dictionary()\n\n\nShow Code\nload_dictionary &lt;- function(filename) {\n    require(tidyverse)\n    dictionary &lt;- read_csv(filename, col_names = TRUE)\n    dictionary &lt;- dictionary[[\"words\"]]\n    dictionary &lt;- toupper(dictionary)\n    # glimpse(dictionary) # to show the structure of 'dictionary' to confirm\n    # that it is a vector\n    return(dictionary)\n}\n\n\n\n\npick_solution()\n\n\nShow Code\npick_solution &lt;- function(dictionary, word_length = 5) {\n    require(tidyverse)\n    possible_solutions &lt;- dictionary[nchar(dictionary) == word_length]\n    solution &lt;- sample(possible_solutions, 1)\n    # print(solution)\n    solution_vector &lt;- str_split(solution, \"\")[[1]]\n    # glimpse(solution_vector) # to show the structure of the solution vector\n    return(solution_vector)\n}\n\n\n\n\nplay_wordle()\n\n\nShow Code\nplay_wordle &lt;- function(solution, valid_list, num_guesses = 6, output_type = \"text\") {\n    ## ARGUMENT 'output_type=' CAN BE CUT IF GRAPHIC OUTPUT NOT NEEDED\n    require(tidyverse)\n    require(sjmisc)\n    require(huxtable)  ## THIS LIBRARY CALL CAN BE CUT IF GRAPHIC OUTPUT NOT NEEDED\n    word_length &lt;- length(solution)\n    print(paste0(\"You have \", num_guesses, \" chances to guess a word of length \",\n        word_length))\n    letters_left &lt;- LETTERS  # a built-in set of capital letters\n    guess_history &lt;- data.frame(matrix(nrow = num_guesses, ncol = word_length))\n    result_history &lt;- data.frame(matrix(nrow = num_guesses, ncol = word_length))\n    if (output_type == \"graphic\") {\n        guess_history &lt;- as_huxtable(guess_history)\n        result_history &lt;- as_huxtable(result_history)\n    }\n    for (i in 1:num_guesses) {\n        # display 'keyboard'\n        print(paste0(c(\"Letters left: \", letters_left), collapse = \" \"))\n        # read in guess and confirm length and validity\n        guess &lt;- readline(paste0(\"Enter guess \", i, \": \")) |&gt;\n            toupper()\n        while (nchar(guess) != word_length) {\n            guess &lt;- readline(paste0(\"Guess must have \", word_length, \" characters. Enter guess \",\n                i, \" again : \")) |&gt;\n                toupper()\n        }\n        while (guess %nin% valid_list) {\n            guess &lt;- readline(paste0(\"Hmm, that word is not in my dictionary of valid words. Enter guess \",\n                i, \" again: \")) |&gt;\n                toupper()\n        }\n        guess &lt;- str_split(guess, \"\")[[1]]\n        # print(guess) # check output\n\n        # evaluate guess\n        result &lt;- evaluate_guess(guess, solution, output_type)\n\n        # update keyboard\n        letters_left &lt;- setdiff(letters_left, guess)\n\n        # print results\n        guess_history[i, ] &lt;- guess\n        result_history[i, ] &lt;- result\n\n        if (output_type == \"text\")\n            {\n                ## THIS LINE CAN BE CUT IF GRAPHIC OUTPUT NOT NEEDED\n                if (all(result == \"*\")) {\n                  guess_history &lt;- guess_history |&gt;\n                    na.omit()\n                  result_history &lt;- result_history |&gt;\n                    na.omit()\n                  print(paste0(\"You won in \", i, \" guesses!\"))\n                  guess_history &lt;- guess_history |&gt;\n                    unite(everything(), sep = \"\", col = \"guess\", remove = TRUE)\n                  result_history &lt;- result_history |&gt;\n                    unite(everything(), sep = \"\", col = \"result\", remove = TRUE)\n                  history &lt;- data.frame(guess = guess_history, result = result_history)\n                  print(history)\n                  return(invisible(history))\n                } else {\n                  history &lt;- data.frame(guess = paste0(guess, collapse = \"\"), result = paste0(result,\n                    collapse = \"\"))\n                  print(history)\n                }\n            }  ## THIS LINE CAN BE CUT IF GRAPHIC OUTPUT NOT NEEDED\n\n        ## THIS WHOLE `if() {} else {}` BLOCK BELOW CAN BE CUT IF GRAPHIC\n        ## OUTPUT IS NOT NEEDED\n        if (output_type == \"graphic\") {\n            if (all(background_color(result) == \"#6BA964\")) {\n                history &lt;- result_history |&gt;\n                  na.omit()\n                print(paste0(\"You won in \", i, \" guesses!\"))\n                print(history, colnames = FALSE)\n                return(invisible(history))\n            } else {\n                print(result, colnames = FALSE)\n            }\n        }\n        ## CLOSE OF BLOCK FOR GRAPHIC OUTPUT\n\n    }\n    print(paste0(\"Sorry, you lost! Solution was \", paste0(solution, collapse = \"\")))\n\n    if (output_type == \"text\")\n        {\n            ## ## THIS LINE CAN BE CUT IF GRAPHIC OUTPUT NOT NEEDED\n            guess_history &lt;- guess_history |&gt;\n                unite(everything(), sep = \"\", col = \"guess\", remove = TRUE)\n            result_history &lt;- result_history |&gt;\n                unite(everything(), sep = \"\", col = \"result\", remove = TRUE)\n            history &lt;- data.frame(guess = guess_history, result = result_history)\n            print(history)\n            return(invisible(history))\n        }  ## THIS LINE CAN BE CUT IF GRAPHIC OUTPUT NOT NEEDED\n\n    ## THE WHOLE `if()` BLOCK BELOW CAN BE CUT IF GRAPHIC OUTPUT IS NOT NEEDED\n    if (output_type == \"graphic\") {\n        history &lt;- result_history\n        print(history, colnames = FALSE)\n        return(invisible(history))\n    }\n    ## CLOSE OF BLOCK FOR GRAPHIC OUTPUT\n\n    return()\n}\n\n\n\n\nevaluate_guess()\n\n\nShow Code\nevaluate_guess &lt;- function(guess, solution, output_type) {\n    word_length &lt;- length(solution)\n    text_result &lt;- rep(\"-\", word_length)\n    # the next lines are an ugly hack to deal with repeat letters...  we first\n    # find the number of times letters in the guess appear in the solution\n    # because we will need to clear 'extra' ones away\n    guess_count &lt;- tibble(letter = guess) |&gt;\n        group_by(letter) |&gt;\n        summarize(n_in_guess = n())\n    solution_count &lt;- tibble(letter = solution) |&gt;\n        group_by(letter) |&gt;\n        summarize(n_in_solution = n())\n    counts &lt;- inner_join(guess_count, solution_count, by = \"letter\") |&gt;\n        mutate(to_clear = n_in_guess - n_in_solution) |&gt;\n        filter(to_clear &gt; 0) |&gt;\n        select(letter, to_clear)\n\n    for (i in 1:word_length) {\n        # these `case_when()` lines are the workhorse of the function...  they\n        # find if each letter in the guess appears in the solution and if it in\n        # the right place\n        text_result[i] &lt;- case_when(guess[i] %in% solution & guess[i] == solution[i] ~\n            \"*\", guess[i] %in% solution & guess[i] != solution[i] ~ \"+\", guess[i] %nin%\n            solution ~ \"-\")\n\n        # this `for()` loop then cycles through cases where the guess contains\n        # more of a particular letter than the solution and clears away the\n        # correct number of matches that are in in the solution but in the\n        # wrong position\n        for (j in counts$letter) {\n            if (guess[i] == j & text_result[i] != \"*\" & counts[counts$letter == j,\n                ]$to_clear &gt; 0) {\n                text_result[i] &lt;- \"-\"\n                counts[counts$letter == j, ]$to_clear &lt;- counts[counts$letter ==\n                  j, ]$to_clear - 1\n            }\n        }\n\n    }\n\n    # format for graphic output\n    graphic_result &lt;- t(data.frame(guess)) |&gt;\n        as_huxtable() |&gt;\n        theme_bright() |&gt;\n        set_all_padding(10) |&gt;\n        set_text_color(\"#FFFFFF\") |&gt;\n        set_align(\"center\") |&gt;\n        set_bold(TRUE) |&gt;\n        set_all_borders(brdr(4, \"solid\", \"white\")) |&gt;\n        set_font(\"arial\") |&gt;\n        set_font_size(18)\n    for (i in 1:word_length) {\n        graphic_result &lt;- set_background_color(graphic_result, 1, i, case_when(text_result[i] ==\n            \"*\" ~ \"#6BA964\", text_result[i] == \"+\" ~ \"#CAB458\", text_result[i] ==\n            \"-\" ~ \"#787C7E\"))\n    }\n    if (output_type == \"text\") {\n        return(text_result)\n    } else {\n        return(graphic_result)\n    }\n}\n\n\n\n\n\nPlaying the Game\n\nCreate Dictionaries\n\nf_solution_list &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/google-10000-english-usa-no-swears.txt\"\nf_valid_list &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/collins-scrabble-words-2019.txt\"\n\nvalid_list &lt;- load_dictionary(f_valid_list)\nsolution_list &lt;- load_dictionary(f_solution_list)\n\n\n\nWinnow Solution Words to Set of Valid Words\n\nsolution_list &lt;- intersect(solution_list, valid_list)\n\n\n\nGame Play and Output\n\nsolution &lt;- pick_solution(solution_list, word_length = 5)\ngame &lt;- play_wordle(solution, valid_list, num_guesses = 6, output = \"graphic\")\nquick_html(game)  # prints results as html\n\n\n\n\nAlternative Functions without Graphic Output\n\n\nShow Code\nplay_wordle &lt;- function(solution, valid_list, num_guesses = 6) {\n    require(tidyverse)\n    require(sjmisc)\n    word_length &lt;- length(solution)\n    print(paste0(\"You have \", num_guesses, \" chances to guess a word of length \",\n        word_length))\n    letters_left &lt;- LETTERS  # a built-in set of capital letters\n    guess_history &lt;- data.frame(matrix(nrow = num_guesses, ncol = word_length))\n    result_history &lt;- data.frame(matrix(nrow = num_guesses, ncol = word_length))\n    for (i in 1:num_guesses) {\n        # display 'keyboard'\n        print(paste0(c(\"Letters left: \", letters_left), collapse = \" \"))\n        # read in guess and confirm length and validity\n        guess &lt;- readline(paste0(\"Enter guess \", i, \": \")) |&gt;\n            toupper()\n        while (nchar(guess) != word_length) {\n            guess &lt;- readline(paste0(\"Guess must have \", word_length, \" characters. Enter guess \",\n                i, \" again : \")) |&gt;\n                toupper()\n        }\n        while (guess %nin% valid_list) {\n            guess &lt;- readline(paste0(\"Hmm, that word is not in my dictionary of valid words. Enter guess \",\n                i, \" again: \")) |&gt;\n                toupper()\n        }\n        guess &lt;- str_split(guess, \"\")[[1]]\n        # print(guess) # check output\n\n        # evaluate guess\n        result &lt;- evaluate_guess(guess, solution)\n\n        # update keyboard\n        letters_left &lt;- setdiff(letters_left, guess)\n\n        # print results\n        guess_history[i, ] &lt;- guess\n        result_history[i, ] &lt;- result\n\n        if (all(result == \"*\")) {\n            guess_history &lt;- guess_history |&gt;\n                na.omit()\n            result_history &lt;- result_history |&gt;\n                na.omit()\n            print(paste0(\"You won in \", i, \" guesses!\"))\n            guess_history &lt;- guess_history |&gt;\n                unite(everything(), sep = \"\", col = \"guess\", remove = TRUE)\n            result_history &lt;- result_history |&gt;\n                unite(everything(), sep = \"\", col = \"result\", remove = TRUE)\n            history &lt;- data.frame(guess = guess_history, result = result_history)\n            print(history)\n            return(invisible(history))\n        } else {\n            history &lt;- data.frame(guess = paste0(guess, collapse = \"\"), result = paste0(result,\n                collapse = \"\"))\n            print(history)\n        }\n\n    }\n    print(paste0(\"Sorry, you lost! Solution was \", paste0(solution, collapse = \"\")))\n\n    guess_history &lt;- guess_history |&gt;\n        unite(everything(), sep = \"\", col = \"guess\", remove = TRUE)\n    result_history &lt;- result_history |&gt;\n        unite(everything(), sep = \"\", col = \"result\", remove = TRUE)\n    history &lt;- data.frame(guess = guess_history, result = result_history)\n    print(history)\n    return(invisible(history))\n\n}\n\nevaluate_guess &lt;- function(guess, solution) {\n    word_length &lt;- length(solution)\n    text_result &lt;- rep(\"-\", word_length)\n    # the next lines are an ugly hack to deal with repeat letters...  we first\n    # find the number of times letters in the guess appear in the solution\n    # because we will need to clear 'extra' ones away\n    guess_count &lt;- tibble(letter = guess) |&gt;\n        group_by(letter) |&gt;\n        summarize(n_in_guess = n())\n    solution_count &lt;- tibble(letter = solution) |&gt;\n        group_by(letter) |&gt;\n        summarize(n_in_solution = n())\n    counts &lt;- inner_join(guess_count, solution_count, by = \"letter\") |&gt;\n        mutate(to_clear = n_in_guess - n_in_solution) |&gt;\n        filter(to_clear &gt; 0) |&gt;\n        select(letter, to_clear)\n\n    for (i in 1:word_length) {\n        # these `case_when()` lines are the workhorse of the function...  they\n        # find if each letter in the guess appears in the solution and if it in\n        # the right place\n        text_result[i] &lt;- case_when(guess[i] %in% solution & guess[i] == solution[i] ~\n            \"*\", guess[i] %in% solution & guess[i] != solution[i] ~ \"+\", guess[i] %nin%\n            solution ~ \"-\")\n        # this `for()` loop then cycles through cases where the guess contains\n        # more of a particular letter than the solution and clears away the\n        # correct number of matches that are in in the solution but in the\n        # wrong position\n        for (j in counts$letter) {\n            if (guess[i] == j & text_result[i] != \"*\" & counts[counts$letter == j,\n                ]$to_clear &gt; 0) {\n                text_result[i] &lt;- \"-\"\n                counts[counts$letter == j, ]$to_clear &lt;- counts[counts$letter ==\n                  j, ]$to_clear - 1\n            }\n        }\n    }\n    return(text_result)\n}",
    "crumbs": [
      "Exercises",
      "Exercise 04 Solution"
    ]
  },
  {
    "objectID": "exercise-05.html",
    "href": "exercise-05.html",
    "title": "Exercise 05",
    "section": "",
    "text": "Generate Sampling Distributions and CIs",
    "crumbs": [
      "Exercises",
      "Exercise 05"
    ]
  },
  {
    "objectID": "exercise-05.html#learning-objectives",
    "href": "exercise-05.html#learning-objectives",
    "title": "Exercise 05",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nGenerating sampling distributions by simulation\nCalculating standard errors\nPlotting with {ggplot2}\nExamining the distributions of variables\nBootstrapping to derive a CI\n\n\nPreliminaries\n\nSet up a new GitHub repo in your GitHub workspace named ‚Äúexercise-05‚Äù and clone that down to your computer as a new RStudio project. The instructions outlined as Method 1 in Module 6 will be helpful.",
    "crumbs": [
      "Exercises",
      "Exercise 05"
    ]
  },
  {
    "objectID": "exercise-05.html#challenge-1",
    "href": "exercise-05.html#challenge-1",
    "title": "Exercise 05",
    "section": "Challenge 1",
    "text": "Challenge 1\n\nStep 1\n\nUsing the {tidyverse} read_csv() function, load the ‚ÄúIMDB-movies.csv‚Äù dataset from this URL as a ‚Äútibble‚Äù named d\n\n\n\nStep 2\n\nUse a one-line statement to filter the dataset to include just movies from 1920 to 1979 and movies that are between 1 and 3 hours long (runtimeMinutes &gt;= 60 and runtimeMinutes &lt;= 180), and add a new column that codes the startYear into a new variable, decade (‚Äú20s‚Äù, ‚Äú30s‚Äù, ‚Ä¶‚Äú70s‚Äù). If you do this correctly, there should be 5651 movies remaining in the dataset.\n\n\nHINT: Use {dplyr} functions and the pipe operator!\n\n\n\nStep 3\n\nUse {ggplot2} (which is part of {tidyverse}) to plot histograms of the distribution of runtimeMinutes for each decade.\n\n\nHINT: Try using facet_wrap() to do this!\n\n\n\nStep 4\n\nUse a one-line statement to calculate the population mean and population standard deviation in runtimeMinutes for each decade and save the results in a new dataframe called results.\n\n\n\nStep 5\n\nDraw a single sample of 100 movies, without replacement, from each decade and calculate the single sample mean and single sample standard deviation in runtimeMinutes for each decades. Recall that your single sample mean for each decade is an estimate of the population mean for each decade.\n\n\nHINT: The {dplyr} functions, sample_n() (which is being deprecated) and its replacement, slice_sample(), lets you randomly sample rows from tabular data the same way that sample() lets you sample items from a vector.\n\n\n\nStep 6\n\nCalculate for each decade the standard error around your estimate of the population mean runtimeMinutes based on the standard deviation and sample size (n=100 movies) of your single sample.\n\n\n\nStep 7\n\nCompare these estimates to the actual population mean runtimeMinutes for each decade and to the calculated SE in the population mean for samples of size 100 based on the population standard deviation for each decade.\n\n\n\nStep 8\n\nGenerate a sampling distribution of mean runtimeMinutes for each decade by [a] drawing 1000 random samples of 100 movies from each decade, without replacement, and, for each sample, [b] calculating the mean runtimeMinutes and the standard deviation in runtimeMinutes for each decade. Use either a standard for( ){ } loop, the do(reps) * formulation from {mosaic}, the rerun() function from {purrr}, or the rep_sample_n() workflow from {infer} to generate your these sampling distributions (see Module 16).\n\n\n\nStep 9\n\nThen, calculate the mean and the standard deviation of the sampling distribution of sample means for each decade (the former should be a very good estimate of the population mean, while the latter is another estimate of the standard error in our estimate of the population mean for a particular sample size) and plot a histogram of the sampling distribution for each decade. What shape does it have?\n\n\n\nStep 10\n\nFinally, compare the standard error in runtimeMinutes for samples of size 100 from each decade [1] as estimated from your first sample of 100 movies, [2] as calculated from the known population standard deviations for each decade, and [3] as estimated from the sampling distribution of sample means for each decade.",
    "crumbs": [
      "Exercises",
      "Exercise 05"
    ]
  },
  {
    "objectID": "exercise-05.html#challenge-2",
    "href": "exercise-05.html#challenge-2",
    "title": "Exercise 05",
    "section": "Challenge 2",
    "text": "Challenge 2\n\n\nStep 1\n\nUsing the {tidyverse} read_csv() function, load the ‚Äúzombies.csv‚Äù dataset from this URL as a ‚Äútibble‚Äù named z. This dataset includes the first and last name and gender of the entire population of 1000 people who have survived the zombie apocalypse and are now ekeing out an existence somewhere on the Gulf Coast, along with several other variables (height, weight, age, number of years of education, number of zombies they have killed, and college major). See here for info on important post-zombie apocalypse majors!\n\n\n\nStep 2\n\nCalculate the population mean and standard deviation for each quantitative random variable in the dataset (height, weight, age, number of zombies killed, and years of education).\n\n\nNOTE: You will not want to use the built in var() and sd() commands as those are for samples.\n\n\n\nStep 3\n\nUse {ggplot} and make boxplots of each of these variables by gender.\n\n\n\nStep 4\n\nUse {ggplot} and make scatterplots of height and weight in relation to age (i.e., use age as the \\(x\\) variable), using different colored points for males versus females. Do these variables seem to be related? In what way?\n\n\n\nStep 5\n\nUsing histograms and Q-Q plots, check whether each of the quantitative variables seem to be drawn from a normal distribution. Which seem to be and which do not?\n\n\nHINT: Not all are drawn from a normal distribution! For those that are not, can you determine what common distribution they are drawn from?\n\n\n\nStep 6\n\nNow use the sample_n() or slice_sample() function from {dplyr} to sample ONE subset of 50 zombie apocalypse survivors (without replacement) from this population and calculate the mean and sample standard deviation for each variable. Also estimate the standard error for each variable based on this one sample and use that to construct a theoretical 95% confidence interval for each mean. You can use either the standard normal or a Student‚Äôs t distribution to derive the critical values needed to calculate the lower and upper limits of the CI.\n\n\n\nStep 7\n\nThen draw another 199 random samples of 50 zombie apocalypse survivors out of the population and calculate the mean for each of the these samples. Together with the first sample you drew out, you now have a set of 200 means for each variable (each of which is based on 50 observations), which constitutes a sampling distribution for each variable. What are the means and standard deviations of the sampling distribution for each variable? How do the standard deviations of the sampling distribution for each variable compare to the standard errors estimated from your first sample of size 50?\n\n\n\nStep 8\n\nPlot the sampling distributions for each variable mean. What do they look like? Are they normally distributed? What about for those variables that you concluded were not originally drawn from a normal distribution?\n\n\n\nStep 9\n\nConstruct a 95% confidence interval for each mean directly from the sampling distribution of sample means using the central 95% that distribution (i.e., by setting the lower and upper CI bounds to 2.5% and 97.5% of the way through that distribution).\n\n\nHINT: You will want to use the quantile() function for this!\n\nHow do the various 95% CIs you estimated compare to one another (i.e., the CI based on one sample and the corresponding sample standard deviation versus the CI based on simulation where you created a sampling distribution across 200 samples)?\n\nNOTE: Remember, too, that the standard deviation of the sampling distribution is the standard error. You could use this value to derive yet another estimate for the 95% CI as the shape of the sampling distribution should be normal.\n\n\n\nStep 10\n\nFinally, use bootstrapping to generate a 95% confidence interval for each variable mean by resampling 1000 samples, with replacement, from your original sample (i.e., by setting the lower and upper CI bounds to 2.5% and 97.5% of the way through the sampling distribution generated by bootstrapping). How does this compare to the CIs generated in Step 9?",
    "crumbs": [
      "Exercises",
      "Exercise 05"
    ]
  },
  {
    "objectID": "exercise-05-solution.html",
    "href": "exercise-05-solution.html",
    "title": "Exercise 05 Solution",
    "section": "",
    "text": "‚Ä¢ Solution",
    "crumbs": [
      "Exercises",
      "Exercise 05 Solution"
    ]
  },
  {
    "objectID": "exercise-05-solution.html#challenge-1",
    "href": "exercise-05-solution.html#challenge-1",
    "title": "Exercise 05 Solution",
    "section": "Challenge 1",
    "text": "Challenge 1\n\nlibrary(tidyverse)\nlibrary(mosaic)  # for do() *\nlibrary(infer)\nlibrary(ggpubr)  # for ggqqplot()\nlibrary(cowplot)  # for arranging plots\nlibrary(kableExtra)  # for kable_styling()\n\n# define a function for calculating the population sd of a variable\nsd_pop &lt;- function(x, na.rm = TRUE) {\n    # adding an argument for na.rm makes the function more robust\n    if (na.rm == TRUE) {\n        x &lt;- x[!is.na(x)]\n    }\n    sqrt(sum((x - mean(x))^2)/(length(x)))\n}\n\n\nStep 1\n\nUsing the {tidyverse} read_csv() function, load the ‚ÄúIMDB-movies.csv‚Äù dataset from this URL as a ‚Äútibble‚Äù named d.\n\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/IMDB-movies.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\n\n## Rows: 28938 Columns: 10\n## ‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n## Delimiter: \",\"\n## chr (6): tconst, titleType, primaryTitle, genres, nconst, director\n## dbl (4): startYear, runtimeMinutes, averageRating, numVotes\n## \n## ‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n## ‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nStep 2\n\nUse a one-line statement to filter the dataset to include just movies from 1920 to 1979 and movies that are between 1 and 3 hours long (runtimeMinutes &gt;= 60 and runtimeMinutes &lt;= 180), and add a new column that codes the startYear into a new variable, decade.\n\n\nd &lt;- d |&gt;\n    filter(runtimeMinutes &gt;= 60 & runtimeMinutes &lt;= 180 & startYear %in% 1920:1979) |&gt;\n    mutate(decade = case_when(startYear %in% 1920:1929 ~ \"20s\", startYear %in% 1930:1939 ~\n        \"30s\", startYear %in% 1940:1949 ~ \"40s\", startYear %in% 1950:1959 ~ \"50s\",\n        startYear %in% 1960:1969 ~ \"60s\", startYear %in% 1970:1979 ~ \"70s\"))\nnrow(d)\n\n## [1] 5651\n\n\n\n\nStep 3\n\nUse {ggplot2} (which is part of {tidyverse}) to plot histograms of the distribution of runtimeMinutes for each decade.\n\n\n(p &lt;- ggplot(data = d, aes(x = runtimeMinutes)) + geom_histogram(stat = \"bin\", bins = 20,\n    colour = \"black\", fill = \"lightblue\")) + facet_wrap(~decade)\n\n\n\n\n\n\n\n\n\n\nStep 4\n\nUse a one-line statement to calculate the population mean and population standard deviation in runtimeMinutes for each decade and save the results in a new dataframe called results.\n\n\nresults &lt;- d |&gt;\n    group_by(decade) |&gt;\n    dplyr::summarise(pop_n = n(), pop_mean = mean(runtimeMinutes, na.rm = TRUE),\n        pop_sd = sd_pop(runtimeMinutes, na.rm = TRUE))\n# user-defined function or use `sdpop()` from {radiant}\n\nkable(results, digits = 3) |&gt;\n    kable_styling(font_size = 12, full_width = FALSE)\n\n\n\n\ndecade\npop_n\npop_mean\npop_sd\n\n\n\n\n20s\n152\n96.257\n26.115\n\n\n30s\n530\n90.300\n17.272\n\n\n40s\n782\n97.203\n19.111\n\n\n50s\n1081\n98.948\n19.198\n\n\n60s\n1386\n105.586\n21.224\n\n\n70s\n1720\n103.750\n17.954\n\n\n\n\n\n\n\n\n\nStep 5 and 6\n\nDraw a single sample of 100 movies, without replacement, from each decade and calculate the single sample mean and single sample standard deviation in runtimeMinutes for each decades.\nCalculate for each decade the standard error around your estimate of the population mean runtimeMinutes based on the standard deviation and sample size (n=100 movies) of your single sample.\n\n\nn &lt;- 100\nset.seed(1)\ns &lt;- d |&gt;\n    group_by(decade) |&gt;\n    sample_n(n, replace = FALSE) |&gt;\n    # or use `slice_sample()` instead of `sample_n()` if we use\n    # `slice_sample()`, we need to pass it either the number of rows ('n=') or\n    # the proportion of rows ('p=') as an argument, i.e., `slice_sample(n=100,\n    # replace=FALSE)`\ndplyr::summarise(samp_size = n(), samp_1_mean = mean(runtimeMinutes, na.rm = TRUE),\n    samp_1_sd = sd(runtimeMinutes, na.rm = TRUE), samp_1_se = sd(runtimeMinutes,\n        na.rm = TRUE)/sqrt(samp_size))\n\nkable(s, digits = 3) |&gt;\n    kable_styling(font_size = 12, full_width = FALSE)\n\n\n\n\ndecade\nsamp_size\nsamp_1_mean\nsamp_1_sd\nsamp_1_se\n\n\n\n\n20s\n100\n97.87\n27.728\n2.773\n\n\n30s\n100\n88.05\n15.331\n1.533\n\n\n40s\n100\n94.27\n18.406\n1.841\n\n\n50s\n100\n100.60\n18.703\n1.870\n\n\n60s\n100\n107.74\n22.977\n2.298\n\n\n70s\n100\n102.71\n15.252\n1.525\n\n\n\n\n\n\n\n\n\nStep 7\n\nCompare these estimates to the actual population mean runtimeMinutes for each decade and to the calculated SE in the population mean for samples of size 100 based on the population standard deviation for each decade.\n\n\nresults &lt;- inner_join(s, results, by = \"decade\") |&gt;\n    mutate(pop_se = pop_sd/sqrt(n))\n\nkable(results, digits = 3) |&gt;\n    kable_styling(font_size = 12, full_width = FALSE)\n\n\n\n\ndecade\nsamp_size\nsamp_1_mean\nsamp_1_sd\nsamp_1_se\npop_n\npop_mean\npop_sd\npop_se\n\n\n\n\n20s\n100\n97.87\n27.728\n2.773\n152\n96.257\n26.115\n2.612\n\n\n30s\n100\n88.05\n15.331\n1.533\n530\n90.300\n17.272\n1.727\n\n\n40s\n100\n94.27\n18.406\n1.841\n782\n97.203\n19.111\n1.911\n\n\n50s\n100\n100.60\n18.703\n1.870\n1081\n98.948\n19.198\n1.920\n\n\n60s\n100\n107.74\n22.977\n2.298\n1386\n105.586\n21.224\n2.122\n\n\n70s\n100\n102.71\n15.252\n1.525\n1720\n103.750\n17.954\n1.795\n\n\n\n\n\n\n\n\n\nStep 8 and 9\n\nGenerate a sampling distribution of mean runtimeMinutes for each decade by [a] drawing 1000 samples of 100 movies from each decade and, for each sample, [b] calculating the mean runtimeMinutes and the standard deviation in runtimeMinutes for each decade.\n\n\nn &lt;- 100\nreps &lt;- 1000\n# using {mosaic}\ns &lt;- {\n    do(reps) * sample_n(group_by(d, decade), n, replace = FALSE)\n} |&gt;\n    # `do(rep) *` needs to be wrapped in braces in order to introduce the\n    # variable '.index' that we need to then pass as an argument to\n    # `group_by()`\ngroup_by(decade, .index) |&gt;\n    summarise(avg_runtimeMinutes = mean(runtimeMinutes, na.rm = TRUE), sd_runtimeMinutes = sd(runtimeMinutes,\n        na.rm = TRUE))\n\n# or...  using {purrr}\ns &lt;- map(1:reps, ~mean(runtimeMinutes ~ decade, data = sample_n(group_by(d, decade),\n    size = n, replace = FALSE))) |&gt;\n    bind_rows() |&gt;\n    pivot_longer(cols = everything(), names_to = \"decade\", values_to = \"avg_runtimeMinutes\")\n\n# or... (and this runs fastest) using {infer}\ns &lt;- tibble(decade = character(), replicate = numeric(), avg_runtimeMinutes = numeric(),\n    sd_runtimeMinutes = numeric())\n\nfor (i in unique(d$decade)) {\n    df &lt;- d |&gt;\n        filter(decade == i) |&gt;\n        rep_sample_n(size = n, reps = reps, replace = FALSE) |&gt;\n        group_by(replicate) |&gt;\n        summarize(avg_runtimeMinutes = mean(runtimeMinutes, na.rm = TRUE), sd_runtimeMinutes = sd(runtimeMinutes,\n            na.rm = TRUE)) |&gt;\n        mutate(decade = i)\n    s &lt;- bind_rows(s, df)\n}\n\n\n(p &lt;- ggplot(data = s, aes(avg_runtimeMinutes)) + geom_histogram(stat = \"bin\", bins = 20,\n    colour = \"black\", fill = \"lightblue\") + facet_wrap(~decade, scales = \"free_x\"))\n\n\n\n\n\n\n\n# now, distributions are not obviously different from NORMAL :)\n\n\n\nStep 10\n\nFinally, compare the standard error in runtimeMinutes for samples of size 100 from each decade [1] as estimated from your first sample of 100 movies, [2] as calculated from the known population standard deviations for each decade, and [3] as estimated from the sampling distribution of sample means for each decade.\n\n\nsamp_dist_stats &lt;- s |&gt;\n    group_by(decade) |&gt;\n    summarize(samp_dist_mean = mean(avg_runtimeMinutes), samp_dist_sd = sd(avg_runtimeMinutes))\n\ncomparison &lt;- inner_join(results, samp_dist_stats, by = \"decade\") |&gt;\n    dplyr::select(decade, pop_se, samp_1_se, samp_dist_sd)\n\nkable(comparison, digits = 3) |&gt;\n    kable_styling(font_size = 12, full_width = FALSE)\n\n\n\n\ndecade\npop_se\nsamp_1_se\nsamp_dist_sd\n\n\n\n\n20s\n2.612\n2.773\n1.495\n\n\n30s\n1.727\n1.533\n1.512\n\n\n40s\n1.911\n1.841\n1.826\n\n\n50s\n1.920\n1.870\n1.897\n\n\n60s\n2.122\n2.298\n2.095\n\n\n70s\n1.795\n1.525\n1.769\n\n\n\n\n\n\n\nThese different estimates of the SE are all pretty close to one another, except for the ‚Äú20s‚Äù, where the total number of movies in the decade is relatively small.",
    "crumbs": [
      "Exercises",
      "Exercise 05 Solution"
    ]
  },
  {
    "objectID": "exercise-05-solution.html#challenge-2",
    "href": "exercise-05-solution.html#challenge-2",
    "title": "Exercise 05 Solution",
    "section": "Challenge 2",
    "text": "Challenge 2\n\nStep 1\n\nUsing the {tidyverse} read_csv() function, load the ‚Äúzombies.csv‚Äù dataset from this URL as a ‚Äútibble‚Äù named z. This dataset includes the first and last name and gender of the entire population of 1000 people who have survived the zombie apocalypse and are now ekeing out an existence somewhere on the Gulf Coast, along with several other variables (height, weight, age, number of years of education, number of zombies they have killed, and college major).\n\n\nn &lt;- 50  # set sample size\nalpha &lt;- 0.05  # set alpha\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/zombies.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\n\nsurvivors &lt;- dplyr::select(d, \"gender\", \"height\", \"weight\", \"age\", \"zombies_killed\",\n    \"years_of_education\")\n\nkable(head(survivors), digits = 3) |&gt;\n    kable_styling(font_size = 12, full_width = FALSE)\n\n\n\n\ngender\nheight\nweight\nage\nzombies_killed\nyears_of_education\n\n\n\n\nFemale\n62.890\n132.087\n17.643\n2\n1\n\n\nMale\n67.803\n146.375\n22.590\n5\n3\n\n\nMale\n72.129\n152.937\n21.913\n1\n1\n\n\nMale\n66.785\n129.742\n18.191\n5\n6\n\n\nFemale\n64.718\n132.426\n21.104\n4\n3\n\n\nMale\n71.243\n152.525\n21.484\n1\n4\n\n\n\n\n\n\n\n\n\nStep 2\n\nCalculate the population mean and standard deviation for each quantitative random variable in the dataset (height, weight, age, number of zombies killed, and years of education).\n\n\npop_means &lt;- dplyr::summarise(survivors, mean(height), mean(weight), mean(age), mean(zombies_killed),\n    mean(years_of_education))  # {dplyr} rocks!\n\npop_sds &lt;- dplyr::summarise(survivors, sd_pop(height), sd_pop(weight), sd_pop(age),\n    sd_pop(zombies_killed), sd_pop(years_of_education))  # {dplyr} rocks!\n\n# or, using `sdpop()` from {radiant} pop_SDs &lt;- dplyr::summarise(survivors,\n# sdpop(height), sdpop(weight), sdpop(age), sdpop(zombies_killed),\n# sdpop(years_of_education))\n\n# ALTERNATIVELY...  we can use new dplyr::summarise(across()) construction this\n# takes two arguments, .cols and .fns .cols is a vector of columns to apply the\n# functions to e.g., c(height, weight, age, zombies_killed, years_of_education)\n# .funs is either...  [a] a single function, e.g., `mean`, [b] a purrr style\n# lambda, e.g., ~ mean(.), [c] a list of functions, e.g., `list(mean=mean,\n# sd=sd)` [d] a list of lambdas, e.g., `list(~ mean(.), ~sd(.)) .funs are\n# applied across the set of .cols\n\n# [a]\npop_means_a &lt;- survivors |&gt;\n    dplyr::summarise(across(c(height, weight, age, zombies_killed, years_of_education),\n        mean))\n\n# [b]\npop_means_b &lt;- survivors |&gt;\n    dplyr::summarise(across(c(height, weight, age, zombies_killed, years_of_education),\n        ~mean(.)))\n\n# [c]\npop_means_c &lt;- survivors |&gt;\n    dplyr::summarise(across(c(height, weight, age, zombies_killed, years_of_education),\n        list(mean = mean)))\n\n# [d]\npop_means_d &lt;- survivors |&gt;\n    dplyr::summarise(across(c(height, weight, age, zombies_killed, years_of_education),\n        list(~mean(.))))\n\npop_ses &lt;- pop_sds/sqrt(n)  # not needed yet, but will be later\n\n# make a pretty table by transposing the 1-row dataframes above\nvariables &lt;- c(\"Height\", \"Weight\", \"Age\", \"Kills\", \"Years of Ed\")\n\npop_summary &lt;- bind_cols(variables, t(pop_means), t(pop_sds))\n\nnames(pop_summary) &lt;- c(\"variable\", \"pop_mean\", \"pop_sd\")\n\nkable(pop_summary, digits = 3) |&gt;\n    kable_styling(font_size = 12, full_width = FALSE)\n\n\n\n\nvariable\npop_mean\npop_sd\n\n\n\n\nHeight\n67.630\n4.308\n\n\nWeight\n143.907\n18.392\n\n\nAge\n20.047\n2.964\n\n\nKills\n2.992\n1.748\n\n\nYears of Ed\n2.996\n1.676\n\n\n\n\n\n\n\n\n\nStep 3\n\nUse {ggplot} and make boxplots of each of these variables by gender.\n\n\np1 &lt;- ggplot(data = survivors, aes(x = gender, y = height))\np1 &lt;- p1 + geom_boxplot(aes(colour = factor(gender)))\np1 &lt;- p1 + theme(legend.position = \"none\")\np2 &lt;- ggplot(data = survivors, aes(x = gender, y = weight))\np2 &lt;- p2 + geom_boxplot(aes(colour = factor(gender)))\np2 &lt;- p2 + theme(legend.position = \"none\")\np3 &lt;- ggplot(data = survivors, aes(x = gender, y = age))\np3 &lt;- p3 + geom_boxplot(aes(colour = factor(gender)))\np3 &lt;- p3 + theme(legend.position = \"none\")\np4 &lt;- ggplot(data = survivors, aes(x = gender, y = zombies_killed))\np4 &lt;- p4 + geom_boxplot(aes(colour = factor(gender)))\np4 &lt;- p4 + theme(legend.position = \"none\")\np5 &lt;- ggplot(data = survivors, aes(x = gender, y = years_of_education))\np5 &lt;- p5 + geom_boxplot(aes(colour = factor(gender)))\np5 &lt;- p5 + theme(legend.position = \"none\")\n\n\nplot_grid(p1, p2, p3, p4, p5, nrow = 2)\n\n\n\n\n\n\n\n# or...  (p11 &lt;- ggplot(data=pivot_longer( survivors, cols = c(height, weight,\n# age, zombies_killed, years_of_education), names_to ='variable', values_to =\n# 'value'), aes(x=factor(gender),y=value)) +\n# geom_boxplot(aes(colour=factor(gender)))+ facet_wrap(~ variable, scales =\n# 'free') + xlab('') + ylab('Frequency') + theme(legend.position = 'none'))\n\n\n\nStep 4\n\nUse {ggplot} and make scatterplots of height and weight in relation to age (i.e., use age as the \\(x\\) variable), using different colored points for males versus females. Do these variables seem to be related? In what way?\n\n\np1 &lt;- ggplot(data = survivors, aes(x = age, y = height, colour = factor(gender))) +\n    geom_point() + theme(legend.position = \"bottom\", legend.title = element_blank())\np2 &lt;- ggplot(data = survivors, aes(x = age, y = weight, colour = factor(gender))) +\n    geom_point() + theme(legend.position = \"bottom\", legend.title = element_blank())\n\n\nplot_grid(p1, p2, nrow = 1)\n\n\n\n\n\n\n\n\nBoth seem to be positive linear functions of age.\n\n\nStep 5\n\nUsing histograms and Q-Q plots, check whether the quantitative variables seem to be drawn from a normal distribution.\n\n\n# plot the distributions\np1 &lt;- ggplot(data = survivors, aes(x = height)) + geom_histogram(bins = 20) + ggtitle(\"Height\")\np2 &lt;- ggqqplot(data = survivors, x = \"height\")\np3 &lt;- ggplot(data = survivors, aes(x = weight)) + geom_histogram(bins = 20) + ggtitle(\"Weight\")\np4 &lt;- ggqqplot(data = survivors, x = \"weight\")\np5 &lt;- ggplot(data = survivors, aes(x = age)) + geom_histogram(bins = 20) + ggtitle(\"Age\")\np6 &lt;- ggqqplot(data = survivors, x = \"age\")\np7 &lt;- ggplot(data = survivors, aes(x = zombies_killed)) + geom_histogram(binwidth = 1) +\n    ggtitle(\"Zombies Killed\")\np8 &lt;- ggqqplot(data = survivors, x = \"zombies_killed\")\np9 &lt;- ggplot(data = survivors, aes(x = years_of_education)) + geom_histogram(binwidth = 1) +\n    ggtitle(\"Years of Education\")\np10 &lt;- ggqqplot(data = survivors, x = \"years_of_education\")\n\n\nplot_grid(p1, p3, p5, p2, p4, p6, nrow = 2)\n\n\n\n\n\n\n\nplot_grid(p7, p9, p8, p10, nrow = 2)\n\n\n\n\n\n\n\n\nThe first three are seemingly drawn from normal distributions, but not the latter two. These are discrete variables, and they seem to be drawn from the Poisson distribution.\n\n\nStep 6\n\nNow use the sample_n() or slice_sample() function from {dplyr} to sample ONE subset of 50 zombie apocalypse survivors (without replacement) from this population and calculate the mean and sample standard deviation for each variable. Also estimate the standard error for each variable based on this sample and use that to construct a 95% confidence interval for each mean. You can use either the standard normal or a Student‚Äôs t distribution to derive the critical values needed to calculate the lower and upper limits of the CI. [As an additional alternative, you could estimate a CI by bootstrap resampling from that first sample as well.]\n\nFirst, we create some functions for SEs and CIs\n\nse &lt;- function(x, type = \"normal\") {\n    if (type == \"normal\") {\n        se &lt;- sd(x)/sqrt(length(x))\n    }\n    if (type == \"poisson\") {\n        se &lt;- sqrt(mean(x)/length(x))\n        # mean(x) is estimate of lambda\n    }\n    return(se)\n}\n\nci_norm &lt;- function(x, alpha = 0.05) {\n    ci &lt;- (mean(x) + c(-1, 1) * qnorm(1 - alpha/2) * se(x)) |&gt;\n        round(3)\n    # confidence interval based on normal distribution\n    ci &lt;- paste0(\"[\", ci[1], \"-\", ci[2], \"]\")\n    names(ci) &lt;- \"CI\"\n    return(ci)\n}\n\nci_t &lt;- function(x, alpha = 0.05) {\n    ci &lt;- (mean(x) + c(-1, 1) * qt(1 - alpha/2, length(x) - 1) * se(x)) |&gt;\n        round(3)\n    # confidence interval based on t distribution\n    ci &lt;- paste0(\"[\", ci[1], \"-\", ci[2], \"]\")\n    names(ci) &lt;- \"CI\"\n    return(ci)\n}\n\nci_boot &lt;- function(x, alpha = 0.05, n_boot = 1000) {\n    boot &lt;- NULL\n    for (i in 1:n_boot) {\n        boot[i] &lt;- mean(sample(x, length(x), replace = TRUE))\n    }\n    ci &lt;- quantile(boot, c(alpha/2, 1 - alpha/2)) |&gt;\n        round(3)\n    ci &lt;- paste0(\"[\", ci[1], \"-\", ci[2], \"]\")\n    names(ci) &lt;- \"CI\"\n    return(ci)\n}\n\nThen, we get a sample of size n from survivors and calculate statistics‚Ä¶\n\nset.seed(1)  # setting the seed makes the random draws the sample across runs of code\nn &lt;- 50\ns &lt;- survivors |&gt;\n    sample_n(size = n, replace = FALSE)\n# or... `s &lt;- survivors |&gt; slice_sample(n = n, replace = FALSE)` head(s) #\n# uncomment to show start of first sample of size n\n\nsamp_1_means &lt;- s |&gt;\n    dplyr::summarise(across(.cols = c(height, weight, age, zombies_killed, years_of_education),\n        .fns = ~mean(.)))\n\nsamp_1_SDs &lt;- s |&gt;\n    dplyr::summarise(across(.cols = c(height, weight, age, zombies_killed, years_of_education),\n        .fns = ~sd(.)))\n\nsamp_1_SEs &lt;- s |&gt;\n    dplyr::summarise(across(.cols = c(height, weight, age), .fns = ~se(., type = \"normal\")),\n        across(.cols = c(zombies_killed, years_of_education), .fns = ~se(., type = \"poisson\")))\n\n# create a tibble of CIs based on normal distribution\nsamp_1_CI_norm &lt;- s |&gt;\n    dplyr::summarise(across(.cols = c(height, weight, age, zombies_killed, years_of_education),\n        .fns = ~ci_norm(.)))\n\n# create a tibble of CIs based on t distribution\nsamp_1_CI_t &lt;- s |&gt;\n    dplyr::summarise(across(.cols = c(height, weight, age, zombies_killed, years_of_education),\n        .fns = ~ci_t(.)))\n\n# create a tibble of CIs based on bootstrapping\nsamp_1_CI_boot &lt;- s |&gt;\n    dplyr::summarise(across(.cols = c(height, weight, age, zombies_killed, years_of_education),\n        .fns = ~ci_boot(., n_boot = 1000)))\n\n# make a pretty table\nsamp_1_stats &lt;- bind_rows(samp_1_means, samp_1_SDs, samp_1_SEs)\nsamp_1_CIs &lt;- bind_rows(samp_1_CI_norm, samp_1_CI_t, samp_1_CI_boot)\n\nsamp_1_summary &lt;- bind_cols(variables, t(samp_1_stats), t(samp_1_CIs))\nnames(samp_1_summary) &lt;- c(\"Variable\", \"Samp 1 mean\", \"Samp 1 SD\", \"Samp 1 SE\", \"Samp 1 CI norm\",\n    \"Samp 1 CI t\", \"Samp 1 CI boot\")\n\nkable(samp_1_summary, digits = 3) |&gt;\n    kable_styling(font_size = 12, full_width = FALSE)\n\n\n\n\nVariable\nSamp 1 mean\nSamp 1 SD\nSamp 1 SE\nSamp 1 CI norm\nSamp 1 CI t\nSamp 1 CI boot\n\n\n\n\nHeight\n67.302\n4.379\n0.619\n[66.088-68.516]\n[66.057-68.546]\n[66.072-68.535]\n\n\nWeight\n143.466\n20.807\n2.943\n[137.699-149.234]\n[137.553-149.38]\n[137.759-148.981]\n\n\nAge\n20.088\n3.106\n0.439\n[19.227-20.948]\n[19.205-20.97]\n[19.298-20.924]\n\n\nKills\n3.080\n1.850\n0.248\n[2.567-3.593]\n[2.554-3.606]\n[2.62-3.62]\n\n\nYears of Ed\n3.040\n1.564\n0.247\n[2.606-3.474]\n[2.595-3.485]\n[2.639-3.46]\n\n\n\n\n\n\n\n\n\nStep 7\n\nThen draw another 199 random samples of 50 zombie apocalypse survivors out of the population and calculate the mean for each of the these samples. Together with the first sample you drew out, you now have a set of 200 means for each variable (each based on 50 observations), which constitutes a sampling distribution for each variable. What are the means and standard deviations of the sampling distribution for each variable?\n\n\nk &lt;- 199  # additional # of sample sets\n# using {mosaic}\nadditional_samples &lt;- do(k) * sample_n(survivors, size = n, replace = FALSE)\n# or `slice_sample(survivors, n = n, replace = FALSE)` each row will have a\n# single individual (with n = 50 rows per sample) and the .index column\n# contains the replicate number\n\n# now, add a .row and .index column to our original sample\ns &lt;- s |&gt;\n    mutate(.row = 1:n, .index = k + 1)\n\n# and bind the additional + original samples into a single data frame\nall_s &lt;- bind_rows(additional_samples, s)\n\nsamp_dist &lt;- all_s |&gt;\n    group_by(.index) |&gt;\n    dplyr::summarise(across(.cols = c(height, weight, age, zombies_killed, years_of_education),\n        .fns = ~mean(.))) |&gt;\n    dplyr::select(-.index)\n\nsamp_SEs &lt;- all_s |&gt;\n    group_by(.index) |&gt;\n    dplyr::summarise(across(.cols = c(height, weight, age), .fns = ~se(., type = \"normal\")),\n        across(.cols = c(zombies_killed, years_of_education), .fns = ~se(., type = \"poisson\"))) |&gt;\n    dplyr::select(-.index)\n\n# head(sampling_distribution) # uncomment to show start of sampling\n# distributions\n\nsamp_dist_means &lt;- samp_dist |&gt;\n    dplyr::summarise(across(.cols = everything(), .fns = ~mean(.)))\n\nsamp_dist_SDs &lt;- samp_dist |&gt;\n    dplyr::summarise(across(.cols = everything(), .fns = ~sd(.)))\n\n# make a pretty table\nvariables &lt;- c(\"Height\", \"Weight\", \"Age\", \"Kills\", \"Years of Ed\")\nsamp_dist_summary &lt;- bind_cols(variables, t(samp_dist_means), t(samp_dist_SDs))\nnames(samp_dist_summary) &lt;- c(\"Variable\", \"Samp Dist mean\", \"Samp Dist SD\")\n\nkable(samp_dist_summary, digits = 3) |&gt;\n    kable_styling(font_size = 12, full_width = FALSE)\n\n\n\n\nVariable\nSamp Dist mean\nSamp Dist SD\n\n\n\n\nHeight\n67.559\n0.551\n\n\nWeight\n143.747\n2.371\n\n\nAge\n20.039\n0.381\n\n\nKills\n3.016\n0.233\n\n\nYears of Ed\n2.996\n0.240\n\n\n\n\n\n\n\n\nHow do the standard deviations of the sampling distribution for each variable compare to the standard errors estimated from your first sample of size 50?\n\n\nsamp_SE_means &lt;- samp_SEs |&gt;\n    dplyr::summarise(across(.cols = everything(), .fns = ~mean(.)))\n\n# again, make a pretty table\ncompare_SEs &lt;- tibble(Variable = c(\"Height\", \"Weight\", \"Age\", \"Kills\", \"Years of Ed\"),\n    samp_dist_mean = samp_dist_summary$`Samp Dist mean`, `**Samp Dist SD**` = samp_dist_summary$`Samp Dist SD`,\n    `SE from Pop SD` = t(pop_sds/sqrt(n)) |&gt;\n        round(3), `**Samp 1 SE**` = samp_1_summary$`Samp 1 SE`, `Mean SE across Samples` = t(samp_SE_means) |&gt;\n        round(3))\nrownames(compare_SEs) &lt;- NULL  # get rid of rownames to make table pretty\n\nkable(compare_SEs, digits = 3) |&gt;\n    kable_styling(font_size = 12, full_width = FALSE)\n\n\n\n\nVariable\nsamp_dist_mean\n**Samp Dist SD**\nSE from Pop SD\n**Samp 1 SE**\nMean SE across Samples\n\n\n\n\nHeight\n67.559\n0.551\n0.609\n0.619\n0.608\n\n\nWeight\n143.747\n2.371\n2.601\n2.943\n2.587\n\n\nAge\n20.039\n0.381\n0.419\n0.439\n0.413\n\n\nKills\n3.016\n0.233\n0.247\n0.248\n0.245\n\n\nYears of Ed\n2.996\n0.240\n0.237\n0.247\n0.245\n\n\n\n\n\n\n\nThese should all be about the same! As the size of each of the k samples increases, the SD of the sampling distribution for each variable should converge to the population estimate of the standard error, i.e., to SD.pop/sqrt(n), or \\(\\frac{\\sigma}{\\sqrt{n}}\\). The SE for each variable within each sample should be an estimator of this standard error, and the mean SE across samples should be really close to the population estimate.\n\nNOTE: The columns in this table with asterisks (‚Äú**‚Äú) are those that you were specifically asked to compare.\n\n\n\nStep 8\n\nPlot the sampling distributions for each variable mean. What do they look like? Are they normally distributed? What about for those variables that you concluded were not originally drawn from a normal distribution?\n\n\n# plot the distributions\np1 &lt;- ggplot(data = samp_dist, aes(x = height)) + geom_histogram(bins = 10) + ggtitle(\"Height Means\")\np2 &lt;- ggqqplot(data = samp_dist, x = \"height\")\np3 &lt;- ggplot(data = samp_dist, aes(x = weight)) + geom_histogram(bins = 10) + ggtitle(\"Weight Means\")\np4 &lt;- ggqqplot(data = samp_dist, x = \"weight\")\np5 &lt;- ggplot(data = samp_dist, aes(x = age)) + geom_histogram(bins = 10) + ggtitle(\"Age Means\")\np6 &lt;- ggqqplot(data = samp_dist, x = \"age\")\np7 &lt;- ggplot(data = samp_dist, aes(x = zombies_killed)) + geom_histogram(bins = 10) +\n    ggtitle(\"Zombies Killed Means\")\np8 &lt;- ggqqplot(data = samp_dist, x = \"zombies_killed\")\np9 &lt;- ggplot(data = samp_dist, aes(x = years_of_education)) + geom_histogram(bins = 10) +\n    ggtitle(\"Years of Education Means\")\np10 &lt;- ggqqplot(data = samp_dist, x = \"years_of_education\")\n\n\nplot_grid(p1, p3, p5, p2, p4, p6, nrow = 2)\n\n\n\n\n\n\n\nplot_grid(p7, p9, p8, p10, nrow = 2)\n\n\n\n\n\n\n\n\nThese all look pretty normally distributed, even for those variables that were not drawn from a normal distribution initially! This becomes even more apparent if we set k to a higher number, e.g., 1000.\n\n\nStep 9\n\nConstruct a 95% confidence interval for each mean directly from the sampling distribution of sample means using the central 95% that distribution.\n\n\n# Here, we use the `quantile()` function... first create a function to pull out\n# CI\nci_quant &lt;- function(x, level = 0.95) {\n    ci &lt;- quantile(x, c((1 - level)/2, 1 - (1 - level)/2)) |&gt;\n        round(3)\n    ci &lt;- paste0(\"[\", ci[1], \"-\", ci[2], \"]\")\n    names(ci) &lt;- \"CI\"\n    return(ci)\n}\n\nsamp_dist_CI &lt;- dplyr::summarise(samp_dist, across(.cols = everything(), .fns = ~ci_quant(.,\n    level = 0.95)))\n\nsamp_dist_summary &lt;- bind_cols(samp_dist_summary, t(samp_dist_CI))\n\nnames(samp_dist_summary) &lt;- c(\"Variable\", \"Samp Dist mean\", \"Samp Dist SD\", \"Samp Dist CI\")\n\nkable(samp_dist_summary, digits = 3) |&gt;\n    kable_styling(font_size = 12, full_width = FALSE)\n\n\n\n\nVariable\nSamp Dist mean\nSamp Dist SD\nSamp Dist CI\n\n\n\n\nHeight\n67.559\n0.551\n[66.503-68.626]\n\n\nWeight\n143.747\n2.371\n[139.071-148.556]\n\n\nAge\n20.039\n0.381\n[19.412-20.828]\n\n\nKills\n3.016\n0.233\n[2.579-3.443]\n\n\nYears of Ed\n2.996\n0.240\n[2.479-3.481]\n\n\n\n\n\n\n\n\nHow do the various 95% CIs you estimated compare to one another (i.e., the CIs based on one sample and the corresponding sample standard deviation versus the CI based on simulation where you created a sampling distribution across 200 samples)?\n\n\n# CIs from Sample 1\ncompare_CIs &lt;- dplyr::select(samp_1_summary, -c(\"Samp 1 mean\", \"Samp 1 SD\", \"Samp 1 SE\")) |&gt;\n    bind_cols(`Samp Dist CI` = samp_dist_summary$`Samp Dist CI`)\n\nkable(compare_CIs, digits = 3) |&gt;\n    kable_styling(font_size = 12, full_width = FALSE)\n\n\n\n\nVariable\nSamp 1 CI norm\nSamp 1 CI t\nSamp 1 CI boot\nSamp Dist CI\n\n\n\n\nHeight\n[66.088-68.516]\n[66.057-68.546]\n[66.072-68.535]\n[66.503-68.626]\n\n\nWeight\n[137.699-149.234]\n[137.553-149.38]\n[137.759-148.981]\n[139.071-148.556]\n\n\nAge\n[19.227-20.948]\n[19.205-20.97]\n[19.298-20.924]\n[19.412-20.828]\n\n\nKills\n[2.567-3.593]\n[2.554-3.606]\n[2.62-3.62]\n[2.579-3.443]\n\n\nYears of Ed\n[2.606-3.474]\n[2.595-3.485]\n[2.639-3.46]\n[2.479-3.481]\n\n\n\n\n\n\n\n\n\nStep 10\n\nFinally, use bootstrapping to generate a 95% confidence interval for each variable mean by bootstrapping 1000 samples, with replacement, from your original sample.\n\n\nNOTE: This was already done in Step 6, where we ran the custom CI_boot() function, and the results are included in the Samp 1 CI boot column in the table above.\n\nThe CI based on the sampling distribution generated via resampling is comparable to those based on the first sample using parametric estimates from a normal and a t distribution, as well as that based on bootstrapping using just the first sample for all of the normally distributed variables (age, height, weight). Even for the Poisson-distributed variables (zombies killed, years of education), the lower and upper bounds for the sampling distribution-based CIs are pretty comparable to those estimated from the first sample by either parametric methods or via bootstrap estimation.",
    "crumbs": [
      "Exercises",
      "Exercise 05 Solution"
    ]
  },
  {
    "objectID": "exercise-06.html",
    "href": "exercise-06.html",
    "title": "Exercise 06",
    "section": "",
    "text": "Practice Simulation-Based Inference",
    "crumbs": [
      "Exercises",
      "Exercise 06"
    ]
  },
  {
    "objectID": "exercise-06.html#learning-objectives",
    "href": "exercise-06.html#learning-objectives",
    "title": "Exercise 06",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nUse a real data set to practice‚Ä¶\n\ngenerating confidence intervals around sample statistics by bootstrapping\ndoing permutation-based tests of independence",
    "crumbs": [
      "Exercises",
      "Exercise 06"
    ]
  },
  {
    "objectID": "exercise-06.html#background-on-the-dataset",
    "href": "exercise-06.html#background-on-the-dataset",
    "title": "Exercise 06",
    "section": "Background on the Dataset",
    "text": "Background on the Dataset\nSpider monkeys (genus Ateles) live in large multimale-multifemale social groups containing a total of ~20-30 adult individuals. Association patterns among the members of these group are very flexible. It is rare to see more than a handful of the adult members of the group together at any given time, and, instead, group members organize themselves in multiple smaller subgroups, or ‚Äúparties‚Äù, that travel separately from one another. Individuals and parties may come together (‚Äúfuse‚Äù), re-assort their membership, and break apart from one another (‚Äúfission‚Äù) multiple times per day. Each individual, then, shows a different pattern of association with other group members and its own pattern of home range use.\nMy research group has collected data on the ranging patterns of one species of spider monkeys (Ateles belzebuth) in Amazonian Ecuador by following focal individuals and recording their location at regular intervals throughout the day using a GPS. We also record information on the other animals associated with focal individuals at those same intervals. This process yields a large set of location records for each individual based on both when those animals are the focus of focal follows and when they are present in subgroups containing a different focal individual.\nUsing location records collected over several years, we have generated several measures of home range size for 9 adult males and 11 adult females who are members of one social group of Ateles belzebuth.\n\nPreliminaries\n\nUsing the {tidyverse} read_csv() function, load this dataset into R as a ‚Äútibble‚Äù named d and look at the variables it contains. For this exercise, we are interested in two variables in particular: sex (‚ÄúM‚Äù or ‚ÄúF‚Äù, for male versus female) and kernel95, which represents the size of a polygon summarizing the location records for an individual as a 95% utilization density kernel.\n\n\n\nStep 1\n\nReduce the dataset to just the two variables of interest.\n\n\n\nStep 2\n\nDetermine the mean, standard deviation, and standard error in ‚Äúkernel95‚Äù home range size for each sex.\n\n\n\nStep 3\n\nCreate boxplots comparing ‚Äúkernel95‚Äù home range size by sex.\n\n\n\nStep 4\n\nFor each sex, generate a bootstrap distribution for mean kernel95 home range size. To do this, for each sex, you will want to generate a set of 10,000 bootstrap samples (i.e., sampling with replacement), calculate the mean kernel95 home range size for each of these samples, and plot the resulting bootstrap sampling distribution.\n\n\n\nStep 5\n\nPlot an appropriate normal distribution over the bootstrap sampling distribution.\n\n\n\nStep 6\n\nCalculate a 95% confidence interval around for the mean kernel95 home range size for each sex‚Ä¶\n\nUsing the quantile() method applied directly to your bootstrap sampling distribution, and‚Ä¶\nUsing the theory-based ‚Äústandard error‚Äù method, based on qnorm() and the standard deviation of your bootstrap sampling distribution.\n\n\n\n\nStep 7\n\nUse simulation-based permutation to evaluate the difference in mean kernel95 home range size for males versus females. To do this, you will want to shuffle either the variable ‚Äúsex‚Äù or ‚Äúkernel95‚Äù home range size a total of 10,000 times, recalculating mean kernel95 size by sex for each permuted sample and then compare the difference in male and female kernel95 means from your original sample to the permutation distribution for the difference in means.\n\nUnder this approach, what is the ‚Äúnull‚Äù hypothesis? What is the test statistic? Is the difference in mean kernel95 home range size ‚Äúsignificant‚Äù?\n\n\n\n\nStep 8\n\nFinally, use a theory-based parametric test (e.g., a t-test) to also calculate an appropriate test statistic and associated p value for the comparison of male and female mean kernel95 home range size. Under this approach, what is the test statistic? Is the difference in mean kernel95 home range size ‚Äúsignificant‚Äù?",
    "crumbs": [
      "Exercises",
      "Exercise 06"
    ]
  },
  {
    "objectID": "exercise-06-solution.html",
    "href": "exercise-06-solution.html",
    "title": "Exercise 06 Solution",
    "section": "",
    "text": "‚Ä¢ Solution\nLoad in dataset and libraries of interest‚Ä¶\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(infer)\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/tbs-2006-2008-ranges.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\nhead(d)\n\n## # A tibble: 6 √ó 9\n##   id       sex   sex.code mcp50 mcp80 mcp95 kernel50 kernel80 kernel95\n##   &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n## 1 Ana      F            1 123.   182.  331.     75.1     185.     330.\n## 2 Andreo   M            2 136.   238.  367.    101.      232.     416.\n## 3 Buka     F            1  98.2  202.  234.     60.5     161.     296.\n## 4 Eva      F            1 104.   193.  352.     82.2     202.     355.\n## 5 Evita    F            1 104.   183.  365.     82.6     190.     367.\n## 6 Geronimo M            2 116.   286.  402.    125.      285.     502.\n\n\n\nStep 1\n\nReduce the dataset to just the two variables of interest.\n\n\nd &lt;- d |&gt;\n    select(sex, kernel95)\nhead(d)\n\n## # A tibble: 6 √ó 2\n##   sex   kernel95\n##   &lt;chr&gt;    &lt;dbl&gt;\n## 1 F         330.\n## 2 M         416.\n## 3 F         296.\n## 4 F         355.\n## 5 F         367.\n## 6 M         502.\n\n\n\n\nStep 2\n\nDetermine the mean, standard deviation, and standard error in ‚Äúkernel95‚Äù home range size for each sex.\n\n\nhr_summary &lt;- d |&gt;\n    group_by(sex) |&gt;\n    summarize(mean = mean(kernel95), sd = sd(kernel95), n = n(), se = sd/sqrt(n))\nhr_summary\n\n## # A tibble: 2 √ó 5\n##   sex    mean    sd     n    se\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n## 1 F      319.  65.7    11  19.8\n## 2 M      430.  58.3     9  19.4\n\n\n\n\nStep 3\n\nCreate boxplots comparing ‚Äúkernel95‚Äù home range size by sex.\n\n\np &lt;- ggplot(data = d, aes(x = sex, y = kernel95)) + geom_boxplot() + geom_jitter()\np\n\n\n\n\n\n\n\n\n\n\nStep 4\n\nFor each sex, generate a bootstrap distribution for mean kernel95 home range size.\n\n\nNOTE: The code below does this for males‚Ä¶ simply replace ‚ÄúM‚Äù with ‚ÄúF‚Äù in the filter() statement to do the same for females.\n\n\n# ci in mean HR size\nn_boot &lt;- 10000\ns &lt;- d |&gt;\n    filter(sex == \"M\")\n\n# option 1 - using {base} R\nboot &lt;- vector()\n# the size of each bootstrap sample should equivalent to the size our original\n# sample\nfor (i in 1:n_boot) {\n    boot[[i]] &lt;- mean(sample(s$kernel95, length(s$kernel95), replace = TRUE))\n}\n\n# option 2 - using {mosaic}\nboot &lt;- do(n_boot) * mean(sample(s$kernel95, size = length(s$kernel95), replace = TRUE))\nboot &lt;- boot$mean  # pull out mean column as vector\n\n# option 3 - using {infer}\nboot &lt;- s %&gt;%\n    rep_sample_n(replace = TRUE, size = nrow(.), reps = 10000) |&gt;\n    group_by(replicate) |&gt;\n    summarize(mean = mean(kernel95))\nboot &lt;- boot$mean  # pull out mean column as vector\n\n\n\nStep 5\n\nPlot the resulting bootstrap sampling distribution and plot an appropriate normal distribution over the bootstrap sampling distribution.\n\n\nse &lt;- sd(boot)\nhistogram(boot)\nplotDist(\"norm\", mean(boot), se, add = TRUE)\n\n\n\n\n\n\n\n\n\n\nStep 6\n\nCalculate a 95% confidence interval around for the mean kernel95 home range size for each sex‚Ä¶\n\nUsing the quantile() method applied directly to your bootstrap sampling distribution, and\nUsing the theory-based ‚Äústandard error‚Äù method, based on qnorm() and the standard deviation of your bootstrap sampling distribution.\n\n\n\n(ci_boot &lt;- c(quantile(boot, 0.025), quantile(boot, 0.975)))\n\n##     2.5%    97.5% \n## 393.7865 466.5024\n\n(ci_theory &lt;- mean(s$kernel95) + c(-1, 1) * qnorm(0.975) * se)\n\n## [1] 393.6588 465.8226\n\n# or\n\n(ci_theory &lt;- mean(s$kernel95) + qnorm(c(0.025, 0.975)) * se)\n\n## [1] 393.6588 465.8226\n\nladd(panel.abline(v = ci_boot, col = \"red\", lty = 3, lwd = 2))\n\nladd(panel.abline(v = ci_theory, col = \"blue\", lty = 1, lwd = 2))\n\n\n\n\n\n\n\n\nWe can also do all of the above using the {infer} package‚Äôs specify() ‚Üí generate() ‚Üí calculate() ‚Üí visualize() workflow‚Ä¶\n\n# option 4 - using {infer}\nboot &lt;- s |&gt;\n    specify(response = kernel95) |&gt;\n    generate(reps = n_boot, type = \"bootstrap\") |&gt;\n    calculate(stat = \"mean\")\n\nci_boot &lt;- boot |&gt;\n    get_confidence_interval(type = \"percentile\", level = 0.95)\n\nci_theory &lt;- boot |&gt;\n    get_confidence_interval(type = \"se\", level = 0.95, point_estimate = mean(s$kernel95))\n\nvisualize(boot) + shade_confidence_interval(endpoints = ci_theory, color = \"blue\",\n    lty = 1, size = 0.5, fill = \"#c0c0c0\") + shade_confidence_interval(endpoints = ci_boot,\n    color = \"red\", lty = 3, size = 0.5, fill = \"#c0c0c0\")\n\n\n\n\n\n\n\n\n\n\nStep 7\n\nUse simulation-based permutation to evaluate the difference in mean kernel95 home range size for males versus females. To do this, you will want to shuffle either the variable ‚Äúsex‚Äù or ‚Äúkernel95‚Äù home range size a total of 10,000 times, recalculating mean kernel95 size by sex for each permuted sample and then compare the difference in male and female kernel95 means from your original sample to the permutation distribution for the difference in means.\n\n\nn_perm &lt;- 10000  # number of permutations\n# create a dummy vector to hold results for each permutation\npermuted_diff &lt;- vector()\npermuted_data &lt;- d\nfor (i in 1:n_perm) {\n    # scramble the sex vector: `sample()` with a vector as an argument yields a\n    # random permutation of the vector\n    permuted_data$sex &lt;- sample(permuted_data$sex)\n    m &lt;- permuted_data[permuted_data$sex == \"M\", ]$kernel95\n    f &lt;- permuted_data[permuted_data$sex == \"F\", ]$kernel95\n    permuted_diff[[i]] &lt;- mean(m) - mean(f)\n}\nhistogram(permuted_diff)\n\n\n\n\n\n\n\nactual_diff &lt;- mean(d[d$sex == \"M\", ]$kernel95) - mean(d[d$sex == \"F\", ]$kernel95)\n\np &lt;- (sum(permuted_diff &gt;= abs(actual_diff)) + sum(permuted_diff &lt;= -abs(actual_diff)))/n_perm\np\n\n## [1] 4e-04\n\nladd(panel.abline(v = actual_diff, col = \"red\", lty = 3, lwd = 2))\n\n\n\n\n\n\n\n\nAgain, we can do this whole process using the {infer} package workflow‚Ä¶\n\nnull_distribution &lt;- d |&gt;\n    specify(formula = kernel95 ~ sex) |&gt;\n    hypothesize(null = \"independence\") |&gt;\n    generate(reps = n_perm, type = \"permute\") |&gt;\n    calculate(stat = \"diff in means\", order = c(\"M\", \"F\"))\n\nactual_diff &lt;- d |&gt;\n    specify(formula = kernel95 ~ sex) |&gt;\n    calculate(stat = \"diff in means\", order = c(\"M\", \"F\"))\nactual_diff\n\n## Response: kernel95 (numeric)\n## Explanatory: sex (factor)\n## # A tibble: 1 √ó 1\n##    stat\n##   &lt;dbl&gt;\n## 1  111.\n\nnull_distribution |&gt;\n    get_p_value(obs_stat = actual_diff, direction = \"both\")\n\n## # A tibble: 1 √ó 1\n##   p_value\n##     &lt;dbl&gt;\n## 1  0.0008\n\nvisualize(null_distribution) + shade_p_value(obs_stat = actual_diff, lty = 1, size = 0.5,\n    fill = \"#c0c0c0\", direction = \"both\")\n\n\n\n\n\n\n\n\n\nUnder this approach, what is the ‚Äúnull‚Äù hypothesis? What is the test statistic? Is the difference in mean kernel95 home range size ‚Äúsignificant‚Äù?\n\nThe null hypothesis here is that the difference in mean kernel95 home range size for males versus females is zero. The test statistic is the actual difference in mean kernel95 home range size for males versus females, and it is evaluated relative to a permutation distribution for this statistic. Based on our very low p value (less than 0.001), under a null hypothesis significance testing framework, we would conclude that the difference is ‚Äúsignificant‚Äù (assuming that p is less than the alpha level we specify).\n\n\nStep 8\n\nFinally, use a theory-based parametric test (e.g., a t-test) to also calculate an appropriate test statistic and associated p value for the comparison of male and female mean kernel95 home range size. Is the difference in mean kernel95 home range size ‚Äúsignificant‚Äù?\n\n\nf &lt;- d |&gt;\n    filter(sex == \"F\")\nm &lt;- d |&gt;\n    filter(sex == \"M\")\n\n# first, what is ratio of variances in our two samples?\nvar_f &lt;- var(f$kernel95)\nvar_m &lt;- var(m$kernel95)\nvar_f/var_m\n\n## [1] 1.272656\n\n# ratio is less than 2.0, so we can use equal variance version of t test...\n\nnum_f &lt;- nrow(f)\nnum_m &lt;- nrow(m)\n\nmean_f &lt;- mean(f$kernel95)\nmean_m &lt;- mean(m$kernel95)\n\n# hand-calculate the test statistic and p value...\ns2 &lt;- ((num_m - 1) * var_m + (num_f - 1) * var_f)/(num_m + num_f - 2)\nt_stat &lt;- (mean_m - mean_f)/sqrt(s2 * (1/num_m + 1/num_f))\nt_stat\n\n## [1] 3.94816\n\ndf &lt;- num_m + num_f - 2\np &lt;- 2 * (1 - pt(t_stat, df))\np\n\n## [1] 0.0009426349\n\n# or use the t.test() function\nt_test &lt;- t.test(x = m$kernel95, y = f$kernel95, var.equal = TRUE)\nt_test\n\n## \n##  Two Sample t-test\n## \n## data:  m$kernel95 and f$kernel95\n## t = 3.9482, df = 18, p-value = 0.0009426\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##   51.91339 169.99890\n## sample estimates:\n## mean of x mean of y \n##  429.7407  318.7846\n\n\n\nUnder this approach, what is the test statistic? Is the difference in mean kernel95 home range size ‚Äúsignificant‚Äù?\n\nHere, the null hypothesis again is that the difference in mean kernel95 home range size for males versus females is zero. The test statistic is the actual difference in means scaled by something equivalent to a standard error that takes into account the variance and size of our two samples. This test statistic is then evaluated relative to a t distribution that depends on the number of degrees of freedom, which depends on the sample size of our two samples. Again, based on the very low p value, we would conclude that the difference is ‚Äúsignificant‚Äù under a null hypothesis significance testing framework (assuming that p is less than the alpha level we specify).",
    "crumbs": [
      "Exercises",
      "Exercise 06 Solution"
    ]
  },
  {
    "objectID": "exercise-07.html",
    "href": "exercise-07.html",
    "title": "Exercise 07",
    "section": "",
    "text": "Explore Distributions and the CLT",
    "crumbs": [
      "Exercises",
      "Exercise 07"
    ]
  },
  {
    "objectID": "exercise-07.html#learning-objectives",
    "href": "exercise-07.html#learning-objectives",
    "title": "Exercise 07",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nPlotting various mathematical distributions\nSampling from distributions that are distinctly non-normal and generating summary statistics\nVisualizing sampling distributions to see the Central Limit Theorem in action\n\n\nlibrary(tidyverse)\nlibrary(infer)  # for rep_sample_n()\nlibrary(ggformula)  # for gf_ functions\nlibrary(cowplot)  # for plot_grid()\nlibrary(mosaic)  # for do() * &lt;function&gt;\nlibrary(kableExtra)  # for kable_styling()\n\nClassic probability and statistical theory - and many parametric statistical tests - assume that the distributions of variables of interest (either things we measure/record about our subjects of study or sample statistics we derived from those measurements) follow certain well-characterized mathematical distributions. For example, when we imagine rolling an unbiased die, we typically assume that we have an equal (i.e., uniform) chance of seeing any given number come up, and when we imagine sampling any particular metric trait from a population, we typically assume that the distribution of that trait in a nature population follows a normal, or Gaussian, distribution (e.g., height). Likewise, we expect or assume that other well-characterized mathematical distributions are appropriate models for the outcomes of other sampling processes. The shape of any of these distributions is governed entirely by a function and one or more tuning parameters for that function.",
    "crumbs": [
      "Exercises",
      "Exercise 07"
    ]
  },
  {
    "objectID": "exercise-07.html#plotting-distributions",
    "href": "exercise-07.html#plotting-distributions",
    "title": "Exercise 07",
    "section": "Plotting Distributions",
    "text": "Plotting Distributions\nBelow is some code we can use to plot some example distributions and visualize how the shape of those distributions changes with different parameter values‚Ä¶\n\n# some continuous distributions - normal, beta, uniform, F, and Chi Square...\nnorm1 &lt;- gf_dist(\"norm\", mean = 2, sd = 1)\nnorm2 &lt;- gf_dist(\"norm\", mean = 30, sd = 15)\nnorm3 &lt;- gf_dist(\"norm\", mean = 100, sd = 15)\nbeta1 &lt;- gf_dist(\"beta\", shape1 = 1, shape2 = 10, xlim = c(-0.1, 1.1))\nbeta2 &lt;- gf_dist(\"beta\", shape1 = 2, shape2 = 10, xlim = c(-0.1, 1.1))\nbeta3 &lt;- gf_dist(\"beta\", shape1 = 3, shape2 = 1, xlim = c(-0.1, 1.1))\nunif1 &lt;- gf_dist(\"unif\", min = 1, max = 2)\nunif2 &lt;- gf_dist(\"unif\", min = 10, max = 25)\nunif3 &lt;- gf_dist(\"unif\", min = 130, max = 240)\nf1 &lt;- gf_dist(\"f\", df1 = 4, df2 = 15)\nf2 &lt;- gf_dist(\"f\", df1 = 4, df2 = 99)\nf3 &lt;- gf_dist(\"f\", df1 = 30, df2 = 199)\nchisq1 &lt;- gf_dist(\"chisq\", df = 2)\nchisq2 &lt;- gf_dist(\"chisq\", df = 3)\nchisq3 &lt;- gf_dist(\"chisq\", df = 20)\n\n# some discrete distributions - Poisson,binomial, negative binomial...\npois1 &lt;- gf_dist(\"pois\", lambda = 3)\npois2 &lt;- gf_dist(\"pois\", lambda = 10)\npois3 &lt;- gf_dist(\"pois\", lambda = 35)\nbinom1 &lt;- gf_dist(\"binom\", size = 10, prob = 0.5, xlim = c(0, 10))\nbinom2 &lt;- gf_dist(\"binom\", size = 20, prob = 0.5, xlim = c(0, 20))\nbinom3 &lt;- gf_dist(\"binom\", size = 20, prob = 0.1, xlim = c(0, 20))\nnbinom1 &lt;- gf_dist(\"nbinom\", size = 5, prob = 0.5, xlim = c(0, 20))\nnbinom2 &lt;- gf_dist(\"nbinom\", size = 10, prob = 0.5, xlim = c(0, 20))\nnbinom3 &lt;- gf_dist(\"nbinom\", size = 15, prob = 0.5, xlim = c(0, 20))\n\n# note that in the code above, we could instead use the `plotDist()` function\n# from {mosaic} this example uses `gf_dist()` simply to create .gg (ggplot)\n# objects rather than .trellis (lattice) objects, which makes visualizing the\n# plots together using {cowplot} look better\n\nrow1 &lt;- plot_grid(norm1, norm2, norm3, nrow = 1) + draw_plot_label(label = \"Normal\",\n    fontface = \"bold\", size = 12, hjust = 0, vjust = -0.5)\nrow2 &lt;- plot_grid(beta1, beta2, beta3, nrow = 1) + draw_plot_label(label = \"Beta\",\n    fontface = \"bold\", size = 12, hjust = 0, vjust = -0.5)\nrow3 &lt;- plot_grid(unif1, unif2, unif3, nrow = 1) + draw_plot_label(label = \"Uniform\",\n    fontface = \"bold\", size = 12, hjust = 0, vjust = -0.5)\nrow4 &lt;- plot_grid(chisq1, chisq2, chisq3, nrow = 1) + draw_plot_label(label = \"Chi Sq\",\n    fontface = \"bold\", size = 12, hjust = 0, vjust = -0.5)\nrow5 &lt;- plot_grid(f1, f2, f3, nrow = 1) + draw_plot_label(label = \"F\", fontface = \"bold\",\n    size = 12, hjust = 0, vjust = -0.5)\n\nblank &lt;- ggplot() + theme_nothing()  # this is just for spacing at the top of the plot\nplot1 &lt;- plot_grid(blank, row1, row2, row3, row4, row5, nrow = 6, rel_heights = c(0.25,\n    1, 1, 1, 1, 1))\nplot1\n\n\n\n\n\n\n\nrow1 &lt;- plot_grid(pois1, pois2, pois3, nrow = 1) + draw_plot_label(label = \"Poisson\",\n    fontface = \"bold\", size = 12, hjust = 0, vjust = -0.5)\nrow2 &lt;- plot_grid(binom1, binom2, binom3, nrow = 1) + draw_plot_label(label = \"Binomial\",\n    fontface = \"bold\", size = 12, hjust = 0, vjust = -0.5)\nrow3 &lt;- plot_grid(nbinom1, nbinom2, nbinom3, nrow = 1) + draw_plot_label(label = \"Negative Binomial\",\n    fontface = \"bold\", size = 12, hjust = 0, vjust = -0.5)\n\nblank &lt;- ggplot() + theme_nothing()  # this is just for spacing at the top of the plot\nplot2 &lt;- plot_grid(blank, row1, row2, row3, nrow = 4, rel_heights = c(0.25, 1, 1,\n    1))\nplot2",
    "crumbs": [
      "Exercises",
      "Exercise 07"
    ]
  },
  {
    "objectID": "exercise-07.html#sampling-distributions-and-the-clt",
    "href": "exercise-07.html#sampling-distributions-and-the-clt",
    "title": "Exercise 07",
    "section": "Sampling Distributions and the CLT",
    "text": "Sampling Distributions and the CLT\nBelow are some examples of [1] drawing random sets of observations from several of these distributions, [2] calculating summary statistics (e.g., means) for each sample, and [3] repeating this process multiple times to generate sampling distributions for these summary statistics. Each of the first three snippets of code below first draws and plots a single sample of size n from a particular distribution, plots a histogram of that sample, and superimposes the distribution is is drawn from (black curve) and the mean of the sample (red line). The last two snippets do the same, but instead plots selected quantile values as example of alternative summary statistics. These are the left-hand plots in the resulting 10-panel figure.\nEach snippet also then draws reps separate samples from the same distributions, calculates the same summary statistics, and plots the resultant sampling distributions of those statistics with a superimposed normal distribution, demonstrating the Central Limit Theorem (CLT). These are the right-hand plots in the resulting 10-panel figure.\nAs a refresher, recall that CLT states that the sampling distribution of a sample mean (and many other sample statistics) is approximately normal if the sample size is large enough, even if the population distribution that the sample is drawn from is not normal.\nAdditionally, the CLT also states that the sampling distribution should have the following properties:\n\nThe mean of the sampling distribution will be equal to the mean of the population distribution.\n\n\\[\\bar{x} = \\mu\\]\n\nThe standard deviation of the sampling distribution will be equal to the standard deviation of the population distribution divided by the square root of the sample size. This is the standard error.\n\n\\[s = \\frac{\\sigma}{\\sqrt{n}}\\]\n\n# sample size\nn &lt;- 10\n# number of replicates\nreps &lt;- 1000\n\n# normal distribution generate 1 sample of size n...\nx &lt;- rnorm(n, mean = 2, sd = 1)\n# and plot it along with the distribution it was drawn from and the mean of the\n# sample\na &lt;- gf_dhistogram(~x) |&gt;\n    gf_dist(\"norm\", mean = 2, sd = 1) |&gt;\n    gf_vline(xintercept = ~c(mean(x)))\n\n# generate a sampling distribution for the sample mean based on *reps* samples\n# of size *n* and put it in a vector...\nn_mean &lt;- tibble(do(reps) * mean(rnorm(n, mean = 2, sd = 1))) |&gt;\n    pull(mean)\n# and plot it along with the sampling distribution and the mean of the sample\nb &lt;- gf_dhistogram(~n_mean, bins = 30) |&gt;\n    gf_dist(\"norm\", mean = mean(n_mean), sd = sd(n_mean)) |&gt;\n    gf_vline(xintercept = ~c(mean(n_mean)))\n\nrow1 &lt;- plot_grid(a, b)\n\n# beta distribution generate 1 sample of size n...\nx &lt;- rbeta(reps, shape1 = 2, shape2 = 10)\n# and plot it along with the distribution it was drawn from and the mean of the\n# sample\na &lt;- gf_dhistogram(~x) |&gt;\n    gf_dist(\"beta\", shape1 = 2, shape2 = 10) |&gt;\n    gf_vline(xintercept = ~c(mean(x)))\n\n# generate a sampling distribution for the sample mean based on *reps* samples\n# of size *n* and put it in a vector...\nb_mean &lt;- tibble(do(reps) * mean(rbeta(reps, shape1 = 2, shape2 = 10))) |&gt;\n    pull(mean)\n# and plot it along with the sampling distribution and the mean of the sample\nb &lt;- gf_dhistogram(~b_mean) |&gt;\n    gf_dist(\"norm\", mean = mean(b_mean), sd = sd(b_mean)) |&gt;\n    gf_vline(xintercept = ~c(mean(b_mean)))\n\nrow2 &lt;- plot_grid(a, b)\n\n# uniform distribution generate 1 sample of size n...\nx &lt;- runif(reps, min = 1, max = 2)\n# and plot it along with the distribution it was drawn from and the mean of the\n# sample\na &lt;- gf_dhistogram(~x) |&gt;\n    gf_dist(\"unif\", min = 1, max = 2) |&gt;\n    gf_vline(xintercept = ~c(mean(x)))\n\n# generate a sampling distribution for the sample mean based on *reps* samples\n# of size *n* and put it in a vector...\nu_mean &lt;- tibble(do(reps) * mean(runif(reps, min = 1, max = 2))) |&gt;\n    pull(mean)\n# and plot it along with the sampling distribution and the mean of the sample\nb &lt;- gf_dhistogram(~u_mean) |&gt;\n    gf_dist(\"norm\", mean = mean(u_mean), sd = sd(u_mean)) |&gt;\n    gf_vline(xintercept = ~c(mean(u_mean)))\n\nrow3 &lt;- plot_grid(a, b)\n\n# normal distribution... with a different statistic generate 1 sample of size\n# n...\nx &lt;- rnorm(n, mean = 2, sd = 1)\n# and plot it along with the distribution it was drawn from and the mean of the\n# sample\na &lt;- gf_dhistogram(~x) |&gt;\n    gf_dist(\"norm\", mean = 2, sd = 1) |&gt;\n    gf_vline(xintercept = ~c(quantile(x, 0.025)))\n\n# generate a sampling distribution for the sample mean based on *reps* samples\n# of size *n* and put it in a vector...\nn_mean &lt;- tibble(do(reps) * quantile(rnorm(n, mean = 2, sd = 1), 0.025)) |&gt;\n    pull(X2.5.)\n# and plot it along with the sampling distribution and the mean of the sample\nb &lt;- gf_dhistogram(~n_mean, bins = 30) |&gt;\n    gf_dist(\"norm\", mean = mean(n_mean), sd = sd(n_mean)) |&gt;\n    gf_vline(xintercept = ~c(mean(n_mean)))\n\nrow4 &lt;- plot_grid(a, b)\n\n# uniform distribution... with a different statistic generate 1 sample of size\n# n...\nx &lt;- runif(reps, min = 1, max = 2)\n# and plot it along with the distribution it was drawn from and the 0.25\n# quantile of the sample\na &lt;- gf_dhistogram(~x) |&gt;\n    gf_dist(\"unif\", min = 1, max = 2) |&gt;\n    gf_vline(xintercept = ~c(quantile(x, 0.25)))\n\n# generate a sampling distribution for the 0.25 quantile based on *reps*\n# samples of size *n* and put it in a vector...\nu_mean &lt;- tibble(do(reps) * quantile(runif(reps, min = 1, max = 2), 0.25)) |&gt;\n    pull(X25.)\n# and plot it along with the sampling distribution and the mean of the sample\nb &lt;- gf_dhistogram(~u_mean) |&gt;\n    gf_dist(\"norm\", mean = mean(u_mean), sd = sd(u_mean)) |&gt;\n    gf_vline(xintercept = ~c(mean(u_mean)))\n\nrow5 &lt;- plot_grid(a, b)\n\nplot3 &lt;- plot_grid(row1, row2, row3, row4, row5, nrow = 5)\nplot3",
    "crumbs": [
      "Exercises",
      "Exercise 07"
    ]
  },
  {
    "objectID": "exercise-07.html#generating-cis-around-a-statistic",
    "href": "exercise-07.html#generating-cis-around-a-statistic",
    "title": "Exercise 07",
    "section": "Generating CIs around a Statistic",
    "text": "Generating CIs around a Statistic\nThe following code draws a single sample of size n from a normal distribution (as above) and calculates the mean and standard deviation of that sample and estimates the standard error of the mean. It then generates several different estimates for a 95% confidence interval around that sample mean:\n\nBased on bootstrap resampling 10,000 times from the original sample and using quantiles from the resulting bootstrap sampling distribution to define the lower and upper bounds of the CI\nUsing the standard deviation of the bootstrap sampling distribution, along with the original sample mean, to generate a theory-based bootstrap CI, presuming that the shape of bootstrap sampling distribution is normal\nUsing the estimate of the standard error generated from the original sample, along with the original sample mean, to generate a different theory-based CI, assuming that the shape of sampling distribution is normal\nUsing the estimate of the standard error generated from the original sample, along with the original sample mean, to generate an alternative theory-based CI, but presuming that the shape of sampling distribution is better modeled as a t-distribution\n\n\n# sample size\nn &lt;- 10\n# number of replicates\nreps &lt;- 10000\n\n# normal distribution generate 1 sample of size n...\nx &lt;- rnorm(n, mean = 2, sd = 1)\nmean(x)\n\n## [1] 2.229151\n\n# close to, but not the same, as the population mean\nse &lt;- sd(x)/sqrt(length(x))\n# estimate of se based on 1 random sample equivalent to...\nse &lt;- sciplot::se(x)\nse_pop &lt;- 1/sqrt(length(x))  # theoretical se calculated from known population sd\n\nboot &lt;- vector()  # set up a dummy variable to hold our bootstrap simulations\n# bootstrap sample size should be the same length as our sample data\nfor (i in 1:reps) {\n    boot[[i]] &lt;- mean(sample(x, length(x), replace = TRUE))\n}\n\n# or...\nboot &lt;- tibble(x) |&gt;\n    rep_sample_n(size = length(x), replace = TRUE, reps = reps) |&gt;\n    group_by(replicate) |&gt;\n    summarize(mean = mean(x)) |&gt;\n    pull(mean)\n\n# or...\nboot &lt;- do(reps) * mean(sample(x, length(x), replace = TRUE))\nboot &lt;- boot$mean\n\n\n# plot a histogram of our bootstrapped sample means with normal curve and\n# original sample mean superimposed\nplot4 &lt;- gf_dhistogram(~boot, title = \"Bootstrap Sampling Distribution\", xlab = \"x\") |&gt;\n    gf_dist(\"norm\", mean = mean(boot), sd = sd(boot)) |&gt;\n    gf_vline(xintercept = ~c(mean(x)))  # mean of our original vector of samples\nplot4\n\n\n\n\n\n\n\n# ci bounds inferred from quantiles of bootstrap distribution\nci.boot.quantiles &lt;- quantile(boot, c(0.025, 0.975))\n\n# ci bounds inferred from original sample mean and sd of bootstrap sampling\n# distribution\nci.boot.theory &lt;- qnorm(c(0.025, 0.975), mean = mean(x), sd = sd(boot))  # 0.025 and 0.975 quantiles of normal with mean and sd of boot\n# equivalent to...  mean + quantiles of standard normal times sd of bootstrap\n# sampling distribution (= standard error)\nci.boot.theory &lt;- mean(x) + qnorm(c(0.025, 0.975), 0, 1) * sd(boot)  # 1.96 SE above and below the mean\n\n# ci bounds inferred from original sample mean + quantiles of standard normal\n# times standard error\nci.norm.theory &lt;- mean(x) + qnorm(c(0.025, 0.975), 0, 1) * se\n# equivalent to...\nci.norm.theory &lt;- mean(x) + c(-1, 1) * qnorm(0.975) * se\n\n# ci bounds inferred from original sample mean + quantiles of t distribution\n# times standard error\nci.t.theory &lt;- mean(x) + qt(c(0.025, 0.975), df = length(x) - 1) * se\n# equivalent to...\nci.t.theory &lt;- mean(x) + c(-1, 1) * qt(0.975, df = length(x) - 1) * se\n\ncomparison &lt;- rbind(ci.boot.quantiles, ci.boot.theory, ci.norm.theory, ci.t.theory)\nkable(comparison, digits = 3) |&gt;\n    kable_styling(font_size = 14, full_width = FALSE)\n\n\n\n\n\n2.5%\n97.5%\n\n\n\n\nci.boot.quantiles\n1.665\n2.853\n\n\nci.boot.theory\n1.636\n2.822\n\n\nci.norm.theory\n1.605\n2.853\n\n\nci.t.theory\n1.509\n2.949\n\n\n\n\n\n\nplot4 &lt;- plot4 |&gt;\n    gf_vline(xintercept = ~ci.boot.quantiles, color = \"blue\") |&gt;\n    gf_vline(xintercept = ~ci.boot.theory, color = \"red\") |&gt;\n    gf_vline(xintercept = ~ci.norm.theory, color = \"green\") |&gt;\n    gf_vline(xintercept = ~ci.t.theory, color = \"purple\")\nplot4",
    "crumbs": [
      "Exercises",
      "Exercise 07"
    ]
  },
  {
    "objectID": "exercise-08.html",
    "href": "exercise-08.html",
    "title": "Exercise 08",
    "section": "",
    "text": "Practice Simple Linear Regression",
    "crumbs": [
      "Exercises",
      "Exercise 08"
    ]
  },
  {
    "objectID": "exercise-08.html#learning-objectives",
    "href": "exercise-08.html#learning-objectives",
    "title": "Exercise 08",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nExplore a published comparative dataset on primate group size, brain size, and life history variables\nConduct simple linear regression analyses with this dataset where you:\n\nGenerate regression coefficients by hand\nUse existing R functions and ‚Äúformula notation‚Äù to generate regression coefficients for simple regression models\nCalculate theory-based standard error estimates and p values for the regression coefficients by hand and also extract these from model summaries\nGenerate theory-based confidence intervals for the regression coefficients by hand and also extract these from model summaries\nGenerate confidence intervals for regression coefficients and p values for regression coefficients using permutation/bootstrapping methods\n\n\n\nData source:\nStreet SE, Navarrete AF, Reader SM, and Laland KN. (2017). Coevolution of cultural intelligence, extended life history, sociality, and brain size in primates. Proceedings of the National Academy of Sciences 114: 7908‚Äì7914.\n\n\nPreliminaries\n\nSet up a new GitHub repo in your GitHub workspace named ‚Äúexercise-09‚Äù and clone that down to your computer as a new RStudio project. The instructions outlined as Method 1 in Module 6 will be helpful.\n\n\n\nStep 1\n\nUsing the {tidyverse} read_csv() function, load the ‚ÄúStreet_et_al_2017.csv‚Äù dataset from this URL as a ‚Äútibble‚Äù named d.\nDo a quick exploratory data analysis where you generate the five-number summary (median, minimum and maximum and 1st and 3rd quartile values), plus mean and standard deviation, for each quantitative variable.\n\n\nHINT: The skim() function from the package {skimr} makes this very easy!\n\n\n\nStep 2\n\nFrom this dataset, plot brain size (ECV) as a function of social group size (Group_size), longevity (Longevity), juvenile period length (Weaning), and reproductive lifespan (Repro_lifespan).\n\n\n\nStep 3\n\nDerive by hand the ordinary least squares regression coefficients \\(\\beta1\\) and \\(\\beta0\\) for ECV as a function of social group size.\n\n\nHINT: You will need to remove rows from your dataset where one of these variables is missing.\n\n\n\nStep 4\n\nConfirm that you get the same results using the lm() function.\n\n\n\nStep 5\n\nRepeat the analysis above for three different major radiations of primates - ‚Äúcatarrhines‚Äù, ‚Äúplatyrrhines‚Äù, and ‚Äústrepsirhines‚Äù) separately. These are stored in the variable Taxonomic_group. Do your regression coefficients differ among groups? How might you determine this?\n\n\n\nStep 6\n\nFor your first regression of ECV on social group size, calculate the standard error for the slope coefficient, the 95% CI, and the p value associated with this coefficient by hand. Also extract this same information from the results of running the lm() function.\n\n\n\nStep 7\n\nUse a permutation approach with 1000 permutations to generate a null sampling distribution for the slope coefficient. What is it that you need to permute? What is the p value associated with your original slope coefficient? You can use either the quantile method (i.e., using quantiles from the actual permutation-based null sampling distribution) or a theory-based method (i.e., using the standard deviation of the permutation-based null sampling distribution as the estimate of the standard error, along with a normal or t distribution), or both, to calculate this p value.\n\n\n\nStep 8\n\nUse bootstrapping to generate a 95% CI for your estimate of the slope coefficient using both the quantile method and the theory-based method (i.e., using the standard deviation of the bootstrapped sampling distribution as an estimate of the standard error). Do these CIs suggest that your slope coefficient is different from zero?",
    "crumbs": [
      "Exercises",
      "Exercise 08"
    ]
  },
  {
    "objectID": "exercise-08-solution.html",
    "href": "exercise-08-solution.html",
    "title": "Exercise 08 Solution",
    "section": "",
    "text": "‚Ä¢ Solution\n\nStep 1\nUsing the {tidyverse} read_csv() function, load the ‚ÄúStreet_et_al_2017.csv‚Äù dataset as a ‚Äútibble‚Äù named d.\n\nlibrary(tidyverse)\nlibrary(broom)\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/Street_et_al_2017.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\n\nDo a quick exploratory data analysis where you generate the five-number summary (median, minimum and maximum and 1st and 3rd quartile values), plus mean and standard deviation, for each quantitative variable.\n\nlibrary(skimr)\nlibrary(kableExtra)\nskim(d) |&gt;\n    kable() |&gt;\n    kable_styling(font_size = 10, full_width = FALSE)\n\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\ncharacter.min\ncharacter.max\ncharacter.empty\ncharacter.n_unique\ncharacter.whitespace\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\n\ncharacter\nSpecies\n0\n1.0000000\n10\n41\n0\n301\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\nTaxonomic_group\n0\n1.0000000\n10\n12\n0\n3\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\nSocial_learning\n98\n0.6744186\nNA\nNA\nNA\nNA\nNA\n2.300493\n16.51382\n0.000\n0.0000\n0.000\n0.0000\n214.00\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nnumeric\nResearch_effort\n115\n0.6179402\nNA\nNA\nNA\nNA\nNA\n38.763441\n80.58897\n1.000\n6.0000\n16.000\n37.7500\n755.00\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nnumeric\nECV\n117\n0.6112957\nNA\nNA\nNA\nNA\nNA\n68.493206\n82.84154\n1.630\n11.8250\n58.550\n86.1975\n491.27\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nnumeric\nGroup_size\n114\n0.6212625\nNA\nNA\nNA\nNA\nNA\n13.263102\n15.19637\n1.000\n3.1250\n7.500\n18.2250\n91.20\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n\n\nnumeric\nGestation\n161\n0.4651163\nNA\nNA\nNA\nNA\nNA\n164.504000\n37.99758\n59.990\n138.3525\n166.030\n183.2650\n274.78\n‚ñÅ‚ñÖ‚ñá‚ñÉ‚ñÅ\n\n\nnumeric\nWeaning\n185\n0.3853821\nNA\nNA\nNA\nNA\nNA\n311.088276\n253.08157\n40.000\n121.6600\n234.165\n388.7825\n1260.81\n‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÅ\n\n\nnumeric\nLongevity\n181\n0.3986711\nNA\nNA\nNA\nNA\nNA\n331.971333\n165.67434\n103.000\n216.0000\n301.200\n393.3000\n1470.00\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n\n\nnumeric\nSex_maturity\n194\n0.3554817\nNA\nNA\nNA\nNA\nNA\n1480.228692\n999.22681\n283.180\n701.5200\n1427.170\n1894.1100\n5582.93\n‚ñá‚ñÜ‚ñÇ‚ñÅ‚ñÅ\n\n\nnumeric\nBody_mass\n63\n0.7906977\nNA\nNA\nNA\nNA\nNA\n6795.184328\n14229.82563\n31.230\n739.4425\n3553.500\n7465.0000\n130000.00\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nnumeric\nMaternal_investment\n197\n0.3455150\nNA\nNA\nNA\nNA\nNA\n478.640000\n292.06808\n99.990\n255.8850\n401.350\n592.2175\n1492.30\n‚ñá‚ñÖ‚ñÇ‚ñÅ‚ñÅ\n\n\nnumeric\nRepro_lifespan\n206\n0.3156146\nNA\nNA\nNA\nNA\nNA\n9064.974702\n4601.56798\n2512.157\n6126.2200\n8325.890\n10716.5950\n39129.57\n‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÅ\n\n\n\n\n\n\ndetach(package:kableExtra)\ndetach(package:skimr)\n\n\n\nStep 2\nFrom this dataset, plot brain size (ECV) as a function of social group size (Group_size), longevity (Longevity), juvenile period length (Weaning), and reproductive lifespan (Repro_lifespan).\n\nlibrary(cowplot)\np1 &lt;- ggplot(data = d, aes(x = Group_size, y = ECV)) + geom_point()\np2 &lt;- ggplot(data = d, aes(x = Longevity, y = ECV)) + geom_point()\np3 &lt;- ggplot(data = d, aes(x = Weaning, y = ECV)) + geom_point()\np4 &lt;- ggplot(data = d, aes(x = Repro_lifespan, y = ECV)) + geom_point()\nplot_grid(p1, p2, p3, p4, nrow = 2)\n\n\n\n\n\n\n\n\n\nNOTE: It looks like most of these variables should probably be log transformed, though that is not essential for this exercise ;)\n\n\n\nStep 3\nDerive by hand the ordinary least squares regression coefficients \\(\\beta1\\) and \\(\\beta0\\) for ECV as a function of social group size.\n\nd_mod &lt;- d |&gt;\n    filter(!is.na(ECV) & !is.na(Group_size))\n# or\nd_mod &lt;- d |&gt;\n    drop_na(ECV, Group_size)\n\n(b1 &lt;- cor(d_mod$ECV, d_mod$Group_size) * sd(d_mod$ECV)/sd(d_mod$Group_size))\n\n## [1] 2.463071\n\n(b0 &lt;- mean(d_mod$ECV) - b1 * mean(d_mod$Group_size))\n\n## [1] 30.35652\n\n\n\n\nStep 4\nConfirm that you get the same results using the lm() function.\n\nm &lt;- lm(ECV ~ Group_size, data = d_mod)\nresults &lt;- m |&gt;\n    summary() |&gt;\n    tidy()\nresults\n\n## # A tibble: 2 √ó 5\n##   term        estimate std.error statistic  p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1 (Intercept)    30.4      6.80       4.47 1.56e- 5\n## 2 Group_size      2.46     0.351      7.02 7.26e-11\n\n\n\n\nStep 5\nRepeat the analysis above for three different major radiations of primates - ‚Äúcatarrhines‚Äù, ‚Äúplatyrrhines‚Äù, and ‚Äústrepsirhines‚Äù) separately. Do your regression coefficients differ among groups? How might you determine this?\n\nplatyrrhini &lt;- d_mod |&gt;\n    filter(Taxonomic_group == \"Platyrrhini\")\ncatarrhini &lt;- d_mod |&gt;\n    filter(Taxonomic_group == \"Catarrhini\")\nstrepsirhini &lt;- d_mod |&gt;\n    filter(Taxonomic_group == \"Strepsirhini\")\n\n(platyrrhini_results &lt;- lm(ECV ~ Group_size, data = platyrrhini) |&gt;\n    summary() |&gt;\n    tidy())\n\n## # A tibble: 2 √ó 5\n##   term        estimate std.error statistic  p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1 (Intercept)    16.2      7.39       2.19 0.0356  \n## 2 Group_size      1.97     0.455      4.32 0.000136\n\n# or\n(beta1_p &lt;- cov(platyrrhini$ECV, platyrrhini$Group_size)/var(platyrrhini$Group_size))\n\n## [1] 1.965176\n\n(beta0_p &lt;- mean(platyrrhini$ECV) - beta1_p * mean(platyrrhini$Group_size))\n\n## [1] 16.18121\n\n(catarrhini_results &lt;- lm(ECV ~ Group_size, data = catarrhini) |&gt;\n    summary() |&gt;\n    tidy())\n\n## # A tibble: 2 √ó 5\n##   term        estimate std.error statistic     p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n## 1 (Intercept)    83.4     14.9        5.59 0.000000438\n## 2 Group_size      1.15     0.579      1.98 0.0518\n\n# or\n(beta1_c &lt;- cov(catarrhini$ECV, catarrhini$Group_size)/var(catarrhini$Group_size))\n\n## [1] 1.146322\n\n(beta0_c &lt;- mean(catarrhini$ECV) - beta1_c * mean(catarrhini$Group_size))\n\n## [1] 83.42059\n\n(strepsirhini_results &lt;- lm(ECV ~ Group_size, data = strepsirhini) |&gt;\n    summary() |&gt;\n    tidy())\n\n## # A tibble: 2 √ó 5\n##   term        estimate std.error statistic  p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1 (Intercept)     8.18     2.14       3.83 0.000404\n## 2 Group_size      1.84     0.473      3.89 0.000332\n\n# or\n(beta1_s &lt;- cov(strepsirhini$ECV, strepsirhini$Group_size)/var(strepsirhini$Group_size))\n\n## [1] 1.840664\n\n(beta0_s &lt;- mean(strepsirhini$ECV) - beta1_s * mean(strepsirhini$Group_size))\n\n## [1] 8.176384\n\n\nAs seen from the results above, the coefficients do differ among groups‚Ä¶ but how might we test if this difference is signficant? One way would be to permute the group assignments randomly, for each permutation, calculate the difference in slopes between pairs of groups to create permutation distributions for that test statistic under a null model of no difference between groups. We could then compare the observed difference in slopes between groups to the ‚Äúexpected‚Äù distribution under that specified null model.\n\n\nStep 6\nFor your first regression of ECV on social group size, calculate the standard error for the slope coefficient, the 95% CI, and the p value associated with this coefficient by hand.\n\n# first define alpha and degrees of freedom for the regression\nalpha &lt;- 0.05\np.lower &lt;- alpha/2\np.upper &lt;- 1 - (alpha/2)\nn &lt;- nrow(d_mod)  # number of observations\ndf &lt;- n - 2\n\n# then, calculate residuals...\nresiduals &lt;- d_mod$ECV - (b0 + b1 * d_mod$Group_size)\n# or residuals &lt;- m$residuals\n\n# then, calculate the SE for b1...\nSSE &lt;- sum(residuals^2)\ndfe &lt;- nrow(d_mod) - 1 - 1  # number of observations - number of predictors - 1 = n - p - 1\nMSE &lt;- SSE/dfe\nSSX &lt;- sum((d_mod$Group_size - mean(d_mod$Group_size))^2)\n\n(SE_b1 &lt;- sqrt(MSE/SSX))\n\n## [1] 0.3508061\n\n# we can calculate an SE for b0 as well...\n(SE_b0 &lt;- SE_b1 * sqrt(sum(d_mod$Group_size^2)/n))\n\n## [1] 6.796325\n\n# calculated 95% CI for b1 assuming a t distribution...\n(CI_b1 &lt;- b1 + c(-1, 1) * qt(p = 1 - (alpha/2), df = df) * SE_b1)\n\n## [1] 1.769874 3.156269\n\n# we can calculate a CI for b0 as well...\n(CI_b0 &lt;- b0 + c(-1, 1) * qt(p = 1 - (alpha/2), df = df) * SE_b0)\n\n## [1] 16.92690 43.78615\n\n# calculate p values...  first, we need t statistics...\nt_b1 = b1/SE_b1\nt_b0 = b0/SE_b0\n\n(p_b1 &lt;- pt(-1 * abs(t_b1), df = df, lower.tail = TRUE) + (1 - pt(abs(t_b1), df = df,\n    lower.tail = TRUE)))\n\n## [1] 7.259436e-11\n\n# or\n(p_b1 &lt;- 2 * pt(abs(t_b1), df = df, lower.tail = FALSE))\n\n## [1] 7.259435e-11\n\n(p_b0 &lt;- 2 * pt(abs(t_b0), df = df, lower.tail = FALSE))\n\n## [1] 1.561214e-05\n\n\nAlso extract this same information from the results of running the lm() function. Compare the hand-calculated output above to that pulled from the model result object, m.\n\n(results &lt;- m |&gt;\n    summary() |&gt;\n    tidy(conf.int = TRUE, conf.level = 1 - alpha))\n\n## # A tibble: 2 √ó 7\n##   term        estimate std.error statistic  p.value conf.low conf.high\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)    30.4      6.80       4.47 1.56e- 5    16.9      43.8 \n## 2 Group_size      2.46     0.351      7.02 7.26e-11     1.77      3.16\n\n\n\n\nStep 7\nThen, use a permutation approach with 1000 permutations to generate a null sampling distribution for the slope coefficient.\n\nlibrary(mosaic)\nlibrary(infer)\n# using a loop to get a permutation distribution for slope\nnperm &lt;- 1000\nperm &lt;- vector(length = nperm)\nperm.sample &lt;- d_mod\nfor (i in 1:nperm) {\n    perm.sample$Group_size &lt;- sample(perm.sample$Group_size)\n    result &lt;- lm(ECV ~ Group_size, data = perm.sample) |&gt;\n        tidy() |&gt;\n        filter(term == \"Group_size\") |&gt;\n        pull(estimate)\n    perm[[i]] &lt;- result\n}\nhistogram(perm, xlim = c(-3, 3))\n\n\n\n\n\n\n\nladd(panel.abline(v = b1, lty = 3, lwd = 2))\n\n\n\n\n\n\n\n# calculate se as sd of permutation distribution\nperm.se &lt;- sd(perm)\n\n# or, using the {infer} workflow...\nperm &lt;- d_mod |&gt;\n    # specify model\nspecify(ECV ~ Group_size) |&gt;\n    # use a null hypothesis of independence\nhypothesize(null = \"independence\") |&gt;\n    # generate permutation replicates\ngenerate(reps = nperm, type = \"permute\") |&gt;\n    # calculate the slope statistic\ncalculate(stat = \"slope\")\n\nvisualize(perm) + shade_p_value(obs_stat = b1, direction = \"two_sided\")\n\n\n\n\n\n\n\n# calculate se as sd of permutation distribution\nperm.se &lt;- sd(perm$stat)\n\nWhat is the p value associated with your original slope coefficient? You can use either the percentile method (i.e., using quantiles from actual permutation-based null sampling distribution) or a theory-based method (i.e., using the standard deviation of the permutation-based null sampling distribution as the estimate of the standard error), or both, to calculate this p value.\n\n(p.percentile &lt;- perm |&gt;\n    mutate(test = abs(stat) &gt;= abs(b1)) |&gt;\n    summarize(p = mean(test)) |&gt;\n    pull(p))\n\n## [1] 0\n\n# p value taken directly from the permutation distribution i.e., how many of\n# the slope estimates from permuted samples are more extreme than the actual\n# b1?  here, absolute value is used to make this a 2-tailed test\n\n# or, using the {infer} package...\n(p.percentile &lt;- perm |&gt;\n    get_p_value(obs_stat = b1, direction = \"two_sided\"))\n\n## Warning: Please be cautious in reporting a p-value of 0. This result is an approximation\n## based on the number of `reps` chosen in the `generate()` step.\n## ‚Ñπ See `get_p_value()` (`?infer::get_p_value()`) for more information.\n\n\n## # A tibble: 1 √ó 1\n##   p_value\n##     &lt;dbl&gt;\n## 1       0\n\n(p.theory &lt;- 2 * pt(abs(b1)/perm.se, df = df, lower.tail = FALSE))\n\n## [1] 5.653821e-09\n\n# the t statistic used in `pt()` to calculate the p values is `b1/se_b1`, where\n# `se_b1` is estimated as the standard deviation of the permutation\n# distribution (i.e., `1/perm_se`) the `2 *` the upper-tail probability in this\n# calculation is to make this a 2-tailed test\n\n\n\nStep 8\nUse bootstrapping to generate a 95% CI for your estimate of the slope coefficient using both the quantile method and the theory-based method (i.e., based on the standard deviation of the bootstrapped sampling distribution). Do these CIs suggest that your slope coefficient is different from zero?\n\n# using a loop to get a bootstrap distribution to generate a CI for the slope\n# coefficient\nnboot &lt;- 1000\nboot &lt;- vector(length = nboot)\nfor (i in 1:nboot) {\n    boot.sample &lt;- sample_n(d_mod, nrow(d_mod), replace = TRUE)\n    result &lt;- lm(ECV ~ Group_size, data = boot.sample) |&gt;\n        tidy() |&gt;\n        filter(term == \"Group_size\") |&gt;\n        pull(estimate)\n    boot[[i]] &lt;- result\n}\nhistogram(boot, xlim = c(b1 - 3, b1 + 3))\n\n\n\n\n\n\n\nCI.quantile &lt;- c(quantile(boot, p.lower), quantile(boot, p.upper))\nladd(panel.abline(v = CI.quantile, lty = 3, lwd = 2, col = \"red\"))\n\n\n\n\n\n\n\nCI.theory &lt;- b1 + c(-1, 1) * qt(p.upper, df = df) * sd(boot)\nladd(panel.abline(v = CI.theory, lty = 3, lwd = 2, col = \"blue\"))\n\n\n\n\n\n\n\n# or, using the {infer} workflow...\nboot.slope &lt;- d_mod |&gt;\n    # specify model\nspecify(ECV ~ Group_size) |&gt;\n    # generate bootstrap replicates\ngenerate(reps = 1000, type = \"bootstrap\") |&gt;\n    # calculate the slope statistic\ncalculate(stat = \"slope\")\n\n(CI.quantile &lt;- c(quantile(boot.slope$stat, p.lower), quantile(boot.slope$stat, p.upper)))\n\n##     2.5%    97.5% \n## 1.467340 3.309761\n\n# or...\n(CI.quantile &lt;- get_ci(boot.slope, level = 1 - alpha, type = \"percentile\", point_estimate = b1))\n\n## # A tibble: 1 √ó 2\n##   lower_ci upper_ci\n##      &lt;dbl&gt;    &lt;dbl&gt;\n## 1     1.47     3.31\n\n(CI.theory &lt;- b1 + c(-1, 1) * qt(p.upper, df = df) * sd(boot.slope$stat))\n\n## [1] 1.544675 3.381468\n\n# or...\n(CI.theory &lt;- get_ci(boot.slope, level = 1 - alpha, type = \"se\", point_estimate = b1))\n\n## # A tibble: 1 √ó 2\n##   lower_ci upper_ci\n##      &lt;dbl&gt;    &lt;dbl&gt;\n## 1     1.55     3.37\n\n# note... the CI_theory values calculated with `get_ci()` differ very slightly\n# from those calculated by hand because {infer} seems to be using upper/lower\n# 0.025% boundaries of a *normal* rather than a *t* distribution I believe this\n# is an 'error' in how `get_ci()` is implemented in {infer}\n\nvisualize(boot.slope) + shade_ci(endpoints = CI.quantile, color = \"red\", fill = NULL) +\n    shade_ci(endpoints = CI.theory, color = \"blue\", fill = NULL)\n\n\n\n\n\n\n\n\nIn all cases, none of these estimated CIs include zero, suggesting that the slope coefficient estimated in our linear model is ‚Äúsignificant‚Äù.",
    "crumbs": [
      "Exercises",
      "Exercise 08 Solution"
    ]
  },
  {
    "objectID": "exercise-09.html",
    "href": "exercise-09.html",
    "title": "Exercise 09",
    "section": "",
    "text": "Check Assumptions of Regression",
    "crumbs": [
      "Exercises",
      "Exercise 09"
    ]
  },
  {
    "objectID": "exercise-09.html#learning-objectives",
    "href": "exercise-09.html#learning-objectives",
    "title": "Exercise 09",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nConduct additional simple linear regression analyses using a comparative dataset on primate group size, brain size, and life history variables\nCompare regression results using untransformed and transformed variables\nPerform residual analysis to consider whether certain assumptions of linear regression are met\n\n\nPreliminaries\n\nSet up a new GitHub repo in your GitHub workspace named ‚Äúexercise-10‚Äù and clone that down to your computer as a new RStudio project. The instructions outlined as Method 1 in Module 6 will be helpful.\n\n\n\nStep 1\n\nUsing the {tidyverse} read_csv() function, load the ‚ÄúKamilarAndCooperData.csv‚Äù dataset from GitHub at this URL as a ‚Äútibble‚Äù named d.\n\n\n\nStep 2\n\nFrom this dataset, plot lifespan (scored as MaxLongevity_m in the dataset) versus female body mass (scored as Body_mass_female_mean). Is the relationship linear? If not, how might you transform one or both variable to more closely approximate a linear relationship?\n\n\n\nStep 3\n\nRun linear models of:\n\nlifespan ~ female body mass\nlifespan ~ log(female body mass)\nlog(lifespan) ~ log(female body mass)\n\n\n\n\nStep 4\n\nGenerate residuals for all three linear models, plot them by hand in relation to the corresponding explanatory variable, and make histograms of the residuals. Do they appear to be normally distributed?\n\n\n\nStep 5\n\nGenerate Q-Q plots for all three linear models. Based on visual inspection of these plots, do the residual appear to deviate from being normally distributed?\n\n\n\nStep 6\n\nRun the plot() command for all three models and visually inspect the resultant plots. What do the plots suggest about whether the assumptions for regression are met for any of these models?\n\n\n\nStep 7\n\nRun Shapiro-Wilks tests (e.g., using the function shapiro.test() on the residuals for all three models. What do the results of these test suggest about the whether your data meet the assumptions for using simple linear regression?",
    "crumbs": [
      "Exercises",
      "Exercise 09"
    ]
  },
  {
    "objectID": "exercise-09-solution.html",
    "href": "exercise-09-solution.html",
    "title": "Exercise 09 Solution",
    "section": "",
    "text": "‚Ä¢ Solution\n\nUsing the {tidyverse} read_csv() function, load the ‚ÄúKamilarAndCooperData.csv‚Äù dataset from GitHub as a ‚Äútibble‚Äù named d.\n\n\nStep 1\n\nlibrary(tidyverse)\nlibrary(cowplot)\nlibrary(mosaic)\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/KamilarAndCooperData.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\n\n\n\nStep 2\n\nFrom this dataset, plot lifespan (scored as MaxLongevity_m in the dataset) versus female body mass (scored as Body_mass_female_mean). Is the relationship linear? If not, how might you transform one or both variable to more closely approximate a linear relationship?\n\n\np1 &lt;- ggplot(data = d, aes(x = Body_mass_female_mean, y = MaxLongevity_m)) + geom_point()\np2 &lt;- ggplot(data = d, aes(x = log(Body_mass_female_mean), y = MaxLongevity_m)) +\n    geom_point()\np3 &lt;- ggplot(data = d, aes(x = log(Body_mass_female_mean), y = log(MaxLongevity_m))) +\n    geom_point()\nplot_grid(p1, p2, p3, nrow = 1)\n\n## Warning: Removed 72 rows containing missing values or values outside the scale range\n## (`geom_point()`).\n## Removed 72 rows containing missing values or values outside the scale range\n## (`geom_point()`).\n## Removed 72 rows containing missing values or values outside the scale range\n## (`geom_point()`).\n\n\n\n\n\n\n\n\n\nWe see the cleanest linear relationship between the two variables when both are log transformed.\n\n\nStep 3\n\nRun linear models of:\n\nlifespan ~ female body mass\nlifespan ~ log(female body mass)\nlog(lifespan) ~ log(female body mass)\n\n\n\nm1 &lt;- lm(MaxLongevity_m ~ Body_mass_female_mean, data = d)\nm2 &lt;- lm(MaxLongevity_m ~ log(Body_mass_female_mean), data = d)\nm3 &lt;- lm(log(MaxLongevity_m) ~ log(Body_mass_female_mean), data = d)\n\n\n\nStep 4\n\nGenerate residuals for all three linear models, plot them by hand in relation to the corresponding explanatory variable, and make histograms of the residuals. Do they appear to be normally distributed?\n\n\np1 &lt;- ggplot(data = NULL, aes(x = m1$model$Body_mass_female_mean, y = m1$residuals)) +\n    geom_point()\np2 &lt;- ggplot(data = NULL, aes(x = m2$model$`log(Body_mass_female_mean)`, y = m2$residuals)) +\n    geom_point()\np3 &lt;- ggplot(data = NULL, aes(x = m3$model$`log(Body_mass_female_mean)`, y = m3$residuals)) +\n    geom_point()\np4 &lt;- histogram(m1$residuals, nint = 20)\np5 &lt;- histogram(m2$residuals, nint = 20)\np6 &lt;- histogram(m3$residuals, nint = 20)\nplot_grid(p1, p4, p2, p5, p3, p6, nrow = 3)\n\n\n\n\n\n\n\n\nBased on these plots, the m3 residuals appear to be most closely to normally distributed. The m1 residuals are clearly not, and the m2 residuals seems to show increasing variable with log(female body mass)\n\n\nStep 5\n\nGenerate QQ plots for all three linear models. Based on visual inspection of these plots, do the residual appear to deviate from being normally distributed?\n\n\npar(mfrow = c(3, 1))\ncar::qqPlot(m1$residuals)\n\n##  43 106 \n##  28  71\n\ncar::qqPlot(m2$residuals)\n\n##  43 106 \n##  28  71\n\ncar::qqPlot(m3$residuals)\n\n\n\n\n\n\n\n\n## 116  43 \n##  77  28\n\n# or\npar(mfrow = c(1, 1))\np1 &lt;- ggpubr::ggqqplot(m1$residuals)\np2 &lt;- ggpubr::ggqqplot(m2$residuals)\np3 &lt;- ggpubr::ggqqplot(m3$residuals)\nplot_grid(p1, p2, p3, nrow = 1)\n\n\n\n\n\n\n\n\nAgain, the m3 residuals look as if they do not deviate substantively from normal. Residuals for m1 and m2, by contrast, do appear to deviate from normal, with a large number of points falling outside of the confidence band in the QQ plots.\n\n\nStep 6\n\nRun the plot() command for all three models and visually inspect the resultant plots. What do the plots suggest about whether the assumptions for regression are met for any of these models?\n\n\npar(mfrow = c(2, 2))\nplot(m1)\n\n\n\n\n\n\n\nplot(m2)\n\n\n\n\n\n\n\nplot(m3)\n\n\n\n\n\n\n\n\nThe plots suggest that m1 and m2 residuals deviate quite markedly from normality,\n\n\nStep 7\n\nRun Shapiro-Wilks tests (e.g., using the function shapiro.test() on the residuals for all three models. What do the results of these test suggest about the whether your data meet the assumptions for using simple linear regression?\n\n\nshapiro.test(m1$residuals)\n\n## \n##  Shapiro-Wilk normality test\n## \n## data:  m1$residuals\n## W = 0.96352, p-value = 0.0008238\n\nshapiro.test(m2$residuals)\n\n## \n##  Shapiro-Wilk normality test\n## \n## data:  m2$residuals\n## W = 0.97515, p-value = 0.01131\n\nshapiro.test(m3$residuals)\n\n## \n##  Shapiro-Wilk normality test\n## \n## data:  m3$residuals\n## W = 0.99319, p-value = 0.7403\n\n\nThe p value for the Shapiro-Wilks test is less than 0.05 for both the m1 and m2 residuals, leading us to reject the null hypothesis that that they are normally distributed (or, better put, that they do not deviate more than expected from normality). The p value for the test of the m3 residuals is &gt;&gt; 0.05, leading us to not reject the null hypothesis that that they are normally distributed.",
    "crumbs": [
      "Exercises",
      "Exercise 09 Solution"
    ]
  },
  {
    "objectID": "exercise-10.html",
    "href": "exercise-10.html",
    "title": "Exercise 10",
    "section": "",
    "text": "Practice ANOVA",
    "crumbs": [
      "Exercises",
      "Exercise 10"
    ]
  },
  {
    "objectID": "exercise-10.html#learning-objectives",
    "href": "exercise-10.html#learning-objectives",
    "title": "Exercise 10",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nConduct one-factor and two-factor ANOVA\nUse permutation-based methods for inference testing\n\n\nPreliminaries\n\nSet up a new GitHub repo in your GitHub workspace named ‚Äúexercise-11‚Äù and clone that down to your computer as a new RStudio project. The instructions outlined as Method 1 in Module 6 will be helpful.\nUsing the {tidyverse} read_csv() function, load the ‚ÄúAVONETdataset1.csv‚Äù dataset from this URL as a ‚Äútibble‚Äù named d. As discussed in class, this is a recently published dataset that compiles morphological measurements and information on various ecological variables and geographic range data for more than 11,000 species of birds.\n\n\nData source:\nTobias JA, et al.¬†(2022). AVONET: Morphological, ecological and geographical data for all birds. Ecology Letters 25: 581‚Äì597.\n\n\nWinnow the dataset to include only the following variables: Species1, Family1, Order1, Beak.Length_Culmen, Beak.Width, Beak.Depth, Tarsus.Length, Wing.Length, Tail.Length, Mass, Habitat, Migration, Trophic.Level, Trophic.Niche, Min.Latitude, Max.Latitude, Centroid.Latitude, Primary.Lifestyle, and Range.Size\nDo a bit of exploratory data analysis with this dataset, e.g., using the {skimr} package. Which of the variables are categorical and which are numeric?",
    "crumbs": [
      "Exercises",
      "Exercise 10"
    ]
  },
  {
    "objectID": "exercise-10.html#challenge-1",
    "href": "exercise-10.html#challenge-1",
    "title": "Exercise 10",
    "section": "Challenge 1",
    "text": "Challenge 1\n\nOne-Factor ANOVA and Inference\n\n\nStep 1\n\nMake boxplots of log(Mass) in relation to Trophic.Level and Migration behavior type. For each plot, drop from the visualization all species records where the categorical variable of interest is missing from the dataset. Also, you will want to convert the variable Migration (which is scored as a number: ‚Äú1‚Äù, ‚Äú2‚Äù, or ‚Äú3‚Äù) from class numeric to either being classified as a factor or as a character (string) variable.\n\n\n\nStep 2\n\nRun linear models using the lm() function to look at the relationship between log(Mass) and Trophic.Level and between log(Mass) and Migration.\nExamine the output of the resultant linear models. Is log(Mass) associated with either Trophic.Level or Migration category? That is, in the global test of significance, is the F statistic large enough to reject the null hypothesis of an F value of zero?\nGiven the regression coefficients returned for your Migration model, which Migration categor(ies) are different than the reference level? What level is the reference level? Relevel and assess differences among the remaining pair of Migration categories.\n\n\n\nStep 3\n\nConduct a post-hoc Tukey Honest Significant Differences test to also evaluate which Migration categories differ ‚Äúsignificantly‚Äù from one another (see Module 20).\n\n\n\nStep 4\n\nUse a permutation approach to inference to generate a null distribution of F statistic values for the model of log(Mass) in relation to Trophic.Level and calculate a p value for your original F statistic. You can do this either by programming your own permutation test (e.g., by shuffling values for the predictor or response variable among observations and calculating an F statistic for each replicate) or by using the {infer} workflow and setting calculate(stat=\"F\").",
    "crumbs": [
      "Exercises",
      "Exercise 10"
    ]
  },
  {
    "objectID": "exercise-10.html#challenge-2",
    "href": "exercise-10.html#challenge-2",
    "title": "Exercise 10",
    "section": "Challenge 2",
    "text": "Challenge 2\n\nData Wrangling plus One- and Two-Factor ANOVA\n\n\nStep 1\n\nCreate the following two new variables and add them to AVONET dataset:\n\nRelative beak length, which you should calculate as the residual of log(Beak.Length_Culmen) on log(Mass).\nRelative tarsus length, which you should calculate as the residual of log(Tarsus.Length) on log(Mass).\n\n\n\n\nStep 2\n\nMake a boxplot or violin plot of your new relative tarsus length variable in relation to Primary.Lifestyle and of your new relative beak length variable in relation to Trophic.Niche\n\n\n\nStep 3\n\nRun ANOVA analyses to look at the association between geographic range size and the variable Migration. You should first drop those observations for which Migration is not scored and also look at the distribution of the variable Range.Size to decide whether and how it might need to be transformed. Based on the global model, is range size associated with form of migration? How much of the variance in your measure of range size is associated with Migration behavior style?\nGiven the regression coefficients returned in the output of the model, which Migration categor(ies) are different than the reference level? What level is the reference level? Relevel and assess differences among the remaining pair of Migration categories. Also conduct a post-hoc Tukey Honest Significant Differences test to also evaluate which Migration categories differ ‚Äúsignificantly‚Äù from one another (see Module 20).\n\n\n\nStep 4\n\nWinnow your original data to just consider birds from the Infraorder ‚ÄúPasseriformes‚Äù (song birds).\nRun separate one-factor ANOVA analyses to look at the association between [1] relative beak length and Primary.Lifestyle and between [2] relative beak length and Trophic.Level. In doing so‚Ä¶\n\nMake boxplots of response variable by each predictor and by the combination of predictors.\nRun linear models for each predictor separately and interpret the model output.\n\n\n\n\nStep 5\n\nRun a two-factor model to look at the association between relative beak length and both Primary.Lifestyle and Trophic.Level among the passeriforms. Based on the model output, what would you conclude about how relative beak length is related to these two variables?\n\n\n\nStep 6\n\nFinally, run an additional two-way model with the same dataset and predictors, but adding the possibility of an interaction term. To do this, you should modify your model formula using the colon operator (:) to specify the interaction, e.g., relative beak length ~ Primary.Lifestyle + Trophic.Level + Primary.Lifestyle:Trophic.Level. Based on the model output, what would you now conclude about how relative beak length is related to these two variables?\n\n\n\nStep 7\n\nUse the interaction.plot() function to visualize the interaction between Primary.Lifestyle and Trophic.Level (see Module 20).\n\n\n\nStep 8\nIn the exercise above, we really did not do any checking with this dataset to see if the data meet the primary assumptions for standard linear regression and ANOVA, which are that variables/residuals within each grouping level are roughly normally distributed and have roughly equal variances. Sample sizes within each grouping level should also be roughly equal. As noted in Module 20, a general rule of thumb for ‚Äúequal‚Äù variances is to compare the largest and smallest within-grouping level standard deviations and, if this value is less than 2, then it is often reasonable to presume the assumption may not be violated.\nUse this approach to see whether variances in across groups in your various models (e.g., for relative beak length ~ trophic level) are roughly equal. Additionally, do a visual check of whether observations and model residuals within groups look to be normally distributed.",
    "crumbs": [
      "Exercises",
      "Exercise 10"
    ]
  },
  {
    "objectID": "exercise-10-solution.html",
    "href": "exercise-10-solution.html",
    "title": "Exercise 10 Solution",
    "section": "",
    "text": "‚Ä¢ Solution\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(cowplot)\nlibrary(broom)\nlibrary(infer)\nlibrary(mosaic)\nlibrary(ggpubr)  # for ggqqplot()\n\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/AVONETdataset1.csv\"\nd &lt;- read_csv(f, col_names = TRUE)\nd &lt;- d |&gt;\n    select(Species1, Family1, Order1, Beak.Length_Culmen, Beak.Width, Beak.Depth,\n        Tarsus.Length, Wing.Length, Tail.Length, Mass, Habitat, Migration, Trophic.Level,\n        Trophic.Niche, Min.Latitude, Max.Latitude, Centroid.Latitude, Range.Size,\n        Primary.Lifestyle)\nskim(d)\n\n\nData summary\n\n\nName\nd\n\n\nNumber of rows\n11009\n\n\nNumber of columns\n19\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n7\n\n\nnumeric\n12\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nSpecies1\n0\n1.00\n9\n36\n0\n11009\n0\n\n\nFamily1\n0\n1.00\n7\n18\n0\n243\n0\n\n\nOrder1\n0\n1.00\n10\n19\n0\n36\n0\n\n\nHabitat\n98\n0.99\n4\n14\n0\n11\n0\n\n\nTrophic.Level\n5\n1.00\n8\n9\n0\n4\n0\n\n\nTrophic.Niche\n10\n1.00\n8\n21\n0\n10\n0\n\n\nPrimary.Lifestyle\n0\n1.00\n6\n11\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nBeak.Length_Culmen\n0\n1.00\n26.36\n24.39\n4.50\n14.70\n19.90\n28.50\n414.20\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nBeak.Width\n0\n1.00\n6.58\n5.15\n0.70\n3.60\n5.00\n7.70\n88.90\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nBeak.Depth\n0\n1.00\n8.06\n7.59\n1.00\n3.80\n5.80\n9.40\n110.90\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nTarsus.Length\n0\n1.00\n28.73\n24.84\n2.50\n17.40\n22.00\n31.30\n481.20\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nWing.Length\n0\n1.00\n124.78\n93.44\n0.10\n66.80\n91.50\n145.50\n789.90\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n\n\nTail.Length\n0\n1.00\n86.65\n61.08\n0.10\n50.20\n68.70\n99.90\n812.80\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nMass\n0\n1.00\n267.15\n1883.03\n1.90\n15.00\n35.50\n121.00\n111000.00\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nMigration\n23\n1.00\n1.29\n0.62\n1.00\n1.00\n1.00\n1.00\n3.00\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nMin.Latitude\n57\n0.99\n-6.44\n22.37\n-85.58\n-21.22\n-7.15\n8.07\n68.08\n‚ñÅ‚ñÉ‚ñá‚ñÉ‚ñÅ\n\n\nMax.Latitude\n57\n0.99\n11.51\n23.32\n-65.12\n-3.33\n9.00\n22.07\n85.01\n‚ñÅ‚ñÉ‚ñá‚ñÇ‚ñÅ\n\n\nCentroid.Latitude\n57\n0.99\n2.95\n22.07\n-71.04\n-9.73\n-0.22\n15.28\n78.43\n‚ñÅ‚ñÉ‚ñá‚ñÇ‚ñÅ\n\n\nRange.Size\n57\n0.99\n2578859.38\n7629310.06\n0.88\n54052.87\n416076.61\n2187040.21\n136304432.20\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\nThe taxomic variables, along with Habitat, Trophic.Level, Trophic.Niche, and Migration, are categorical, although Migration is scored using integer factors as ‚Äú1‚Äù, ‚Äú2‚Äù, or ‚Äú3‚Äù. The remaining variables are numeric.",
    "crumbs": [
      "Exercises",
      "Exercise 10 Solution"
    ]
  },
  {
    "objectID": "exercise-10-solution.html#challenge-1",
    "href": "exercise-10-solution.html#challenge-1",
    "title": "Exercise 10 Solution",
    "section": "Challenge 1",
    "text": "Challenge 1\n\nStep 1\n\nMake boxplots of log(Mass) in relation to Trophic.Level and Migration and convert the variable Migration into a factor.\n\n\nd &lt;- d |&gt;\n    mutate(logMass = log(Mass), logRS = log(Range.Size), logBeak = log(Beak.Length_Culmen),\n        logTarsus = log(Tarsus.Length), Migration = as.factor(Migration))\n\np1 &lt;- ggplot(data = d |&gt;\n    drop_na(Trophic.Level), aes(x = Trophic.Level, y = log(Mass))) + geom_boxplot() +\n    geom_jitter(alpha = 0.05)\n\np2 &lt;- ggplot(data = d |&gt;\n    drop_na(Migration), aes(x = Migration, y = log(Mass))) + geom_boxplot() + geom_jitter(alpha = 0.05)\nplot_grid(p1, p2, nrow = 1)\n\n\n\n\n\n\n\n\n\n\nStep 2\n\nRun linear models using the lm() function to look at the relationship between log(Mass) and Trophic.Level and between log(Mass) and Migration.\n\n\nm1 &lt;- lm(logMass ~ Trophic.Level, data = d)\nm2 &lt;- lm(logMass ~ Migration, data = d)\n\n\nExamine the output of the resultant linear models. Is log(Mass) associated with either Trophic.Level or Migration category? That is, in the global test of significance, is the F statistic large enough to reject the null hypothesis of an F value of zero?\n\n\nsummary(m1)\n\n## \n## Call:\n## lm(formula = logMass ~ Trophic.Level, data = d)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.4229 -1.1551 -0.3028  0.8982  7.5526 \n## \n## Coefficients:\n##                        Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)             3.80834    0.01967 193.632  &lt; 2e-16 ***\n## Trophic.LevelHerbivore  0.25639    0.03406   7.528 5.54e-14 ***\n## Trophic.LevelOmnivore   0.01422    0.04116   0.345     0.73    \n## Trophic.LevelScavenger  4.63189    0.34447  13.446  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.538 on 11000 degrees of freedom\n##   (5 observations deleted due to missingness)\n## Multiple R-squared:  0.02094,    Adjusted R-squared:  0.02067 \n## F-statistic: 78.42 on 3 and 11000 DF,  p-value: &lt; 2.2e-16\n\nsummary(m2)\n\n## \n## Call:\n## lm(formula = logMass ~ Migration, data = d)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.8924 -1.1769 -0.3088  0.9152  7.8427 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  3.77457    0.01636 230.710  &lt; 2e-16 ***\n## Migration2   0.75971    0.04731  16.059  &lt; 2e-16 ***\n## Migration3   0.37647    0.05155   7.303 3.02e-13 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.535 on 10983 degrees of freedom\n##   (23 observations deleted due to missingness)\n## Multiple R-squared:  0.02563,    Adjusted R-squared:  0.02546 \n## F-statistic: 144.5 on 2 and 10983 DF,  p-value: &lt; 2.2e-16\n\n\nIn both cases, the global test is significant.\n\nGiven the regression coefficients returned for your Migration model, which Migration categor(ies) are different than the reference level? What level is the reference level?\n\nIn m2, Migration levels 2 and 3 are both different from Migration level 1, which is the reference level.\n\nRelevel and assess differences among the remaining pair of Migration categories.\n\n\nd &lt;- d |&gt;\n    mutate(Migration = relevel(Migration, ref = \"3\"))\nm2 &lt;- lm(logMass ~ Migration, data = d)\nsummary(m2)\n\n## \n## Call:\n## lm(formula = logMass ~ Migration, data = d)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.8924 -1.1769 -0.3088  0.9152  7.8427 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  4.15104    0.04889  84.909  &lt; 2e-16 ***\n## Migration1  -0.37647    0.05155  -7.303 3.02e-13 ***\n## Migration2   0.38324    0.06603   5.804 6.67e-09 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.535 on 10983 degrees of freedom\n##   (23 observations deleted due to missingness)\n## Multiple R-squared:  0.02563,    Adjusted R-squared:  0.02546 \n## F-statistic: 144.5 on 2 and 10983 DF,  p-value: &lt; 2.2e-16\n\n\nBased on the re-leveled model results, Migration levels 2 and 3 are also different from one another.\n\n\nStep 3\n\nConduct a post-hoc Tukey Significant Differences test to also evaluate which Migration categories differ ‚Äúsignificantly‚Äù from one another.\n\n\n# we need an ANOVA object to run a Tukey test\nm2 &lt;- aov(logMass ~ Migration, data = d)\n(posthoc &lt;- TukeyHSD(m2, which = \"Migration\", conf.level = 0.95))\n\n##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n## \n## Fit: aov(formula = logMass ~ Migration, data = d)\n## \n## $Migration\n##           diff        lwr        upr p adj\n## 1-3 -0.3764693 -0.4973105 -0.2556282     0\n## 2-3  0.3832374  0.2284536  0.5380211     0\n## 2-1  0.7597067  0.6488157  0.8705977     0\n\nplot(posthoc)\n\n\n\n\n\n\n\n\nComparisons among all pairs of levels are significant.\n\n\nStep 4\n\nUse a permutation approach to inference to generate a null distribution of F statistic values for the model of log(Mass) in relation to Trophic.Level and calculate a p value for your original F statistic.\n\n\noriginal.F &lt;- aov(logMass ~ Trophic.Level, data = d) |&gt;\n    tidy() |&gt;\n    filter(term == \"Trophic.Level\") |&gt;\n    pull(statistic)\n# show aov results for F statistic and p value for omnibus F test\noriginal.F\n\n## [1] 78.42283\n\n# using {infer}\npermuted.F &lt;- d |&gt;\n    specify(logMass ~ Trophic.Level) |&gt;\n    hypothesize(null = \"independence\") |&gt;\n    generate(reps = 1000, type = \"permute\") |&gt;\n    calculate(stat = \"F\")\n\nvisualize(permuted.F) + shade_p_value(obs_stat = original.F, direction = \"greater\")\n\n\n\n\n\n\n\np.value &lt;- permuted.F |&gt;\n    get_p_value(obs_stat = original.F, direction = \"greater\")\n\n# or...  using {mosaic}\nreps &lt;- 1000\npermuted.F &lt;- do(reps) * {\n    d |&gt;\n        mutate(Trophic.Level = sample(Trophic.Level)) %&gt;%\n        # note the use of the %&gt;% pipe operator in the line above... for some\n        # reason, the native pipe operator (`|&gt;`) throws an error!\n    aov(logMass ~ Trophic.Level, data = .) |&gt;\n        tidy() |&gt;\n        filter(term == \"Trophic.Level\") |&gt;\n        pull(statistic)\n}\n\npermuted.F &lt;- as_tibble(permuted.F) |&gt;\n    rename(stat = \"result\")\n\np.value &lt;- permuted.F |&gt;\n    get_p_value(obs_stat = original.F, direction = \"greater\")\np.value\n\n## # A tibble: 1 √ó 1\n##   p_value\n##     &lt;dbl&gt;\n## 1       0",
    "crumbs": [
      "Exercises",
      "Exercise 10 Solution"
    ]
  },
  {
    "objectID": "exercise-10-solution.html#challenge-2",
    "href": "exercise-10-solution.html#challenge-2",
    "title": "Exercise 10 Solution",
    "section": "Challenge 2",
    "text": "Challenge 2\n\nStep 1\n\nCreate the following two new variables and add them to AVONET dataset:\n\nRelative beak length, which you should calculate as the residual of log(Beak.Length_Culmen) on log(Mass).\nRelative tarsus length, which you should calculate as the residual of log(Tarsus.Length) on log(Mass).\n\n\n\nrelBeak &lt;- lm(logBeak ~ logMass, data = d)\nrelTarsus &lt;- lm(logTarsus ~ logMass, data = d)\nd &lt;- d |&gt;\n    mutate(relBeak = relBeak$residuals, relTarsus = relTarsus$residuals)\n\n\n\nStep 2\n\nMake a boxplot or violin plot of your new relative tarsus length variable in relation to Primary.Lifestyle and of your new relative beak length variable in relation to Trophic.Niche\n\n\nd &lt;- d |&gt;\n    mutate(Primary.Lifestyle = factor(Primary.Lifestyle, levels = c(\"Aerial\", \"Aquatic\",\n        \"Insessorial\", \"Terrestrial\", \"Generalist\")))\np1 &lt;- ggplot(data = d |&gt;\n    drop_na(Primary.Lifestyle), aes(x = Primary.Lifestyle, y = relTarsus)) + geom_boxplot() +\n    # geom_jitter(alpha = 0.05) +\ntheme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nd &lt;- d |&gt;\n    mutate(Trophic.Niche = factor(Trophic.Niche, levels = c(\"Nectarivore\", \"Herbivore aquatic\",\n        \"Frugivore\", \"Granivore\", \"Herbivore terrestrial\", \"Aquatic predator\", \"Invertivore\",\n        \"Vertivore\", \"Scavenger\", \"Omnivore\")))\np2 &lt;- ggplot(data = d |&gt;\n    drop_na(Trophic.Niche), aes(x = Trophic.Niche, y = relBeak)) + geom_boxplot() +\n    # geom_jitter(alpha = 0.05) +\ntheme(axis.text.x = element_text(angle = 45, hjust = 1))\nplot_grid(p1, p2, nrow = 1)\n\n\n\n\n\n\n\n\n\n\nStep 3\n\nRun ANOVA analyses to look at the association between geographic range size and the variable Migration. You should first drop those observations for which Migration is not scored and also look at the distribution of the variable Range.Size to decide whether and how it might need to be transformed.\n\n\nmigration &lt;- d |&gt;\n    drop_na(Migration)\nhistogram(migration$Range.Size)\n\n\n\n\n\n\n\nhistogram(migration$logRS)\n\n\n\n\n\n\n\n\nIt looks like Range.Size should be log transformed!\n\n# look at distribution of the data across levels\nggplot(data = migration, aes(x = Migration, y = logRS)) + geom_violin() + geom_jitter(alpha = 0.05,\n    width = 0.5)\n\n\n\n\n\n\n\n\n\nm3 &lt;- lm(logRS ~ Migration, data = migration)\nsummary(m3)\n\n## \n## Call:\n## lm(formula = logRS ~ Migration, data = migration)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -14.5710  -1.4521   0.4357   1.9763   5.9271 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 14.55082    0.08896 163.568  &lt; 2e-16 ***\n## Migration1  -2.51702    0.09380 -26.834  &lt; 2e-16 ***\n## Migration2  -0.73233    0.12015  -6.095 1.13e-09 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.785 on 10934 degrees of freedom\n##   (49 observations deleted due to missingness)\n## Multiple R-squared:  0.0869, Adjusted R-squared:  0.08674 \n## F-statistic: 520.3 on 2 and 10934 DF,  p-value: &lt; 2.2e-16\n\n\n\nBased on the global model, is range size associated with form of migration? How much of the variance in your measure of range size is associated with Migration behavior level?\n\nFrom the model summary(), we can see that the F statistic is significant, although the R-squared value suggests that only ~9% of the variation in log(range size) is associated with Migration behavior style.\n\nNOTE: We can calculate the amount of variance explained by the model (i.e., R-squared, or, a.k.a. \\(\\eta\\)-squared) ourselves from the ANOVA table output from running aov(). To do so, we calculate the ratio of the regression sum of squares to the total sum of squares. The code below pulls these value from a tidy() table of the model coefficients‚Ä¶\n\n\nm3 &lt;- aov(logRS ~ Migration, data = migration)\nsummary(m3)\n\n##                Df Sum Sq Mean Sq F value Pr(&gt;F)    \n## Migration       2   8071    4035   520.3 &lt;2e-16 ***\n## Residuals   10934  84798       8                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## 49 observations deleted due to missingness\n\ncoefs &lt;- m3 |&gt;\n    tidy() %&gt;%\n    select(term, sumsq)\n(r2 &lt;- 1 - (coefs |&gt;\n    filter(term == \"Residuals\") |&gt;\n    pull(sumsq))/(coefs |&gt;\n    summarize(totalSS = sum(sumsq)) |&gt;\n    pull(totalSS)))\n\n## [1] 0.0869031\n\n\n\nGiven the regression coefficients returned in the output of the model, which Migration categor(ies) are different than the reference level? What level is the reference level?\n\nIn the summary() of the m3 linear model, Migration levels 1 and 2 are both different from Migration level 3, which is the reference level (because that is what we releveled to above).\n\nRe-level and assess differences among the remaining pair of Migration categories.\n\n\nmigration &lt;- migration |&gt;\n    mutate(Migration = relevel(Migration, ref = \"1\"))\nm3 &lt;- aov(logRS ~ Migration, data = migration)\nsummary(m3)\n\n##                Df Sum Sq Mean Sq F value Pr(&gt;F)    \n## Migration       2   8071    4035   520.3 &lt;2e-16 ***\n## Residuals   10934  84798       8                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## 49 observations deleted due to missingness\n\n\nAfter re-leveling, we can see that Migration levels 1 and 2 are also different from one another.\n\nAlso conduct a post-hoc Tukey Honest Significant Differences test to also evaluate which Migration categories differ ‚Äúsignificantly‚Äù from one another.\n\n\n(posthoc &lt;- TukeyHSD(m3, which = \"Migration\", conf.level = 0.95))\n\n##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n## \n## Fit: aov(formula = logRS ~ Migration, data = migration)\n## \n## $Migration\n##           diff       lwr       upr p adj\n## 3-1  2.5170168  2.297150  2.736883     0\n## 2-1  1.7846901  1.582952  1.986428     0\n## 2-3 -0.7323266 -1.013964 -0.450689     0\n\nplot(posthoc)\n\n\n\n\n\n\n\n\n\n\nStep 4\n\nWinnow your original data to just consider birds from the Infraorder ‚ÄúPasseriformes‚Äù (song birds).\n\n\npass &lt;- d |&gt;\n    filter(Order1 == \"Passeriformes\")\n\n\nRun separate one-factor ANOVA analyses to look at the association between [1] relative beak length and Primary.Lifestyle and between [2] relative beak length and Trophic.Level. In doing so‚Ä¶\n\nMake boxplots of the response variable by each predictor and by the combination of predictors‚Ä¶\n\np1 &lt;- ggplot(data = pass, aes(x = Primary.Lifestyle, y = relBeak)) + geom_boxplot() +\n    geom_jitter(alpha = 0.05) + theme(axis.text.x = element_text(angle = 45, hjust = 1))\np2 &lt;- ggplot(data = pass, aes(x = Trophic.Level, y = relBeak)) + geom_boxplot() +\n    geom_jitter(alpha = 0.05) + theme(axis.text.x = element_text(angle = 45, hjust = 1))\np3 &lt;- ggplot(data = pass, aes(x = Primary.Lifestyle, y = relBeak)) + geom_boxplot() +\n    geom_jitter(alpha = 0.05) + facet_wrap(~Trophic.Level) + theme(axis.text.x = element_text(angle = 45,\n    hjust = 1))\np4 &lt;- ggplot(data = pass, aes(x = Trophic.Level, y = relBeak)) + geom_boxplot() +\n    geom_jitter(alpha = 0.05) + facet_wrap(~Primary.Lifestyle) + theme(axis.text.x = element_text(angle = 45,\n    hjust = 1))\nplot_grid(plot_grid(p1, p2, nrow = 1), p3, p4, nrow = 3)\n\n\n\n\n\n\n\n\n‚Ä¶ and run linear models for each predictor separately and interpret the model output.\n\nm5 &lt;- aov(relBeak ~ Primary.Lifestyle, data = pass)\n# or m5 &lt;- lm(relBeak ~ Primary.Lifestyle, data = pass)\nsummary(m5)\n\n##                     Df Sum Sq Mean Sq F value Pr(&gt;F)    \n## Primary.Lifestyle    3   18.2   6.067   130.2 &lt;2e-16 ***\n## Residuals         6610  307.9   0.047                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nm6 &lt;- aov(relBeak ~ Trophic.Level, data = pass)\n# or m6 &lt;- lm(relBeak ~ Trophic.Level, data = pass)\nsummary(m6)\n\n##                 Df Sum Sq Mean Sq F value Pr(&gt;F)    \n## Trophic.Level    2  16.31   8.154     174 &lt;2e-16 ***\n## Residuals     6611 309.81   0.047                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nRelative beak length is significantly associated with both variables, but the R-squared (or eta- squared) values are quite modest.\n\n\nStep 5\n\nRun a two-factor model to look at the association between relative beak length and both Primary.Lifestyle and Trophic.Level among the passeriforms. Based on the model output, what would you conclude about how relative beak length is related to these two variables?\n\n\nm7 &lt;- aov(relBeak ~ Primary.Lifestyle + Trophic.Level, data = pass)\n# or m7 &lt;- lm(relBeak ~ Primary.Lifestyle + Trophic.Level, data = pass)\nsummary(m7)\n\n##                     Df Sum Sq Mean Sq F value Pr(&gt;F)    \n## Primary.Lifestyle    3  18.20   6.067   138.1 &lt;2e-16 ***\n## Trophic.Level        2  17.68   8.838   201.2 &lt;2e-16 ***\n## Residuals         6608 290.24   0.044                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBoth variables are important predictors of relative beak length.\n\n\nStep 6\n\nFinally, run an additional two-way model with the same dataset and predictors, but adding the possibility of an interaction term. Based on the model output, what would you now conclude about how relative beak length is related to these two variables?\n\n\nm8 &lt;- aov(relBeak ~ Primary.Lifestyle + Trophic.Level + Primary.Lifestyle:Trophic.Level,\n    data = pass)\n# or m8 &lt;- lm(relBeak ~ Primary.Lifestyle + Trophic.Level +\n# Primary.Lifestyle:Trophic.Level, data = pass) or m8 &lt;- lm(relBeak ~\n# Primary.Lifestyle * Trophic.Level, data = pass)\nsummary(m8)\n\n##                                   Df Sum Sq Mean Sq F value Pr(&gt;F)    \n## Primary.Lifestyle                  3  18.20   6.067  142.15 &lt;2e-16 ***\n## Trophic.Level                      2  17.68   8.838  207.11 &lt;2e-16 ***\n## Primary.Lifestyle:Trophic.Level    4   8.41   2.102   49.26 &lt;2e-16 ***\n## Residuals                       6604 281.83   0.043                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nPrimary.Lifestyle, Trophic.Level, and the interaction of these two variables are all significant predictors of relative beak length.\n\n\nStep 7\n\nUse the interaction.plot() function to visualize the interaction between Primary.Lifestyle and Trophic.Level.\n\n\npar(mfrow=c(2,1))\ninteraction.plot(\n  x.factor = pass$Primary.Lifestyle,\n  xlab = \"Primary Lifestyle\",\n  trace.factor = pass$Trophic.Level,\n  trace.label = \"Trophic Level\",\n  response = pass$relBeak,\n  fun = base::mean, # make sure we use {base} version\n  ylab = \"Mean Relative Beak Length\"\n)\n\ninteraction.plot(\n  x.factor = pass$Trophic.Level,\n  xlab = \"Trophic Level\",\n  trace.factor = pass$Primary.Lifestyle,\n  trace.label = \"Primary Lifestyle\",\n  response = pass$relBeak,\n  fun = base::mean, # make sure we use {base} version\n  ylab = \"Mean Relative Beak Length\"\n)",
    "crumbs": [
      "Exercises",
      "Exercise 10 Solution"
    ]
  },
  {
    "objectID": "exercise-10-solution.html#step-8",
    "href": "exercise-10-solution.html#step-8",
    "title": "Exercise 10 Solution",
    "section": "Step 8",
    "text": "Step 8\nLet‚Äôs look at the model of relative beak size ~ trophic level.\nFirst, we check the ratio of variances among different levels of the predictor‚Ä¶\n\nsd_ratio &lt;- pass |&gt;\n    group_by(Trophic.Level) |&gt;\n    summarize(sd = sd(relBeak, na.rm = TRUE)) |&gt;\n    pull(sd)\n(sd_ratio &lt;- max(sd_ratio)/min(sd_ratio))\n\n## [1] 1.336194\n\n\nThen, we do visual checks for normality of observations and residuals within groups‚Ä¶ all seem to deviate from normality!\n\np1 &lt;- ggplot(data = pass, aes(x = relBeak)) + geom_histogram() + facet_wrap(~Trophic.Level)\nm6data &lt;- tibble(residuals = m6$residuals, Trophic.Level = m6$model$Trophic.Level)\np2 &lt;- ggqqplot(data = m6data, x = \"residuals\") + facet_wrap(~Trophic.Level)\nplot_grid(p1, p2, nrow = 2)\n\n\n\n\n\n\n\n\nWe can do the same for our model of log(range size) ~ migration level.\nRatio of variances‚Ä¶\n\nsd_ratio &lt;- migration |&gt;\n    group_by(Migration) |&gt;\n    summarize(sd = sd(logRS, na.rm = TRUE)) |&gt;\n    pull(sd)\n(sd_ratio &lt;- max(sd_ratio)/min(sd_ratio))\n\n## [1] 1.332764\n\n\nVisual checks‚Ä¶ again all seem to deviate from normality!\n\np1 &lt;- ggplot(data = migration, aes(x = logRS)) + geom_histogram() + facet_wrap(~Migration)\nm3data &lt;- tibble(residuals = m3$residuals, Migration = m3$model$Migration)\np2 &lt;- ggqqplot(data = m3data, x = \"residuals\") + facet_wrap(~Migration)\nplot_grid(p1, p2, nrow = 2)\n\n\n\n\n\n\n\n\nAlthough the variance ratios are under 2, given the clear deviation from normality for the residuals, we are not really justified in using standard (or general) linear regression for the analyses done above.",
    "crumbs": [
      "Exercises",
      "Exercise 10 Solution"
    ]
  },
  {
    "objectID": "exercise-11.html",
    "href": "exercise-11.html",
    "title": "Exercise 11",
    "section": "",
    "text": "Practice Model Selection",
    "crumbs": [
      "Exercises",
      "Exercise 11"
    ]
  },
  {
    "objectID": "exercise-11.html#learning-objectives",
    "href": "exercise-11.html#learning-objectives",
    "title": "Exercise 11",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nRun exploratory multivariate regression to evaluate different models\n\n\nPreliminaries\n\nSet up a new GitHub repo in your GitHub workspace named ‚Äúexercise-11‚Äù and clone that down to your computer as a new RStudio project. The instructions outlined as Method 1 in Module 6 will be helpful.\nUsing the {tidyverse} read_tsv() function, load the ‚ÄúMammal_lifehistories_v2.txt‚Äù dataset from this URL as a ‚Äútibble‚Äù named d. As discussed in class, this is dataset that compiles life history and other variables for over 1400 species of placental mammals from 17 different Orders.\n\n\nData source:\nErnest SKM. (2003). Life history characteristics of placental nonvolant mammals. Ecology 84: 3402‚Äì3402.\n\n\nDo a bit of exploratory data analysis with this dataset, e.g., using the {skimr} package. Which of the variables are categorical and which are numeric?",
    "crumbs": [
      "Exercises",
      "Exercise 11"
    ]
  },
  {
    "objectID": "exercise-11.html#challenge",
    "href": "exercise-11.html#challenge",
    "title": "Exercise 11",
    "section": "Challenge",
    "text": "Challenge\n\nStep 1\n\nReplace all values of -999 (the authors‚Äô code for missing data) with NA.\n\n\nHINT: This is easy to do in base {R}, but you can also check out the replace_with_na_all() function from the {naniar} package.\n\n\n\nStep 2\n\nDrop the variables litter size and refs.\nRename the variable max. life(mo) to maxlife(mo).\nRename the variable wean mass(g) to weanmass(g).\n\n\n\nStep 3\n\nLog transform all of the other numeric variables.\n\n\nHINT: There are lots of ways to do this‚Ä¶ look into mutate(across(where(), .funs)) for an efficient motif.\n\n\n\nStep 4\n\nRegress the (now log transformed) age [gestation(mo), weaning(mo), AFR(mo) (i.e., age at first reproduction), and maxlife(mo) (i.e., maximum lifespan)] and mass [newborn(g) and weanmass(g)] variables on (now log transformed) overall body mass(g) and add the residuals to the dataframe as new variables [relGest, relWean, relAFR, relLife, relNewbornMass, and relWeaningMass].\n\n\nHINT: Use ‚Äúna.action=na.exclude‚Äù in yourlm() calls. With this argument set, the residuals will be padded to the correct length by inserting NAs for cases with missing data. To access these correctly, however, where the length of the vector of residuals is equal in length to the length of your original vector (i.e., where missing residuals are padded out as NAs) you will need to call the resid(m) function on the model object (assuming the model is named m) rather than by calling m$residuals. The former function returns a vector of residuals with ‚ÄúNA‚Äù for cases where the value of one of the formula variables is missing, while the latter returns a vector with the NAs dropped, which may be shorter than the length of the original data frame!\n\n\n\nStep 5\n\nPlot residuals of max lifespan (relLife) in relation to Order. Which mammalian orders have the highest residual lifespan?\nPlot residuals of newborn mass (relNewbornMass) in relation to Order. Which mammalian orders have the have highest residual newborn mass?\nPlot residuals of weaning mass (relWeaningMass) in relation to Order. Which mammalian orders have the have highest residual weaning mass?\n\n\nNOTE: There will be lots of missing data for the latter two variables!\n\n\n\nStep 6\n\nRun models and a model selection process to evaluate what (now log transformed) variables best predict each of the two response variables, maxlife(mo) and AFR(mo), from the set of the following predictors: gestation(mo), newborn(g), weaning(mo), weanmass(g), litters/year, and overall body mass(g).\n\n\nHINT: Before running models, winnow your dataset to drop rows that are missing the respective response variable or any of the predictors, e.g., by using drop_na().\n\n\nFor each of the two response variables, indicate what is the best model overall based on AICc and how many models have a delta AICc of 4 or less?\nWhat variables, if any, appear in all of this set of ‚Äútop‚Äù models?\nCalculate and plot the model-averaged coefficients and their CIs across this set of top models.",
    "crumbs": [
      "Exercises",
      "Exercise 11"
    ]
  },
  {
    "objectID": "exercise-11-solution.html",
    "href": "exercise-11-solution.html",
    "title": "Exercise 11 Solution",
    "section": "",
    "text": "‚Ä¢ Solution\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(cowplot)\nlibrary(broom)\nlibrary(MuMIn)\nf &lt;- \"https://raw.githubusercontent.com/difiore/ada-datasets/main/Mammal_lifehistories_v2.txt\"\nd &lt;- read_tsv(f, col_names = TRUE)\nskim(d)\n\n\nData summary\n\n\nName\nd\n\n\nNumber of rows\n1440\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nnumeric\n10\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\norder\n0\n1\n7\n14\n0\n17\n0\n\n\nfamily\n0\n1\n6\n15\n0\n96\n0\n\n\nGenus\n0\n1\n3\n16\n0\n618\n0\n\n\nspecies\n0\n1\n3\n17\n0\n1191\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nmass(g)\n0\n1\n3.835767e+05\n5.055163e+06\n-999\n50\n403.02\n7009.17\n1.490000e+08\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\ngestation(mo)\n0\n1\n-2.872500e+02\n4.553600e+02\n-999\n-999\n1.05\n4.50\n2.146000e+01\n‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñá\n\n\nnewborn(g)\n0\n1\n6.703150e+03\n9.091252e+04\n-999\n-999\n2.65\n98.00\n2.250000e+06\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nweaning(mo)\n0\n1\n-4.271700e+02\n4.967100e+02\n-999\n-999\n0.73\n2.00\n4.800000e+01\n‚ñÜ‚ñÅ‚ñÅ‚ñÅ‚ñá\n\n\nwean mass(g)\n0\n1\n1.604893e+04\n5.036502e+05\n-999\n-999\n-999.00\n10.00\n1.907500e+07\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nAFR(mo)\n0\n1\n-4.081200e+02\n5.049700e+02\n-999\n-999\n2.50\n15.61\n2.100000e+02\n‚ñÜ‚ñÅ‚ñÅ‚ñÅ‚ñá\n\n\nmax. life(mo)\n0\n1\n-4.902600e+02\n6.153000e+02\n-999\n-999\n-999.00\n147.25\n1.368000e+03\n‚ñá‚ñÅ‚ñÖ‚ñÅ‚ñÅ\n\n\nlitter size\n0\n1\n-5.563000e+01\n2.348800e+02\n-999\n1\n2.27\n3.84\n1.418000e+01\n‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá\n\n\nlitters/year\n0\n1\n-4.771400e+02\n5.000300e+02\n-999\n-999\n0.38\n1.15\n7.500000e+00\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñá\n\n\nrefs\n0\n1\n1.054762e+12\n3.619709e+13\n1\n116\n1229.00\n1242249.75\n1.368101e+15\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\nThe variables order, family, Genus, and species are categorical, the rest are numeric.",
    "crumbs": [
      "Exercises",
      "Exercise 11 Solution"
    ]
  },
  {
    "objectID": "exercise-11-solution.html#challenge-1",
    "href": "exercise-11-solution.html#challenge-1",
    "title": "Exercise 11 Solution",
    "section": "Challenge 1",
    "text": "Challenge 1\n\nStep 1\n\nReplace all values of -999 (the authors‚Äô code for missing data) with NA.\n\n\nd[d == -999] &lt;- NA  # using base ***R***\n\n# OR library(naniar) d &lt;- d |&gt; replace_with_na_all(condition = ~.x == -999)\n\n\n\nStep 2\n\nDrop the variables litter size and refs.\n\n\nd &lt;- d |&gt;\n    select(-c(\"litter size\", \"refs\"))\n\n\nRename the variable max. life(mo) to maxlife(mo).\nRename the variable wean mass(g) to weanmass(g).\n\n\nd &lt;- d |&gt;\n    rename(`maxlife(mo)` = `max. life(mo)`) |&gt;\n    rename(`weanmass(g)` = `wean mass(g)`)\n\n\n\nStep 3\n\nLog transform all of the other numeric variables.\n\n\nd &lt;- d |&gt;\n    mutate(across(where(is.numeric), log))\n\n# OR vars &lt;- c('mass(g)', 'gestation(mo)', 'newborn(g)', 'weaning(mo)',\n# 'weanmass(g)', 'AFR(mo)', 'maxlife(mo)', 'litters/year') d &lt;- d\n# |&gt;mutate(across(all_of(vars), log))\n\n\n\nStep 4\n\nRegress the (now log transformed) age [gestation(mo), weaning(mo), AFR(mo) (i.e., age at first reproduction), and maxlife(mo) (i.e., maximum lifespan)] and mass [newborn(g) and weanmass(g)] variables on (now log transformed) overall body mass(g) and add the residuals to the dataframe as new variables [relGest, relWean, relAFR, relLife, relNewbornMass, and relWeaningMass].\n\n\nd$relGest &lt;- resid(lm(`gestation(mo)` ~ `mass(g)`, data = d, na.action = na.exclude))\nd$relWean &lt;- resid(lm(`weaning(mo)` ~ `mass(g)`, data = d, na.action = na.exclude))\nd$relAFR &lt;- resid(lm(`AFR(mo)` ~ `mass(g)`, data = d, na.action = na.exclude))\nd$relLife &lt;- resid(lm(`maxlife(mo)` ~ `mass(g)`, data = d, na.action = na.exclude))\nd$relNewbornMass &lt;- resid(lm(`newborn(g)` ~ `mass(g)`, data = d, na.action = na.exclude))\nd$relWeaningMass &lt;- resid(lm(`weanmass(g)` ~ `mass(g)`, data = d, na.action = na.exclude))\n\n# OR d &lt;- d |&gt; mutate( relGest = resid(lm(`gestation(mo)` ~ `mass(g)`, data =\n# d, na.action=na.exclude)), relWean = resid(lm(`weaning(mo)` ~ `mass(g)`, data\n# = d,na.action=na.exclude)), relAFR = resid(lm(`AFR(mo)` ~ `mass(g)`, data =\n# d, na.action=na.exclude)), relLife = resid(lm(`maxlife(mo)` ~ `mass(g)`, data\n# = d, na.action=na.exclude)), relNewbornMass = resid(lm(`newborn(g)` ~\n# `mass(g)`, data = d, na.action=na.exclude)), relWeaningMass =\n# resid(lm(`weanmass(g)` ~ `mass(g)`, data = d, na.action=na.exclude)) )\n\n\n\nStep 5\n\nPlot residuals of max lifespan (relLife) in relation to Order. Which mammalian orders have the highest residual lifespan?\nPlot residuals of newborn mass (relNewbornMass) in relation to Order. Which mammalian orders have the have highest residual newborn mass?\nPlot residuals of weaning mass (relWeaningMass) in relation to Order. Which mammalian orders have the have highest residual weaning mass?\n\n\np1 &lt;- ggplot(data = d, aes(x = order, y = relLife)) + geom_boxplot(na.rm = TRUE) +\n    geom_jitter(na.rm = TRUE, alpha = 0.1) + theme(axis.text.x = element_text(angle = 90))\n\np2 &lt;- ggplot(data = d, aes(x = order, y = relNewbornMass)) + geom_boxplot(na.rm = TRUE) +\n    geom_jitter(na.rm = TRUE, alpha = 0.1) + theme(axis.text.x = element_text(angle = 90))\n\np3 &lt;- ggplot(data = d, aes(x = order, y = relWeaningMass)) + geom_boxplot(na.rm = TRUE) +\n    geom_jitter(na.rm = TRUE, alpha = 0.1) + theme(axis.text.x = element_text(angle = 90))\n\nplot_grid(p1, p2, p3, nrow = 3)\n\n\n\n\n\n\n\n\n\nOrder Primates has the highest residual lifespan.\nOrders Macroscelidae and Cetacea have the highest residual newborn mass.\nOrder Perrisodactyla has the highest residual weaning mass.\n\n\n\nStep 6\n\nRun models and a model selection process to evaluate what (now log transformed) variables best predict each of the two response variables, maxlife(mo) and AFR(mo), from the set of the following predictors: gestation(mo), newborn(g), weaning(mo), weanmass(g), litters/year, and overall body mass(g).\n\n\nMaximum Lifespan\n\nUsing Stepwise Model Selection\n\nd1 &lt;- d |&gt;\n    drop_na(c(\"maxlife(mo)\", \"gestation(mo)\", \"newborn(g)\", \"weaning(mo)\", \"weanmass(g)\",\n        \"litters/year\", \"mass(g)\"))\nML_full &lt;- lm(`maxlife(mo)` ~ `gestation(mo)` + `newborn(g)` + `weaning(mo)` + `weanmass(g)` +\n    `litters/year` + `mass(g)`, data = d1, na.action = na.fail)\nsummary(ML_full)\n\n## \n## Call:\n## lm(formula = `maxlife(mo)` ~ `gestation(mo)` + `newborn(g)` + \n##     `weaning(mo)` + `weanmass(g)` + `litters/year` + `mass(g)`, \n##     data = d1, na.action = na.fail)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.41516 -0.24524 -0.02146  0.30217  1.02653 \n## \n## Coefficients:\n##                  Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)      3.861775   0.173884  22.209  &lt; 2e-16 ***\n## `gestation(mo)`  0.376680   0.098926   3.808 0.000186 ***\n## `newborn(g)`    -0.067921   0.058839  -1.154 0.249715    \n## `weaning(mo)`    0.126879   0.054254   2.339 0.020334 *  \n## `weanmass(g)`    0.008042   0.069187   0.116 0.907585    \n## `litters/year`  -0.227594   0.079839  -2.851 0.004816 ** \n## `mass(g)`        0.128250   0.056485   2.271 0.024230 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.4482 on 202 degrees of freedom\n## Multiple R-squared:  0.7713, Adjusted R-squared:  0.7645 \n## F-statistic: 113.6 on 6 and 202 DF,  p-value: &lt; 2.2e-16\n\nML_null &lt;- lm(`maxlife(mo)` ~ 1, data = d1, na.action = na.fail)\n\n# using backwards selection\ndrop1(ML_full, test = \"F\")\n\n## Single term deletions\n## \n## Model:\n## `maxlife(mo)` ~ `gestation(mo)` + `newborn(g)` + `weaning(mo)` + \n##     `weanmass(g)` + `litters/year` + `mass(g)`\n##                 Df Sum of Sq    RSS     AIC F value   Pr(&gt;F)    \n## &lt;none&gt;                       40.579 -328.57                     \n## `gestation(mo)`  1   2.91254 43.491 -316.08 14.4986 0.000186 ***\n## `newborn(g)`     1   0.26769 40.846 -329.20  1.3326 0.249715    \n## `weaning(mo)`    1   1.09864 41.677 -324.99  5.4690 0.020334 *  \n## `weanmass(g)`    1   0.00271 40.581 -330.56  0.0135 0.907585    \n## `litters/year`   1   1.63243 42.211 -322.33  8.1262 0.004816 ** \n## `mass(g)`        1   1.03563 41.614 -325.30  5.1553 0.024230 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nb2 &lt;- update(ML_full, . ~ . - `weanmass(g)`)\ndrop1(b2, test = \"F\")\n\n## Single term deletions\n## \n## Model:\n## `maxlife(mo)` ~ `gestation(mo)` + `newborn(g)` + `weaning(mo)` + \n##     `litters/year` + `mass(g)`\n##                 Df Sum of Sq    RSS     AIC F value    Pr(&gt;F)    \n## &lt;none&gt;                       40.581 -330.56                      \n## `gestation(mo)`  1   2.91040 43.492 -318.08 14.5587 0.0001803 ***\n## `newborn(g)`     1   0.30876 40.890 -330.97  1.5445 0.2153774    \n## `weaning(mo)`    1   1.12655 41.708 -326.83  5.6353 0.0185347 *  \n## `litters/year`   1   1.65592 42.237 -324.20  8.2834 0.0044291 ** \n## `mass(g)`        1   1.98136 42.563 -322.59  9.9114 0.0018901 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nb3 &lt;- update(b2, . ~ . - `newborn(g)`)\ndrop1(b3, test = \"F\")\n\n## Single term deletions\n## \n## Model:\n## `maxlife(mo)` ~ `gestation(mo)` + `weaning(mo)` + `litters/year` + \n##     `mass(g)`\n##                 Df Sum of Sq    RSS     AIC F value    Pr(&gt;F)    \n## &lt;none&gt;                       40.890 -330.97                      \n## `gestation(mo)`  1    3.2383 44.128 -317.04 16.1557 8.207e-05 ***\n## `weaning(mo)`    1    1.4682 42.358 -325.60  7.3248  0.007377 ** \n## `litters/year`   1    2.0193 42.909 -322.90 10.0743  0.001736 ** \n## `mass(g)`        1    4.0694 44.959 -313.14 20.3024 1.113e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# no more to drop\n\n# using forward selection\nadd1(ML_null, . ~ . + `gestation(mo)` + `newborn(g)` + `weaning(mo)` + `weanmass(g)` +\n    `litters/year` + `mass(g)`, test = \"F\")\n\n## Single term additions\n## \n## Model:\n## `maxlife(mo)` ~ 1\n##                 Df Sum of Sq     RSS     AIC F value    Pr(&gt;F)    \n## &lt;none&gt;                       177.452  -32.20                      \n## `gestation(mo)`  1   122.169  55.283 -273.94  457.45 &lt; 2.2e-16 ***\n## `newborn(g)`     1   113.172  64.280 -242.43  364.44 &lt; 2.2e-16 ***\n## `weaning(mo)`    1    99.056  78.396 -200.94  261.55 &lt; 2.2e-16 ***\n## `weanmass(g)`    1   117.341  60.111 -256.44  404.08 &lt; 2.2e-16 ***\n## `litters/year`   1   100.053  77.398 -203.61  267.59 &lt; 2.2e-16 ***\n## `mass(g)`        1   116.049  61.402 -252.00  391.23 &lt; 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nf2 &lt;- update(ML_null, . ~ . + `gestation(mo)`)\nadd1(f2, . ~ . + `newborn(g)` + `weaning(mo)` + `weanmass(g)` + `litters/year` +\n    `mass(g)`, test = \"F\")\n\n## Single term additions\n## \n## Model:\n## `maxlife(mo)` ~ `gestation(mo)`\n##                Df Sum of Sq    RSS     AIC F value    Pr(&gt;F)    \n## &lt;none&gt;                      55.283 -273.94                      \n## `newborn(g)`    1    2.2088 53.074 -280.47  8.5731  0.003795 ** \n## `weaning(mo)`   1    5.5215 49.761 -293.94 22.8579 3.316e-06 ***\n## `weanmass(g)`   1    5.4940 49.789 -293.82 22.7312 3.519e-06 ***\n## `litters/year`  1    9.6219 45.661 -311.91 43.4095 3.639e-10 ***\n## `mass(g)`       1    6.6254 48.657 -298.62 28.0498 3.024e-07 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nf3 &lt;- update(f2, . ~ . + `litters/year`)\nadd1(f3, . ~ . + `newborn(g)` + `weaning(mo)` + `weanmass(g)` + `mass(g)`, test = \"F\")\n\n## Single term additions\n## \n## Model:\n## `maxlife(mo)` ~ `gestation(mo)` + `litters/year`\n##               Df Sum of Sq    RSS     AIC F value    Pr(&gt;F)    \n## &lt;none&gt;                     45.661 -311.91                      \n## `newborn(g)`   1    1.6040 44.057 -317.38  7.4635 0.0068455 ** \n## `weaning(mo)`  1    0.7013 44.959 -313.14  3.1978 0.0752155 .  \n## `weanmass(g)`  1    2.6677 42.993 -322.49 12.7201 0.0004503 ***\n## `mass(g)`      1    3.3026 42.358 -325.60 15.9834 8.915e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nf4 &lt;- update(f3, . ~ . + `mass(g)`)\nadd1(f4, . ~ . + `newborn(g)` + `weaning(mo)` + `weanmass(g)`, test = \"F\")\n\n## Single term additions\n## \n## Model:\n## `maxlife(mo)` ~ `gestation(mo)` + `litters/year` + `mass(g)`\n##               Df Sum of Sq    RSS     AIC F value   Pr(&gt;F)   \n## &lt;none&gt;                     42.358 -325.60                    \n## `newborn(g)`   1   0.65041 41.708 -326.83  3.1813 0.075974 . \n## `weaning(mo)`  1   1.46819 40.890 -330.97  7.3248 0.007377 **\n## `weanmass(g)`  1   0.04196 42.316 -323.81  0.2023 0.653345   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nf5 &lt;- update(f4, . ~ . + `weaning(mo)`)\nadd1(f5, . ~ . + `newborn(g)` + `weanmass(g)`, test = \"F\")\n\n## Single term additions\n## \n## Model:\n## `maxlife(mo)` ~ `gestation(mo)` + `litters/year` + `mass(g)` + \n##     `weaning(mo)`\n##               Df Sum of Sq    RSS     AIC F value Pr(&gt;F)\n## &lt;none&gt;                     40.890 -330.97               \n## `newborn(g)`   1  0.308765 40.581 -330.56  1.5445 0.2154\n## `weanmass(g)`  1  0.043789 40.846 -329.20  0.2176 0.6414\n\n# no more to add\n\nNote that forward and backward selection using F ratio tests yield the same best model‚Ä¶\n\\[maxlife(mo) \\sim gestation(mo) + litters/year + mass(g) + weaning(mo)\\]\n\n\nUsing AICc\n\nML_mods &lt;- dredge(ML_full)\n\n## Fixed term is \"(Intercept)\"\n\nhead(coef(ML_mods), 10)  # top 10 models\n\n##    (Intercept) `gestation(mo)` `litters/year`  `mass(g)` `weaning(mo)`\n## 24    3.998909       0.2940783     -0.2474480 0.08591653     0.1421767\n## 32    3.865397       0.3764905     -0.2283866 0.13261110     0.1276109\n## 56    3.988708       0.3076744     -0.2469755 0.10955724     0.1422660\n## 64    3.861775       0.3766799     -0.2275937 0.12825043     0.1268787\n## 52    4.110776       0.2894074     -0.2669214         NA     0.1346416\n## 16    3.927433       0.4902189     -0.3201992 0.14352724            NA\n## 8     4.136516       0.3869647     -0.3641313 0.07593844            NA\n## 60    4.064502       0.3181420     -0.2599531         NA     0.1273638\n## 48    3.914163       0.4886737     -0.3157965 0.12877056            NA\n## 30    3.558604       0.4042026             NA 0.17090404     0.2030174\n##    `newborn(g)` `weanmass(g)`\n## 24           NA            NA\n## 32  -0.06477636            NA\n## 56           NA  -0.028684385\n## 64  -0.06792115   0.008041606\n## 52           NA   0.087704974\n## 16  -0.09175290            NA\n## 8            NA            NA\n## 60  -0.02962093   0.112382090\n## 48  -0.10172794   0.026827785\n## 30  -0.09377062            NA\n\n(ML_mods.avg &lt;- summary(model.avg(ML_mods, subset = delta &lt; 4)))\n\n## \n## Call:\n## model.avg(object = ML_mods, subset = delta &lt; 4)\n## \n## Component model call: \n## lm(formula = `maxlife(mo)` ~ &lt;5 unique rhs&gt;, data = d1, na.action = \n##      na.fail)\n## \n## Component models: \n##        df  logLik   AICc delta weight\n## 1235    6 -126.07 264.56  0.00   0.39\n## 12345   7 -125.28 265.12  0.56   0.30\n## 12356   7 -125.96 266.48  1.92   0.15\n## 123456  8 -125.27 267.27  2.71   0.10\n## 1256    6 -128.05 268.51  3.95   0.05\n## \n## Term codes: \n## `gestation(mo)`  `litters/year`       `mass(g)`    `newborn(g)`   `weaning(mo)` \n##               1               2               3               4               5 \n##   `weanmass(g)` \n##               6 \n## \n## Model-averaged coefficients:  \n## (full average) \n##                  Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)    \n## (Intercept)      3.949706   0.166665    0.167466  23.585  &lt; 2e-16 ***\n## `gestation(mo)`  0.328869   0.093972    0.094438   3.482 0.000497 ***\n## `litters/year`  -0.240736   0.079388    0.079854   3.015 0.002572 ** \n## `mass(g)`        0.103022   0.049693    0.049867   2.066 0.038835 *  \n## `weaning(mo)`    0.135875   0.053586    0.053901   2.521 0.011708 *  \n## `newborn(g)`    -0.026237   0.046859    0.047008   0.558 0.576748    \n## `weanmass(g)`    0.001288   0.040439    0.040600   0.032 0.974699    \n##  \n## (conditional average) \n##                  Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)    \n## (Intercept)      3.949706   0.166665    0.167466  23.585  &lt; 2e-16 ***\n## `gestation(mo)`  0.328869   0.093972    0.094438   3.482 0.000497 ***\n## `litters/year`  -0.240736   0.079388    0.079854   3.015 0.002572 ** \n## `mass(g)`        0.108990   0.044295    0.044501   2.449 0.014320 *  \n## `weaning(mo)`    0.135875   0.053586    0.053901   2.521 0.011708 *  \n## `newborn(g)`    -0.065577   0.053928    0.054252   1.209 0.226761    \n## `weanmass(g)`    0.004185   0.072820    0.073111   0.057 0.954353    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nWhat is the best model overall based on AICc and how many models have a delta AICc of 4 or less?\n\nThe best model overall is maxlife(mo) ~ gestation(mo) + litters/year + mass(g) + weaning(mo), the same as identified by both forward and backwards selection. Five models have a delta AICc of &lt; 4.\n\nWhat variables, if any, appear in all of this set of ‚Äútop‚Äù models?\n\nThe variables gestation(mo), litters/year, mass(g), and weaning(mo) appear in all of the models with delta AICc of ‚â§ 4.\n\nCalculate and plot the model-averaged coefficients and their CIs across this set of top models.\n\n\nconfint(ML_mods.avg)\n\n##                       2.5 %      97.5 %\n## (Intercept)      3.62147891  4.27793347\n## `gestation(mo)`  0.14377472  0.51396308\n## `litters/year`  -0.39724571 -0.08422565\n## `mass(g)`        0.02176916  0.19621043\n## `weaning(mo)`    0.03023157  0.24151902\n## `newborn(g)`    -0.17190901  0.04075518\n## `weanmass(g)`   -0.13911025  0.14748030\n\nplot(ML_mods.avg, full = TRUE, intercept = FALSE)\n\n\n\n\n\n\n\n\nBased on the plot, we can again see that set of top models includes four predictors for which the 95% CI around the coefficient does not include zero. These are the same predictors identified by forward and backwards selection: gestation(mo), litters/year, mass(g), and weaning(mo).\n\n\n\nAge at First Reproduction\n\nUsing Stepwise Model Selection\n\nd2 &lt;- d |&gt;\n    drop_na(c(\"AFR(mo)\", \"gestation(mo)\", \"newborn(g)\", \"weaning(mo)\", \"weanmass(g)\",\n        \"litters/year\", \"mass(g)\"))\nAFR_full &lt;- lm(`AFR(mo)` ~ `gestation(mo)` + `newborn(g)` + `weaning(mo)` + `weanmass(g)` +\n    `litters/year` + `mass(g)`, data = d2, na.action = na.fail)\nAFR_null &lt;- lm(`AFR(mo)` ~ 1, data = d2, na.action = na.fail)\n\n# using backwards selection\ndrop1(AFR_full, test = \"F\")\n\n## Single term deletions\n## \n## Model:\n## `AFR(mo)` ~ `gestation(mo)` + `newborn(g)` + `weaning(mo)` + \n##     `weanmass(g)` + `litters/year` + `mass(g)`\n##                 Df Sum of Sq    RSS     AIC F value    Pr(&gt;F)    \n## &lt;none&gt;                       69.997 -336.46                      \n## `gestation(mo)`  1    3.6852 73.682 -324.91 13.5306 0.0002859 ***\n## `newborn(g)`     1    0.1963 70.193 -337.72  0.7206 0.3967307    \n## `weaning(mo)`    1    1.4553 71.452 -333.03  5.3434 0.0215917 *  \n## `weanmass(g)`    1    0.1490 70.146 -337.90  0.5472 0.4601285    \n## `litters/year`   1   22.6366 92.634 -264.49 83.1123 &lt; 2.2e-16 ***\n## `mass(g)`        1    0.1974 70.194 -337.72  0.7247 0.3954149    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nb2 &lt;- update(AFR_full, . ~ . - `weanmass(g)`)\ndrop1(b2, test = \"F\")\n\n## Single term deletions\n## \n## Model:\n## `AFR(mo)` ~ `gestation(mo)` + `newborn(g)` + `weaning(mo)` + \n##     `litters/year` + `mass(g)`\n##                 Df Sum of Sq    RSS     AIC F value    Pr(&gt;F)    \n## &lt;none&gt;                       70.146 -337.90                      \n## `gestation(mo)`  1    3.6422 73.788 -326.53 13.3960 0.0003058 ***\n## `newborn(g)`     1    0.0910 70.237 -339.56  0.3349 0.5633153    \n## `weaning(mo)`    1    1.6144 71.760 -333.89  5.9380 0.0154944 *  \n## `litters/year`   1   22.9076 93.054 -265.29 84.2552 &lt; 2.2e-16 ***\n## `mass(g)`        1    0.8376 70.984 -336.76  3.0808 0.0804066 .  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nb3 &lt;- update(b2, . ~ . - `newborn(g)`)\ndrop1(b3, test = \"F\")\n\n## Single term deletions\n## \n## Model:\n## `AFR(mo)` ~ `gestation(mo)` + `weaning(mo)` + `litters/year` + \n##     `mass(g)`\n##                 Df Sum of Sq    RSS     AIC F value    Pr(&gt;F)    \n## &lt;none&gt;                       70.237 -339.56                      \n## `gestation(mo)`  1    5.4185 75.655 -321.94 19.9807 1.171e-05 ***\n## `weaning(mo)`    1    1.8804 72.117 -334.58  6.9341  0.008965 ** \n## `litters/year`   1   24.9554 95.192 -261.29 92.0233 &lt; 2.2e-16 ***\n## `mass(g)`        1    2.1463 72.383 -333.61  7.9144  0.005280 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# no more to drop\n\n# forward selection\nadd1(AFR_null, . ~ . + `gestation(mo)` + `newborn(g)` + `weaning(mo)` + `weanmass(g)` +\n    `litters/year` + `mass(g)`, test = \"F\")\n\n## Single term additions\n## \n## Model:\n## `AFR(mo)` ~ 1\n##                 Df Sum of Sq    RSS     AIC F value    Pr(&gt;F)    \n## &lt;none&gt;                       391.65  106.12                      \n## `gestation(mo)`  1    266.36 125.28 -192.78  557.03 &lt; 2.2e-16 ***\n## `newborn(g)`     1    233.51 158.14 -131.30  386.87 &lt; 2.2e-16 ***\n## `weaning(mo)`    1    244.14 147.51 -149.67  433.65 &lt; 2.2e-16 ***\n## `weanmass(g)`    1    244.96 146.69 -151.13  437.51 &lt; 2.2e-16 ***\n## `litters/year`   1    278.73 112.92 -220.22  646.75 &lt; 2.2e-16 ***\n## `mass(g)`        1    239.63 152.01 -141.72  413.02 &lt; 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nf2 &lt;- update(AFR_null, . ~ . + `litters/year`)\nadd1(f2, . ~ . + `gestation(mo)` + `newborn(g)` + `weaning(mo)` + `weanmass(g)` +\n    `mass(g)`, test = \"F\")\n\n## Single term additions\n## \n## Model:\n## `AFR(mo)` ~ `litters/year`\n##                 Df Sum of Sq     RSS     AIC F value    Pr(&gt;F)    \n## &lt;none&gt;                       112.916 -220.22                      \n## `gestation(mo)`  1    39.294  73.622 -331.13 139.303 &lt; 2.2e-16 ***\n## `newborn(g)`     1    32.477  80.438 -307.75 105.379 &lt; 2.2e-16 ***\n## `weaning(mo)`    1    17.618  95.298 -263.00  48.251 2.979e-11 ***\n## `weanmass(g)`    1    31.485  81.430 -304.52 100.916 &lt; 2.2e-16 ***\n## `mass(g)`        1    28.882  84.034 -296.21  89.703 &lt; 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nf3 &lt;- update(f2, . ~ . + `gestation(mo)`)\nadd1(f3, . ~ . + `newborn(g)` + `weaning(mo)` + `weanmass(g)` + `mass(g)`, test = \"F\")\n\n## Single term additions\n## \n## Model:\n## `AFR(mo)` ~ `litters/year` + `gestation(mo)`\n##               Df Sum of Sq    RSS     AIC F value  Pr(&gt;F)  \n## &lt;none&gt;                     73.622 -331.13                  \n## `newborn(g)`   1   0.71369 72.908 -331.70  2.5451 0.11185  \n## `weaning(mo)`  1   1.23838 72.383 -333.61  4.4482 0.03589 *\n## `weanmass(g)`  1   1.52304 72.099 -334.65  5.4924 0.01985 *\n## `mass(g)`      1   1.50422 72.117 -334.58  5.4231 0.02064 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nf4 &lt;- update(f3, . ~ . + `weanmass(g)`)\nadd1(f4, . ~ . + `newborn(g)` + `weaning(mo)` + `mass(g)`, test = \"F\")\n\n## Single term additions\n## \n## Model:\n## `AFR(mo)` ~ `litters/year` + `gestation(mo)` + `weanmass(g)`\n##               Df Sum of Sq    RSS     AIC F value  Pr(&gt;F)  \n## &lt;none&gt;                     72.099 -334.65                  \n## `newborn(g)`   1   0.44927 71.649 -334.30  1.6240 0.20367  \n## `weaning(mo)`  1   1.80288 70.296 -339.33  6.6426 0.01051 *\n## `mass(g)`      1   0.03965 72.059 -332.79  0.1425 0.70612  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nf5 &lt;- update(f4, . ~ . + `weaning(mo)`)\nadd1(f5, . ~ . + `newborn(g)` + `mass(g)`, test = \"F\")\n\n## Single term additions\n## \n## Model:\n## `AFR(mo)` ~ `litters/year` + `gestation(mo)` + `weanmass(g)` + \n##     `weaning(mo)`\n##              Df Sum of Sq    RSS     AIC F value Pr(&gt;F)\n## &lt;none&gt;                    70.296 -339.33               \n## `newborn(g)`  1   0.10143 70.194 -337.72  0.3728 0.5420\n## `mass(g)`     1   0.10253 70.193 -337.72  0.3769 0.5398\n\n# no more to add\n\nHere, forward and backward selection using F ratio tests yield slightly different models‚Ä¶\nBACKWARD: \\[AFR(mo) \\sim gestation(mo) + litters/year + mass(g) + weaning(mo)\\] BACKWARD: \\[AFR(mo) \\sim gestation(mo) + litters/year + weaning(mo) + weanmass(g)\\]\n\n\nUsing AICc\n\nAFR_mods &lt;- dredge(AFR_full)\n\n## Fixed term is \"(Intercept)\"\n\nhead(coef(AFR_mods), 10)  # top 10 models\n\n##    (Intercept) `gestation(mo)` `litters/year`  `mass(g)` `weaning(mo)`\n## 24    1.865571       0.3515017     -0.7311936 0.05757463     0.1443987\n## 52    1.918063       0.3343795     -0.7402756         NA     0.1408658\n## 32    1.797585       0.3939747     -0.7205625 0.08179348     0.1371941\n## 56    1.876551       0.3394831     -0.7329602 0.03572031     0.1438754\n## 60    1.862965       0.3716340     -0.7319557         NA     0.1314640\n## 28    2.074817       0.3214482     -0.7666004         NA     0.1487558\n## 64    1.776504       0.3964944     -0.7173683 0.05226162     0.1314778\n## 36    2.033261       0.4347639     -0.8415227         NA            NA\n## 8     1.996944       0.4535600     -0.8367332 0.04731896            NA\n## 44    1.906060       0.4966642     -0.8109774         NA            NA\n##    `weanmass(g)` `newborn(g)`\n## 24            NA           NA\n## 52    0.06388861           NA\n## 32            NA  -0.03372466\n## 56    0.02627408           NA\n## 60    0.09482855  -0.03778513\n## 28            NA   0.05814792\n## 64    0.05424074  -0.05542620\n## 36    0.05377311           NA\n## 8             NA           NA\n## 44    0.11782202  -0.07654879\n\n(AFR_mods.avg &lt;- summary(model.avg(AFR_mods, subset = delta &lt; 4)))\n\n## \n## Call:\n## model.avg(object = AFR_mods, subset = delta &lt; 4)\n## \n## Component model call: \n## lm(formula = `AFR(mo)` ~ &lt;7 unique rhs&gt;, data = d2, na.action = \n##      na.fail)\n## \n## Component models: \n##        df  logLik   AICc delta weight\n## 1235    6 -199.82 411.97  0.00   0.29\n## 1256    6 -199.93 412.19  0.22   0.26\n## 12345   7 -199.65 413.74  1.77   0.12\n## 12356   7 -199.74 413.92  1.95   0.11\n## 12456   7 -199.74 413.92  1.95   0.11\n## 1245    6 -201.22 414.76  2.79   0.07\n## 123456  8 -199.37 415.30  3.33   0.05\n## \n## Term codes: \n## `gestation(mo)`  `litters/year`       `mass(g)`    `newborn(g)`   `weaning(mo)` \n##               1               2               3               4               5 \n##   `weanmass(g)` \n##               6 \n## \n## Model-averaged coefficients:  \n## (full average) \n##                  Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)    \n## (Intercept)      1.881883   0.157221    0.157826  11.924  &lt; 2e-16 ***\n## `gestation(mo)`  0.353306   0.093289    0.093702   3.771 0.000163 ***\n## `litters/year`  -0.734295   0.077192    0.077547   9.469  &lt; 2e-16 ***\n## `mass(g)`        0.032778   0.043499    0.043601   0.752 0.452196    \n## `weaning(mo)`    0.140805   0.055623    0.055883   2.520 0.011747 *  \n## `weanmass(g)`    0.032328   0.049236    0.049353   0.655 0.512445    \n## `newborn(g)`    -0.006927   0.041658    0.041781   0.166 0.868313    \n##  \n## (conditional average) \n##                 Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)    \n## (Intercept)      1.88188    0.15722     0.15783  11.924  &lt; 2e-16 ***\n## `gestation(mo)`  0.35331    0.09329     0.09370   3.771 0.000163 ***\n## `litters/year`  -0.73429    0.07719     0.07755   9.469  &lt; 2e-16 ***\n## `mass(g)`        0.05795    0.04343     0.04362   1.329 0.183972    \n## `weaning(mo)`    0.14081    0.05562     0.05588   2.520 0.011747 *  \n## `weanmass(g)`    0.06151    0.05308     0.05329   1.154 0.248332    \n## `newborn(g)`    -0.01977    0.06854     0.06875   0.287 0.773741    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nWhat is the best model overall based on AICc and how many models have a delta AICc of 4 or less?\n\nThe best model overall is AFR(mo) ~ gestation(mo) + litters/year + mass(g) + weaning(mo), the same as identified via backward selection. Seven models have a delta AICc of &lt; 4.\n\nWhat variables, if any, appear in all of this set of ‚Äútop‚Äù models?\n\nHere, only three variables - gestation(mo), litters/year, and weaning(mo) - appear in all of the models with delta AICc of ‚â§ 4. The variables mass(g) and newborn(g) each appears in 4 of the top 6 models, while the variable weanmass(g) appears in 3.\n\nCalculate and plot the model-averaged coefficients and their CIs across this set of top models.\n\n\nconfint(AFR_mods.avg)\n\n##                       2.5 %     97.5 %\n## (Intercept)      1.57255043  2.1912166\n## `gestation(mo)`  0.16965399  0.5369572\n## `litters/year`  -0.88628517 -0.5823047\n## `mass(g)`       -0.02753636  0.1434341\n## `weaning(mo)`    0.03127700  0.2503339\n## `weanmass(g)`   -0.04292452  0.1659521\n## `newborn(g)`    -0.15452137  0.1149898\n\nplot(AFR_mods.avg, full = TRUE, intercept = FALSE)\n\n\n\n\n\n\n\n\nBased on the plot, we again see that the set of top models only includes 3 predictors whose 95% CIs around the regression coefficients do not include 0: gestation(mo), litters/year, and weaning(mo).",
    "crumbs": [
      "Exercises",
      "Exercise 11 Solution"
    ]
  },
  {
    "objectID": "data-analysis-replication-assignment.html",
    "href": "data-analysis-replication-assignment.html",
    "title": "Data Analysis Replication",
    "section": "",
    "text": "Objectives\nThe objective of this assignment is to use your skills in R to replicate as closely as you can a set of statistical analyses and results reported in a paper of your choosing from the primary literature in your field.",
    "crumbs": [
      "Assignments",
      "Data Analysis Replication"
    ]
  },
  {
    "objectID": "data-analysis-replication-assignment.html#what-to-do",
    "href": "data-analysis-replication-assignment.html#what-to-do",
    "title": "Data Analysis Replication",
    "section": "What to Do",
    "text": "What to Do\nYou will need to create a new RStudio project and a ‚Äú.qmd‚Äù or ‚Äú.Rmd‚Äù report detailing your work on replicating the results presented in the paper you choose. You should start your reanalysis report with a text description of the study and of the specific data and reanalyses you will be doing, to orient your reader. Outline (briefly!) the goal of the original paper, the data set used, and the analyses conducted, then describe which ones you will replicate. You should also demonstrate how you read in any data file(s) and show a few lines of raw data in your output (e.g., using head()).\nNote that I will be looking for you to clearly take your reader through all of the elements of data manipulation, analysis, and, if appropriate, visualization. You should provide as much coding detail, explanation, and output tables as necessary to compare your results to those published!\nYou do not need to replicate ALL of the analyses presented in the paper (although the more the better)! At a bare minimum, you need to repeat at least three analyses, including at least one descriptive statistical analysis, one visualization, and one inferential statistical analysis.\nAs assessment, I will be looking at several different elements of your report and code for this assignment. Below, I outline some of the main things:\n\nOrganization and Logistics\n\nRepo set up on GitHub, named correctly, and shared with me (URL submitted via Canvas)\nPDF of paper included in the top level of the repository\n‚Äú.qmd‚Äù or ‚Äú.Rmd‚Äù file for your report stored at the top level of the repo\n‚Äú.qmd‚Äù or ‚Äú.Rmd‚Äù file includes text and code and embedded images/tables from the paper for comparison\n‚Äú.qmd‚Äù or ‚Äú.Rmd‚Äù file is well organized into subsections, including blocks of explanatory text and R code blocks, plus output\nR code follows a consistent convention with respect to variable and function names and formatting\nFile(s) with original data are included in a directory (folder) called ‚Äúdata‚Äù within the repo\nImages from the original paper that are referenced in the report stored in a separate directory called ‚Äúimages‚Äù within the repo\n\nIntroduction and Framing\n\nReport includes a short, introductory description of the goal of the original paper, the data used, the analyses conducted, and the conclusions of the original study\nReport outlines the specific data and reanalyses you will be doing\nCode correctly loads all required packages not included in {base} R\n‚Äú.qmd‚Äù or ‚Äú.Rmd‚Äù file renders and produces HTML output without requiring edits or additional modifications\n‚Äú.qmd‚Äù or ‚Äú.Rmd‚Äù file includes a dictionary defining variable names used in dataset and R code\nR code successfully reads in data file(s) from the local ‚Äúdata‚Äù directory within the repo and shows a few lines of raw data (e.g., using head())\nR code successfully reads in any image file(s) for comparison from within the local ‚Äúimage‚Äù directory within the repo\n\nData Analysis/Visualization Replications\n\nFor each of the analyses/visualizations being done‚Ä¶\n\nText of the report clearly takes the reader through all of the elements of data manipulation/analysis/visualization, from raw data to presentation\nReport text is thorough and the R code is well-documented and provides as much explanation and output (tables, figures, etc.) as necessary to understand how the code works\nReport includes side-by-side comparisons of the results of the replication attempts to the published original results\n\n\nDiscussion and Reflection\n\nReport includes a narrative summary about how successful the analysis replications were or were not - i.e., how well do the replicated results compare to those presented in the original paper?\nReport discusses and addresses any challenges encountered‚Ä¶ missing data, unclear information about how data were processed in the original publication, etc.\nReport discusses where and possible reasons why the analysis replications might differ from the authors‚Äô original results",
    "crumbs": [
      "Assignments",
      "Data Analysis Replication"
    ]
  },
  {
    "objectID": "data-analysis-replication-assignment.html#what-to-turn-in",
    "href": "data-analysis-replication-assignment.html#what-to-turn-in",
    "title": "Data Analysis Replication",
    "section": "What to Turn In",
    "text": "What to Turn In\nStart a new GitHub repo and R project and using one of the methods outlined in Module 06. You should call it ‚Äúdata-analysis-replication‚Äù.\nThe top level of your repo should include a ‚Äú.qmd‚Äù or ‚Äú.Rmd‚Äù file called ‚Äúdata-analysis-replication.Rmd‚Äù where you thoroughly describe and run the code for all of the steps in your reanalysis. You can begin with the standard Quarto or RMarkdown document template created by choosing File &gt; New File and the desired file type and then choosing HTML as the ‚ÄúDefault Output Format‚Äù. Be sure to remove any extraneous code that is included, by default, in the template.\nThe top level of your repository should also include a PDF copy of the paper you are reanalyzing data from.\nWithin the repository, make a ‚Äúdata‚Äù directory in which you include any ‚Äú.csv‚Äù or ‚Äú.xlsx‚Äù or similar files that contain the original data for the paper as you either downloaded them as supplementary material or received them from the paper‚Äôs author.\nWithin the repository, also make an ‚Äúimages‚Äù directory in which you include ‚Äú.jpeg‚Äù or ‚Äú.png‚Äù or similar files that show figures or tabular results from the original paper. In your ‚Äú.qmd‚Äù or ‚Äú.Rmd‚Äù file, near your own results, you will want to embed some of these results from the original paper that you replicate so that I can see them together. You can include code in one of the following formats to reference files in your ‚Äúimages‚Äù folder for inclusion in your document. The first alternative uses raw HTML in markdown, outside of a code block; the second and third accomplish the same thing using {knitr} in an {r} code block in either a Quarto (Alternative 2) or RMarkdown (Alternative 3) document.\nAlternative 1: Raw HTML\nThe line below is included in your markdown file, outside of a code block, positioned where you want the image to go.\n&lt;img src=\"images/imagename.filetype\" width=\"###px\"/&gt;\n\n```{r}\n# some code here...\n```\nAlternative 2: Quarto\nHere, the reference to the image is included within a code block.\n```{r}\n#| out-width: ###px\nknitr::include_graphics(\"images/imagename.filetype\")\n```\n‚Ä¶ OR ‚Ä¶\n```{r}\n#| out-width: \"##%\"\nknitr::include_graphics(\"images/imagename.filetype\")\n```\nAlternative 3: RMarkdown\nAgain, here, the reference to the image is included within a code block.\n```{r echo=FALSE, width=\"###px\"}\nknitr::include_graphics(\"images/imagename.filetype\")\n```\n‚Ä¶ OR ‚Ä¶\n```{r echo=FALSE, out.width=\"##%\"}\nknitr::include_graphics(\"images/imagename.filetype\")\n```\nIn each case, you would replace imagename.filetype with the name of your file, e.g., ‚Äúfigure-1.jpeg‚Äù and ## or ### with a integer number of pixels (e.g., 200px) or a percent of the window width (e.g., ‚Äú70%‚Äù).\nYou shoudl also add the following line to the initial code chunk in your ‚Äú.qmd‚Äù or ‚Äú.Rmd‚Äù file, which tells {knitr} where to output all of the figures associated with your chunks of code as well as where to find any images that you want to include in your ‚Äú.html‚Äù output.\nknitr::opts_chunk$set(fig.path = \"images/\")\nIf you wish, you can download either the file ‚Äúdata-analysis-replication-template.Rmd‚Äù or ‚Äúdata-analysis-replication-template.qmd‚Äù from https://github.com/difiore/ada-datasets and use that as a starting point for your file.\nBefore turning in this assignment, you should confirm that you can render your document successfully to HTML and make sure that the entire repo is pushed to GitHub repository as well. I should be able to clone your entire repo to my own computer, open the ‚Äú.qmd‚Äù or ‚Äú.Rmd‚Äù file, and render it to produce a nicely formatted ‚Äú.html‚Äù report describing what you did and seeing your results.\nBy the due date for this assignment, you should push all of the above to your GitHub repository for this assignment. Then, in Canvas, please submit the URL for the repository (grabbed from the green CODE button on the repo‚Äôs base page) into the assignment submission text field.\n\nTL/DR: I should be able to CLONE your repository and knit your ‚Äú.qmd‚Äù or ‚Äú.Rmd‚Äù file to show all of your completed work for the data analysis replication assignment. Practically speaking, this means that if your code reads in data from external files, such as ‚Äú.xlsx‚Äù or ‚Äú.csv‚Äù files, it should be general enough to work on ANY machine. Thus, you will want to have your code read any data from local files housed within your repository , i.e., in the ‚Äúdata‚Äù folder described above. Your repository should also include a PDF of the original paper with the analyses being replicated at the top level of the repo. The structure of your repo thus should look pretty similar to what is shown below:\n\n\nLocal Repository\n\n\n\n\n\n\n\n\n\n\n\nRemote Repository",
    "crumbs": [
      "Assignments",
      "Data Analysis Replication"
    ]
  },
  {
    "objectID": "collaborative-data-science-assignment.html",
    "href": "collaborative-data-science-assignment.html",
    "title": "Collaborative Data Science Project",
    "section": "",
    "text": "Objectives\nThe objective of this assignment is to work with one or two of your classmates over the final weeks of the class to complete your choice of one of the following options:",
    "crumbs": [
      "Assignments",
      "Collaborative Data Science Project"
    ]
  },
  {
    "objectID": "collaborative-data-science-assignment.html#objectives",
    "href": "collaborative-data-science-assignment.html#objectives",
    "title": "Collaborative Data Science Project",
    "section": "",
    "text": "A ‚ÄúMethods Exploration‚Äù Project\n\nIn consultation with the instructor, your group should select a particular statistical method, data analysis workflow, and/or data visualization tool that we do not cover in the class and that you would like to explore further and prepare a 10-15 minute presentation for your peers. In your presentation, you will describe the purpose of the method and walk the class through examples/applications of its use using a publicly available dataset (e.g., from Kaggle, Google Dataset Search, Data.gov, etc.).\nIn addition to your presentation, you will develop an accompanying R package and ‚Äúvignette‚Äù taking a user through the method/workflow that will be shared through GitHub.\nIdeally, the method you choose to explore would be one that you and your partner will find useful for your own research. This is your chance both to be creative and to delve more deeply into the statistics, data science, and R literature on a topic of your own choice! Below, I list a large number of possible topics: these are by no means exclusive‚Ä¶ your group should feel free to suggest any other topics you might find interesting, just confirm with me that what you would like to focus on is appropriate.\n\nPrincipal components analysis (PCA)\nDiscriminant function analysis\nFactor analysis\nStructured equation modeling\nClustering and classification tools (e.g., k-means clustering, classification trees, random forests)\nNonlinear regression, ridge regression, lasso regression\nSpecies occupancy/distribution modeling\nSocial/biological network construction and visualization\nGeospatial data visualization and spatial queries\nText mining and natural language processing\nInteracting with relational/nonrelational databases and query languages\nManipulating and analyzing DNA sequence data\nPhylogeny construction, phylogenetic comparative analysis\nBioinformatics data processing\nImage analysis (e.g., supervised and unsupervised classification, feature extraction)\nMachine learning\nAcoustic data analysis\n\n\n\n\nA ‚ÄúNovel Data Science‚Äù Project\n\nImplement a modest data science analysis using your one or both of your group members‚Äô own data and addressing a theoretically interesting problem in the natural or social sciences. The goal is to incorporate several of the tools learned in this classroom into a project of your own design that, ideally, might also help you move your own research forward. The project should include both exploratory (descriptive) and inferential data analyses. Your group will give a 10-15 minute presentation on your project to your peers.\nIn addition to your presentation, you will develop an accompanying R package and ‚Äúvignette‚Äù that walks the reader though the your analysis and workflow that will be shared through GitHub.",
    "crumbs": [
      "Assignments",
      "Collaborative Data Science Project"
    ]
  },
  {
    "objectID": "collaborative-data-science-assignment.html#what-to-do",
    "href": "collaborative-data-science-assignment.html#what-to-do",
    "title": "Collaborative Data Science Project",
    "section": "What to Do",
    "text": "What to Do\nYou and your groupmates will work together to develop a short (roughly 10-15 minute) but reasonably comprehensive presentation and associated HTML ‚Äúvignette‚Äù that can be rendered from a ‚Äú.qmd‚Äù or ‚Äú.Rmd‚Äù document). Depending on which type of project you are doing, your vignette will provide background on the statistical method/topic/data visualization procedures that you have chosen to explore or on the dataset and theory that you are exploring with your own data. It should then take the user through a set of analyses either demonstrating the method being explored or applying approaches we have learned in class to your own data. Your module should be organized similarly to those that I have prepared for various other topics this semester, though need not be as long.\nThe presentation can either be done from the module itself or as a PowerPoint/Keynote/Google Slides/R presentation/whatever slideshow.\nThus, all groups will need to produce a ‚Äú.qmd‚Äù or ‚Äú.Rmd‚Äù file that will be the basis for a module vignette that you build and will bundle into a custom R package. Besides the vignette, your package should also bundle at least some of the data you are working with into one or more included datasets and should also bundle code for least two functions that are called in the vignette. The functions you include can be custom functions that you create, but it is also perfectly fine to use functions that you have copied from other packages and include into your own package. The important thing is that you are bundling them together with your dataset and vignette for easy distribution. You will want to create your own documentation for each function, as discussed in the Miscellany module on Building Custom R Packages. One of the main objectives for this exercise is to give you experience with tools for distributing shared data and code to other researchers and pull the curtain back a bit on just what producing an R package entails.\nPlease be sure to divide up the work with your partner so that each of you are contributing more-or-less equally!",
    "crumbs": [
      "Assignments",
      "Collaborative Data Science Project"
    ]
  },
  {
    "objectID": "collaborative-data-science-assignment.html#what-to-turn-in",
    "href": "collaborative-data-science-assignment.html#what-to-turn-in",
    "title": "Collaborative Data Science Project",
    "section": "What to Turn In",
    "text": "What to Turn In\nYou will collectively take the class through your presentation/module during our meeting final class meeting period. We will aim for 7-8 presentations total of ~15 minutes each with time for questions. I will record these video presentations and share them via the Canvas site.\nYour group‚Äôs work also should result in a custom R package that can be shared as a single file and loaded into an R workspace (e.g., using the install.packages() function. As noted above, that package should combine the following:\n\nA set of functions and associated function documentation appropriate to the topic you have chosen.\nOne or more relevant datasets (either data you have simulated or, preferably, real data), that you use in a module vignette about your chosen topic,\nA vignette that walks users through demonstrations of relevant analyses,\nAppropriate METADATA for your package (e.g., information on dependencies, etc.),\n\n\nNOTE: The Miscellany module on Building Custom R Packages takes the user through all of the step of package development, so use it as a resource! We will work through this module during our one of our last class period.\n\nAdditionally, by the due date for this assignment - the last day of the formal final exam period - one group member should upload the final R Package‚Äôs bundled ‚Äú.gz.tar‚Äù file to the Canvas site for the course, and your group should collectively upload your entire R Package project to a GitHub repository and share with me, via the assignment submission text field on Canvas, the URL to that repository.\n\nNOTE: I should be able to CLONE your repository and see all of the components associated with your package development.",
    "crumbs": [
      "Assignments",
      "Collaborative Data Science Project"
    ]
  },
  {
    "objectID": "creative-dataviz-assignment.html",
    "href": "creative-dataviz-assignment.html",
    "title": "Creative Data Visualization",
    "section": "",
    "text": "Objectives\nThe objective of this assignment is to use your skills in R to aid in producing a creative-nontraditional-artistic (or choose your own similar adjective!) data visualization using either a dataset you find or, ideally, some of your own data! This is your chance to both showcase your R coding and data wrangling skills and explore your creative side.\nYour visualization does not have to be sophisticated, but it must involve loading, transforming, and visualizing set of custom data - either a publicly-available dataset or data relevant to your own work or your field of study - into some kind of interesting, creative representation. Mostly, I am looking for you try think outside the box and be experimental or nontraditional in how you represent data‚Ä¶ and to that end, it would be okay if you simply want to present a cool algorithm/protocol/description about how you would like to transform your data even if you are not able to implement it.\nCheck out this website for an example of the kind of thing I am thinking of‚Ä¶",
    "crumbs": [
      "Assignments",
      "Creative Data Visualization"
    ]
  },
  {
    "objectID": "creative-dataviz-assignment.html#objectives",
    "href": "creative-dataviz-assignment.html#objectives",
    "title": "Creative Data Visualization",
    "section": "",
    "text": "NOTE: I am using the term ‚Äúvisualization‚Äù here to mean something more general than that term would usually imply‚Ä¶ i.e., your ‚Äúvisualization‚Äù can be in a sensory modality OTHER than sight. For example, you might imagine creating an auditory or olfactory or tactile representation of data!\n\n\n\n\nDNA Sonification refers to the use of audio to convey the information content of a DNA sequence. Audio is created using the rules of gene expression and codons are played as musical notes.\n\n\nTemple, Mark D. 2017. An auditory display tool for DNA sequence analysis.‚Äù BMC Bioinformatics 18: 221. https://doi.org/10.1186/s12859-017-1632-x.",
    "crumbs": [
      "Assignments",
      "Creative Data Visualization"
    ]
  },
  {
    "objectID": "creative-dataviz-assignment.html#what-to-turn-in",
    "href": "creative-dataviz-assignment.html#what-to-turn-in",
    "title": "Creative Data Visualization",
    "section": "What to Turn In",
    "text": "What to Turn In\nIf you can do your visualization entirely with code, then start a new R project in a new GitHub repository using one of the methods outlined in Module 06. You should call it ‚Äúcreative-data-visualization‚Äù.\nBy the due date for this assignment, you should post to your code along with the dataset you are visualizing to this GitHub repository and submit the URL into the assignment submission text field.\nIf your visualization cannot be done entirely with code (i.e., if you produce ‚Äúsomething else‚Äù with your data after wrangling/transforming with code: a recording, painting, physical object, text or graphic representation of a process/algorithm for visualization that you cannot yet implement), please follow the same instructions above, but then also take a photo/video/recording of your finished product or a sketch of your (unrealized) ideas and include it in your renders ‚Äú.Rmd‚Äù or ‚Äú.qmd‚Äù file.\n\nNOTE: I should be able to CLONE whatever is in your repository and render your associated ‚Äú.qmd‚Äù or ‚Äú.Rmd‚Äù file to show all of the work for this creative data visualization.",
    "crumbs": [
      "Assignments",
      "Creative Data Visualization"
    ]
  },
  {
    "objectID": "packages.html",
    "href": "packages.html",
    "title": "List of Packages Used",
    "section": "",
    "text": "By Module",
    "crumbs": [
      "References",
      "List of Packages Used"
    ]
  },
  {
    "objectID": "packages.html#by-module",
    "href": "packages.html#by-module",
    "title": "List of Packages Used",
    "section": "",
    "text": "Module 03\n\n{easypackages}: Sherman (2016)\n\n\n\nModule 05\n\n{usethis}: Wickham and Bryan (2020)\n\n\n\nModule 07\n\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n\n{ggplot2}: Wickham, Chang, et al. (2020), Wickham (2016)\n{tibble}: M√ºller and Wickham (2020)\n{tidyr}: Wickham and Henry (2020)\n{readr}: Wickham, Hester, and Francois (2018)\n{purrr}: Henry and Wickham (2020)\n{dplyr}: Wickham, Fran√ßois, Henry, and M√ºller (2020)\n{stringr}: Wickham (2019a)\n{forcats}: Wickham (2020))\n\n{data.table}: Dowle and Srinivasan (2019)\n\n\n\nModule 08\n\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{readxl}: Wickham and Bryan (2019)\n{XLConnect}: Mirai Solutions GmbH (2020)\n{gdata}: Warnes et al. (2017)\n{xlsx}: Dragulescu and Arendt (2020)\n{curl}: Ooms (2019)\n{rdrop2}: Ram and Yochum (2017)\n{repmis}: Gandrud (2016)\n{googlesheets4}: Bryan (2020)\n{googledrive}: D‚ÄôAgostino McGowan and Bryan (2019)\n\n\n\nModule 09\n\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{magrittr}: Bache and Wickham (2014)\n{tidylog}: Elbers (2020)\n\n\n\nModule 10\n\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{curl}: Ooms (2019)\n{skimr}: Waring et al. (2020)\n{summarytools}: Comtois (2020)\n{dataMaid}: Petersen and Ekstr√∏m (2019a), Petersen and Ekstr√∏m (2019b)\n{psych}: Revelle (2020)\n{pastecs}: Grosjean and Ibanez (2018)\n{Hmisc}: Harrell (2020)\n{ggExtra}: Attali and Baker (2019)\n{car}: Fox, Weisberg, and Price (2020), Fox and Weisberg (2019)\n{GGally}: Schloerke et al. (2020)\n{corrplot}: Wei and Simko (2017a), Wei and Simko (2017b)\n{patchwork}: Pedersen (2019)\n{cowplot}: Wilke (2019)\n{gridExtra}: Auguie (2017)\n\n\n\nModule 11\n\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{sjmisc}: L√ºdecke (2020), L√ºdecke (2018)\n\n\n\nModule 12\n\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{moments}: Komsta and Novomestky (2022)\n{mosaic}: Pruim, Kaplan, and Horton (2020), Pruim, Kaplan, and Horton (2017)\n{radiant}: Nijs (2020)\n{sciplot}: Morales, with code developed by the R Development Core Team, and with general advice from the R-help listserv community and especially Duncan Murdoch. (2020)\n\n\n\nModule 13\n\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{mosaic}: Pruim et al. (2020), Pruim et al. (2017)\n{cowplot}: Wilke (2019)\n{manipulate}: Allaire (2014)\n\n\n\nModule 14\n\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{mosaic}: Pruim et al. (2020), Pruim et al. (2017)\n{manipulate}: Allaire (2014)\n{boot}: Canty and Ripley (2020), Davison and Hinkley (1997)\n\n\n\nModule 15\n\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{mosaic}: Pruim et al. (2020), Pruim et al. (2017)\n\n\n\nModule 16\n\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{mosaic}: Pruim et al. (2020), Pruim et al. (2017)\n{coin}: Hothorn, Winell, Hornik, van de Wiel, and Zeileis (2019), Hothorn, Hornik, van de Wiel, and Zeileis (2006), Hothorn, Hornik, van de Wiel, and Zeileis (2008)\n{jmuOutlier}: Garren (2019)\n{infer}: Bray, Ismay, Chasnovski, Baumer, and Cetinkaya-Rundel (2019)\n\n\n\nModule 17\n\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{manipulate}: Allaire (2014)\n\n\n\nModule 18\n\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{manipulate}: Allaire (2014)\n{patchwork}: Pedersen (2019)\n{infer}: Bray et al. (2019)\n{broom}: Robinson and Hayes (2020)\n{lmodel2}: Legendre (2018)\n\n\n\nModule 19\n\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{car}: Fox et al. (2020), Fox and Weisberg (2019)\n{ggpubr}: @-R-ggpubr\n\n\n\nModule 20\n\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{car}: Fox et al. (2020), Fox and Weisberg (2019)\n{broom}: Robinson and Hayes (2020)\n{coin}: Hothorn, Winell, et al. (2019), Hothorn et al. (2006), Hothorn et al. (2008)\n{infer}: Bray et al. (2019)\n{permuco}: Frossard and Renaud (2019)\n{dunn.test}: Dinno (2017b)\n{conover.test}: Dinno (2017a)\n{effectsize}: Ben-Shachar, L√ºdecke, and Makowski (2020a), Ben-Shachar, L√ºdecke, and Makowski (2020b)\n\n\n\nModule 21\n\n{jtools}: Long (2020)\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{broom}: Robinson and Hayes (2020)\n{car}: Fox et al. (2020), Fox and Weisberg (2019)\n{gridExtra}: Auguie (2017)\n{effects}: Fox and Hong (2009a), Fox and Hong (2009b)\n\n\n\nModule 22\n\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{MASS}: Ripley (2019), Venables and Ripley (2002)\n{AICcmodavg}: Mazerolle and portions of code contributed by Dan Linden. (2019)\n{MuMIn}: Barto≈Ñ (2020)\n\n\n\nModule 23\n\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{broom}: Robinson and Hayes (2020)\n{patchwork}: Pedersen (2019)\n{lmtest}: Hothorn, Zeileis, Farebrother, and Cummins (2019), Zeileis and Hothorn (2002)\n\n\n\nModule 24\n\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{lmtest}: Hothorn, Zeileis, et al. (2019), Zeileis and Hothorn (2002)\n{AICcmodavg}: Mazerolle and portions of code contributed by Dan Linden. (2019)\n{lme4}: Bates, Maechler, Bolker, and Walker (2020), Bates, M√§chler, Bolker, and Walker (2015)\n{redres}: Goode, McClernon, Zhao, Zhang, and Huo. (2024)\n{effects}: Fox and Hong (2009a), Fox and Hong (2009b)\n{cowplot}: Wilke (2019)\n{sjPlots}: L√ºdecke (2023)\n{MuMIn}: Barto≈Ñ (2020)\n{mixedup}: Clark (2024)\n{glmmML}: Brostr√∂m (2020)\n{MASS}: Ripley (2019)\n\n\n\nMaximum Likelihood Estimation\n\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{mosaic}: Pruim et al. (2020), Pruim et al. (2017)\n{bbmle}: Bolker and R Development Core Team (2023)\n{maxLik}: Toomet and Henningsen (2024)\n\n\n\nUsing Python from R\n\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{broom}: Robinson and Hayes (2020)\n{reticulate}: Ushey, Allaire, and Tang (2020)\n\n\n\nBuilding Custom R Packages\n\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{devtools}: Wickham, Hester, and Chang (2020)\n{usethis}: Wickham and Bryan (2020)\n{roxygen2}: Wickham, Danenberg, Cs√°rdi, and Eugster (2020)\n{withr}: Hester, M√ºller, Ushey, Wickham, and Chang (2020)\n{manipulate}: Allaire (2014)\n\n\n\nBuilding Interactive Web Apps\n\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{shiny}: Chang, Cheng, Allaire, Xie, and McPherson (2020)\n{DT}: Xie, Cheng, and Tan (2020)\n\n\n\nWorking with and Managing Packages\n\n{easypackages}\n{pacman}\n{librarian}\n{Require}",
    "crumbs": [
      "References",
      "List of Packages Used"
    ]
  },
  {
    "objectID": "packages.html#in-exercises",
    "href": "packages.html#in-exercises",
    "title": "List of Packages Used",
    "section": "In Exercises",
    "text": "In Exercises\n\n{emayili}: Collier (2021)\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{mailR}: Premraj (2021),\n{blastula}: Iannone and Cheng (2020)\n{usethis}: Wickham and Bryan (2020)\n{emo}: Wickham, Fran√ßois, and D‚ÄôAgostino McGowan (2019)\n{sjmisc}: L√ºdecke (2020)\n{huxtable}: Hugh-Jones (2021)\n{cowplot}: Wilke (2019)\n{mosaic}: Pruim et al. (2020), Pruim et al. (2017)\n{kableExtra}: Zhu (2019)\n{ggpubr}: Kassambara (2020)\n{infer}: Bray et al. (2019)",
    "crumbs": [
      "References",
      "List of Packages Used"
    ]
  },
  {
    "objectID": "packages.html#complete-list",
    "href": "packages.html#complete-list",
    "title": "List of Packages Used",
    "section": "Complete List",
    "text": "Complete List\n\n{AICcmodavg}: Mazerolle and portions of code contributed by Dan Linden. (2019)\n{BBmisc}: Bischl et al. (2017)\n{bbmle}: Bolker and R Development Core Team (2023)\n{blastula}: Iannone and Cheng (2020)\n{boot}: Canty and Ripley (2020), Davison and Hinkley (1997)\n{broom}: Robinson and Hayes (2020)\n{car}: Fox et al. (2020), Fox and Weisberg (2019)\n{coin}: Hothorn, Winell, et al. (2019), Hothorn et al. (2006), Hothorn et al. (2008)\n{collape}: Krantz (2020)\n{conover.test}: Dinno (2017a)\n{corrplot}: Wei and Simko (2017a), Wei and Simko (2017b)\n{cowplot}: Wilke (2019)\n{curl}: Ooms (2019)\n{data.table}: Dowle and Srinivasan (2019)\n{dataMaid}: Petersen and Ekstr√∏m (2019a), Petersen and Ekstr√∏m (2019b)\n{devtools}: Wickham, Hester, et al. (2020)\n{dplyr}: Wickham, Fran√ßois, et al. (2020)\n{DT}: Xie et al. (2020)\n{dunn.test}: Dinno (2017b)\n{emayili}: Collier (2021)\n{easypackages}: Sherman (2016)\n{effects}: Fox and Hong (2009a), Fox and Hong (2009b)\n{effectsize}: Ben-Shachar et al. (2020a), Ben-Shachar et al. (2020b)\n{emo}: Wickham, Fran√ßois, et al. (2019)\n{forcats}: Wickham (2020)\n{gdata}: Warnes et al. (2017)\n{GGally}: Schloerke et al. (2020)\n{ggExtra}: Attali and Baker (2019)\n{ggplot2}: Wickham, Chang, et al. (2020), Wickham (2016)\n{ggpubr}: Kassambara (2020)\n{glmmML}: Brostr√∂m (2020)\n{googledrive}: D‚ÄôAgostino McGowan and Bryan (2019)\n{googlesheets4}: Bryan (2020)\n{gridExtra}: Auguie (2017)\n{Hmisc}: Harrell (2020)\n{huxtable}: Hugh-Jones (2021)\n{infer}: Bray et al. (2019)\n{jmuOutlier}: Garren (2019)\n{jtools}: Long (2020)\n{kableExtra}: Zhu (2019)\n{knitr}: Xie (2020), Xie (2014), Xie (2015)\n{librarian}:\n{lme4}: Bates et al. (2020), Bates et al. (2015)\n{lmodel2}: Legendre (2018)\n{lmtest}: Hothorn, Zeileis, et al. (2019), Zeileis and Hothorn (2002)\n{magrittr}: Bache and Wickham (2014)\n{manipulate}: Allaire (2014)\n{MASS}: Ripley (2019), Venables and Ripley (2002)\n{maxLik}: Toomet and Henningsen (2024), Henningsen and Toomet (2011)\n{mixedup}: Clark (2024)\n{moments}: Komsta and Novomestky (2022)\n{mosaic}: Pruim et al. (2020), Pruim et al. (2017)\n{MuMIn}: Barto≈Ñ (2020)\n{pacman}:\n{pastecs}: Grosjean and Ibanez (2018)\n{patchwork}: Pedersen (2019)\n{permuco}: Frossard and Renaud (2019)\n{psych}: Revelle (2020)\n{purrr}: Henry and Wickham (2020)\n{radiant}: Nijs (2020)\n{rdrop2}: Ram and Yochum (2017)\n{readr}: Wickham et al. (2018)\n{redres}: Goode et al. (2024)\n{readxl}: Wickham and Bryan (2019)\n{repmis}: Gandrud (2016)\n{Require}:\n{reticulate}: Ushey et al. (2020)\n{roxygen2}: Wickham, Danenberg, et al. (2020)\n{scales}: Wickham and Seidel (2019)\n{sciplot}: Morales et al. (2020)\n{shiny}: Chang et al. (2020)\n{sjmisc}: L√ºdecke (2020), L√ºdecke (2018)\n{skimr}: Waring et al. (2020)\n{sjPlot}: L√ºdecke (2023)\n{stringr}: Wickham (2019a)\n{summarytools}: Comtois (2020)\n{tibble}: M√ºller and Wickham (2020)\n{tictoc}: Izrailev (2014)\n{tidyr}: Wickham and Henry (2020)\n{tidyverse}: Wickham (2019b), Wickham, Averick, et al. (2019)\n{usethis}: Wickham and Bryan (2020)\n{withr}: Hester et al. (2020)\n{XLConnect}: Mirai Solutions GmbH (2020)\n{xlsx}: Dragulescu and Arendt (2020)\n\n\n\n\n\n\n\nAllaire J. (2014). manipulate: Interactive plots for RStudio. https://CRAN.R-project.org/package=manipulate\n\n\nAttali D, and Baker C. (2019). ggExtra: Add marginal histograms to ‚Äôggplot2‚Äô, and more ‚Äôggplot2‚Äô enhancements. https://CRAN.R-project.org/package=ggExtra\n\n\nAuguie B. (2017). gridExtra: Miscellaneous functions for \"grid\" graphics. https://CRAN.R-project.org/package=gridExtra\n\n\nBache SM, and Wickham H. (2014). magrittr: A forward-pipe operator for R. https://CRAN.R-project.org/package=magrittr\n\n\nBarto≈Ñ K. (2020). MuMIn: Multi-model inference. https://CRAN.R-project.org/package=MuMIn\n\n\nBates D, M√§chler M, Bolker B, and Walker S. (2015). Fitting linear mixed-effects models using lme4. Journal of Statistical Software 67: 1‚Äì48. https://doi.org/10.18637/jss.v067.i01\n\n\nBates D, Maechler M, Bolker B, and Walker S. (2020). lme4: Linear mixed-effects models using ‚Äôeigen‚Äô and S4. https://CRAN.R-project.org/package=lme4\n\n\nBen-Shachar MS, L√ºdecke D, and Makowski D. (2020a). effectsize: Estimation of effect size indices and standardized parameters. https://CRAN.R-project.org/package=effectsize\n\n\nBen-Shachar MS, L√ºdecke D, and Makowski D. (2020b). effectsize: Estimation of effect size indices and standardized parameters. Journal of Open Source Software 5: 2815. https://doi.org/10.21105/joss.02815\n\n\nBischl B, Lang M, Bossek J, Horn D, Richter J, and Surmann D. (2017). BBmisc: Miscellaneous helper functions for B. Bischl. https://CRAN.R-project.org/package=BBmisc\n\n\nBolker B, and R Development Core Team. (2023). Bbmle: Tools for general maximum likelihood estimation. https://CRAN.R-project.org/package=bbmle\n\n\nBray A, Ismay C, Chasnovski E, Baumer B, and Cetinkaya-Rundel M. (2019). infer: Tidy statistical inference. https://CRAN.R-project.org/package=infer\n\n\nBrostr√∂m G. (2020). glmmML: Generalized linear models with clustering. https://CRAN.R-project.org/package=glmmML\n\n\nBryan J. (2020). googlesheets4: Access Google Sheets using the Sheets API V4. https://CRAN.R-project.org/package=googlesheets4\n\n\nCanty A, and Ripley B. (2020). boot: Bootstrap functions (originally by Angelo Canty for S). https://CRAN.R-project.org/package=boot\n\n\nChang W, Cheng J, Allaire J, Xie Y, and McPherson J. (2020). shiny: Web application framework for R. https://CRAN.R-project.org/package=shiny\n\n\nClark M. (2024). Mixedup: Miscellaneous functions for mixed models. https://m-clark.github.io/mixedup\n\n\nCollier AB. (2021). Emayili: Send email messages. https://CRAN.R-project.org/package=emayili\n\n\nComtois D. (2020). summarytools: Tools to quickly and neatly summarize data. https://CRAN.R-project.org/package=summarytools\n\n\nD‚ÄôAgostino McGowan L, and Bryan J. (2019). googledrive: An interface to Google Drive. https://CRAN.R-project.org/package=googledrive\n\n\nDavison AC, and Hinkley DV. (1997). Bootstrap Methods and Their Applications. Cambridge: Cambridge University Press. http://statwww.epfl.ch/davison/BMA/\n\n\nDinno A. (2017a). conover.test: Conover-Iman test of multiple comparisons using rank sums. https://CRAN.R-project.org/package=conover.test\n\n\nDinno A. (2017b). dunn.test: Dunn‚Äôs test of multiple comparisons using rank sums. https://CRAN.R-project.org/package=dunn.test\n\n\nDowle M, and Srinivasan A. (2019). data.table: Extension of ‚Äôdata.frame‚Äô. https://CRAN.R-project.org/package=data.table\n\n\nDragulescu A, and Arendt C. (2020). xlsx: Read, write, format Excel 2007 and Excel 97/2000/XP/2003 files. https://CRAN.R-project.org/package=xlsx\n\n\nElbers B. (2020). tidylog: Logging for ‚Äôdplyr‚Äô and ‚Äôtidyr‚Äô functions. https://CRAN.R-project.org/package=tidylog\n\n\nFox J, and Hong J. (2009a). Effect displays in R for multinomial and proportional-odds logit models: Extensions to the effects package. Journal of Statistical Software. https://CRAN.R-project.org/package=effects\n\n\nFox J, and Hong J. (2009b). Effect displays in R for multinomial and proportional-odds logit models: Extensions to the effects package. Journal of Statistical Software 32: 1‚Äì24. https://doi.org/10.18637/jss.v032.i01\n\n\nFox J, and Weisberg S. (2019). An R Companion to Applied Regression (Third Edition). Thousand Oaks CA: Sage. https://socialsciences.mcmaster.ca/jfox/Books/Companion/\n\n\nFox J, Weisberg S, and Price B. (2020). car: Companion to applied regression. https://CRAN.R-project.org/package=car\n\n\nFrossard J, and Renaud O. (2019). permuco: Permutation tests for regression, (repeated measures) ANOVA/ANCOVA and comparison of signals. https://CRAN.R-project.org/package=permuco\n\n\nGandrud C. (2016). repmis: Miscellaneous tools for reproducible research. https://CRAN.R-project.org/package=repmis\n\n\nGarren ST. (2019). jmuOutlier: Permutation tests for nonparametric statistics. https://CRAN.R-project.org/package=jmuOutlier\n\n\nGoode K, McClernon K, Zhao J, Zhang Y, and Huo. Y. (2024). Redres: Residuals and diagnostic plots for mixed models. https://github.com/goodekat/redres.git\n\n\nGrosjean P, and Ibanez F. (2018). pastecs: Package for analysis of space-time ecological series. https://CRAN.R-project.org/package=pastecs\n\n\nHarrell FE Jr. (2020). Hmisc: Harrell miscellaneous. https://CRAN.R-project.org/package=Hmisc\n\n\nHenningsen A, and Toomet O. (2011). maxLik: A package for maximum likelihood estimation in R. Computational Statistics 26: 443‚Äì458. http://dx.doi.org/10.1007/s00180-010-0217-1\n\n\nHenry L, and Wickham H. (2020). purrr: Functional programming tools. https://CRAN.R-project.org/package=purrr\n\n\nHester J, M√ºller K, Ushey K, Wickham H, and Chang W. (2020). withr: Run code ‚Äôwith‚Äô temporarily modified global state. https://CRAN.R-project.org/package=withr\n\n\nHothorn T, Hornik K, van de Wiel MA, and Zeileis A. (2006). A Lego system for conditional inference. The American Statistician 60: 257‚Äì263. https://doi.org/10.1198/000313006X118430\n\n\nHothorn T, Hornik K, van de Wiel MA, and Zeileis A. (2008). Implementing a class of permutation tests: The coin package. Journal of Statistical Software 28: 1‚Äì23. https://doi.org/10.18637/jss.v028.i08\n\n\nHothorn T, Winell H, Hornik K, van de Wiel MA, and Zeileis A. (2019). coin: Conditional inference procedures in a permutation test framework. https://CRAN.R-project.org/package=coin\n\n\nHothorn T, Zeileis A, Farebrother RW, and Cummins C. (2019). lmtest: Testing linear regression models. https://CRAN.R-project.org/package=lmtest\n\n\nHugh-Jones D. (2021). Huxtable: Easily create and style tables for LaTeX, HTML and other formats. https://CRAN.R-project.org/package=huxtable\n\n\nIannone R, and Cheng J. (2020). Blastula: Easily send HTML email messages. https://CRAN.R-project.org/package=blastula\n\n\nIzrailev S. (2014). tictoc: Functions for timing R scripts, as well as implementations of stack and list structures. https://CRAN.R-project.org/package=tictoc\n\n\nKassambara A. (2020). Ggpubr: ggplot2 based publication ready plots. https://rpkgs.datanovia.com/ggpubr/\n\n\nKomsta L, and Novomestky F. (2022). Moments: Moments, cumulants, skewness, kurtosis and related tests. https://CRAN.R-project.org/package=moments\n\n\nKrantz S. (2020). collapse: Advanced and fast data transformation. https://CRAN.R-project.org/package=collapse\n\n\nLegendre P. (2018). lmodel2: Model II regression. https://CRAN.R-project.org/package=lmodel2\n\n\nLong JA. (2020). jtools: Analysis and presentation of social scientific data. https://CRAN.R-project.org/package=jtools\n\n\nL√ºdecke D. (2018). sjmisc: Data and variable transformation functions. Journal of Open Source Software 3: 754. https://doi.org/10.21105/joss.00754\n\n\nL√ºdecke D. (2020). sjmisc: Data and variable transformation functions. https://CRAN.R-project.org/package=sjmisc\n\n\nL√ºdecke D. (2023). sjPlot: Data visualization for statistics in social science. https://CRAN.R-project.org/package=sjPlot\n\n\nMazerolle MJ, and portions of code contributed by Dan Linden. (2019). AICcmodavg: Model selection and multimodel inference based on (Q)AIC(c). https://CRAN.R-project.org/package=AICcmodavg\n\n\nMirai Solutions GmbH. (2020). XLConnect: Excel connector for R. https://CRAN.R-project.org/package=XLConnect\n\n\nMorales M, with code developed by the R Development Core Team, and with general advice from the R-help listserv community and especially Duncan Murdoch. (2020). sciplot: Scientific graphing functions for factorial designs. https://CRAN.R-project.org/package=sciplot\n\n\nM√ºller K, and Wickham H. (2020). tibble: Simple data frames. https://CRAN.R-project.org/package=tibble\n\n\nNijs V. (2020). radiant: Business analytics using R and Shiny. https://CRAN.R-project.org/package=radiant\n\n\nOoms J. (2019). curl: A modern and flexible web client for R. https://CRAN.R-project.org/package=curl\n\n\nPedersen TL. (2019). patchwork: The composer of plots. https://CRAN.R-project.org/package=patchwork\n\n\nPetersen AH, and Ekstr√∏m CT. (2019a). dataMaid: A suite of checks for identification of potential errors in a data frame as part of the data screening process. https://CRAN.R-project.org/package=dataMaid\n\n\nPetersen AH, and Ekstr√∏m CT. (2019b). dataMaid: Your assistant for documenting supervised data quality screening in R. Journal of Statistical Software 90: 1‚Äì38. https://doi.org/10.18637/jss.v090.i06\n\n\nPremraj R. (2021). mailR: A utility to send emails from R. https://CRAN.R-project.org/package=mailR\n\n\nPruim R, Kaplan DT, and Horton NJ. (2017). The mosaic package: Helping students to ‚Äôthink with data‚Äô using R. The R Journal 9: 77‚Äì102. https://journal.r-project.org/archive/2017/RJ-2017-024/index.html\n\n\nPruim R, Kaplan DT, and Horton NJ. (2020). mosaic: Project MOSAIC statistics and mathematics teaching utilities. https://CRAN.R-project.org/package=mosaic\n\n\nRam K, and Yochum C. (2017). rdrop2: Programmatic interface to the ‚ÄôDropbox‚Äô API. https://CRAN.R-project.org/package=rdrop2\n\n\nRevelle W. (2020). psych: Procedures for psychological, psychometric, and personality research. https://CRAN.R-project.org/package=psych\n\n\nRipley B. (2019). MASS: Support functions and datasets for Venables and Ripley‚Äôs MASS. https://CRAN.R-project.org/package=MASS\n\n\nRobinson D, and Hayes A. (2020). broom: Convert statistical analysis objects into tidy tibbles. https://CRAN.R-project.org/package=broom\n\n\nSchloerke B, Crowley J, Cook D, Briatte F, Marbach M, Thoen E, Elberg A, and Larmarange J. (2020). GGally: Extension to ‚Äôggplot2‚Äô. https://CRAN.R-project.org/package=GGally\n\n\nSherman J. (2016). Easypackages: Easy loading and installing of packages. https://CRAN.R-project.org/package=easypackages\n\n\nToomet O, and Henningsen A. (2024). maxLik: Maximum likelihood estimation and related tools. https://CRAN.R-project.org/package=maxLik\n\n\nUshey K, Allaire J, and Tang Y. (2020). reticulate: Interface to ‚Äôpython‚Äô. https://CRAN.R-project.org/package=reticulate\n\n\nVenables WN, and Ripley BD. (2002). Modern Applied Statistics with S (Fourth Edition). New York: Springer. http://www.stats.ox.ac.uk/pub/MASS4\n\n\nWaring E, Quinn M, McNamara A, Arino de la Rubia E, Zhu H, and Ellis S. (2020). skimr: Compact and flexible summaries of data. https://CRAN.R-project.org/package=skimr\n\n\nWarnes GR, Bolker B, Gorjanc G, Grothendieck G, Korosec A, Lumley T, MacQueen D, Magnusson A, Rogers J, and others. (2017). gdata: Various R programming tools for data manipulation. https://CRAN.R-project.org/package=gdata\n\n\nWei T, and Simko V. (2017a). corrplot: Visualization of a correlation matrix. https://CRAN.R-project.org/package=corrplot\n\n\nWei T, and Simko V. (2017b). R package \"corrplot\": Visualization of a correlation matrix. https://github.com/taiyun/corrplot\n\n\nWickham H. (2016). ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org\n\n\nWickham H. (2019a). stringr: Simple, consistent wrappers for common string operations. https://CRAN.R-project.org/package=stringr\n\n\nWickham H. (2019b). tidyverse: Easily install and load the ‚Äôtidyverse‚Äô. https://CRAN.R-project.org/package=tidyverse\n\n\nWickham H. (2020). forcats: Tools for working with categorical variables (factors). https://CRAN.R-project.org/package=forcats\n\n\nWickham H, Averick M, Bryan J, Chang W, McGowan LD, Fran√ßois R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, M√ºller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, and Yutani H. (2019). Welcome to the tidyverse. Journal of Open Source Software 4: 1686. https://doi.org/10.21105/joss.01686\n\n\nWickham H, and Bryan J. (2019). readxl: Read Excel files. https://CRAN.R-project.org/package=readxl\n\n\nWickham H, and Bryan J. (2020). usethis: Automate package and project setup. https://CRAN.R-project.org/package=usethis\n\n\nWickham H, Chang W, Henry L, Pedersen TL, Takahashi K, Wilke C, Woo K, Yutani H, and Dunnington D. (2020). ggplot2: Create elegant data visualisations using the grammar of graphics. https://CRAN.R-project.org/package=ggplot2\n\n\nWickham H, Danenberg P, Cs√°rdi G, and Eugster M. (2020). roxygen2: In-line documentation for R. https://CRAN.R-project.org/package=roxygen2\n\n\nWickham H, Fran√ßois R, and D‚ÄôAgostino McGowan L. (2019). emo: Easily insert ‚Äôemoji‚Äô. https://github.com/hadley/emo\n\n\nWickham H, Fran√ßois R, Henry L, and M√ºller K. (2020). dplyr: A grammar of data manipulation. https://CRAN.R-project.org/package=dplyr\n\n\nWickham H, and Henry L. (2020). tidyr: Tidy messy data. https://CRAN.R-project.org/package=tidyr\n\n\nWickham H, Hester J, and Chang W. (2020). devtools: Tools to make developing R packages easier. https://CRAN.R-project.org/package=devtools\n\n\nWickham H, Hester J, and Francois R. (2018). readr: Read rectangular text data. https://CRAN.R-project.org/package=readr\n\n\nWickham H, and Seidel D. (2019). scales: Scale functions for visualization. https://CRAN.R-project.org/package=scales\n\n\nWilke CO. (2019). cowplot: Streamlined plot theme and plot annotations for ‚Äôggplot2‚Äô. https://CRAN.R-project.org/package=cowplot\n\n\nXie Y. (2014). knitr: A comprehensive tool for reproducible research in R. In V Stodden, F Leisch, and RD Peng (Eds.), Implementing reproducible computational research. Chapman; Hall/CRC. http://www.crcpress.com/product/isbn/9781466561595\n\n\nXie Y. (2015). Dynamic Documents with R and knitr (Second Edition). Boca Raton, Florida: Chapman; Hall/CRC. https://yihui.org/knitr/\n\n\nXie Y. (2020). knitr: A general-purpose package for dynamic report generation in R. https://CRAN.R-project.org/package=knitr\n\n\nXie Y, Cheng J, and Tan X. (2020). DT: A wrapper of the JavaScript library ‚ÄôDataTables‚Äô. https://CRAN.R-project.org/package=DT\n\n\nZeileis A, and Hothorn T. (2002). Diagnostic checking in regression relationships. R News 2: 7‚Äì10. https://CRAN.R-project.org/doc/Rnews/\n\n\nZhu H. (2019). kableExtra: Construct complex table with ‚Äôkable‚Äô and pipe syntax. https://CRAN.R-project.org/package=kableExtra",
    "crumbs": [
      "References",
      "List of Packages Used"
    ]
  }
]